ERROR    The specified wabTarget for Weights & Biases tracking does not exist: ARR
INFO     X training matrix of shape (4128, 2048) and type float32
INFO     Y training matrix of shape (4128,) and type float32
INFO     Training of fold number: 1
INFO     Training sample distribution: train data: {-1.2016366720199585: 7, -1.2016383409500122: 5, -1.2016324996948242: 4, -1.2016342878341675: 3, -1.2016351222991943: 3, -1.2016377449035645: 3, -1.201635479927063: 3, -1.2016339302062988: 2, -1.201621651649475: 2, -1.201596975326538: 2, -1.201636552810669: 2, -1.201636791229248: 2, -1.2016295194625854: 2, -1.2016310691833496: 2, -1.201627254486084: 2, -1.2016315460205078: 2, -1.201635718345642: 2, -1.2016253471374512: 2, -1.2016288042068481: 2, -1.2016302347183228: 2, -1.2016254663467407: 2, -1.2016345262527466: 2, -1.2016304731369019: 2, -1.201633095741272: 2, -1.2016290426254272: 2, -1.201622486114502: 2, -1.2016384601593018: 2, -1.2016363143920898: 2, -1.2016353607177734: 2, -1.2016355991363525: 2, 1.1870161294937134: 1, 0.5256680846214294: 1, 0.08119866997003555: 1, -0.35442060232162476: 1, 1.3986883163452148: 1, -0.31709083914756775: 1, -0.16630633175373077: 1, -0.16336557269096375: 1, 1.5455868244171143: 1, -0.4970245659351349: 1, 1.4836735725402832: 1, 0.7778733968734741: 1, 1.0667099952697754: 1, 0.6829988956451416: 1, 0.5299685597419739: 1, 0.831580638885498: 1, -0.425843745470047: 1, 0.02149348333477974: 1, -1.2016195058822632: 1, 0.002237366745248437: 1, -0.2558962106704712: 1, 0.22109845280647278: 1, 1.4535586833953857: 1, -0.6966411471366882: 1, -1.1963087320327759: 1, 1.2952117919921875: 1, -1.2015975713729858: 1, 0.28807583451271057: 1, 0.8313724994659424: 1, -1.1840753555297852: 1, 1.1950836181640625: 1, 1.3501269817352295: 1, 0.34516385197639465: 1, 0.11128426343202591: 1, -0.5620038509368896: 1, -0.24468590319156647: 1, 1.478103518486023: 1, 1.3625078201293945: 1, 1.5410983562469482: 1, 0.12603989243507385: 1, 1.621954083442688: 1, 1.5633379220962524: 1, 1.593016266822815: 1, 0.9703378081321716: 1, 1.1153340339660645: 1, 0.44327667355537415: 1, 1.3818247318267822: 1, 0.2669691741466522: 1, -0.7750424146652222: 1, -0.6442912220954895: 1, 0.7218993902206421: 1, 0.0860099047422409: 1, -1.201627492904663: 1, 0.3664189279079437: 1, 0.6573249697685242: 1, 0.4933412969112396: 1, 0.3488617241382599: 1, -0.5330178141593933: 1, -0.09881063550710678: 1, 0.25783771276474: 1, -0.008235386572778225: 1, 0.2470504194498062: 1, -0.9223102331161499: 1, -0.297276109457016: 1, -1.201623558998108: 1, 1.5667779445648193: 1, -0.4459679424762726: 1, 0.25592291355133057: 1, 0.10525540262460709: 1, 1.4251669645309448: 1, 0.31522658467292786: 1, 0.695344090461731: 1, 0.2743425965309143: 1, -0.19042591750621796: 1, 0.2500914931297302: 1, 1.9054079055786133: 1, 0.15692748129367828: 1, -0.026365874335169792: 1, 0.7279888391494751: 1, 0.7412875294685364: 1, 0.015656888484954834: 1, 1.5035943984985352: 1, 0.34191834926605225: 1, -0.10739652067422867: 1, 0.643775999546051: 1, 1.4653488397598267: 1, 1.4158756732940674: 1, 1.1683435440063477: 1, -0.9669303297996521: 1, 1.4923255443572998: 1, 0.31334978342056274: 1, 0.49536043405532837: 1, 1.2379846572875977: 1, 0.23716896772384644: 1, -0.10035426914691925: 1, 0.0010552277090027928: 1, -1.189130187034607: 1, 1.4424492120742798: 1, -0.20045150816440582: 1, -1.160044550895691: 1, 0.8569538593292236: 1, -0.2393776774406433: 1, 0.6820655465126038: 1, 0.7402693033218384: 1, -1.0873202085494995: 1, 0.540539026260376: 1, 0.10738043487071991: 1, 1.3042255640029907: 1, -1.0201867818832397: 1, 1.168498158454895: 1, 0.15129715204238892: 1, 1.5149681568145752: 1, 0.9998847246170044: 1, 0.19487899541854858: 1, 0.21748410165309906: 1, -0.06838630884885788: 1, 0.31109386682510376: 1, 0.5247108340263367: 1, 1.8254425525665283: 1, 0.8825770616531372: 1, 1.6950387954711914: 1, 1.3083291053771973: 1, -0.3040960133075714: 1, 1.4907145500183105: 1, 0.20332399010658264: 1, -0.9010018706321716: 1, 0.2900018095970154: 1, 0.5601941347122192: 1, 1.415509581565857: 1, -0.18418414890766144: 1, 0.2954026460647583: 1, 0.2610865831375122: 1, 0.2609155476093292: 1, -1.194837212562561: 1, 0.3422311544418335: 1, 1.5088152885437012: 1, 0.36003923416137695: 1, 0.17204155027866364: 1, 0.04832748696208: 1, -0.17784874141216278: 1, 0.22961212694644928: 1, 1.595760464668274: 1, 0.8440163731575012: 1, -0.5706648826599121: 1, -0.4307803809642792: 1, 0.26908448338508606: 1, 0.06883639097213745: 1, 1.6063342094421387: 1, -0.1421377956867218: 1, -1.2012096643447876: 1, 1.6047093868255615: 1, -1.201625108718872: 1, -1.1956098079681396: 1, -1.1269394159317017: 1, -0.7543148398399353: 1, -1.1961579322814941: 1, -0.15980762243270874: 1, -0.8842195272445679: 1, -0.7196366786956787: 1, -0.2302703857421875: 1, -0.5718616843223572: 1, 1.1079081296920776: 1, 0.8374537825584412: 1, -0.26343750953674316: 1, 1.4342314004898071: 1, 0.9686956405639648: 1, 0.28549933433532715: 1, -0.5639210939407349: 1, -0.39423879981040955: 1, -1.1962995529174805: 1, -1.1962871551513672: 1, 0.16395387053489685: 1, -1.198028802871704: 1, -1.196489930152893: 1, -1.1964974403381348: 1, -1.1852269172668457: 1, -1.1929398775100708: 1, -1.1945018768310547: 1, -0.34237241744995117: 1, -1.1871466636657715: 1, -1.197901725769043: 1, -1.1866750717163086: 1, -1.1237342357635498: 1, -1.1664562225341797: 1, 0.005114047322422266: 1, -1.1939657926559448: 1, -1.2009947299957275: 1, -1.2016159296035767: 1, -1.2008297443389893: 1, -1.1980621814727783: 1, -0.4268537759780884: 1, 0.5479292273521423: 1, -0.9927355051040649: 1, 1.6281145811080933: 1, -0.6085047125816345: 1, 1.0926192998886108: 1, 0.21325090527534485: 1, 1.8768579959869385: 1, 0.6321052312850952: 1, 1.8398889303207397: 1, -1.009895920753479: 1, 0.7517246007919312: 1, 1.3204209804534912: 1, -1.2016119956970215: 1, -0.4007585644721985: 1, 1.5550175905227661: 1, -0.8897131681442261: 1, 0.7365891337394714: 1, 1.0860453844070435: 1, 1.0165932178497314: 1, -0.6538054943084717: 1, -0.19289743900299072: 1, -0.4147615134716034: 1, -0.5222632884979248: 1, -0.9769929647445679: 1, -0.27864202857017517: 1, -0.32787415385246277: 1, 0.2115481197834015: 1, 1.4029184579849243: 1, -0.3579169511795044: 1, 1.603068470954895: 1, -0.8235211968421936: 1, 0.2787120044231415: 1, -0.2865978181362152: 1, -0.683555543422699: 1, 1.4387927055358887: 1, 0.18236172199249268: 1, 1.2996296882629395: 1, -0.30772721767425537: 1, 0.5139665007591248: 1, 1.3062021732330322: 1, -0.37773171067237854: 1, -1.1987890005111694: 1, -1.197718858718872: 1, 1.5627902746200562: 1, 0.031881630420684814: 1, -0.4260459542274475: 1, -0.3525408208370209: 1, -1.180529236793518: 1, -0.6465518474578857: 1, 0.8243502378463745: 1, -1.1526696681976318: 1, -1.0348321199417114: 1, -1.2015955448150635: 1, -0.5828713178634644: 1, -1.2016127109527588: 1, 0.10008653253316879: 1, -0.9153497219085693: 1, 0.1308048814535141: 1, -1.2015149593353271: 1, -0.5025617480278015: 1, -0.6686071157455444: 1, -1.201509714126587: 1, -1.2016046047210693: 1, -0.8519163131713867: 1, -0.7713357210159302: 1, -1.1730265617370605: 1, 0.670230507850647: 1, -1.1937233209609985: 1, -0.13643299043178558: 1, -1.1647279262542725: 1, -1.2015366554260254: 1, 0.37413451075553894: 1, 0.16982176899909973: 1, -0.6204319000244141: 1, -1.2013626098632812: 1, -1.201349139213562: 1, -0.916256844997406: 1, -1.201637625694275: 1, -0.9931707978248596: 1, -1.2013576030731201: 1, -1.092740535736084: 1, -1.2016162872314453: 1, -0.3419588804244995: 1, -1.2007629871368408: 1, -1.2016221284866333: 1, -1.1976145505905151: 1, -0.8503693342208862: 1, -1.201615810394287: 1, -1.2014594078063965: 1, -0.27123570442199707: 1, 0.5888639092445374: 1, -1.1972260475158691: 1, -1.201310396194458: 1, -0.9657180905342102: 1, -1.1830931901931763: 1, -0.6864959597587585: 1, -0.33821895718574524: 1, -1.1987498998641968: 1, -0.512914776802063: 1, -0.37911587953567505: 1, -1.1482487916946411: 1, -1.2000701427459717: 1, -1.2007057666778564: 1, -1.2015410661697388: 1, -1.2014657258987427: 1, 0.7523799538612366: 1, 0.812082827091217: 1, -0.06465810537338257: 1, -1.201629400253296: 1, -0.21383443474769592: 1, 0.21984633803367615: 1, -1.1999688148498535: 1, -1.201606273651123: 1, -1.2015992403030396: 1, -1.2016280889511108: 1, -1.201348900794983: 1, -1.1382324695587158: 1, -1.1506352424621582: 1, -0.8027163147926331: 1, -1.201614260673523: 1, -1.2002341747283936: 1, -1.201564908027649: 1, -1.2016369104385376: 1, 0.1561044156551361: 1, -0.09781666100025177: 1, 0.16810350120067596: 1, -0.2711203992366791: 1, 0.3245508372783661: 1, -0.9981153011322021: 1, 1.7476170063018799: 1, 0.19889964163303375: 1, 1.9090871810913086: 1, 0.006390336435288191: 1, -0.5620161294937134: 1, -1.18631112575531: 1, -0.3605387806892395: 1, 0.292474627494812: 1, -0.9736310243606567: 1, 0.5786248445510864: 1, -0.5120261311531067: 1, 0.05307941138744354: 1, -0.43377333879470825: 1, 0.6492028832435608: 1, 0.5685755014419556: 1, 0.20812031626701355: 1, -1.1569617986679077: 1, -1.0893759727478027: 1, -1.0622771978378296: 1, -0.9751180410385132: 1, -0.4610443115234375: 1, 0.4990178942680359: 1, -1.1573199033737183: 1, 0.7257186770439148: 1, -1.098894715309143: 1, -0.24214474856853485: 1, -1.1312958002090454: 1, -1.2013144493103027: 1, -0.41735291481018066: 1, 1.5167564153671265: 1, -0.2154475301504135: 1, -1.111975073814392: 1, -1.0634658336639404: 1, -1.1138004064559937: 1, -0.8241826295852661: 1, -1.0799649953842163: 1, -1.1919200420379639: 1, -0.04164140671491623: 1, -0.8634552955627441: 1, -1.1004241704940796: 1, -0.26597675681114197: 1, 1.3899286985397339: 1, -0.8622487187385559: 1, 1.4774726629257202: 1, -1.0084080696105957: 1, -0.9050359129905701: 1, -0.5784834027290344: 1, 1.1575602293014526: 1, -1.1544640064239502: 1, 0.40835040807724: 1, -0.9035985469818115: 1, -1.0024443864822388: 1, 1.3982725143432617: 1, -0.8754668831825256: 1, -0.2101057469844818: 1, -0.38419657945632935: 1, -0.27164560556411743: 1, -0.09434226900339127: 1, -0.0960833728313446: 1, 1.1112544536590576: 1, 0.7951827049255371: 1, -1.1873440742492676: 1, 0.9346560835838318: 1, 0.22341269254684448: 1, -0.748826265335083: 1, 1.281778335571289: 1, -0.4367877244949341: 1, -0.8407037854194641: 1, -1.1983946561813354: 1, -0.5144169330596924: 1, -0.7376867532730103: 1, 1.2280430793762207: 1, 0.16367900371551514: 1, -1.1570571660995483: 1, 0.5357815027236938: 1, -0.5850008726119995: 1, -1.1196062564849854: 1, -1.17500901222229: 1, -0.03840658441185951: 1, -1.2006902694702148: 1, -0.5724524259567261: 1, 0.4252239763736725: 1, 1.5516252517700195: 1, -0.9432769417762756: 1, 1.5324621200561523: 1, 0.9532740116119385: 1, 1.114488959312439: 1, 1.0554637908935547: 1, 3.259727716445923: 1, -1.1946264505386353: 1, 0.025435535237193108: 1, -0.09378552436828613: 1, -0.8962797522544861: 1, -1.199107050895691: 1, 0.8790757060050964: 1, -0.9642779231071472: 1, -0.9627416729927063: 1, -0.9982122778892517: 1, -1.1845873594284058: 1, -1.0507465600967407: 1, 1.8212419748306274: 1, -0.36594024300575256: 1, -1.1659823656082153: 1, -1.0714704990386963: 1, -0.1391475349664688: 1, -1.0221163034439087: 1, 1.6904795169830322: 1, -1.078048825263977: 1, -0.7701697945594788: 1, -0.5982488989830017: 1, -0.3465023636817932: 1, -0.3573164939880371: 1, -1.0539400577545166: 1, -1.1605626344680786: 1, -0.8580590486526489: 1, -0.8890491724014282: 1, 0.6293842196464539: 1, 1.1045739650726318: 1, 0.04404761642217636: 1, 0.0663938894867897: 1, 1.4690353870391846: 1, 0.17207567393779755: 1, 1.4842547178268433: 1, -0.981871485710144: 1, 0.2662027180194855: 1, -1.194373369216919: 1, -1.1041064262390137: 1, -0.3497014045715332: 1, -1.1412583589553833: 1, -0.930094301700592: 1, -1.1848548650741577: 1, -0.6980454921722412: 1, -0.6311256289482117: 1, -1.188598394393921: 1, -1.1560314893722534: 1, -0.9288858771324158: 1, -0.5754680633544922: 1, 1.4887964725494385: 1, -1.2012841701507568: 1, 0.7494425773620605: 1, -0.8920286297798157: 1, -0.556195080280304: 1, 1.788353681564331: 1, -0.896551251411438: 1, 0.43905025720596313: 1, 0.6965684294700623: 1, -1.0395952463150024: 1, -0.45158419013023376: 1, 0.2903974652290344: 1, -0.8680421113967896: 1, -1.1370965242385864: 1, -0.19042661786079407: 1, -0.2147151529788971: 1, -0.9940837025642395: 1, 1.3862724304199219: 1, 1.6431117057800293: 1, -0.1989370882511139: 1, -1.0411947965621948: 1, -0.923246443271637: 1, -0.8935588002204895: 1, -1.18907630443573: 1, -1.0768063068389893: 1, -1.0751264095306396: 1, -0.35407769680023193: 1, -1.197619080543518: 1, -0.2589719295501709: 1, -1.2003082036972046: 1, -0.4400210678577423: 1, 1.6006011962890625: 1, 0.342952162027359: 1, -0.9859256744384766: 1, 1.6178103685379028: 1, 0.7533062696456909: 1, 1.5823100805282593: 1, 1.3024239540100098: 1, 0.9470359086990356: 1, 1.2427196502685547: 1, -0.5881722569465637: 1, -0.8510425090789795: 1, -0.930620014667511: 1, -1.2016338109970093: 1, 1.573736548423767: 1, 0.15925046801567078: 1, -0.37734130024909973: 1, 1.0202414989471436: 1, 1.1998642683029175: 1, 0.22981515526771545: 1, 1.2730098962783813: 1, 0.7611046433448792: 1, -1.113705039024353: 1, -0.39828142523765564: 1, -0.16857680678367615: 1, 1.427193522453308: 1, -0.5825971961021423: 1, -0.8986467123031616: 1, -0.15746326744556427: 1, -1.1914112567901611: 1, 1.5811641216278076: 1, -0.027011625468730927: 1, 1.3250715732574463: 1, 0.12876805663108826: 1, 0.465168297290802: 1, 0.2403424084186554: 1, 0.2308363914489746: 1, 0.8504006266593933: 1, 1.2588475942611694: 1, 0.4723914861679077: 1, -0.24819114804267883: 1, -0.08030800521373749: 1, -0.06996402144432068: 1, 0.36215391755104065: 1, 1.2712597846984863: 1, 0.3604428768157959: 1, -1.1895103454589844: 1, 0.17100460827350616: 1, 0.7471779584884644: 1, -1.115770697593689: 1, 1.1508636474609375: 1, 1.0995265245437622: 1, -0.7282882928848267: 1, -1.1950759887695312: 1, 0.34950539469718933: 1, -0.993471086025238: 1, 0.3443826735019684: 1, 0.4633817672729492: 1, 1.286658525466919: 1, 1.5370275974273682: 1, 0.7614589929580688: 1, 0.6386443972587585: 1, 1.4500402212142944: 1, 1.088638186454773: 1, 0.9285130500793457: 1, -1.1701372861862183: 1, 0.912930965423584: 1, 1.5277636051177979: 1, -0.21738800406455994: 1, 1.0551327466964722: 1, 0.35480692982673645: 1, 1.4536134004592896: 1, -0.004194003064185381: 1, 0.23328566551208496: 1, 1.6003168821334839: 1, -0.20539666712284088: 1, 0.7950616478919983: 1, -0.22176611423492432: 1, -0.4253336787223816: 1, 0.2726168930530548: 1, -0.02133699133992195: 1, 0.03207547590136528: 1, 0.4048600494861603: 1, 1.4308977127075195: 1, 0.637361466884613: 1, 1.3996703624725342: 1, 1.7721431255340576: 1, 1.3002121448516846: 1, 0.6696186065673828: 1, 1.0170906782150269: 1, 0.9138351678848267: 1, 0.3607413172721863: 1, -0.10225572437047958: 1, 1.366266131401062: 1, -0.16261765360832214: 1, -0.1799832135438919: 1, 0.900846004486084: 1, 1.0683897733688354: 1, -0.514695405960083: 1, 0.36046868562698364: 1, 1.3494455814361572: 1, 0.12815426290035248: 1, 1.7614071369171143: 1, -0.16177639365196228: 1, 0.9005318284034729: 1, -0.18478137254714966: 1, -0.539240837097168: 1, 0.2775695323944092: 1, 0.28860634565353394: 1, 1.3262649774551392: 1, 0.7724436521530151: 1, 0.3313300311565399: 1, -0.19720852375030518: 1, 1.425097107887268: 1, -1.1913617849349976: 1, -1.0719594955444336: 1, 1.287703037261963: 1, 1.4413039684295654: 1, -0.8234555125236511: 1, -1.1940946578979492: 1, -0.9339156150817871: 1, 1.9238320589065552: 1, -1.1969212293624878: 1, 0.1708119511604309: 1, -0.23679472506046295: 1, 0.2964378595352173: 1, 0.739153265953064: 1, 0.03332117572426796: 1, 1.4062000513076782: 1, -0.2782626748085022: 1, -0.021469445899128914: 1, -0.589444637298584: 1, -0.32526895403862: 1, 1.0541952848434448: 1, 0.9129876494407654: 1, -0.01062939316034317: 1, 1.0303751230239868: 1, 1.5870535373687744: 1, 0.11351441591978073: 1, -1.107023000717163: 1, -0.16252407431602478: 1, 0.9657272696495056: 1, 0.27488669753074646: 1, 1.2950940132141113: 1, 0.7110782265663147: 1, 1.210314393043518: 1, 0.5456136465072632: 1, -0.16512484848499298: 1, 0.1011820137500763: 1, 1.495969295501709: 1, 1.2984728813171387: 1, 0.5222756862640381: 1, 0.4746771454811096: 1, 1.4644434452056885: 1, 1.5755736827850342: 1, 0.6039040088653564: 1, -1.031872034072876: 1, 0.1467430293560028: 1, 0.31799715757369995: 1, 0.1881372481584549: 1, 0.16384904086589813: 1, 0.2640135586261749: 1, 1.3437533378601074: 1, 0.9666041731834412: 1, 0.9499619007110596: 1, 0.02830575592815876: 1, 1.3615977764129639: 1, 0.8268551230430603: 1, 1.7417840957641602: 1, 1.0402084589004517: 1, -0.6611031889915466: 1, 0.6606081128120422: 1, -0.408608615398407: 1, -0.47762712836265564: 1, -1.1897622346878052: 1, -0.2686515748500824: 1, -1.1663979291915894: 1, 1.8360271453857422: 1, 1.5100682973861694: 1, -1.1264077425003052: 1, 1.0540943145751953: 1, 1.5793328285217285: 1, 0.42848649621009827: 1, -0.41689032316207886: 1, -1.061113953590393: 1, 0.591416597366333: 1, -1.201619267463684: 1, 1.2202951908111572: 1, -0.4105451703071594: 1, 1.280470848083496: 1, -0.807515025138855: 1, -1.2013123035430908: 1, -1.2016263008117676: 1, 1.3301595449447632: 1, -1.0099024772644043: 1, 0.5131713151931763: 1, 0.3117649555206299: 1, -0.2495342493057251: 1, 0.27857378125190735: 1, 1.1086541414260864: 1, 1.7298698425292969: 1, 1.0101338624954224: 1, 0.9832457304000854: 1, 1.891126036643982: 1, 0.19680047035217285: 1, 0.5497986674308777: 1, 0.08808748424053192: 1, 1.6464146375656128: 1, 1.9098440408706665: 1, 0.523788332939148: 1, 1.4320223331451416: 1, 1.7558945417404175: 1, -0.4567924439907074: 1, -0.962668240070343: 1, -1.2016352415084839: 1, 0.4860861599445343: 1, -1.1416743993759155: 1, 1.1998889446258545: 1, -1.201629877090454: 1, 0.14208731055259705: 1, 1.8529928922653198: 1, -0.10222127288579941: 1, 1.7350994348526: 1, 1.7166484594345093: 1, 1.5665374994277954: 1, 1.0374422073364258: 1, 0.5713223814964294: 1, 1.5698747634887695: 1, 1.3496158123016357: 1, 1.9196945428848267: 1, 1.9020731449127197: 1, -1.2016111612319946: 1, -0.5369133949279785: 1, -0.6532332301139832: 1, 0.9807740449905396: 1, 1.796111822128296: 1, 0.6003371477127075: 1, 0.804813027381897: 1, -0.9704957008361816: 1, 1.2709132432937622: 1, 1.5992790460586548: 1, -1.2016212940216064: 1, -0.5300003886222839: 1, 1.2604477405548096: 1, 1.2889325618743896: 1, -0.864342451095581: 1, -1.1658029556274414: 1, -1.2016136646270752: 1, 1.633592128753662: 1, 1.3466922044754028: 1, -1.2002416849136353: 1, 1.5722275972366333: 1, 0.5880253314971924: 1, -1.042400598526001: 1, 1.4280599355697632: 1, -0.2810556888580322: 1, -0.5811882019042969: 1, 0.24442099034786224: 1, 1.5315228700637817: 1, -0.46823152899742126: 1, 0.766596257686615: 1, -0.03557446971535683: 1, -0.7948459982872009: 1, 0.81379234790802: 1, 1.6143009662628174: 1, 0.7481110692024231: 1, 0.04922454059123993: 1, -0.08275699615478516: 1, 1.4337563514709473: 1, 0.9173278212547302: 1, 0.5438616275787354: 1, 0.14515815675258636: 1, 0.7045637369155884: 1, -0.03429622948169708: 1, 0.03225273638963699: 1, 1.545008897781372: 1, 1.3033519983291626: 1, 0.19334031641483307: 1, 1.8893579244613647: 1, -0.7589280009269714: 1, 0.24924464523792267: 1, 0.8739101886749268: 1, 1.6735926866531372: 1, -0.23398016393184662: 1, 0.7566526532173157: 1, -0.4275071322917938: 1, 1.231010913848877: 1, -0.3100615441799164: 1, 1.692647099494934: 1, 1.8799982070922852: 1, -0.8965012431144714: 1, 1.5299123525619507: 1, 1.9158005714416504: 1, 0.4917255938053131: 1, 1.6278821229934692: 1, 1.6179100275039673: 1, 0.16001862287521362: 1, 0.3740043342113495: 1, -0.22299611568450928: 1, 0.056893277913331985: 1, -1.0097246170043945: 1, -0.03317539766430855: 1, 1.7539278268814087: 1, 1.7069745063781738: 1, 1.0135881900787354: 1, -0.31705647706985474: 1, 0.8423707485198975: 1, 1.0885628461837769: 1, -0.19816404581069946: 1, -0.2079317569732666: 1, 0.1083393469452858: 1, 0.7741647958755493: 1, 1.6500415802001953: 1, 0.6886505484580994: 1, 0.5059344172477722: 1, -0.04758370667695999: 1, -0.2531147599220276: 1, 0.9454357028007507: 1, 0.9327585697174072: 1, 0.521833598613739: 1, 0.26564425230026245: 1, 1.7395148277282715: 1, 1.6153100728988647: 1, 1.0263571739196777: 1, -0.5665901303291321: 1, -0.4220041036605835: 1, 0.29674577713012695: 1, 0.7860816717147827: 1, 0.48093563318252563: 1, 1.0254307985305786: 1, -0.12864308059215546: 1, 1.6906383037567139: 1, 1.8846700191497803: 1, 0.6905592679977417: 1, 1.5815212726593018: 1, 1.1628241539001465: 1, 1.7223035097122192: 1, 0.34220972657203674: 1, 0.6320856809616089: 1, 1.9327584505081177: 1, 0.6905925273895264: 1, 1.4504951238632202: 1, -0.94952791929245: 1, -0.5870760679244995: 1, 0.5326334834098816: 1, 0.38025134801864624: 1, 0.261216938495636: 1, -0.7838892936706543: 1, 1.271135687828064: 1, 0.562964141368866: 1, 1.6581584215164185: 1, 0.8071705102920532: 1, -1.2015928030014038: 1, 0.6071187853813171: 1, 1.4854387044906616: 1, -0.4422439932823181: 1, 0.9784976243972778: 1, 1.9389169216156006: 1, 1.0590068101882935: 1, 1.610649824142456: 1, -1.0813920497894287: 1, -0.9450681209564209: 1, 0.8005363941192627: 1, 0.10180643945932388: 1, 0.5681759119033813: 1, -1.201594352722168: 1, 0.2591017186641693: 1, 1.0987391471862793: 1, 0.8198267817497253: 1, 0.12914451956748962: 1, 1.5291143655776978: 1, 1.7461694478988647: 1, 0.46223318576812744: 1, 1.127722144126892: 1, 1.8749902248382568: 1, 0.3816104531288147: 1, -1.002498984336853: 1, -1.2015867233276367: 1, -0.538144052028656: 1, -1.040601134300232: 1, -0.8136187195777893: 1, 1.519827961921692: 1, 1.16111421585083: 1, -1.085170865058899: 1, 0.8646785616874695: 1, 1.2595564126968384: 1, -1.012891411781311: 1, -0.13542917370796204: 1, -0.7379482984542847: 1, 1.1129239797592163: 1, 1.3335411548614502: 1, 1.3847272396087646: 1, -0.8793452382087708: 1, 1.2369016408920288: 1, -0.9954119920730591: 1, 1.8261455297470093: 1, -0.3587249517440796: 1, -0.20700129866600037: 1, 0.9158507585525513: 1, 0.8895251154899597: 1, -1.1936933994293213: 1, -0.7448302507400513: 1, -1.2016236782073975: 1, 0.8969712257385254: 1, -0.8653848767280579: 1, 1.8480027914047241: 1, -0.5476839542388916: 1, -0.24342182278633118: 1, -1.2016215324401855: 1, -0.13886263966560364: 1, 0.963599681854248: 1, 0.6376197934150696: 1, 0.6801310777664185: 1, 0.4253986179828644: 1, -0.2255825698375702: 1, 1.3490053415298462: 1, 1.5920872688293457: 1, 1.7425659894943237: 1, -0.9789384603500366: 1, -0.6306192278862: 1, 1.3510494232177734: 1, 1.6034055948257446: 1, -0.6586742401123047: 1, 0.1718989461660385: 1, -1.200726866722107: 1, -1.2011404037475586: 1, 1.339892864227295: 1, -1.1579643487930298: 1, -0.21755054593086243: 1, 1.455495834350586: 1, 0.25442907214164734: 1, -0.9175146222114563: 1, 0.9006003737449646: 1, 1.0329307317733765: 1, 1.0343732833862305: 1, -1.2016191482543945: 1, 0.05316608399152756: 1, 1.9172451496124268: 1, 1.8285820484161377: 1, 0.43482455611228943: 1, 1.8706549406051636: 1, -0.22918304800987244: 1, -0.2059621661901474: 1, 1.7338974475860596: 1, -1.1562846899032593: 1, 1.760259985923767: 1, -0.11584310233592987: 1, -0.9069592952728271: 1, 1.636014699935913: 1, -0.4143541157245636: 1, 0.08071509003639221: 1, 0.8007993698120117: 1, -0.9877029061317444: 1, 0.23050570487976074: 1, -0.7923315763473511: 1, -0.009660118259489536: 1, 1.114802360534668: 1, -1.174880027770996: 1, 1.7429335117340088: 1, 1.92928946018219: 1, 1.7397531270980835: 1, -0.8557604551315308: 1, -0.08804576843976974: 1, 1.2302463054656982: 1, -0.8916471600532532: 1, 0.04224063828587532: 1, 1.4848576784133911: 1, 1.8896540403366089: 1, -1.2014458179473877: 1, 0.8217967748641968: 1, 0.8788592219352722: 1, 1.8354456424713135: 1, 1.0276689529418945: 1, 0.019096076488494873: 1, -0.1335328370332718: 1, 1.058468222618103: 1, 0.32922476530075073: 1, 1.7499927282333374: 1, -0.7876200675964355: 1, 0.13984030485153198: 1, 1.9223994016647339: 1, -0.11489463597536087: 1, 1.0331618785858154: 1, -0.4194781482219696: 1, 1.5742621421813965: 1, 0.1478143036365509: 1, 1.2849032878875732: 1, -0.420527845621109: 1, 1.6787821054458618: 1, 1.0356202125549316: 1, 1.0119178295135498: 1, -0.7692103385925293: 1, 1.5984208583831787: 1, 1.7267848253250122: 1, 1.7614209651947021: 1, 0.8861773610115051: 1, 1.887890100479126: 1, 0.09368380159139633: 1, -0.796614408493042: 1, 1.7114263772964478: 1, -0.8602240085601807: 1, 0.9352988600730896: 1, 1.597861409187317: 1, 0.9059203267097473: 1, 1.0176738500595093: 1, 1.703546404838562: 1, 0.9797016382217407: 1, 1.746630072593689: 1, 0.9968640804290771: 1, 5.270293235778809: 1, 0.7037346363067627: 1, -0.6276612877845764: 1, -0.4433523118495941: 1, -0.4171464443206787: 1, -0.6007823944091797: 1, -0.9877877235412598: 1, -1.0387167930603027: 1, -0.3422335684299469: 1, -0.29665860533714294: 1, -0.4729682505130768: 1, -0.8232591152191162: 1, -0.558110237121582: 1, 0.9276106357574463: 1, -0.7505127787590027: 1, -0.0690179392695427: 1, 0.017721591517329216: 1, -1.1957098245620728: 1, 1.3759360313415527: 1, 1.1735796928405762: 1, -0.4622204601764679: 1, -0.03591597080230713: 1, -0.14395083487033844: 1, 0.2143784463405609: 1, -0.4592617452144623: 1, 1.2142442464828491: 1, -0.7915438413619995: 1, -0.46590861678123474: 1, -0.06991042196750641: 1, 1.4788439273834229: 1, 1.052090048789978: 1, 1.2964091300964355: 1, 1.3913298845291138: 1, -0.466978520154953: 1, -1.2009780406951904: 1, -0.92970210313797: 1, -0.34284013509750366: 1, -0.47152507305145264: 1, -0.23732632398605347: 1, 1.8227369785308838: 1, 0.7785534858703613: 1, -0.3081098794937134: 1, 0.006228437647223473: 1, -0.8751227855682373: 1, 1.456301212310791: 1, 0.5354000329971313: 1, 0.171223446726799: 1, -0.34326422214508057: 1, 0.15484686195850372: 1, -1.1479105949401855: 1, -0.24207068979740143: 1, -0.3858093321323395: 1, 1.3953920602798462: 1, 1.878305196762085: 1, -0.46113380789756775: 1, 1.5919677019119263: 1, 1.7606215476989746: 1, 1.3363116979599: 1, -0.9762966632843018: 1, -0.8456059098243713: 1, -1.0356733798980713: 1, 0.22667939960956573: 1, 0.43118155002593994: 1, 0.6444940567016602: 1, -0.2761753499507904: 1, -0.5378462672233582: 1, -0.5857658982276917: 1, -0.44544142484664917: 1, 1.4956955909729004: 1, 1.3399251699447632: 1, 1.9232004880905151: 1, -0.33291712403297424: 1, -0.3439752459526062: 1, -0.07709828019142151: 1, -0.7822737693786621: 1, -0.28641819953918457: 1, -0.9299617409706116: 1, -1.1991149187088013: 1, 1.8707987070083618: 1, 0.6785101294517517: 1, 0.29917436838150024: 1, 0.6026607155799866: 1, -1.185569167137146: 1, 1.6845048666000366: 1, -1.2014148235321045: 1, -1.193153738975525: 1, -1.1891201734542847: 1, -0.3753896951675415: 1, 0.012689988128840923: 1, -0.8769359588623047: 1, -0.46731990575790405: 1, -0.1967451572418213: 1, 0.9746946096420288: 1, 0.47734835743904114: 1, 0.16428887844085693: 1, -0.35491982102394104: 1, -0.5582360029220581: 1, -0.5378177762031555: 1, -0.960394024848938: 1, -0.200442373752594: 1, -0.29989245533943176: 1, -1.1109116077423096: 1, 1.7616217136383057: 1, -0.4605187475681305: 1, 0.06790906190872192: 1, -0.2670007050037384: 1, 3.323413610458374: 1, 0.3248298168182373: 1, -0.5843047499656677: 1, -0.9987406134605408: 1, 1.9010276794433594: 1, -1.1868388652801514: 1, -0.5717604756355286: 1, -0.8472578525543213: 1, -1.1767117977142334: 1, 0.6789939403533936: 1, 1.3883335590362549: 1, -0.90742427110672: 1, -0.9396678805351257: 1, -0.5413272380828857: 1, -0.7269378900527954: 1, 1.6493046283721924: 1, -0.2937408983707428: 1, 0.15043634176254272: 1, -0.5880576372146606: 1, -0.13842415809631348: 1, -0.3907565474510193: 1, 1.491416096687317: 1, -0.17455770075321198: 1, 1.4998488426208496: 1, -0.879592776298523: 1, -0.3702247440814972: 1, 0.5128249526023865: 1, -0.5174428224563599: 1, -0.3349834084510803: 1, -0.21868036687374115: 1, 1.6888245344161987: 1, 1.2261123657226562: 1, 0.37810415029525757: 1, -1.0713788270950317: 1, -0.5210259556770325: 1, -0.2992294430732727: 1, -1.179208755493164: 1, 1.6146701574325562: 1, -0.36514076590538025: 1, -0.29448574781417847: 1, -0.8708242177963257: 1, 0.025362450629472733: 1, -1.0031712055206299: 1, -0.014453819021582603: 1, -0.8232764601707458: 1, 0.3796524703502655: 1, -0.288830429315567: 1, 0.3571058511734009: 1, -0.8833669424057007: 1, 0.6150393486022949: 1, 1.187366247177124: 1, -0.238590806722641: 1, -0.5713714957237244: 1, -1.0160380601882935: 1, -0.5338919758796692: 1, 0.9851498603820801: 1, 0.273947536945343: 1, -1.1766016483306885: 1, 1.376474142074585: 1, 1.8017815351486206: 1, -0.4548023045063019: 1, 1.6648616790771484: 1, 0.9130324721336365: 1, 0.48826223611831665: 1, -0.6655375957489014: 1, -0.21501778066158295: 1, 1.058951735496521: 1, 0.1769435554742813: 1, -1.0319366455078125: 1, -0.04817575961351395: 1, 1.2531977891921997: 1, -0.9147067666053772: 1, 0.8109627962112427: 1, 0.04068145155906677: 1, -0.39972129464149475: 1, 1.7796648740768433: 1, 0.2126206010580063: 1, 0.2422785758972168: 1, 1.1691573858261108: 1, -0.21730461716651917: 1, -0.03495830297470093: 1, 1.6034691333770752: 1, 0.3213244378566742: 1, 0.5874748826026917: 1, -1.0406304597854614: 1, -0.9834045767784119: 1, -0.1501469612121582: 1, 0.2097160518169403: 1, -1.1448076963424683: 1, -0.13741447031497955: 1, 1.552185297012329: 1, -0.8602992296218872: 1, -1.201583981513977: 1, -0.3992172181606293: 1, -1.1680830717086792: 1, -0.5809049010276794: 1, -1.2016146183013916: 1, 0.5123543739318848: 1, -1.1336802244186401: 1, -1.155177116394043: 1, 0.5790113210678101: 1, -0.392406165599823: 1, -1.103760838508606: 1, -0.8279891610145569: 1, -1.1760764122009277: 1, -0.6904935240745544: 1, 1.9273109436035156: 1, 0.3162490129470825: 1, -0.3135724663734436: 1, 1.7555813789367676: 1, -1.1569743156433105: 1, -0.6562255024909973: 1, -0.26246213912963867: 1, 0.21555371582508087: 1, 0.04079057276248932: 1, -0.3426608741283417: 1, 1.7097703218460083: 1, 1.6683679819107056: 1, -0.09393322467803955: 1, 1.905008316040039: 1, -0.14373785257339478: 1, 0.611980676651001: 1, 0.11937177926301956: 1, -0.5307965278625488: 1, -0.27935805916786194: 1, -0.2874172031879425: 1, -0.9310538172721863: 1, 0.8171444535255432: 1, 1.8530430793762207: 1, 0.21643884479999542: 1, 0.7304840087890625: 1, -0.6055639982223511: 1, 1.6697852611541748: 1, -0.28438881039619446: 1, 0.12007596343755722: 1, -0.4845651388168335: 1, 1.0491001605987549: 1, -0.25207433104515076: 1, 0.6704513430595398: 1, 0.15644948184490204: 1, 1.8505216836929321: 1, 0.3744315505027771: 1, 1.852098822593689: 1, 1.5747997760772705: 1, 0.7721536755561829: 1, -0.01187801081687212: 1, 1.4984081983566284: 1, 0.956155002117157: 1, -1.0813374519348145: 1, 1.525660514831543: 1, -1.2013746500015259: 1, -1.1236215829849243: 1, -0.5361114740371704: 1, -0.3123464286327362: 1, 0.18035785853862762: 1, -0.8952922821044922: 1, -0.9930511116981506: 1, -0.30786851048469543: 1, 0.4937743842601776: 1, -0.364163339138031: 1, -0.3189155161380768: 1, -0.539626955986023: 1, -0.25122812390327454: 1, -0.9730278849601746: 1, 1.5011951923370361: 1, -1.1689732074737549: 1, -0.44949105381965637: 1, -1.1831696033477783: 1, -0.7939702272415161: 1, -0.5884281992912292: 1, -1.1422733068466187: 1, -1.201271653175354: 1, 1.896323323249817: 1, 0.4647309482097626: 1, -1.061142921447754: 1, 1.6570682525634766: 1, 1.4193427562713623: 1, 1.93668532371521: 1, 0.16830426454544067: 1, 0.1589841991662979: 1, 0.7185215353965759: 1, -0.608808159828186: 1, 1.8601784706115723: 1, 0.13262629508972168: 1, 1.629822015762329: 1, -0.43086937069892883: 1, -1.1991721391677856: 1, -0.4168057143688202: 1, -0.2183121144771576: 1, -0.7995052933692932: 1, -0.9981749057769775: 1, -1.1617506742477417: 1, -0.3383774757385254: 1, -0.6015652418136597: 1, 0.7078472375869751: 1, -1.1108695268630981: 1, -1.1475187540054321: 1, 0.9462845921516418: 1, -1.1829675436019897: 1, 4.112330913543701: 1, -1.2015608549118042: 1, -1.1969398260116577: 1, 4.5118513107299805: 1, -0.9751223921775818: 1, -1.199577808380127: 1, -0.7289202809333801: 1, -1.1694631576538086: 1, -0.13113076984882355: 1, -1.1207038164138794: 1, 3.341240167617798: 1, -1.1994960308074951: 1, -1.2015869617462158: 1, -0.2223142683506012: 1, -1.0460647344589233: 1, -1.2015472650527954: 1, 0.012895430438220501: 1, -1.1953339576721191: 1, -0.36108481884002686: 1, -0.8523722290992737: 1, -0.7296833395957947: 1, -1.2016171216964722: 1, -1.1925454139709473: 1, -1.0483030080795288: 1, -1.2002716064453125: 1, 0.5143935680389404: 1, -1.201569676399231: 1, -1.110012173652649: 1, -0.6403390765190125: 1, 4.282702445983887: 1, -1.1945738792419434: 1, -1.1494790315628052: 1, -1.201560139656067: 1, -1.2015953063964844: 1, -1.1635701656341553: 1, 0.6749281883239746: 1, -0.8671839237213135: 1, -1.0858170986175537: 1, 1.3031054735183716: 1, -1.0164505243301392: 1, -1.150406837463379: 1, -1.1756606101989746: 1, 0.032617583870887756: 1, -1.1632294654846191: 1, -1.1747305393218994: 1, -1.1993433237075806: 1, -0.20172715187072754: 1, -0.7597726583480835: 1, -1.0657732486724854: 1, -0.3311309218406677: 1, -0.4824763238430023: 1, -0.4976504147052765: 1, -1.2016228437423706: 1, 1.2888545989990234: 1, -1.1091188192367554: 1, 0.8944936394691467: 1, -0.4707425832748413: 1, -1.1992580890655518: 1, -0.012192374095320702: 1, -1.0079307556152344: 1, -0.11124473065137863: 1, -1.2016023397445679: 1, 1.2057219743728638: 1, 0.5111210942268372: 1, -0.7557051777839661: 1, 0.7875488996505737: 1, -1.1981743574142456: 1, -1.2011059522628784: 1, -0.8982016444206238: 1, -0.35893186926841736: 1, -0.6715388298034668: 1, -0.6006320714950562: 1, -0.12521179020404816: 1, -0.5657321810722351: 1, -0.39080610871315: 1, -0.10903797298669815: 1, -0.07614605128765106: 1, -1.1906999349594116: 1, -0.40984249114990234: 1, -0.8984062075614929: 1, 1.1114397048950195: 1, -0.881252646446228: 1, -0.5050346255302429: 1, -1.1873303651809692: 1, -1.200080394744873: 1, 0.17733901739120483: 1, -0.04230939969420433: 1, -0.35624369978904724: 1, -0.03235474228858948: 1, -0.3301823139190674: 1, -0.8901136517524719: 1, 0.49005118012428284: 1, -1.2016326189041138: 1, -0.7893494963645935: 1, -1.2016328573226929: 1, 0.5382747054100037: 1, -0.25587567687034607: 1, 0.810942530632019: 1, 0.03431294485926628: 1, -0.10418325662612915: 1, -1.1882494688034058: 1, -0.7642948627471924: 1, -0.01552529539912939: 1, -0.0996609777212143: 1, -0.14265145361423492: 1, -1.1139642000198364: 1, 0.944926917552948: 1, 0.5761882066726685: 1, -0.03975825384259224: 1, -1.1809542179107666: 1, 0.00948107335716486: 1, 0.4138732850551605: 1, -0.8764730095863342: 1, -0.24605818092823029: 1, -0.5374748706817627: 1, 0.040903303772211075: 1, 1.0700626373291016: 1, -0.7673166990280151: 1, -0.15097910165786743: 1, -1.2008600234985352: 1, 0.264765202999115: 1, 1.0866621732711792: 1, 1.1708778142929077: 1, -0.13237591087818146: 1, 0.6366953253746033: 1, 1.177069067955017: 1, -1.20154869556427: 1, 1.1094199419021606: 1, 1.0952398777008057: 1, 1.258391261100769: 1, -0.07435453683137894: 1, 1.1807068586349487: 1, -0.15606260299682617: 1, -1.1549781560897827: 1, 0.8677871227264404: 1, 1.0160198211669922: 1, -0.8395327925682068: 1, -0.5897277593612671: 1, -1.192929744720459: 1, -0.08644621819257736: 1, 0.4334481656551361: 1, 0.9452794194221497: 1, -0.7184330821037292: 1, -0.5807573795318604: 1, -0.39197561144828796: 1, -1.0416687726974487: 1, -0.042717207223176956: 1, -0.5342384576797485: 1, -0.39157962799072266: 1, -0.5935716032981873: 1, 1.0410280227661133: 1, -0.3055903911590576: 1, 1.2313082218170166: 1, -0.3122004270553589: 1, 0.8209275603294373: 1, -0.41031748056411743: 1, -0.005674127489328384: 1, 0.03998654708266258: 1, -1.1028809547424316: 1, -1.1866058111190796: 1, -0.24963954091072083: 1, -0.7811260223388672: 1, -1.201612949371338: 1, -0.3552163541316986: 1, 0.7848809361457825: 1, 0.3299405574798584: 1, 0.9907589554786682: 1, 1.143233299255371: 1, -1.187011957168579: 1, -0.671938955783844: 1, 0.8696494698524475: 1, -0.7864221334457397: 1, -0.136034294962883: 1, -0.17077305912971497: 1, -0.7683218121528625: 1, 0.8018452525138855: 1, -1.193078875541687: 1, -1.186597228050232: 1, -1.1420694589614868: 1, -1.1992239952087402: 1, 0.02189079485833645: 1, -1.169080138206482: 1, 1.25115966796875: 1, 1.1413462162017822: 1, -1.0178923606872559: 1, 0.7787987589836121: 1, -1.083876609802246: 1, -0.5272284746170044: 1, -1.1375747919082642: 1, -0.19534148275852203: 1, -1.201613426208496: 1, -0.3075839579105377: 1, -1.0825824737548828: 1, -0.29541367292404175: 1, -1.1735186576843262: 1, 0.3790889084339142: 1, 0.7587617635726929: 1, 0.6585915088653564: 1, -0.16215069591999054: 1, -1.2009185552597046: 1, 0.043348729610443115: 1, -0.4258005619049072: 1, 0.19746625423431396: 1, 0.6161129474639893: 1, -0.27813780307769775: 1, 1.4876383543014526: 1, -0.11206144839525223: 1, 1.1690500974655151: 1, -0.7266088128089905: 1, 0.022439440712332726: 1, -0.618209183216095: 1, 1.0350605249404907: 1, -0.3780987560749054: 1, -0.36745402216911316: 1, 0.5923774242401123: 1, -0.19212104380130768: 1, -1.2016348838806152: 1, 0.9105262160301208: 1, 0.9495259523391724: 1, 0.41225412487983704: 1, 0.6296795010566711: 1, -1.1068792343139648: 1, -0.6869497895240784: 1, 0.07662157714366913: 1, -0.11562295258045197: 1, -0.028692159801721573: 1, 1.0399528741836548: 1, 0.8076177835464478: 1, -0.13060888648033142: 1, -0.5774872899055481: 1, -1.198868989944458: 1, 1.1278204917907715: 1, 0.2748369872570038: 1, 1.1562855243682861: 1, -0.12876513600349426: 1, -1.1697360277175903: 1, -0.2588912844657898: 1, 1.3108209371566772: 1, -0.5649014711380005: 1, -0.1045512706041336: 1, -0.01294313371181488: 1, -0.21699510514736176: 1, 1.0150052309036255: 1, -0.1227283924818039: 1, -0.7380070686340332: 1, 1.274375557899475: 1, 1.196900725364685: 1, 1.1462621688842773: 1, 0.43160757422447205: 1, 0.34432777762413025: 1, 0.02654222585260868: 1, -0.32669803500175476: 1, -0.253052294254303: 1, -0.12913857400417328: 1, 0.5908839702606201: 1, 1.0087279081344604: 1, 0.167218878865242: 1, -0.5622422695159912: 1, 1.2926610708236694: 1, 0.12959904968738556: 1, -0.34817975759506226: 1, -1.1653438806533813: 1, 1.0467203855514526: 1, 1.0611008405685425: 1, 1.2197721004486084: 1, -0.7624772787094116: 1, -1.1104997396469116: 1, 1.2668349742889404: 1, -0.09158548712730408: 1, -0.4923703670501709: 1, -0.4809620976448059: 1, 0.2788977324962616: 1, -0.17342792451381683: 1, -1.1826090812683105: 1, -1.200330376625061: 1, 1.0692790746688843: 1, 0.606712281703949: 1, -0.5744653940200806: 1, -1.182140827178955: 1, 0.028185075148940086: 1, 0.5745441317558289: 1, 0.666642963886261: 1, 0.2699461281299591: 1, -0.024909501895308495: 1, 1.2464758157730103: 1, 0.8297379016876221: 1, -0.27802959084510803: 1, -0.7992537021636963: 1, 0.05971863865852356: 1, 0.0014178809942677617: 1, -0.5088394284248352: 1, 1.2035483121871948: 1, -1.2008949518203735: 1, -0.999508798122406: 1, -0.05626079440116882: 1, -0.5931430459022522: 1, 0.9011586904525757: 1, 0.762050449848175: 1, -0.22990889847278595: 1, 1.2829649448394775: 1, -0.23146884143352509: 1, 0.03461417928338051: 1, 0.7100340127944946: 1, 1.030333161354065: 1, 0.5583640336990356: 1, -1.109034538269043: 1, -0.5908447504043579: 1, -0.019765598699450493: 1, -0.3548189401626587: 1, -0.6266202926635742: 1, -0.0026253368705511093: 1, 0.05442709103226662: 1, -0.9430715441703796: 1, -0.7650251388549805: 1, -1.0579359531402588: 1, -0.16474246978759766: 1, -1.1513574123382568: 1, 1.3037681579589844: 1, 1.0595874786376953: 1, -1.155191421508789: 1, -1.1991511583328247: 1, -0.16514527797698975: 1, 1.1214849948883057: 1, -1.0054869651794434: 1, -1.1934514045715332: 1, -0.002401667181402445: 1, -0.11618001013994217: 1, -1.1654343605041504: 1, -0.26975223422050476: 1, -0.15591250360012054: 1, 0.12906195223331451: 1, -0.15388239920139313: 1, -0.34825557470321655: 1, -0.21185417473316193: 1, -0.02432066947221756: 1, 0.2524677515029907: 1, 0.7206960916519165: 1, -0.6903147101402283: 1, 1.00320303440094: 1, -1.193916916847229: 1, -0.15946216881275177: 1, -0.24131079018115997: 1, -0.048064157366752625: 1, -0.06410756707191467: 1, -0.024461327120661736: 1, -0.2519146203994751: 1, 0.6152291297912598: 1, 0.01674770377576351: 1, 1.182131290435791: 1, -0.06836508214473724: 1, -0.06601618975400925: 1, -0.3034926950931549: 1, 0.021945519372820854: 1, -0.15634003281593323: 1, -0.32367783784866333: 1, -0.5897085666656494: 1, 0.9278587102890015: 1, -0.19999556243419647: 1, 0.26051849126815796: 1, 0.053155358880758286: 1, -0.056251220405101776: 1, -1.1260875463485718: 1, -0.9567206501960754: 1, -1.0963338613510132: 1, -0.07029284536838531: 1, 1.1092147827148438: 1, 0.6174435615539551: 1, -0.7010860443115234: 1, -0.2265346348285675: 1, -0.9494366645812988: 1, -0.18796367943286896: 1, -1.144519329071045: 1, -1.2016303539276123: 1, -1.0224525928497314: 1, -1.1722731590270996: 1, -1.1582452058792114: 1, -1.191677212715149: 1, -1.0668615102767944: 1, -1.0160771608352661: 1, -0.5631559491157532: 1, -1.1923733949661255: 1, -1.1721937656402588: 1, -1.0433918237686157: 1, -1.1339654922485352: 1, -1.1858601570129395: 1, -0.6137503981590271: 1, -0.788809597492218: 1, -1.1685314178466797: 1, -1.1470420360565186: 1, -1.1350377798080444: 1, -1.1558103561401367: 1, -0.862193763256073: 1, -0.4475323557853699: 1, -1.1744723320007324: 1, -1.201621413230896: 1, -1.200749397277832: 1, -1.2015254497528076: 1, -1.026742935180664: 1, -0.4338090121746063: 1, -1.1509727239608765: 1, -0.9300684928894043: 1, -1.157931923866272: 1, -0.7443411946296692: 1, -0.8867827653884888: 1, -1.194840431213379: 1, -1.1729844808578491: 1, -1.010536789894104: 1, -1.1989647150039673: 1, -0.6588079929351807: 1, -1.1501390933990479: 1, -1.194190263748169: 1, -0.19914157688617706: 1, -0.2236318439245224: 1, -1.180148720741272: 1, -0.5804309248924255: 1, -1.05448579788208: 1, -1.2016135454177856: 1, -1.1871360540390015: 1, -0.6205415725708008: 1, -1.2015451192855835: 1, -1.0986745357513428: 1, -0.7456731200218201: 1, -1.0944702625274658: 1, -1.1019858121871948: 1, -1.1784441471099854: 1, -0.7622874975204468: 1, -1.1929867267608643: 1, -1.1482324600219727: 1, -0.5079992413520813: 1, -1.1857080459594727: 1, -1.1485586166381836: 1, -1.0865159034729004: 1, -1.1643073558807373: 1, -0.40432149171829224: 1, -1.1961623430252075: 1, -1.1300313472747803: 1, -1.2016000747680664: 1, -1.2015275955200195: 1, -0.832706868648529: 1, -1.1969208717346191: 1, -1.1333073377609253: 1, -1.1568188667297363: 1, -0.17838822305202484: 1, -1.175873041152954: 1, -1.1691778898239136: 1, -1.0323843955993652: 1, -1.1832531690597534: 1, -0.8899372816085815: 1, -0.9966712594032288: 1, -1.2013036012649536: 1, -0.6249680519104004: 1, -1.1552096605300903: 1, -1.1936330795288086: 1, -1.1981604099273682: 1, -1.1905823945999146: 1, -0.6411266922950745: 1, -0.694696843624115: 1, 0.9692907333374023: 1, -1.0461647510528564: 1, -0.8267379403114319: 1, -1.0589720010757446: 1, -1.1486388444900513: 1, -1.1036909818649292: 1, -1.0501618385314941: 1, -1.1981457471847534: 1, -1.200010895729065: 1, -0.9485399723052979: 1, -1.2006280422210693: 1, -1.1891672611236572: 1, -1.1977088451385498: 1, -1.144382119178772: 1, -1.1986464262008667: 1, -1.0186684131622314: 1, -0.4794164001941681: 1, -1.1468044519424438: 1, -0.967963457107544: 1, -0.9021695256233215: 1, -0.7001152634620667: 1, -0.8666650652885437: 1, -0.21827441453933716: 1, 0.057195477187633514: 1, 4.863577365875244: 1, -0.9046985507011414: 1, 4.722275257110596: 1, 0.939198911190033: 1, -1.1445417404174805: 1, 4.302996635437012: 1, -1.1364892721176147: 1, -0.07361169159412384: 1, 4.875144004821777: 1, 5.066896438598633: 1, 5.072229385375977: 1, 0.4937030076980591: 1, -1.1785725355148315: 1, -0.4824954569339752: 1, -0.6635236740112305: 1, -0.7420345544815063: 1, -0.47614291310310364: 1, -0.9453350901603699: 1, -1.1463063955307007: 1, 0.29684698581695557: 1, -1.145255446434021: 1, -1.1212965250015259: 1, -1.2005667686462402: 1, -1.0699774026870728: 1, -1.0538722276687622: 1, -0.9141108393669128: 1, -0.712040364742279: 1, -0.8772833347320557: 1, -0.5385211110115051: 1, -0.5520062446594238: 1, -0.9129625558853149: 1, -1.0223268270492554: 1, -1.0399388074874878: 1, -1.1062737703323364: 1, -0.7228063344955444: 1, -0.9656466245651245: 1, -1.0281414985656738: 1, -0.7968862652778625: 1, 0.18425099551677704: 1, -0.8791208267211914: 1, -1.1150031089782715: 1, -1.1503796577453613: 1, -0.9824236631393433: 1, -1.189503788948059: 1, -0.4648326635360718: 1, -1.0434380769729614: 1, -1.00275719165802: 1, -0.6867276430130005: 1, -0.669135332107544: 1, -1.2014120817184448: 1, 0.216363787651062: 1, -0.38882899284362793: 1, -1.1946135759353638: 1, -0.9539603590965271: 1, -1.1790684461593628: 1, -1.167614459991455: 1, -1.0620733499526978: 1, -1.1940333843231201: 1, -1.1610828638076782: 1, -1.188031554222107: 1, -0.6259949207305908: 1, -0.3561877906322479: 1, 1.585684895515442: 1, 1.1463862657546997: 1, -1.2015637159347534: 1, -1.154268741607666: 1, -1.171905517578125: 1, 0.009732205420732498: 1, -1.1621276140213013: 1, 1.123262643814087: 1, -0.45371508598327637: 1, -0.411021888256073: 1, 0.8030492067337036: 1, 0.5780434608459473: 1, -0.32108673453330994: 1, -0.4003660976886749: 1, 0.6591589450836182: 1, -0.8111313581466675: 1, -0.8244988322257996: 1, -0.34684932231903076: 1, 1.5706232786178589: 1, -0.4005826711654663: 1, 1.0962333679199219: 1, -0.9092623591423035: 1, 1.593440055847168: 1, 1.0043728351593018: 1, 1.6096405982971191: 1, -0.433001846075058: 1, 1.5002496242523193: 1, -1.1626108884811401: 1, -0.8635501265525818: 1, 1.8255465030670166: 1, 0.2436400204896927: 1, 1.036679744720459: 1, 0.2817704975605011: 1, 1.2748090028762817: 1, -0.750208854675293: 1, -0.7198786735534668: 1, 0.05797763168811798: 1, 1.0823159217834473: 1, -0.5816406607627869: 1, -0.6496835947036743: 1, -1.186979055404663: 1, 0.5858985185623169: 1, -1.2016358375549316: 1, -0.5739816427230835: 1, 0.7670885920524597: 1, -0.4656817615032196: 1, -1.0180860757827759: 1, -1.2016373872756958: 1, -0.576421856880188: 1, -1.195853590965271: 1, 0.821479320526123: 1, -1.1690752506256104: 1, -0.5976389050483704: 1, 1.2671806812286377: 1, 1.0670119524002075: 1, -0.7073397040367126: 1, 0.6389100551605225: 1, -1.0666824579238892: 1, 1.282943606376648: 1, -0.30494359135627747: 1, -0.9499993324279785: 1, -0.7252834439277649: 1, 0.842113196849823: 1, -0.7019102573394775: 1, -0.5334532856941223: 1, 1.0577486753463745: 1, -0.5885310769081116: 1, 1.2156920433044434: 1, -0.8736492395401001: 1, 0.4960012137889862: 1, 1.0147548913955688: 1, -0.35478705167770386: 1, 1.2256038188934326: 1, 0.9517895579338074: 1, -0.3316475749015808: 1, 0.880368709564209: 1, -1.084214448928833: 1, 0.21256738901138306: 1, 0.605282723903656: 1, -1.1689379215240479: 1, 0.7936035394668579: 1, -0.1682627946138382: 1, 1.1564749479293823: 1, -1.1865227222442627: 1, -1.1811505556106567: 1, 1.6846264600753784: 1, -1.0972120761871338: 1, -1.2010711431503296: 1, -1.1966667175292969: 1, -1.198758602142334: 1, -1.196737289428711: 1, -1.1950762271881104: 1, -0.9986592531204224: 1, -1.1970906257629395: 1, 1.7719837427139282: 1, -1.1995155811309814: 1, -1.2008846998214722: 1, -1.0349972248077393: 1, -0.9958106875419617: 1, -1.0542218685150146: 1, 0.4647902548313141: 1, -1.1973918676376343: 1, -0.23024950921535492: 1, -0.15451399981975555: 1, 0.275499552488327: 1, -1.0963668823242188: 1, -1.1594713926315308: 1, -0.9010990262031555: 1, -0.5026354193687439: 1, -1.1999870538711548: 1, -1.1910632848739624: 1, -0.7359880805015564: 1, -1.195172905921936: 1, 0.741217315196991: 1, -1.1993547677993774: 1, -1.2002415657043457: 1, -1.164048194885254: 1, -1.1929975748062134: 1, 0.3444092869758606: 1, 0.7184975147247314: 1, -1.157366394996643: 1, -1.175083041191101: 1, -1.1728081703186035: 1, -0.6460915207862854: 1, -1.1685289144515991: 1, -1.1994209289550781: 1, 1.6113003492355347: 1, 0.29118791222572327: 1, -0.2950673997402191: 1, -0.15851348638534546: 1, -0.7970835566520691: 1, 0.5490202307701111: 1, -0.16204535961151123: 1, 0.8697875142097473: 1, 0.7108749151229858: 1, 0.5599294900894165: 1, -0.6900844573974609: 1, 0.7067909836769104: 1, -1.1882411241531372: 1, 1.5164122581481934: 1, -1.2010202407836914: 1, 0.8852934837341309: 1, -0.23251420259475708: 1, -1.201418161392212: 1, 0.8645955324172974: 1, -0.39169713854789734: 1, -1.1990872621536255: 1, -1.173497200012207: 1, -0.638097882270813: 1, -1.2015430927276611: 1, 0.23683039844036102: 1, -1.0081939697265625: 1, -0.04312499612569809: 1, -1.1948130130767822: 1, -1.188680648803711: 1, 0.6052579283714294: 1, -0.17054051160812378: 1, -1.1964715719223022: 1, 0.3594037592411041: 1, -1.2015913724899292: 1, -1.016434669494629: 1, -0.7167530059814453: 1, 0.6880602836608887: 1, 0.6605957746505737: 1, -0.019019143655896187: 1, -1.1852235794067383: 1, 0.7352478504180908: 1, 1.4972656965255737: 1, 1.0554699897766113: 1, 1.3231751918792725: 1, 0.592692494392395: 1, 1.556430697441101: 1, -0.9130086302757263: 1, 1.1509000062942505: 1, 0.7825908064842224: 1, 1.260263442993164: 1, 1.375723958015442: 1, 1.4145740270614624: 1, 0.7438257932662964: 1, -0.2681446373462677: 1, 0.18295887112617493: 1, 0.2222539335489273: 1, 0.3024345636367798: 1, 0.35236239433288574: 1, 1.0590577125549316: 1, -0.9236016869544983: 1, 1.4752575159072876: 1, 1.1449819803237915: 1, 0.5713070034980774: 1, 0.24058645963668823: 1, 0.3425867557525635: 1, 0.35902467370033264: 1, 1.561331033706665: 1, 0.1928955614566803: 1, 0.23876602947711945: 1, 0.23767612874507904: 1, -0.13019442558288574: 1, -0.1196289211511612: 1, 1.2999117374420166: 1, 1.4899855852127075: 1, -0.1709771454334259: 1, -0.812441349029541: 1, 0.3157300651073456: 1, 0.6854439377784729: 1, -0.6113037467002869: 1, 1.6643083095550537: 1, -0.19414900243282318: 1, 1.481839656829834: 1, 1.4051384925842285: 1, -0.20360040664672852: 1, 0.2816910445690155: 1, -0.15476621687412262: 1, 0.339995801448822: 1, 0.11328306794166565: 1, 0.5472216606140137: 1, 0.2568971812725067: 1, 0.5611000061035156: 1, 0.8098611235618591: 1, 0.4243623912334442: 1, 1.5731940269470215: 1, 1.5127235651016235: 1, 1.2513844966888428: 1, 1.4644227027893066: 1, 0.2846507132053375: 1, 1.3862555027008057: 1, 1.2155990600585938: 1, 0.1144905686378479: 1, -0.30866673588752747: 1, -1.1556631326675415: 1, 0.822748601436615: 1, -0.2946179509162903: 1, -0.48821160197257996: 1, 1.4599251747131348: 1, 0.31122344732284546: 1, -0.05793027952313423: 1, 1.0482161045074463: 1, 0.5573470592498779: 1, 0.81052166223526: 1, 1.233654499053955: 1, 1.215183138847351: 1, 0.3554361164569855: 1, 1.3589857816696167: 1, 0.9935461282730103: 1, 0.2826254069805145: 1, -0.11455459892749786: 1, 0.340713769197464: 1, 0.2596873939037323: 1, 1.3110328912734985: 1, 1.4727312326431274: 1, -0.4757806956768036: 1, 1.0209075212478638: 1, 0.7781831622123718: 1, 1.4282907247543335: 1, 0.19260334968566895: 1, 0.36868804693222046: 1, 0.01178812701255083: 1, -0.5199218392372131: 1, 0.21266861259937286: 1, -0.03138621151447296: 1, 0.5634517669677734: 1, 1.256608247756958: 1, 1.4527193307876587: 1, -0.03743843734264374: 1, 1.5988372564315796: 1, 1.3192821741104126: 1, 1.533202886581421: 1, 0.31844547390937805: 1, 0.19674475491046906: 1, 0.11682117730379105: 1, 0.7520656585693359: 1, -0.11516252905130386: 1, 0.004675476811826229: 1, -0.06609977036714554: 1, 1.5724915266036987: 1, 0.7346283793449402: 1, 0.11165464669466019: 1, 1.111115574836731: 1, 1.5909359455108643: 1, 0.017385760322213173: 1, -0.014552570879459381: 1, 0.853833794593811: 1, -0.3219013214111328: 1, 0.3231067657470703: 1, 1.7802213430404663: 1, 0.2615222632884979: 1, -0.06798223406076431: 1, 1.3918044567108154: 1, 1.126368761062622: 1, 0.7675086855888367: 1, -0.3147757053375244: 1, 0.7623692750930786: 1, -0.10369612276554108: 1, 0.9434877038002014: 1, -0.0526600144803524: 1, -0.21601253747940063: 1, 1.5148382186889648: 1, 0.11681299656629562: 1, 0.06557659059762955: 1, 0.2561040222644806: 1, 0.06706182658672333: 1, 0.18826737999916077: 1, 0.30889958143234253: 1, -0.632728099822998: 1, 0.3140702247619629: 1, -0.10326965153217316: 1, 0.12041562795639038: 1, 0.7718502283096313: 1, 0.3626178801059723: 1, 1.622856616973877: 1, -0.40100592374801636: 1, 1.379611611366272: 1, 1.1887938976287842: 1, -1.1871042251586914: 1, 0.987504780292511: 1, -0.1960129290819168: 1, 0.9693878293037415: 1, -1.0057076215744019: 1, 0.8502970337867737: 1, -0.6454114317893982: 1, 0.2539007067680359: 1, 0.002965901279821992: 1, 1.4364150762557983: 1, 1.4786081314086914: 1, -0.16230207681655884: 1, 1.4751214981079102: 1, 0.3319496512413025: 1, 0.18870316445827484: 1, 0.858607292175293: 1, 1.282376766204834: 1, 0.7782679200172424: 1, 1.0659617185592651: 1, -1.1900941133499146: 1, 0.8500742316246033: 1, 1.4411144256591797: 1, -1.1040070056915283: 1, -0.56696617603302: 1, 0.7161386013031006: 1, 0.7092652916908264: 1, 0.0384686179459095: 1, 0.27865079045295715: 1, -0.5194405913352966: 1, 0.009009350091218948: 1, 1.4103199243545532: 1, 1.3759030103683472: 1, 0.8865175843238831: 1, 0.7280606031417847: 1, 0.7922017574310303: 1, 1.5027039051055908: 1, 0.6448225975036621: 1, 1.4735376834869385: 1, -0.870884895324707: 1, 1.5217971801757812: 1, 1.1847658157348633: 1, -0.2038322389125824: 1, 0.48444247245788574: 1, 2.0702569484710693: 1, 0.8618729114532471: 1, 0.9195832014083862: 1, -0.7541334629058838: 1, 0.8805737495422363: 1, 0.9053120613098145: 1, -0.16789886355400085: 1, -0.17545948922634125: 1, 0.263934850692749: 1, 1.3844947814941406: 1, 0.7684015035629272: 1, 0.22540217638015747: 1, 0.041503969579935074: 1, -0.11932859569787979: 1, -1.1504056453704834: 1, 1.405086636543274: 1, 0.7253832817077637: 1, 1.5550216436386108: 1, 0.008224710822105408: 1, -0.19083698093891144: 1, 0.02118554152548313: 1, 0.46070119738578796: 1, 1.0112850666046143: 1, -0.9134752750396729: 1, -0.8270519375801086: 1, -0.04211708903312683: 1, -0.9460977911949158: 1, -1.19014310836792: 1, -1.193596363067627: 1, 0.49092090129852295: 1, 0.638069212436676: 1, 1.4807751178741455: 1, 0.6570013761520386: 1, -0.08619289845228195: 1, 0.5575955510139465: 1, 0.39953649044036865: 1, -0.9919153451919556: 1, -1.1343045234680176: 1, -0.8108161091804504: 1, 1.4680920839309692: 1, -1.1999285221099854: 1, -0.091583751142025: 1, -1.1579349040985107: 1, 0.2824403941631317: 1, -0.009973025880753994: 1, 0.3276282548904419: 1, 0.3697844445705414: 1, -1.1851680278778076: 1, -1.086830973625183: 1, -0.6757361888885498: 1, 0.6463801264762878: 1, 1.4938876628875732: 1, 0.31239357590675354: 1, 0.7275790572166443: 1, 0.3124358654022217: 1, 1.556864857673645: 1, -0.04237562045454979: 1, 0.9760450720787048: 1, 0.3074534237384796: 1, 0.8999701142311096: 1, 0.05755604803562164: 1, 1.6108869314193726: 1, 0.29415011405944824: 1, 1.5599995851516724: 1, 1.5686708688735962: 1, 1.7404301166534424: 1, -0.2210041582584381: 1, 0.9266675710678101: 1, -0.3739321231842041: 1, 0.8433452844619751: 1, 0.5241292119026184: 1, 0.5769325494766235: 1, 1.916335940361023: 1, 1.5063830614089966: 1, -0.4644731879234314: 1, 1.4151798486709595: 1, -0.11317183822393417: 1, 0.3683989942073822: 1, 0.1563034951686859: 1, -0.1884830892086029: 1, 0.35628634691238403: 1, -1.0314160585403442: 1, -0.28925973176956177: 1, -0.16245944797992706: 1, 0.8222253322601318: 1, 1.622040867805481: 1, 1.1854524612426758: 1, 0.8342987895011902: 1, 0.968934953212738: 1, 1.007346510887146: 1, -0.6390724778175354: 1, 1.0041277408599854: 1, 0.23464715480804443: 1, 0.9422100782394409: 1, -0.6223658919334412: 1, 2.4050354957580566: 1, -0.09898030012845993: 1, 0.06307864934206009: 1, -0.20047886669635773: 1, 0.3499395549297333: 1, -0.1565595418214798: 1, 0.9952307939529419: 1, -1.0716415643692017: 1, -0.2168547362089157: 1, 0.30258285999298096: 1, 0.6513680219650269: 1, 1.547389030456543: 1, 1.471611738204956: 1, 1.4821670055389404: 1, -0.07051629573106766: 1, 1.3494865894317627: 1, -0.6971253156661987: 1, 0.24872112274169922: 1, 1.4069277048110962: 1, -1.1632437705993652: 1, -1.1837621927261353: 1, 1.5597952604293823: 1, 0.2389289289712906: 1, 1.1072840690612793: 1, 0.09744428098201752: 1, -0.8828660249710083: 1, 1.5512200593948364: 1, -0.19494549930095673: 1, 0.8533394932746887: 1, 1.5009726285934448: 1, 0.6865427494049072: 1, 1.302850365638733: 1, -0.040548212826251984: 1, 1.1731380224227905: 1, -0.20986565947532654: 1, -1.024586796760559: 1, 0.8943637013435364: 1, 1.4529069662094116: 1, 0.8744557499885559: 1, 0.7159395813941956: 1, -0.43688738346099854: 1, 1.368375539779663: 1, 0.8933335542678833: 1, -0.11868320405483246: 1, 0.3126457929611206: 1, 1.5239653587341309: 1, 0.5069756507873535: 1, 1.3772739171981812: 1, -0.0920228511095047: 1, 1.059990644454956: 1, 1.104429006576538: 1, 0.938378632068634: 1, -0.39580973982810974: 1, -1.201277732849121: 1, 1.2822530269622803: 1, -0.27776581048965454: 1, 1.2413678169250488: 1, 1.2350752353668213: 1, -0.2299586683511734: 1, 0.008748067542910576: 1, 0.9413108229637146: 1, 0.6657525897026062: 1, 0.8562594652175903: 1, -0.9053927063941956: 1, 0.8952587246894836: 1, -0.37779542803764343: 1, -0.4684484004974365: 1, -0.5749701261520386: 1, 0.9305616617202759: 1, 0.008470889180898666: 1, 0.9848727583885193: 1, 0.7325264811515808: 1, -1.126828908920288: 1, 0.055386364459991455: 1, -0.2671576142311096: 1, 0.009196557104587555: 1, 0.23997803032398224: 1, 0.24701066315174103: 1, -1.1247143745422363: 1, -0.007039392367005348: 1, -0.621107280254364: 1, 0.23272329568862915: 1, 1.0945255756378174: 1, -0.9168434739112854: 1, -0.8882886171340942: 1, 0.8631362915039062: 1, 0.008864369243383408: 1, -0.20541705191135406: 1, 0.23268936574459076: 1, -0.35842111706733704: 1, 0.6777730584144592: 1, 0.4251300096511841: 1, -0.674019455909729: 1, 0.09383262693881989: 1, 0.6232529878616333: 1, -0.5104274749755859: 1, -0.14243346452713013: 1, 0.5468651056289673: 1, 0.637002170085907: 1, -1.2005233764648438: 1, -1.1812138557434082: 1, -0.05695538595318794: 1, -0.28555259108543396: 1, 0.2588636577129364: 1, -0.5884501934051514: 1, 0.9394524693489075: 1, -0.8740671277046204: 1, -0.34523066878318787: 1, 1.1960792541503906: 1, 0.08744630217552185: 1, -0.029267514124512672: 1, 0.36468705534935: 1, 1.310013771057129: 1, 1.2978951930999756: 1, 0.015011060051620007: 1, 0.729964017868042: 1, -0.8322789072990417: 1, -0.33581042289733887: 1, -0.2597183287143707: 1, 1.035197138786316: 1, 1.196738839149475: 1, -0.16826894879341125: 1, -0.9032037854194641: 1, 1.0347987413406372: 1, 0.6233600974082947: 1, 1.113309621810913: 1, 1.2229245901107788: 1, -1.2015588283538818: 1, -0.7430113554000854: 1, 1.3061707019805908: 1, -1.0537559986114502: 1, -0.08886344730854034: 1, -1.1972460746765137: 1, 0.889022946357727: 1, -0.22985504567623138: 1, 0.02054119110107422: 1, -0.9307324290275574: 1, -1.2004859447479248: 1, 0.41254743933677673: 1, -1.20163094997406: 1, 0.15707539021968842: 1, -0.11954380571842194: 1, 0.29174771904945374: 1, 1.039072036743164: 1, 1.0578463077545166: 1, -0.5372467637062073: 1, -0.0449078269302845: 1, -0.8713244199752808: 1, -0.14272816479206085: 1, -1.2016297578811646: 1, 1.1865397691726685: 1, -0.04175446555018425: 1, 1.2327803373336792: 1, -0.9308528304100037: 1, 1.0013794898986816: 1, 0.4987215995788574: 1, 0.44458866119384766: 1, 0.525627076625824: 1, -0.6482374668121338: 1, -1.1774789094924927: 1, -0.13838951289653778: 1, -0.11709770560264587: 1, -0.20500345528125763: 1, -0.23531334102153778: 1, -0.15267345309257507: 1, -0.19495585560798645: 1, -1.2016282081604004: 1, -0.7914682626724243: 1, 1.0062413215637207: 1, -0.02250954695045948: 1, -0.11078709363937378: 1, 1.2373515367507935: 1, -0.586733877658844: 1, 1.0134633779525757: 1, -0.19223442673683167: 1, -0.42109400033950806: 1, 0.9730016589164734: 1, 0.838370680809021: 1, 0.654328465461731: 1, 1.1694233417510986: 1, 0.23339834809303284: 1, -1.2016277313232422: 1, -0.0411478653550148: 1, -1.1211071014404297: 1, -0.42487016320228577: 1, -0.002975589595735073: 1, 0.5979693531990051: 1, 0.8318881988525391: 1, -1.0489486455917358: 1, -0.012521528638899326: 1, -0.12074612081050873: 1, 1.150854468345642: 1, 1.2192449569702148: 1, -0.8759819269180298: 1, 1.009254813194275: 1, -0.41674312949180603: 1, -0.18254651129245758: 1, -0.816495954990387: 1, -0.24156887829303741: 1, 0.4205377995967865: 1, 0.033690646290779114: 1, 0.8838387131690979: 1, -0.9900954961776733: 1, -0.20062661170959473: 1, -1.1656997203826904: 1, 1.0187321901321411: 1, 1.1683013439178467: 1, 1.1942393779754639: 1, 1.2479115724563599: 1, -0.22866979241371155: 1, -1.1798354387283325: 1, 0.6535213589668274: 1, 1.2529374361038208: 1, 1.2188843488693237: 1, -0.5479841232299805: 1, 0.8476697206497192: 1, -1.1963841915130615: 1, -0.21881107985973358: 1, 0.33690160512924194: 1, -1.2015154361724854: 1, -0.4057319462299347: 1, -0.08719541132450104: 1, -1.1954847574234009: 1, -1.2016379833221436: 1, -1.194821834564209: 1, -1.2013384103775024: 1, -0.045158740133047104: 1, -1.1490250825881958: 1, -0.11703558266162872: 1, 0.1581522524356842: 1, 0.9015107750892639: 1, -0.28742867708206177: 1, -0.518195390701294: 1, -0.5834662318229675: 1, -0.9676279425621033: 1, -1.2015115022659302: 1, -1.2014601230621338: 1, -1.2011888027191162: 1, -1.2016382217407227: 1, -0.21048426628112793: 1, -1.1363192796707153: 1, -1.1318936347961426: 1, -1.1987295150756836: 1, -1.2013877630233765: 1, -0.6771114468574524: 1, -0.07099064439535141: 1, 0.1497194468975067: 1, -1.2014774084091187: 1, -1.2016361951828003: 1, -1.2015197277069092: 1, -1.201493740081787: 1, -1.201407790184021: 1, -0.8910970091819763: 1, -1.193373441696167: 1, -1.1923046112060547: 1, -1.1979888677597046: 1, -0.053435858339071274: 1, -1.2015819549560547: 1, 0.86496901512146: 1, -0.9951181411743164: 1, -0.14633353054523468: 1, -0.06736226379871368: 1, -0.5093275308609009: 1, -1.2015849351882935: 1, -0.0922759622335434: 1, -0.8935246467590332: 1, 0.10832516103982925: 1, -1.140577793121338: 1, 0.4845307767391205: 1, 0.021630888804793358: 1, 0.05546194687485695: 1, 1.189407229423523: 1, -1.0867854356765747: 1, 0.9222752451896667: 1, -0.16844011843204498: 1, -0.4890022277832031: 1, -1.1736985445022583: 1, 0.26258811354637146: 1, -0.20083148777484894: 1, -0.5888482928276062: 1, -1.2014681100845337: 1, -1.2016206979751587: 1, -0.8209242224693298: 1, -0.9857455492019653: 1, -0.009843221865594387: 1, -0.5475073456764221: 1, -1.2014092206954956: 1, -1.1383882761001587: 1, -1.2012838125228882: 1, -0.41403406858444214: 1, -1.180970549583435: 1, 0.20136801898479462: 1, -0.416104793548584: 1, -0.11705746501684189: 1, -1.1903178691864014: 1, -1.171350359916687: 1, -1.150158166885376: 1, -1.1623584032058716: 1, 0.3900337219238281: 1, -0.6426911950111389: 1, -1.2015341520309448: 1, -1.2015831470489502: 1, -1.2016292810440063: 1, -0.015705665573477745: 1, -0.011501064524054527: 1, -1.1947468519210815: 1, -1.175829291343689: 1, 3.9672465324401855: 1, -0.37206733226776123: 1, 0.7298784255981445: 1, 0.8610304594039917: 1, 1.28579580783844: 1, 0.29349300265312195: 1, 1.2526917457580566: 1, 0.3951069414615631: 1, 1.0069524049758911: 1, 0.047311048954725266: 1, 1.1649004220962524: 1, -0.34107181429862976: 1, 1.295539140701294: 1, -0.33075013756752014: 1, 0.8673886060714722: 1, -0.19123347103595734: 1, -1.2016286849975586: 1, -0.9734629392623901: 1, 0.04192548990249634: 1, -1.2016375064849854: 1, 0.27843114733695984: 1, 0.6842910647392273: 1, 0.5252094268798828: 1, -0.14631414413452148: 1, 1.2064505815505981: 1, -0.84548020362854: 1, 1.200035810470581: 1, 0.9529565572738647: 1, -0.9972583055496216: 1, -0.30726683139801025: 1, 1.2879470586776733: 1, -0.9601404666900635: 1, 0.9265954494476318: 1, 1.2916063070297241: 1, 0.9066123366355896: 1, 1.1717408895492554: 1, 1.2726079225540161: 1, 0.30879271030426025: 1, -0.24580752849578857: 1, -1.2015316486358643: 1, -1.1484540700912476: 1, -1.201595425605774: 1, -1.201473593711853: 1, 0.19668835401535034: 1, 2.26233172416687: 1, -1.2015864849090576: 1, -1.2016347646713257: 1, -0.848010778427124: 1, -1.1948002576828003: 1, -0.5937166213989258: 1, -1.2016299962997437: 1, -1.2015916109085083: 1, -1.179785132408142: 1, -1.2016327381134033: 1, 0.9705496430397034: 1, 0.469992071390152: 1, -1.2015215158462524: 1, 0.1331777125597: 1, -0.16572129726409912: 1, -0.21433545649051666: 1, -0.10037212073802948: 1, -1.0237786769866943: 1, 0.005373222753405571: 1, 1.0674824714660645: 1, 0.09893729537725449: 1, 1.0808240175247192: 1, -0.47594985365867615: 1, 1.0520529747009277: 1, -0.10643515735864639: 1, 0.9786769151687622: 1, -0.10290281474590302: 1, -0.16829612851142883: 1, -0.1688508540391922: 1, 0.6481048464775085: 1, -0.3213060796260834: 1, -1.1992485523223877: 1, -1.1320221424102783: 1} test data: {-1.20163094997406: 3, -1.201636552810669: 2, -1.2016041278839111: 2, -1.2016366720199585: 2, -1.201637625694275: 2, -1.2016327381134033: 2, -1.2016369104385376: 2, -1.2016353607177734: 2, -1.2016363143920898: 2, 0.6442342400550842: 1, 1.2753889560699463: 1, -0.3344474732875824: 1, -0.4993174076080322: 1, 0.36155056953430176: 1, 0.29451489448547363: 1, -0.31671836972236633: 1, 0.21625953912734985: 1, 0.9650787115097046: 1, -0.17338967323303223: 1, 0.10213274508714676: 1, -0.16027171909809113: 1, 0.5403873920440674: 1, 1.2994223833084106: 1, 1.2997812032699585: 1, 1.166581153869629: 1, 1.4755654335021973: 1, 0.6465133428573608: 1, 1.571059226989746: 1, 0.663663923740387: 1, 1.4237861633300781: 1, -0.30124133825302124: 1, 0.6158161163330078: 1, -0.2516374886035919: 1, -1.0873279571533203: 1, 1.6106586456298828: 1, -0.5348793864250183: 1, 1.5720155239105225: 1, -0.37317758798599243: 1, 0.8396581411361694: 1, 1.6089627742767334: 1, 0.32629087567329407: 1, 0.8664619326591492: 1, 0.2661615014076233: 1, 0.5732383131980896: 1, 0.31096193194389343: 1, 0.1385408341884613: 1, 1.605971336364746: 1, -0.48075738549232483: 1, 0.8028292059898376: 1, 1.4458893537521362: 1, 1.427628755569458: 1, 1.4694101810455322: 1, 0.7825468182563782: 1, 1.3784377574920654: 1, 0.71575528383255: 1, 1.1383615732192993: 1, 1.4938230514526367: 1, 1.1008855104446411: 1, -0.9218448400497437: 1, 1.224327802658081: 1, -0.5288078188896179: 1, 1.9331234693527222: 1, -0.25767362117767334: 1, 1.4811328649520874: 1, -0.6162902116775513: 1, 0.6780659556388855: 1, 1.0355088710784912: 1, 0.699573814868927: 1, 0.3411409258842468: 1, 0.394859254360199: 1, -1.1746125221252441: 1, 1.457643747329712: 1, 0.8274267315864563: 1, 0.5170465111732483: 1, -0.22351212799549103: 1, 1.5124294757843018: 1, -0.24653010070323944: 1, 0.7171676754951477: 1, -0.040320102125406265: 1, -0.030014334246516228: 1, 0.9149655103683472: 1, 1.090896725654602: 1, 0.20390741527080536: 1, 1.5805106163024902: 1, 0.13578376173973083: 1, -0.059458885341882706: 1, -0.4297850430011749: 1, 0.2713952362537384: 1, -0.3774075210094452: 1, 0.06667295098304749: 1, 1.520749807357788: 1, -1.1972938776016235: 1, -0.7745821475982666: 1, -1.045175313949585: 1, -1.2007761001586914: 1, -1.1639471054077148: 1, 0.5436715483665466: 1, -1.1476235389709473: 1, -1.1323087215423584: 1, 0.034827083349227905: 1, 0.3489625155925751: 1, 0.4001854956150055: 1, 1.14208984375: 1, -1.1674489974975586: 1, 0.39954620599746704: 1, -1.1996524333953857: 1, -1.186226725578308: 1, -0.2831217646598816: 1, -1.2015609741210938: 1, -1.1962217092514038: 1, -1.1927686929702759: 1, -1.2016384601593018: 1, 1.7539664506912231: 1, 1.8953936100006104: 1, 1.7107861042022705: 1, 0.6560998558998108: 1, 0.2407364845275879: 1, 0.0290671493858099: 1, -0.883184015750885: 1, 0.7675377726554871: 1, -0.3334684669971466: 1, -1.177890419960022: 1, -0.5042855143547058: 1, -0.29486238956451416: 1, -0.09802207350730896: 1, -0.00951747503131628: 1, 1.4295574426651: 1, -0.07565759867429733: 1, -1.1402051448822021: 1, -0.005905755329877138: 1, -1.201635479927063: 1, -1.1205617189407349: 1, -0.4154701232910156: 1, 0.6741006970405579: 1, -1.137963891029358: 1, 0.339458167552948: 1, -0.46576231718063354: 1, -0.8657602667808533: 1, 0.007752139586955309: 1, 0.32263097167015076: 1, -0.399366557598114: 1, -0.5027830004692078: 1, 0.007953275926411152: 1, 0.21878471970558167: 1, 0.21768306195735931: 1, -0.29657527804374695: 1, 1.1970174312591553: 1, -0.3203604817390442: 1, 1.5978947877883911: 1, -0.41311657428741455: 1, 0.8892757296562195: 1, -0.9888867139816284: 1, 0.9611315131187439: 1, -1.0070439577102661: 1, -0.506174623966217: 1, -0.7784246206283569: 1, -1.2014739513397217: 1, -1.2014697790145874: 1, 0.46835482120513916: 1, 0.6647039651870728: 1, -1.2006030082702637: 1, -1.2015589475631714: 1, -1.2015974521636963: 1, -1.1959139108657837: 1, -1.2004907131195068: 1, -1.055851936340332: 1, -0.1883729249238968: 1, -1.201597809791565: 1, -0.4781683385372162: 1, 0.603252649307251: 1, -1.2014310359954834: 1, -1.2015936374664307: 1, 1.2882025241851807: 1, 0.8741052150726318: 1, -0.116549052298069: 1, 0.3200169503688812: 1, -0.9299888610839844: 1, -1.1325860023498535: 1, -1.2009141445159912: 1, -1.2001420259475708: 1, 1.0116826295852661: 1, -1.0076677799224854: 1, -0.608140766620636: 1, 0.30057549476623535: 1, -0.8923998475074768: 1, -1.1712636947631836: 1, -0.7628646492958069: 1, 0.5474697947502136: 1, -1.094030499458313: 1, 1.0025136470794678: 1, 1.8480533361434937: 1, 1.536348581314087: 1, -1.1791499853134155: 1, -1.1212905645370483: 1, -0.4354245066642761: 1, -1.1646332740783691: 1, 0.8417104482650757: 1, -0.9047161340713501: 1, 1.3791615962982178: 1, -1.0090559720993042: 1, -0.26542410254478455: 1, -1.1941906213760376: 1, -1.1907743215560913: 1, 1.8447388410568237: 1, -0.6316778063774109: 1, -1.0422825813293457: 1, 0.10187211632728577: 1, -1.0393953323364258: 1, 0.7908697128295898: 1, -0.5884833335876465: 1, -0.18136551976203918: 1, 1.192413568496704: 1, -1.193503737449646: 1, -0.7096079587936401: 1, -1.0519613027572632: 1, 1.6304298639297485: 1, -0.8917420506477356: 1, -1.1898142099380493: 1, -0.7244925498962402: 1, 1.5179330110549927: 1, 0.7332795858383179: 1, 1.1921144723892212: 1, -0.004799619782716036: 1, 0.9344565272331238: 1, 0.80833899974823: 1, 0.35639217495918274: 1, 0.3431006968021393: 1, -0.20943275094032288: 1, 0.8287729024887085: 1, 1.3890480995178223: 1, 1.4127790927886963: 1, -0.025239035487174988: 1, -0.9639919996261597: 1, 1.3463716506958008: 1, 0.6183062195777893: 1, 0.2642950117588043: 1, 1.5557059049606323: 1, 0.22126778960227966: 1, 1.4580349922180176: 1, 0.2692132890224457: 1, -0.30212122201919556: 1, 1.2598285675048828: 1, -1.024053692817688: 1, -0.9951556921005249: 1, 0.5211433172225952: 1, -0.233070969581604: 1, 1.0163378715515137: 1, 0.4876076281070709: 1, -0.9803085327148438: 1, -1.0776159763336182: 1, -0.8936908841133118: 1, 1.8432141542434692: 1, -0.67027348279953: 1, -0.7166287899017334: 1, -0.974824070930481: 1, -0.49170398712158203: 1, -1.1716605424880981: 1, -0.13872799277305603: 1, -0.20395949482917786: 1, 1.577986240386963: 1, -0.3594827950000763: 1, -0.8993450403213501: 1, -0.4185396134853363: 1, 0.32160520553588867: 1, 0.1897212415933609: 1, 0.9568199515342712: 1, -0.18094922602176666: 1, -0.11414719372987747: 1, -0.047685928642749786: 1, 0.002877143444493413: 1, 0.7527873516082764: 1, -0.14438967406749725: 1, 1.9067574739456177: 1, -1.07969331741333: 1, -0.13197508454322815: 1, 1.1217374801635742: 1, -1.1607003211975098: 1, -1.1507015228271484: 1, 0.35307395458221436: 1, -0.9158876538276672: 1, -0.8877851366996765: 1, -0.8486149311065674: 1, 0.3255096673965454: 1, 0.3698660731315613: 1, 0.2634084224700928: 1, -0.7170498371124268: 1, -0.7180438041687012: 1, -1.2016340494155884: 1, -0.9881011247634888: 1, 0.6718612909317017: 1, 0.16298139095306396: 1, 1.4656307697296143: 1, -0.26631277799606323: 1, 0.2808535397052765: 1, 0.003300704760476947: 1, -0.5022063255310059: 1, 0.10751932114362717: 1, -0.016411839053034782: 1, 0.46373531222343445: 1, 0.3286954462528229: 1, 1.001185417175293: 1, 1.053705096244812: 1, 0.07571198046207428: 1, -0.48030614852905273: 1, -1.1072322130203247: 1, -1.1999518871307373: 1, 0.8691079020500183: 1, 0.46832725405693054: 1, 0.20924636721611023: 1, 0.9790754318237305: 1, -1.1358855962753296: 1, -1.1800106763839722: 1, 1.5657020807266235: 1, 0.06941241025924683: 1, -1.0407896041870117: 1, 1.6284458637237549: 1, -0.12463472783565521: 1, 0.15106460452079773: 1, -0.5990430116653442: 1, 0.2268732339143753: 1, -1.1442792415618896: 1, -0.07864277809858322: 1, 1.8358889818191528: 1, -1.1329883337020874: 1, -1.0213873386383057: 1, -1.1798655986785889: 1, -0.700495719909668: 1, -0.9755853414535522: 1, -0.6876721978187561: 1, -0.8956496715545654: 1, 1.2659807205200195: 1, 1.7851945161819458: 1, -0.9131625890731812: 1, -0.45086827874183655: 1, -0.05734042823314667: 1, -0.731924295425415: 1, -0.8747767210006714: 1, -0.5242934823036194: 1, -1.199570655822754: 1, 0.784543514251709: 1, -0.7811231017112732: 1, -1.0523524284362793: 1, 1.324471354484558: 1, -0.9793020486831665: 1, -1.1970343589782715: 1, -0.30134135484695435: 1, -0.6546655297279358: 1, -0.47023993730545044: 1, 0.28526046872138977: 1, 0.35712626576423645: 1, 0.454349547624588: 1, -0.9806917309761047: 1, -1.102870225906372: 1, -0.35984915494918823: 1, -0.7447215914726257: 1, -0.9635469913482666: 1, 1.6493014097213745: 1, -1.2014974355697632: 1, -0.7435175180435181: 1, 0.11243647336959839: 1, 1.7208062410354614: 1, 1.8881990909576416: 1, -1.2016189098358154: 1, -1.2006548643112183: 1, 0.9834553003311157: 1, -1.2016271352767944: 1, 1.76934015750885: 1, 1.9024626016616821: 1, 1.6134487390518188: 1, 1.5396013259887695: 1, -1.0555497407913208: 1, 0.5422061085700989: 1, -0.7931265234947205: 1, 1.8800233602523804: 1, 0.3571586012840271: 1, 1.6013163328170776: 1, 1.1523829698562622: 1, -1.0791629552841187: 1, 1.901644229888916: 1, -1.0994056463241577: 1, 0.5419895052909851: 1, -0.44384631514549255: 1, -0.253825306892395: 1, 0.9758270978927612: 1, 0.10260368138551712: 1, -0.7490406036376953: 1, -1.088794231414795: 1, -0.8662946820259094: 1, -0.26848453283309937: 1, 1.6441566944122314: 1, 1.477781891822815: 1, -0.6814844608306885: 1, 0.949316143989563: 1, 1.3716888427734375: 1, -0.0515512116253376: 1, -0.10395042598247528: 1, -1.054260492324829: 1, 1.4221618175506592: 1, -0.22097758948802948: 1, 0.4076603651046753: 1, -0.180640310049057: 1, 0.2675780653953552: 1, -0.2709258496761322: 1, 0.9761800765991211: 1, -0.4791189730167389: 1, -0.1754150390625: 1, 0.39559683203697205: 1, 0.5755087733268738: 1, 1.590468406677246: 1, -0.22210338711738586: 1, 1.865704894065857: 1, 0.9889004230499268: 1, 0.5330069065093994: 1, 1.612230896949768: 1, 1.6939563751220703: 1, -1.0590986013412476: 1, 0.8425688147544861: 1, 0.014584558084607124: 1, -0.2232077419757843: 1, 1.4182209968566895: 1, -0.025178229436278343: 1, 0.7021965980529785: 1, 0.27008119225502014: 1, 0.9952530264854431: 1, 0.18436919152736664: 1, 0.8505056500434875: 1, 1.7850080728530884: 1, 1.8263726234436035: 1, 0.9554869532585144: 1, 0.723430871963501: 1, -0.3399538993835449: 1, 1.5719680786132812: 1, -1.2016080617904663: 1, 0.8521577715873718: 1, -1.2015763521194458: 1, -1.2016057968139648: 1, 0.9156394600868225: 1, -1.2015223503112793: 1, 0.7080846428871155: 1, -1.0571757555007935: 1, 1.2539737224578857: 1, 1.7376213073730469: 1, 1.3225599527359009: 1, -0.0861414223909378: 1, -0.7990305423736572: 1, -0.43114355206489563: 1, 1.2727433443069458: 1, -0.7696746587753296: 1, 1.3174397945404053: 1, 1.896134853363037: 1, 1.7389863729476929: 1, 1.0316096544265747: 1, 1.6759966611862183: 1, 1.0934252738952637: 1, -1.14544677734375: 1, 0.9750880599021912: 1, -1.1302224397659302: 1, -0.7542659640312195: 1, 0.4616207182407379: 1, -1.2016278505325317: 1, 1.5496872663497925: 1, 1.3129916191101074: 1, -0.6896651983261108: 1, -1.2007230520248413: 1, -0.24564200639724731: 1, -1.130736231803894: 1, -0.40272125601768494: 1, -0.06900987029075623: 1, -0.34224218130111694: 1, -1.0611162185668945: 1, 0.4620521366596222: 1, 0.5805153250694275: 1, 1.7776927947998047: 1, 1.110692024230957: 1, 1.8291547298431396: 1, 0.780617356300354: 1, 0.20394694805145264: 1, 1.2843722105026245: 1, 1.4813575744628906: 1, -0.010684509761631489: 1, 0.63859623670578: 1, 1.788615107536316: 1, 1.7511584758758545: 1, 1.8416829109191895: 1, 1.446770191192627: 1, 1.6221997737884521: 1, 1.4475048780441284: 1, 0.10331395268440247: 1, 0.9160022139549255: 1, -1.2014168500900269: 1, 1.6676998138427734: 1, 0.9216548800468445: 1, 0.6508381366729736: 1, 0.8335886001586914: 1, -1.201361894607544: 1, -0.907404899597168: 1, 1.473099708557129: 1, -0.20698606967926025: 1, 1.5016316175460815: 1, -1.201613187789917: 1, 1.8303353786468506: 1, -0.655949056148529: 1, -0.24759358167648315: 1, 1.8067305088043213: 1, 1.4667671918869019: 1, 1.8180814981460571: 1, -0.5466570258140564: 1, 1.5142070055007935: 1, 0.35647323727607727: 1, -1.156775951385498: 1, 1.7661612033843994: 1, 1.8266348838806152: 1, 1.7602899074554443: 1, 1.8225407600402832: 1, -0.5458275079727173: 1, 1.6772441864013672: 1, 0.2889866232872009: 1, -0.2772659659385681: 1, 0.43645578622817993: 1, -0.47733497619628906: 1, -0.48933231830596924: 1, -0.4369107186794281: 1, -0.6786049008369446: 1, -0.37577909231185913: 1, -0.7031784653663635: 1, 0.3244344890117645: 1, -0.24040797352790833: 1, -0.014680324122309685: 1, 1.2115764617919922: 1, 1.5116616487503052: 1, -0.032225385308265686: 1, -0.9143604040145874: 1, -0.5345551371574402: 1, -0.6440175771713257: 1, 1.542358636856079: 1, -0.16766004264354706: 1, -1.093284010887146: 1, 0.9121710062026978: 1, -1.0817426443099976: 1, -0.31910526752471924: 1, -0.5629591941833496: 1, 0.6444322466850281: 1, -0.22242674231529236: 1, -0.906280517578125: 1, -0.5209143161773682: 1, 1.7544053792953491: 1, 1.2151012420654297: 1, 1.8596769571304321: 1, -0.8671026229858398: 1, -0.3237372636795044: 1, 0.3824501037597656: 1, -0.0695866122841835: 1, -0.42767956852912903: 1, 1.8792171478271484: 1, -0.06596078723669052: 1, 1.6555308103561401: 1, -1.1333893537521362: 1, -0.8443267941474915: 1, -0.3169466555118561: 1, 1.569981575012207: 1, -0.9003047943115234: 1, 1.5233625173568726: 1, 1.5933094024658203: 1, -0.5030357241630554: 1, -0.13618627190589905: 1, -0.4835919141769409: 1, -0.766767144203186: 1, -0.007546336855739355: 1, 0.8612715601921082: 1, -0.30685290694236755: 1, 1.4534815549850464: 1, 0.4303920269012451: 1, -0.7417653203010559: 1, -0.3424704372882843: 1, -0.1308683305978775: 1, 1.0024393796920776: 1, 1.5701237916946411: 1, -0.07945768535137177: 1, -0.03796708956360817: 1, 1.0432312488555908: 1, 0.672914445400238: 1, 0.7212937474250793: 1, 1.483720302581787: 1, -0.5381679534912109: 1, 0.2551988363265991: 1, 0.19413244724273682: 1, -0.0211151335388422: 1, 1.5722923278808594: 1, -0.0832226425409317: 1, 0.7159720659255981: 1, -0.6234576106071472: 1, -0.023513898253440857: 1, 1.7650138139724731: 1, 0.31867286562919617: 1, 0.17652635276317596: 1, 1.3037792444229126: 1, -0.8521113991737366: 1, -0.49563685059547424: 1, 0.6790488958358765: 1, 0.04851381108164787: 1, 0.5300026535987854: 1, 1.7963730096817017: 1, -0.24450848996639252: 1, 0.26557838916778564: 1, 0.10666077584028244: 1, -1.2016232013702393: 1, -1.2016154527664185: 1, -0.22874142229557037: 1, 0.3141234815120697: 1, -0.061833277344703674: 1, -0.3255411982536316: 1, -0.9726563692092896: 1, -1.2015986442565918: 1, 0.49887171387672424: 1, -0.3431845009326935: 1, 0.3915187418460846: 1, -0.24634599685668945: 1, 0.2092854529619217: 1, 1.0318180322647095: 1, 1.6552691459655762: 1, 0.5530001521110535: 1, -1.1350090503692627: 1, -0.5802597403526306: 1, 0.6405824422836304: 1, -0.8678878545761108: 1, 0.5500550270080566: 1, 1.4624748229980469: 1, 1.1236602067947388: 1, -0.7293344736099243: 1, 0.7833142876625061: 1, 0.0023630079813301563: 1, 0.11675674468278885: 1, 1.7392624616622925: 1, -0.15924076735973358: 1, 1.2042120695114136: 1, -0.22727444767951965: 1, -0.6431723237037659: 1, -0.024080123752355576: 1, -0.2558027505874634: 1, -0.2089931219816208: 1, 0.7334970831871033: 1, -1.2016234397888184: 1, 1.2401331663131714: 1, 0.46222802996635437: 1, 1.5428107976913452: 1, -0.18857857584953308: 1, -0.9720814824104309: 1, -0.9465845823287964: 1, -1.1498054265975952: 1, 0.7230984568595886: 1, -0.29477736353874207: 1, -0.8816695213317871: 1, -0.26783594489097595: 1, -0.6922621726989746: 1, -0.9465891718864441: 1, -0.26545509696006775: 1, 1.571593999862671: 1, -1.2009780406951904: 1, -0.7234905958175659: 1, -0.7983001470565796: 1, -0.27475300431251526: 1, -0.37186357378959656: 1, -1.1008306741714478: 1, -0.4653182625770569: 1, -0.563433825969696: 1, -1.1406184434890747: 1, -1.201536774635315: 1, 0.3640252351760864: 1, -1.1448140144348145: 1, -1.1006834506988525: 1, -0.5899453163146973: 1, 2.0270323753356934: 1, -0.46826034784317017: 1, -0.9971945881843567: 1, -1.1205850839614868: 1, -1.0625981092453003: 1, -0.7790790796279907: 1, -0.2970311641693115: 1, -1.188301682472229: 1, -0.8350648880004883: 1, -1.1232612133026123: 1, -1.1971925497055054: 1, -1.1624175310134888: 1, -1.1515345573425293: 1, -1.201555848121643: 1, 0.8056516647338867: 1, 1.5033998489379883: 1, 0.8847638368606567: 1, 1.2778698205947876: 1, 0.669349730014801: 1, -0.32589226961135864: 1, -0.06760650873184204: 1, -1.09720778465271: 1, -0.7514510154724121: 1, 0.050834186375141144: 1, 0.8621184229850769: 1, 0.5890273451805115: 1, 0.5067287683486938: 1, -0.5920382142066956: 1, 0.4001394212245941: 1, -0.7601802349090576: 1, 1.1708914041519165: 1, 0.3778545558452606: 1, 1.0215860605239868: 1, -0.5910298824310303: 1, 0.8628989458084106: 1, -1.1580485105514526: 1, -0.002722974168136716: 1, -1.192414402961731: 1, -0.6031686067581177: 1, -1.2016347646713257: 1, -0.861803412437439: 1, 0.9220188856124878: 1, -1.15325927734375: 1, -0.7482910752296448: 1, -0.03033183142542839: 1, -0.5369265675544739: 1, -1.051085352897644: 1, -1.2001534700393677: 1, -0.05180063098669052: 1, 0.8550933599472046: 1, 1.488932728767395: 1, -0.16389766335487366: 1, 0.8666924238204956: 1, 0.7242860198020935: 1, -0.9202075004577637: 1, 1.2778337001800537: 1, 0.1461368203163147: 1, -1.1379677057266235: 1, -0.13443662226200104: 1, 0.7907249331474304: 1, -0.5142630338668823: 1, -0.8211961388587952: 1, -0.8594576716423035: 1, 0.5329916477203369: 1, 1.229008674621582: 1, -1.1707801818847656: 1, -0.44190311431884766: 1, 0.9127581715583801: 1, 0.6975425481796265: 1, -0.9352316856384277: 1, -1.1959316730499268: 1, -1.2016355991363525: 1, -0.1892606019973755: 1, -1.1028145551681519: 1, -0.7776852250099182: 1, -1.2016229629516602: 1, -1.2016305923461914: 1, 0.6509128212928772: 1, -1.199397087097168: 1, -0.35862404108047485: 1, 0.369517058134079: 1, -0.3642917275428772: 1, 1.2405850887298584: 1, -0.8366453051567078: 1, -1.1760457754135132: 1, -1.1941806077957153: 1, -1.1992988586425781: 1, -0.09844925999641418: 1, -1.2015101909637451: 1, -0.6867349743843079: 1, 0.004596139770001173: 1, -1.1276121139526367: 1, -0.21156159043312073: 1, 0.9924389719963074: 1, -0.2299073189496994: 1, -1.2002339363098145: 1, -0.3503449857234955: 1, -0.4774172008037567: 1, 0.6907184720039368: 1, 0.48456287384033203: 1, -0.1598413586616516: 1, -1.1774768829345703: 1, 0.5863446593284607: 1, 1.0157313346862793: 1, 0.7746310830116272: 1, 1.043671727180481: 1, -0.01721060648560524: 1, 0.7346874475479126: 1, -0.016240552067756653: 1, -0.6019781231880188: 1, 0.048983871936798096: 1, 1.1558300256729126: 1, -0.23161835968494415: 1, 0.3949566185474396: 1, 0.004652172327041626: 1, -0.08915898948907852: 1, -1.0697368383407593: 1, -0.07907160371541977: 1, -0.29025569558143616: 1, 0.5070648193359375: 1, 0.9881972074508667: 1, 1.0766181945800781: 1, -1.1586220264434814: 1, -0.3917173445224762: 1, 0.2549906373023987: 1, 0.6609118580818176: 1, 0.6649335622787476: 1, -0.0404001921415329: 1, 0.008013189770281315: 1, 0.9726781249046326: 1, -0.21517714858055115: 1, -0.8245752453804016: 1, -1.1966403722763062: 1, -0.12709279358386993: 1, -0.3348834216594696: 1, -0.003650385420769453: 1, -1.0131398439407349: 1, 0.9348665475845337: 1, 0.6744810342788696: 1, 1.30207359790802: 1, -0.32544630765914917: 1, -0.42906203866004944: 1, -0.6155209541320801: 1, 0.033837273716926575: 1, -0.9566242694854736: 1, -0.455705851316452: 1, -1.1322532892227173: 1, 0.68455970287323: 1, -0.9743238091468811: 1, -0.14771254360675812: 1, 0.9156388640403748: 1, 1.257509708404541: 1, -0.8429122567176819: 1, -0.171332448720932: 1, -1.1908975839614868: 1, 0.013616573065519333: 1, -1.2010724544525146: 1, -1.20045006275177: 1, -0.17132940888404846: 1, -0.9249427914619446: 1, -0.6981508731842041: 1, -0.04764068126678467: 1, 0.6273993253707886: 1, -1.1074978113174438: 1, -0.05808640271425247: 1, -1.192280888557434: 1, 1.0808086395263672: 1, 1.0084635019302368: 1, -1.1338447332382202: 1, 1.2993144989013672: 1, -0.3634156584739685: 1, -0.7420557737350464: 1, 1.023429274559021: 1, -0.2878003716468811: 1, -0.2553582787513733: 1, -1.0076838731765747: 1, -0.07969757169485092: 1, -0.03992554917931557: 1, 0.8519235253334045: 1, -1.2016372680664062: 1, 1.1363762617111206: 1, -1.1914066076278687: 1, -0.3076569437980652: 1, -1.006115436553955: 1, -0.0674244612455368: 1, -1.201583743095398: 1, -0.8054187893867493: 1, 0.11520501971244812: 1, -1.0309724807739258: 1, -1.1520825624465942: 1, -0.63347989320755: 1, -1.0135008096694946: 1, -1.17979097366333: 1, -0.5772318840026855: 1, -0.6635865569114685: 1, -1.201562523841858: 1, -0.8834558129310608: 1, -1.0467379093170166: 1, -0.5632508397102356: 1, -0.6325321793556213: 1, -1.193282127380371: 1, -0.5115338563919067: 1, -0.2650972902774811: 1, -1.1344302892684937: 1, -0.4774998426437378: 1, -0.8366922736167908: 1, -0.41631850600242615: 1, -1.167817234992981: 1, -0.9714952111244202: 1, -1.0810678005218506: 1, -0.6395750641822815: 1, -1.1859160661697388: 1, -1.1349726915359497: 1, -0.9732658267021179: 1, -1.1563661098480225: 1, -0.9933805465698242: 1, -1.2008585929870605: 1, -1.126016616821289: 1, -0.7897263169288635: 1, -1.0068711042404175: 1, -1.161582589149475: 1, -1.0831377506256104: 1, -1.1954984664916992: 1, -0.8829452991485596: 1, -1.2006478309631348: 1, -1.162119746208191: 1, -1.194927453994751: 1, -1.1800310611724854: 1, -0.7736660242080688: 1, -0.5409148931503296: 1, -1.02364981174469: 1, -1.087907314300537: 1, -0.86516273021698: 1, -1.1933567523956299: 1, -0.25539615750312805: 1, -0.4444347023963928: 1, 1.7253838777542114: 1, 5.037554740905762: 1, -1.1754673719406128: 1, 5.012721538543701: 1, 5.006033897399902: 1, -1.1136521100997925: 1, 3.830434560775757: 1, 5.05051851272583: 1, -1.1480247974395752: 1, 4.58308219909668: 1, -0.7939543128013611: 1, 4.900933742523193: 1, 5.001932144165039: 1, -0.9225814342498779: 1, -0.3156169652938843: 1, -0.9272821545600891: 1, -1.1703253984451294: 1, -0.75212162733078: 1, -0.8758900761604309: 1, -0.6818874478340149: 1, -0.5497206449508667: 1, -1.0416630506515503: 1, -0.6967374682426453: 1, -0.5638068318367004: 1, -1.0074002742767334: 1, -0.8599510192871094: 1, -0.1397843062877655: 1, -1.0246316194534302: 1, -0.4437340795993805: 1, -1.1434556245803833: 1, -1.1181161403656006: 1, -1.0933367013931274: 1, -0.9449849724769592: 1, -0.9726890325546265: 1, -1.1816785335540771: 1, -1.2016375064849854: 1, 1.3093386888504028: 1, 0.05473056063055992: 1, -1.1721446514129639: 1, 0.7324214577674866: 1, -0.9458178281784058: 1, 0.18603742122650146: 1, -0.31994664669036865: 1, -0.4477367699146271: 1, -1.1971783638000488: 1, -1.201563835144043: 1, -0.6645460724830627: 1, -1.1857632398605347: 1, -1.1948115825653076: 1, -1.1924408674240112: 1, -1.1862393617630005: 1, -1.1612766981124878: 1, -1.1819733381271362: 1, -1.1986582279205322: 1, -0.40274444222450256: 1, -0.6273359060287476: 1, -0.23552395403385162: 1, 1.6066374778747559: 1, 0.8136993050575256: 1, -0.726171612739563: 1, 0.6412140727043152: 1, 0.5601547360420227: 1, -0.17489729821681976: 1, -0.1808653175830841: 1, -1.201631784439087: 1, 1.7609484195709229: 1, 1.2443398237228394: 1, -0.44575440883636475: 1, 0.5944019556045532: 1, 1.6532078981399536: 1, 0.9053057432174683: 1, 0.4647965729236603: 1, -1.0877141952514648: 1, -0.6111128330230713: 1, 0.9798484444618225: 1, 0.6101611256599426: 1, 1.094826102256775: 1, -0.7029581665992737: 1, -0.4641222357749939: 1, -1.1912822723388672: 1, -1.2009553909301758: 1, -1.1821529865264893: 1, -1.1595861911773682: 1, -1.1066040992736816: 1, -1.0014375448226929: 1, -1.0521938800811768: 1, -1.1405699253082275: 1, -1.0740134716033936: 1, -0.957834780216217: 1, -1.170555830001831: 1, -0.7500604391098022: 1, -0.6185922622680664: 1, -1.2012261152267456: 1, -1.1855055093765259: 1, -1.2014809846878052: 1, -1.091170310974121: 1, -0.40758535265922546: 1, -1.198327660560608: 1, -1.076475739479065: 1, -1.1624016761779785: 1, -1.1539435386657715: 1, -1.18364417552948: 1, -1.171040415763855: 1, 0.6086026430130005: 1, -1.194710373878479: 1, -1.2014538049697876: 1, 0.19575154781341553: 1, 1.8886953592300415: 1, -1.1054575443267822: 1, -0.459964781999588: 1, 5.645717620849609: 1, -1.196357011795044: 1, -1.1365838050842285: 1, -1.201351284980774: 1, -1.1963162422180176: 1, -1.1676386594772339: 1, -0.6870049834251404: 1, -1.1970044374465942: 1, -1.0951330661773682: 1, 1.1997345685958862: 1, -0.26567548513412476: 1, 0.15262554585933685: 1, 0.021963730454444885: 1, 1.3780083656311035: 1, 0.5367603302001953: 1, -0.5888111591339111: 1, 0.8642333745956421: 1, 0.8655160665512085: 1, 1.3268651962280273: 1, 1.466652274131775: 1, 0.3591007590293884: 1, 1.5059622526168823: 1, 0.33931559324264526: 1, 1.244690179824829: 1, 0.8794386982917786: 1, 1.0407052040100098: 1, 0.29691436886787415: 1, 0.8458276391029358: 1, -0.22581757605075836: 1, 1.0225938558578491: 1, 0.31476086378097534: 1, 1.1870133876800537: 1, 1.2197668552398682: 1, 1.5651975870132446: 1, -0.11230547726154327: 1, 0.22853031754493713: 1, -0.22873257100582123: 1, -0.1031346470117569: 1, 0.22794750332832336: 1, 1.55558443069458: 1, 1.3110779523849487: 1, 0.753506600856781: 1, 0.35881760716438293: 1, -1.1865193843841553: 1, 1.5112932920455933: 1, 0.7778939008712769: 1, -0.1758507341146469: 1, 1.3359390497207642: 1, 1.533963680267334: 1, 0.9224774241447449: 1, 1.466456651687622: 1, -0.5517370104789734: 1, 0.9252344369888306: 1, 0.3153885304927826: 1, 0.34270647168159485: 1, -0.006573406048119068: 1, -1.1257261037826538: 1, -0.6758065819740295: 1, -0.6676098704338074: 1, -1.0628424882888794: 1, -1.2016332149505615: 1, 0.1322988122701645: 1, -1.2015478610992432: 1, -1.1849600076675415: 1, -0.4687884449958801: 1, -1.201636791229248: 1, -0.4132004976272583: 1, -1.201630711555481: 1, -1.2016191482543945: 1, 0.2464127391576767: 1, -1.0203382968902588: 1, 0.3015405535697937: 1, -0.7989376187324524: 1, -1.2016159296035767: 1, -0.5612114071846008: 1, -1.2015819549560547: 1, 0.07849415391683578: 1, -1.2016303539276123: 1, -0.9999420642852783: 1, -1.1100589036941528: 1, -1.201610803604126: 1, -1.201634168624878: 1, -1.2016377449035645: 1, -1.2015372514724731: 1, -1.2015061378479004: 1, -1.2016048431396484: 1, -1.2016196250915527: 1, -1.2016316652297974: 1, 0.15111742913722992: 1, -1.1779894828796387: 1, 0.5647678971290588: 1, -1.2003253698349: 1, -1.201079249382019: 1, 0.8498935103416443: 1, -0.6209532618522644: 1, 0.3338538706302643: 1, -0.09314227104187012: 1, 1.5401815176010132: 1, 0.7008569240570068: 1, 0.31707215309143066: 1, 0.2925977110862732: 1, -0.08143828064203262: 1, 0.29443448781967163: 1, 1.4056357145309448: 1, -1.1742432117462158: 1, 0.98076331615448: 1, 0.3661552369594574: 1, 1.914624810218811: 1, 0.881543755531311: 1, 1.5978120565414429: 1, -0.41700541973114014: 1, 1.5425565242767334: 1, 0.39437854290008545: 1, 0.1911333203315735: 1, -0.30856987833976746: 1, 1.069445013999939: 1, 0.8512904644012451: 1, -0.11862857639789581: 1, -0.4737872779369354: 1, -0.2417261004447937: 1, 1.3578754663467407: 1, -0.622269868850708: 1, 1.133412480354309: 1, 0.9983642101287842: 1, 0.8997297883033752: 1, 0.19180983304977417: 1, 1.5916389226913452: 1, 0.0863155722618103: 1, 1.3597513437271118: 1, 0.9064841866493225: 1, 1.0842758417129517: 1, 1.5040947198867798: 1, 0.9953116178512573: 1, 0.5000193119049072: 1, 1.4793431758880615: 1, 1.0762102603912354: 1, 0.6668532490730286: 1, 1.473071813583374: 1, 0.39806419610977173: 1, 0.6104775071144104: 1, -0.10372047126293182: 1, -0.6031805276870728: 1, 0.21012525260448456: 1, -0.31073465943336487: 1, 1.5044559240341187: 1, -0.44012218713760376: 1, 0.11391282081604004: 1, 0.14948004484176636: 1, 1.5503476858139038: 1, 0.7984791398048401: 1, -0.20442236959934235: 1, 0.20913533866405487: 1, 0.4515918791294098: 1, 0.05425111949443817: 1, 1.6200270652770996: 1, -0.11514480412006378: 1, 1.431997537612915: 1, 0.6983891129493713: 1, 0.6647680401802063: 1, 1.1246896982192993: 1, -0.10053509473800659: 1, 0.276736855506897: 1, 1.2591054439544678: 1, 1.0418422222137451: 1, 1.540098786354065: 1, 1.1571227312088013: 1, 1.4639532566070557: 1, 0.30982303619384766: 1, 0.04298178479075432: 1, 0.3602719008922577: 1, -0.3440307080745697: 1, 1.3900312185287476: 1, -0.5775644183158875: 1, 0.44933900237083435: 1, 1.3920340538024902: 1, 1.5357881784439087: 1, 0.2801852524280548: 1, 0.07517638802528381: 1, -0.10448039323091507: 1, -1.2016335725784302: 1, -1.201634407043457: 1, -1.1967262029647827: 1, 0.1316474825143814: 1, -0.1855221837759018: 1, -0.6826592683792114: 1, -0.29299479722976685: 1, 0.7244752645492554: 1, -0.1759326308965683: 1, -0.2098180055618286: 1, 0.02958042360842228: 1, -0.24153171479701996: 1, -1.1981785297393799: 1, -1.2016326189041138: 1, 0.7561059594154358: 1, 1.1279875040054321: 1, 0.042822714895009995: 1, -1.0583271980285645: 1, -0.1827705055475235: 1, -0.39703771471977234: 1, -0.35524412989616394: 1, -1.0349785089492798: 1, -1.1928194761276245: 1, 1.179612159729004: 1, -0.37873539328575134: 1, 0.9235488772392273: 1, 1.256977915763855: 1, -0.28629931807518005: 1, -0.24570585787296295: 1, -0.4453357756137848: 1, 1.3004077672958374: 1, 0.7288377285003662: 1, 0.5795131325721741: 1, 1.0863690376281738: 1, -1.2004797458648682: 1, -0.24846623837947845: 1, 0.840314507484436: 1, -0.16393840312957764: 1, 1.2736730575561523: 1, 0.7264453172683716: 1, -0.12660875916481018: 1, -0.22304323315620422: 1, 0.4542813301086426: 1, -0.4121406674385071: 1, 0.7884150743484497: 1, -0.3953478932380676: 1, 1.1819933652877808: 1, 1.2238743305206299: 1, -0.8852211833000183: 1, 0.017293494194746017: 1, 1.1265980005264282: 1, -0.809281051158905: 1, 0.21761490404605865: 1, -1.0983095169067383: 1, -0.5160067081451416: 1, 0.05186900869011879: 1, 0.09289468824863434: 1, 0.5615567564964294: 1, -0.27063482999801636: 1, -0.257107138633728: 1, -0.19042982161045074: 1, 0.7786849141120911: 1, 1.2882169485092163: 1, -0.1383906453847885: 1, -1.194933295249939: 1, -1.194725751876831: 1, -1.095828652381897: 1, 0.029728559777140617: 1, -0.23942992091178894: 1, -0.3641962707042694: 1, 0.004906347021460533: 1, 0.0796559751033783: 1, 1.1985303163528442: 1, 1.1650327444076538: 1, -0.12005668878555298: 1, -0.8494775891304016: 1, -1.1878859996795654: 1, 1.2513697147369385: 1, 0.31131237745285034: 1, -0.16533638536930084: 1, -0.6921688914299011: 1, 0.32448264956474304: 1, -0.17582924664020538: 1, -0.025104349479079247: 1, -0.35746997594833374: 1, 1.1922990083694458: 1, -0.8705235719680786: 1, 0.8834699392318726: 1, 1.018097996711731: 1, 1.4819786548614502: 1, -0.2904500663280487: 1, -0.040736664086580276: 1, 0.5694330334663391: 1, 0.8222996592521667: 1, 1.1995047330856323: 1, -0.04390183836221695: 1, 1.176633596420288: 1, 0.9712859988212585: 1, -1.0558182001113892: 1, 0.8965467214584351: 1, -0.11945565789937973: 1, 1.0769490003585815: 1, -0.463926762342453: 1, 0.3886134922504425: 1, -0.3390061855316162: 1, -1.193730115890503: 1, -0.2126571536064148: 1, 0.2079305797815323: 1, -0.8751402497291565: 1, -1.1586899757385254: 1, -1.2016255855560303: 1, 0.36895880103111267: 1, -0.14289136230945587: 1, -0.11716806888580322: 1, -1.1943039894104004: 1, 4.363720893859863: 1, -0.3404213488101959: 1, -1.201629877090454: 1, -1.201523780822754: 1, 0.0891273021697998: 1, -1.2016350030899048: 1, -1.201620101928711: 1, -1.201588749885559: 1, -1.2015702724456787: 1, -1.2016273736953735: 1, -1.2015695571899414: 1, -1.0591267347335815: 1, -1.1107620000839233: 1, 0.04932570457458496: 1, -0.09275590628385544: 1, 1.2749443054199219: 1, -0.1517976075410843: 1, 0.6872115731239319: 1, 0.4259852468967438: 1, -0.2664187550544739: 1, 0.8119110465049744: 1, -0.25068432092666626: 1, 1.0972230434417725: 1, 1.150696039199829: 1, -1.1879688501358032: 1, -0.2164342701435089: 1, 0.9840258359909058: 1, -0.1916339248418808: 1, 1.185150146484375: 1, 0.6013088822364807: 1, 0.8532567620277405: 1, -1.2016304731369019: 1, -0.06864805519580841: 1, -0.09905305504798889: 1, -1.1755220890045166: 1, 0.01943967677652836: 1, -1.0965174436569214: 1, 0.18457916378974915: 1, 0.8548043966293335: 1, -0.0580214224755764: 1, -0.4360010027885437: 1, -0.3879204988479614: 1, -1.1634924411773682: 1, -0.3469093143939972: 1, -0.6421114802360535: 1, 0.01664043217897415: 1, -0.0332360677421093: 1, -0.1427413374185562: 1, 0.8218473196029663: 1, 0.5997416377067566: 1, 0.5362502932548523: 1, 0.029840881004929543: 1, -0.6103007793426514: 1, -1.1905653476715088: 1, -1.2003904581069946: 1}
2024-07-15 15:01:36.399658: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory
2024-07-15 15:01:36.399702: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)
2024-07-15 15:01:36.399734: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (UFZ544049): /proc/driver/nvidia/version does not exist
2024-07-15 15:01:36.400186: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
INFO     Network type: REG
INFO     Model: "sequential"
INFO     _________________________________________________________________
INFO     Layer (type)                 Output Shape              Param #
INFO     =================================================================
INFO     dense (Dense)                (None, 1024)              2098176
INFO     _________________________________________________________________
INFO     dropout (Dropout)            (None, 1024)              0
INFO     _________________________________________________________________
INFO     dense_1 (Dense)              (None, 512)               524800
INFO     _________________________________________________________________
INFO     dropout_1 (Dropout)          (None, 512)               0
INFO     _________________________________________________________________
INFO     dense_2 (Dense)              (None, 256)               131328
INFO     _________________________________________________________________
INFO     dropout_2 (Dropout)          (None, 256)               0
INFO     _________________________________________________________________
INFO     dense_3 (Dense)              (None, 128)               32896
INFO     _________________________________________________________________
INFO     dropout_3 (Dropout)          (None, 128)               0
INFO     _________________________________________________________________
INFO     dense_4 (Dense)              (None, 1)                 129
INFO     =================================================================
INFO     Total params: 2,787,329
INFO     Trainable params: 2,787,329
INFO     Non-trainable params: 0
INFO     _________________________________________________________________
WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.
WARNING  `period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.
2024-07-15 15:01:37.362609: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0
2024-07-15 15:01:37.362750: I tensorflow/core/grappler/clusters/single_machine.cc:357] Starting new session
2024-07-15 15:01:37.364846: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:1137] Optimization results for grappler item: graph_to_optimize
  function_optimizer: function_optimizer did nothing. time = 0.005ms.
  function_optimizer: function_optimizer did nothing. time = 0.001ms.
WARNING:tensorflow:From /home/shanavas/miniforge3/envs/dfpl_env/lib/python3.9/site-packages/tensorflow/python/ops/math_ops.py:3777: tensor_shape_from_node_def_name (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.graph_util.tensor_shape_from_node_def_name`
WARNING  From /home/shanavas/miniforge3/envs/dfpl_env/lib/python3.9/site-packages/tensorflow/python/ops/math_ops.py:3777: tensor_shape_from_node_def_name (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.graph_util.tensor_shape_from_node_def_name`
2024-07-15 15:01:37.515351: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)
Epoch 1/2000
22/22 - 1s - loss: 5.3135 - val_loss: 5.2331
Epoch 2/2000
22/22 - 1s - loss: 5.1202 - val_loss: 5.0548
Epoch 3/2000
22/22 - 1s - loss: 4.9455 - val_loss: 4.8873
Epoch 4/2000
22/22 - 1s - loss: 4.7909 - val_loss: 4.7506
Epoch 5/2000
22/22 - 1s - loss: 4.6814 - val_loss: 4.6633
Epoch 6/2000
22/22 - 1s - loss: 4.6225 - val_loss: 4.6170
Epoch 7/2000
22/22 - 1s - loss: 4.5910 - val_loss: 4.5918
Epoch 8/2000
22/22 - 1s - loss: 4.5664 - val_loss: 4.5747
Epoch 9/2000
22/22 - 1s - loss: 4.5442 - val_loss: 4.5597
Epoch 10/2000
22/22 - 1s - loss: 4.5231 - val_loss: 4.5459
Epoch 00010: val_loss improved from inf to 4.54591, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 11/2000
22/22 - 1s - loss: 4.5042 - val_loss: 4.5324
Epoch 12/2000
22/22 - 1s - loss: 4.4865 - val_loss: 4.5195
Epoch 13/2000
22/22 - 1s - loss: 4.4715 - val_loss: 4.5075
Epoch 14/2000
22/22 - 1s - loss: 4.4542 - val_loss: 4.4965
Epoch 15/2000
22/22 - 1s - loss: 4.4396 - val_loss: 4.4853
Epoch 16/2000
22/22 - 1s - loss: 4.4249 - val_loss: 4.4741
Epoch 17/2000
22/22 - 1s - loss: 4.4115 - val_loss: 4.4632
Epoch 18/2000
22/22 - 1s - loss: 4.3962 - val_loss: 4.4526
Epoch 19/2000
22/22 - 1s - loss: 4.3797 - val_loss: 4.4420
Epoch 20/2000
22/22 - 1s - loss: 4.3658 - val_loss: 4.4321
Epoch 00020: val_loss improved from 4.54591 to 4.43210, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 21/2000
22/22 - 1s - loss: 4.3542 - val_loss: 4.4219
Epoch 22/2000
22/22 - 1s - loss: 4.3390 - val_loss: 4.4126
Epoch 23/2000
22/22 - 1s - loss: 4.3261 - val_loss: 4.4027
Epoch 24/2000
22/22 - 1s - loss: 4.3126 - val_loss: 4.3937
Epoch 25/2000
22/22 - 1s - loss: 4.2986 - val_loss: 4.3841
Epoch 26/2000
22/22 - 1s - loss: 4.2852 - val_loss: 4.3758
Epoch 27/2000
22/22 - 1s - loss: 4.2732 - val_loss: 4.3665
Epoch 28/2000
22/22 - 1s - loss: 4.2612 - val_loss: 4.3580
Epoch 29/2000
22/22 - 1s - loss: 4.2469 - val_loss: 4.3486
Epoch 30/2000
22/22 - 1s - loss: 4.2343 - val_loss: 4.3405
Epoch 00030: val_loss improved from 4.43210 to 4.34047, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 31/2000
22/22 - 1s - loss: 4.2240 - val_loss: 4.3322
Epoch 32/2000
22/22 - 1s - loss: 4.2099 - val_loss: 4.3230
Epoch 33/2000
22/22 - 1s - loss: 4.1956 - val_loss: 4.3158
Epoch 34/2000
22/22 - 1s - loss: 4.1841 - val_loss: 4.3063
Epoch 35/2000
22/22 - 1s - loss: 4.1716 - val_loss: 4.2997
Epoch 36/2000
22/22 - 1s - loss: 4.1583 - val_loss: 4.2905
Epoch 37/2000
22/22 - 1s - loss: 4.1481 - val_loss: 4.2829
Epoch 38/2000
22/22 - 1s - loss: 4.1360 - val_loss: 4.2737
Epoch 39/2000
22/22 - 1s - loss: 4.1228 - val_loss: 4.2656
Epoch 40/2000
22/22 - 1s - loss: 4.1124 - val_loss: 4.2583
Epoch 00040: val_loss improved from 4.34047 to 4.25825, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 41/2000
22/22 - 1s - loss: 4.1012 - val_loss: 4.2495
Epoch 42/2000
22/22 - 1s - loss: 4.0862 - val_loss: 4.2422
Epoch 43/2000
22/22 - 1s - loss: 4.0765 - val_loss: 4.2346
Epoch 44/2000
22/22 - 1s - loss: 4.0640 - val_loss: 4.2292
Epoch 45/2000
22/22 - 1s - loss: 4.0525 - val_loss: 4.2213
Epoch 46/2000
22/22 - 1s - loss: 4.0426 - val_loss: 4.2130
Epoch 47/2000
22/22 - 1s - loss: 4.0301 - val_loss: 4.2067
Epoch 48/2000
22/22 - 1s - loss: 4.0206 - val_loss: 4.1985
Epoch 49/2000
22/22 - 1s - loss: 4.0086 - val_loss: 4.1927
Epoch 50/2000
22/22 - 1s - loss: 3.9975 - val_loss: 4.1841
Epoch 00050: val_loss improved from 4.25825 to 4.18412, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 51/2000
22/22 - 1s - loss: 3.9898 - val_loss: 4.1788
Epoch 52/2000
22/22 - 1s - loss: 3.9768 - val_loss: 4.1698
Epoch 53/2000
22/22 - 1s - loss: 3.9640 - val_loss: 4.1642
Epoch 54/2000
22/22 - 1s - loss: 3.9553 - val_loss: 4.1567
Epoch 55/2000
22/22 - 1s - loss: 3.9467 - val_loss: 4.1504
Epoch 56/2000
22/22 - 1s - loss: 3.9347 - val_loss: 4.1436
Epoch 57/2000
22/22 - 1s - loss: 3.9257 - val_loss: 4.1370
Epoch 58/2000
22/22 - 1s - loss: 3.9153 - val_loss: 4.1308
Epoch 59/2000
22/22 - 1s - loss: 3.9040 - val_loss: 4.1265
Epoch 60/2000
22/22 - 1s - loss: 3.8982 - val_loss: 4.1207
Epoch 00060: val_loss improved from 4.18412 to 4.12066, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 61/2000
22/22 - 1s - loss: 3.8866 - val_loss: 4.1121
Epoch 62/2000
22/22 - 1s - loss: 3.8751 - val_loss: 4.1072
Epoch 63/2000
22/22 - 1s - loss: 3.8687 - val_loss: 4.1019
Epoch 64/2000
22/22 - 1s - loss: 3.8555 - val_loss: 4.0952
Epoch 65/2000
22/22 - 1s - loss: 3.8471 - val_loss: 4.0885
Epoch 66/2000
22/22 - 1s - loss: 3.8409 - val_loss: 4.0823
Epoch 67/2000
22/22 - 1s - loss: 3.8290 - val_loss: 4.0805
Epoch 68/2000
22/22 - 1s - loss: 3.8230 - val_loss: 4.0710
Epoch 69/2000
22/22 - 1s - loss: 3.8140 - val_loss: 4.0650
Epoch 70/2000
22/22 - 1s - loss: 3.8024 - val_loss: 4.0588
Epoch 00070: val_loss improved from 4.12066 to 4.05875, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 71/2000
22/22 - 1s - loss: 3.7953 - val_loss: 4.0529
Epoch 72/2000
22/22 - 1s - loss: 3.7856 - val_loss: 4.0473
Epoch 73/2000
22/22 - 1s - loss: 3.7783 - val_loss: 4.0432
Epoch 74/2000
22/22 - 1s - loss: 3.7700 - val_loss: 4.0381
Epoch 75/2000
22/22 - 1s - loss: 3.7624 - val_loss: 4.0313
Epoch 76/2000
22/22 - 1s - loss: 3.7533 - val_loss: 4.0272
Epoch 77/2000
22/22 - 1s - loss: 3.7431 - val_loss: 4.0214
Epoch 78/2000
22/22 - 1s - loss: 3.7371 - val_loss: 4.0139
Epoch 79/2000
22/22 - 1s - loss: 3.7272 - val_loss: 4.0102
Epoch 80/2000
22/22 - 1s - loss: 3.7203 - val_loss: 4.0036
Epoch 00080: val_loss improved from 4.05875 to 4.00355, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 81/2000
22/22 - 1s - loss: 3.7125 - val_loss: 3.9990
Epoch 82/2000
22/22 - 1s - loss: 3.7054 - val_loss: 3.9937
Epoch 83/2000
22/22 - 1s - loss: 3.6967 - val_loss: 3.9887
Epoch 84/2000
22/22 - 1s - loss: 3.6912 - val_loss: 3.9819
Epoch 85/2000
22/22 - 1s - loss: 3.6810 - val_loss: 3.9787
Epoch 86/2000
22/22 - 1s - loss: 3.6746 - val_loss: 3.9736
Epoch 87/2000
22/22 - 1s - loss: 3.6667 - val_loss: 3.9676
Epoch 88/2000
22/22 - 1s - loss: 3.6597 - val_loss: 3.9631
Epoch 89/2000
22/22 - 1s - loss: 3.6523 - val_loss: 3.9592
Epoch 90/2000
22/22 - 1s - loss: 3.6473 - val_loss: 3.9528
Epoch 00090: val_loss improved from 4.00355 to 3.95280, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 91/2000
22/22 - 1s - loss: 3.6386 - val_loss: 3.9462
Epoch 92/2000
22/22 - 1s - loss: 3.6329 - val_loss: 3.9407
Epoch 93/2000
22/22 - 1s - loss: 3.6264 - val_loss: 3.9358
Epoch 94/2000
22/22 - 1s - loss: 3.6198 - val_loss: 3.9313
Epoch 95/2000
22/22 - 1s - loss: 3.6076 - val_loss: 3.9263
Epoch 96/2000
22/22 - 1s - loss: 3.5998 - val_loss: 3.9213
Epoch 97/2000
22/22 - 1s - loss: 3.5976 - val_loss: 3.9164
Epoch 98/2000
22/22 - 1s - loss: 3.5910 - val_loss: 3.9137
Epoch 99/2000
22/22 - 1s - loss: 3.5870 - val_loss: 3.9079
Epoch 100/2000
22/22 - 1s - loss: 3.5782 - val_loss: 3.9030
Epoch 00100: val_loss improved from 3.95280 to 3.90302, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 101/2000
22/22 - 1s - loss: 3.5693 - val_loss: 3.8987
Epoch 102/2000
22/22 - 1s - loss: 3.5654 - val_loss: 3.8923
Epoch 103/2000
22/22 - 1s - loss: 3.5562 - val_loss: 3.8881
Epoch 104/2000
22/22 - 1s - loss: 3.5545 - val_loss: 3.8848
Epoch 105/2000
22/22 - 1s - loss: 3.5466 - val_loss: 3.8821
Epoch 106/2000
22/22 - 1s - loss: 3.5393 - val_loss: 3.8752
Epoch 107/2000
22/22 - 1s - loss: 3.5340 - val_loss: 3.8700
Epoch 108/2000
22/22 - 1s - loss: 3.5268 - val_loss: 3.8661
Epoch 109/2000
22/22 - 1s - loss: 3.5193 - val_loss: 3.8615
Epoch 110/2000
22/22 - 1s - loss: 3.5153 - val_loss: 3.8566
Epoch 00110: val_loss improved from 3.90302 to 3.85663, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 111/2000
22/22 - 1s - loss: 3.5076 - val_loss: 3.8516
Epoch 112/2000
22/22 - 1s - loss: 3.5030 - val_loss: 3.8491
Epoch 113/2000
22/22 - 1s - loss: 3.4963 - val_loss: 3.8439
Epoch 114/2000
22/22 - 1s - loss: 3.4917 - val_loss: 3.8400
Epoch 115/2000
22/22 - 1s - loss: 3.4840 - val_loss: 3.8341
Epoch 116/2000
22/22 - 1s - loss: 3.4774 - val_loss: 3.8294
Epoch 117/2000
22/22 - 1s - loss: 3.4736 - val_loss: 3.8276
Epoch 118/2000
22/22 - 1s - loss: 3.4684 - val_loss: 3.8221
Epoch 119/2000
22/22 - 1s - loss: 3.4632 - val_loss: 3.8177
Epoch 120/2000
22/22 - 1s - loss: 3.4543 - val_loss: 3.8134
Epoch 00120: val_loss improved from 3.85663 to 3.81339, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 121/2000
22/22 - 1s - loss: 3.4493 - val_loss: 3.8084
Epoch 122/2000
22/22 - 1s - loss: 3.4452 - val_loss: 3.8048
Epoch 123/2000
22/22 - 1s - loss: 3.4393 - val_loss: 3.8004
Epoch 124/2000
22/22 - 1s - loss: 3.4387 - val_loss: 3.7982
Epoch 125/2000
22/22 - 1s - loss: 3.4307 - val_loss: 3.7936
Epoch 126/2000
22/22 - 1s - loss: 3.4271 - val_loss: 3.7868
Epoch 127/2000
22/22 - 1s - loss: 3.4191 - val_loss: 3.7829
Epoch 128/2000
22/22 - 1s - loss: 3.4120 - val_loss: 3.7783
Epoch 129/2000
22/22 - 1s - loss: 3.4061 - val_loss: 3.7745
Epoch 130/2000
22/22 - 1s - loss: 3.4020 - val_loss: 3.7729
Epoch 00130: val_loss improved from 3.81339 to 3.77291, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 131/2000
22/22 - 1s - loss: 3.4003 - val_loss: 3.7664
Epoch 132/2000
22/22 - 1s - loss: 3.3935 - val_loss: 3.7614
Epoch 133/2000
22/22 - 1s - loss: 3.3892 - val_loss: 3.7563
Epoch 134/2000
22/22 - 1s - loss: 3.3800 - val_loss: 3.7552
Epoch 135/2000
22/22 - 1s - loss: 3.3768 - val_loss: 3.7492
Epoch 136/2000
22/22 - 1s - loss: 3.3688 - val_loss: 3.7461
Epoch 137/2000
22/22 - 1s - loss: 3.3680 - val_loss: 3.7413
Epoch 138/2000
22/22 - 1s - loss: 3.3616 - val_loss: 3.7372
Epoch 139/2000
22/22 - 1s - loss: 3.3588 - val_loss: 3.7340
Epoch 140/2000
22/22 - 1s - loss: 3.3471 - val_loss: 3.7303
Epoch 00140: val_loss improved from 3.77291 to 3.73028, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 141/2000
22/22 - 1s - loss: 3.3456 - val_loss: 3.7275
Epoch 142/2000
22/22 - 1s - loss: 3.3468 - val_loss: 3.7222
Epoch 143/2000
22/22 - 1s - loss: 3.3350 - val_loss: 3.7173
Epoch 144/2000
22/22 - 1s - loss: 3.3303 - val_loss: 3.7147
Epoch 145/2000
22/22 - 1s - loss: 3.3256 - val_loss: 3.7112
Epoch 146/2000
22/22 - 1s - loss: 3.3232 - val_loss: 3.7072
Epoch 147/2000
22/22 - 1s - loss: 3.3189 - val_loss: 3.7028
Epoch 148/2000
22/22 - 1s - loss: 3.3121 - val_loss: 3.6987
Epoch 149/2000
22/22 - 1s - loss: 3.3067 - val_loss: 3.6943
Epoch 150/2000
22/22 - 1s - loss: 3.3020 - val_loss: 3.6902
Epoch 00150: val_loss improved from 3.73028 to 3.69019, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 151/2000
22/22 - 1s - loss: 3.2978 - val_loss: 3.6855
Epoch 152/2000
22/22 - 1s - loss: 3.2940 - val_loss: 3.6828
Epoch 153/2000
22/22 - 1s - loss: 3.2927 - val_loss: 3.6790
Epoch 154/2000
22/22 - 1s - loss: 3.2829 - val_loss: 3.6781
Epoch 155/2000
22/22 - 1s - loss: 3.2811 - val_loss: 3.6731
Epoch 156/2000
22/22 - 1s - loss: 3.2765 - val_loss: 3.6683
Epoch 157/2000
22/22 - 1s - loss: 3.2690 - val_loss: 3.6651
Epoch 158/2000
22/22 - 1s - loss: 3.2649 - val_loss: 3.6612
Epoch 159/2000
22/22 - 1s - loss: 3.2590 - val_loss: 3.6610
Epoch 160/2000
22/22 - 1s - loss: 3.2578 - val_loss: 3.6546
Epoch 00160: val_loss improved from 3.69019 to 3.65459, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 161/2000
22/22 - 1s - loss: 3.2550 - val_loss: 3.6495
Epoch 162/2000
22/22 - 1s - loss: 3.2462 - val_loss: 3.6457
Epoch 163/2000
22/22 - 1s - loss: 3.2429 - val_loss: 3.6418
Epoch 164/2000
22/22 - 1s - loss: 3.2411 - val_loss: 3.6377
Epoch 165/2000
22/22 - 1s - loss: 3.2371 - val_loss: 3.6342
Epoch 166/2000
22/22 - 1s - loss: 3.2329 - val_loss: 3.6301
Epoch 167/2000
22/22 - 1s - loss: 3.2294 - val_loss: 3.6273
Epoch 168/2000
22/22 - 1s - loss: 3.2231 - val_loss: 3.6225
Epoch 169/2000
22/22 - 1s - loss: 3.2174 - val_loss: 3.6198
Epoch 170/2000
22/22 - 1s - loss: 3.2131 - val_loss: 3.6160
Epoch 00170: val_loss improved from 3.65459 to 3.61602, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 171/2000
22/22 - 1s - loss: 3.2134 - val_loss: 3.6114
Epoch 172/2000
22/22 - 1s - loss: 3.2065 - val_loss: 3.6094
Epoch 173/2000
22/22 - 1s - loss: 3.2015 - val_loss: 3.6090
Epoch 174/2000
22/22 - 1s - loss: 3.1992 - val_loss: 3.6048
Epoch 175/2000
22/22 - 1s - loss: 3.1958 - val_loss: 3.6007
Epoch 176/2000
22/22 - 1s - loss: 3.1863 - val_loss: 3.5970
Epoch 177/2000
22/22 - 1s - loss: 3.1858 - val_loss: 3.5968
Epoch 178/2000
22/22 - 1s - loss: 3.1790 - val_loss: 3.5893
Epoch 179/2000
22/22 - 1s - loss: 3.1784 - val_loss: 3.5868
Epoch 180/2000
22/22 - 1s - loss: 3.1731 - val_loss: 3.5861
Epoch 00180: val_loss improved from 3.61602 to 3.58611, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 181/2000
22/22 - 1s - loss: 3.1673 - val_loss: 3.5784
Epoch 182/2000
22/22 - 1s - loss: 3.1648 - val_loss: 3.5775
Epoch 183/2000
22/22 - 1s - loss: 3.1627 - val_loss: 3.5719
Epoch 184/2000
22/22 - 1s - loss: 3.1586 - val_loss: 3.5692
Epoch 185/2000
22/22 - 1s - loss: 3.1532 - val_loss: 3.5639
Epoch 186/2000
22/22 - 1s - loss: 3.1474 - val_loss: 3.5620
Epoch 187/2000
22/22 - 1s - loss: 3.1419 - val_loss: 3.5579
Epoch 188/2000
22/22 - 1s - loss: 3.1418 - val_loss: 3.5539
Epoch 189/2000
22/22 - 1s - loss: 3.1374 - val_loss: 3.5504
Epoch 190/2000
22/22 - 1s - loss: 3.1325 - val_loss: 3.5470
Epoch 00190: val_loss improved from 3.58611 to 3.54697, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 191/2000
22/22 - 1s - loss: 3.1280 - val_loss: 3.5448
Epoch 192/2000
22/22 - 1s - loss: 3.1252 - val_loss: 3.5407
Epoch 193/2000
22/22 - 1s - loss: 3.1229 - val_loss: 3.5362
Epoch 194/2000
22/22 - 1s - loss: 3.1192 - val_loss: 3.5333
Epoch 195/2000
22/22 - 1s - loss: 3.1143 - val_loss: 3.5324
Epoch 196/2000
22/22 - 1s - loss: 3.1117 - val_loss: 3.5276
Epoch 197/2000
22/22 - 1s - loss: 3.1080 - val_loss: 3.5254
Epoch 198/2000
22/22 - 1s - loss: 3.1047 - val_loss: 3.5214
Epoch 199/2000
22/22 - 1s - loss: 3.1025 - val_loss: 3.5171
Epoch 200/2000
22/22 - 1s - loss: 3.0924 - val_loss: 3.5144
Epoch 00200: val_loss improved from 3.54697 to 3.51437, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 201/2000
22/22 - 1s - loss: 3.0919 - val_loss: 3.5149
Epoch 202/2000
22/22 - 1s - loss: 3.0880 - val_loss: 3.5112
Epoch 203/2000
22/22 - 1s - loss: 3.0880 - val_loss: 3.5053
Epoch 204/2000
22/22 - 1s - loss: 3.0831 - val_loss: 3.5037
Epoch 205/2000
22/22 - 1s - loss: 3.0754 - val_loss: 3.4988
Epoch 206/2000
22/22 - 1s - loss: 3.0740 - val_loss: 3.4950
Epoch 207/2000
22/22 - 1s - loss: 3.0712 - val_loss: 3.4938
Epoch 208/2000
22/22 - 1s - loss: 3.0660 - val_loss: 3.4892
Epoch 209/2000
22/22 - 1s - loss: 3.0647 - val_loss: 3.4880
Epoch 210/2000
22/22 - 1s - loss: 3.0582 - val_loss: 3.4857
Epoch 00210: val_loss improved from 3.51437 to 3.48574, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 211/2000
22/22 - 1s - loss: 3.0548 - val_loss: 3.4827
Epoch 212/2000
22/22 - 1s - loss: 3.0489 - val_loss: 3.4786
Epoch 213/2000
22/22 - 1s - loss: 3.0500 - val_loss: 3.4749
Epoch 214/2000
22/22 - 1s - loss: 3.0450 - val_loss: 3.4715
Epoch 215/2000
22/22 - 1s - loss: 3.0444 - val_loss: 3.4690
Epoch 216/2000
22/22 - 1s - loss: 3.0394 - val_loss: 3.4692
Epoch 217/2000
22/22 - 1s - loss: 3.0392 - val_loss: 3.4621
Epoch 218/2000
22/22 - 1s - loss: 3.0316 - val_loss: 3.4574
Epoch 219/2000
22/22 - 1s - loss: 3.0280 - val_loss: 3.4566
Epoch 220/2000
22/22 - 1s - loss: 3.0218 - val_loss: 3.4523
Epoch 00220: val_loss improved from 3.48574 to 3.45227, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 221/2000
22/22 - 1s - loss: 3.0241 - val_loss: 3.4493
Epoch 222/2000
22/22 - 1s - loss: 3.0195 - val_loss: 3.4449
Epoch 223/2000
22/22 - 1s - loss: 3.0138 - val_loss: 3.4444
Epoch 224/2000
22/22 - 1s - loss: 3.0126 - val_loss: 3.4386
Epoch 225/2000
22/22 - 1s - loss: 3.0064 - val_loss: 3.4358
Epoch 226/2000
22/22 - 1s - loss: 3.0066 - val_loss: 3.4360
Epoch 227/2000
22/22 - 1s - loss: 3.0001 - val_loss: 3.4307
Epoch 228/2000
22/22 - 1s - loss: 2.9986 - val_loss: 3.4298
Epoch 229/2000
22/22 - 1s - loss: 2.9938 - val_loss: 3.4264
Epoch 230/2000
22/22 - 1s - loss: 2.9885 - val_loss: 3.4233
Epoch 00230: val_loss improved from 3.45227 to 3.42329, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 231/2000
22/22 - 1s - loss: 2.9856 - val_loss: 3.4187
Epoch 232/2000
22/22 - 1s - loss: 2.9818 - val_loss: 3.4163
Epoch 233/2000
22/22 - 1s - loss: 2.9815 - val_loss: 3.4157
Epoch 234/2000
22/22 - 1s - loss: 2.9790 - val_loss: 3.4121
Epoch 235/2000
22/22 - 1s - loss: 2.9755 - val_loss: 3.4107
Epoch 236/2000
22/22 - 1s - loss: 2.9719 - val_loss: 3.4083
Epoch 237/2000
22/22 - 1s - loss: 2.9687 - val_loss: 3.4038
Epoch 238/2000
22/22 - 1s - loss: 2.9644 - val_loss: 3.3996
Epoch 239/2000
22/22 - 1s - loss: 2.9633 - val_loss: 3.3981
Epoch 240/2000
22/22 - 1s - loss: 2.9574 - val_loss: 3.3941
Epoch 00240: val_loss improved from 3.42329 to 3.39409, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 241/2000
22/22 - 1s - loss: 2.9551 - val_loss: 3.3912
Epoch 242/2000
22/22 - 1s - loss: 2.9509 - val_loss: 3.3871
Epoch 243/2000
22/22 - 1s - loss: 2.9501 - val_loss: 3.3844
Epoch 244/2000
22/22 - 1s - loss: 2.9456 - val_loss: 3.3820
Epoch 245/2000
22/22 - 1s - loss: 2.9449 - val_loss: 3.3775
Epoch 246/2000
22/22 - 1s - loss: 2.9405 - val_loss: 3.3759
Epoch 247/2000
22/22 - 1s - loss: 2.9382 - val_loss: 3.3719
Epoch 248/2000
22/22 - 1s - loss: 2.9322 - val_loss: 3.3701
Epoch 249/2000
22/22 - 1s - loss: 2.9300 - val_loss: 3.3676
Epoch 250/2000
22/22 - 1s - loss: 2.9273 - val_loss: 3.3643
Epoch 00250: val_loss improved from 3.39409 to 3.36431, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 251/2000
22/22 - 1s - loss: 2.9249 - val_loss: 3.3619
Epoch 252/2000
22/22 - 1s - loss: 2.9218 - val_loss: 3.3592
Epoch 253/2000
22/22 - 1s - loss: 2.9165 - val_loss: 3.3550
Epoch 254/2000
22/22 - 1s - loss: 2.9103 - val_loss: 3.3510
Epoch 255/2000
22/22 - 1s - loss: 2.9145 - val_loss: 3.3505
Epoch 256/2000
22/22 - 1s - loss: 2.9106 - val_loss: 3.3481
Epoch 257/2000
22/22 - 1s - loss: 2.9060 - val_loss: 3.3450
Epoch 258/2000
22/22 - 1s - loss: 2.9021 - val_loss: 3.3450
Epoch 259/2000
22/22 - 1s - loss: 2.8977 - val_loss: 3.3403
Epoch 260/2000
22/22 - 1s - loss: 2.8982 - val_loss: 3.3373
Epoch 00260: val_loss improved from 3.36431 to 3.33729, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 261/2000
22/22 - 1s - loss: 2.8930 - val_loss: 3.3365
Epoch 262/2000
22/22 - 1s - loss: 2.8871 - val_loss: 3.3325
Epoch 263/2000
22/22 - 1s - loss: 2.8885 - val_loss: 3.3304
Epoch 264/2000
22/22 - 1s - loss: 2.8838 - val_loss: 3.3247
Epoch 265/2000
22/22 - 1s - loss: 2.8817 - val_loss: 3.3212
Epoch 266/2000
22/22 - 1s - loss: 2.8771 - val_loss: 3.3192
Epoch 267/2000
22/22 - 1s - loss: 2.8783 - val_loss: 3.3166
Epoch 268/2000
22/22 - 1s - loss: 2.8701 - val_loss: 3.3137
Epoch 269/2000
22/22 - 1s - loss: 2.8713 - val_loss: 3.3102
Epoch 270/2000
22/22 - 1s - loss: 2.8649 - val_loss: 3.3092
Epoch 00270: val_loss improved from 3.33729 to 3.30918, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 271/2000
22/22 - 1s - loss: 2.8626 - val_loss: 3.3056
Epoch 272/2000
22/22 - 1s - loss: 2.8583 - val_loss: 3.3025
Epoch 273/2000
22/22 - 1s - loss: 2.8579 - val_loss: 3.3000
Epoch 274/2000
22/22 - 1s - loss: 2.8518 - val_loss: 3.2973
Epoch 275/2000
22/22 - 1s - loss: 2.8501 - val_loss: 3.2955
Epoch 276/2000
22/22 - 1s - loss: 2.8469 - val_loss: 3.2941
Epoch 277/2000
22/22 - 1s - loss: 2.8447 - val_loss: 3.2906
Epoch 278/2000
22/22 - 1s - loss: 2.8426 - val_loss: 3.2887
Epoch 279/2000
22/22 - 1s - loss: 2.8430 - val_loss: 3.2873
Epoch 280/2000
22/22 - 1s - loss: 2.8368 - val_loss: 3.2821
Epoch 00280: val_loss improved from 3.30918 to 3.28212, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 281/2000
22/22 - 1s - loss: 2.8349 - val_loss: 3.2789
Epoch 282/2000
22/22 - 1s - loss: 2.8312 - val_loss: 3.2757
Epoch 283/2000
22/22 - 1s - loss: 2.8279 - val_loss: 3.2727
Epoch 284/2000
22/22 - 1s - loss: 2.8239 - val_loss: 3.2697
Epoch 285/2000
22/22 - 1s - loss: 2.8216 - val_loss: 3.2707
Epoch 286/2000
22/22 - 1s - loss: 2.8189 - val_loss: 3.2683
Epoch 287/2000
22/22 - 1s - loss: 2.8169 - val_loss: 3.2639
Epoch 288/2000
22/22 - 1s - loss: 2.8135 - val_loss: 3.2605
Epoch 289/2000
22/22 - 1s - loss: 2.8117 - val_loss: 3.2586
Epoch 290/2000
22/22 - 1s - loss: 2.8085 - val_loss: 3.2551
Epoch 00290: val_loss improved from 3.28212 to 3.25507, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 291/2000
22/22 - 1s - loss: 2.8053 - val_loss: 3.2544
Epoch 292/2000
22/22 - 1s - loss: 2.8025 - val_loss: 3.2512
Epoch 293/2000
22/22 - 1s - loss: 2.8010 - val_loss: 3.2501
Epoch 294/2000
22/22 - 1s - loss: 2.7963 - val_loss: 3.2476
Epoch 295/2000
22/22 - 1s - loss: 2.7939 - val_loss: 3.2459
Epoch 296/2000
22/22 - 1s - loss: 2.7894 - val_loss: 3.2429
Epoch 297/2000
22/22 - 1s - loss: 2.7878 - val_loss: 3.2394
Epoch 298/2000
22/22 - 1s - loss: 2.7886 - val_loss: 3.2369
Epoch 299/2000
22/22 - 1s - loss: 2.7815 - val_loss: 3.2352
Epoch 300/2000
22/22 - 1s - loss: 2.7796 - val_loss: 3.2298
Epoch 00300: val_loss improved from 3.25507 to 3.22975, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 301/2000
22/22 - 1s - loss: 2.7793 - val_loss: 3.2280
Epoch 302/2000
22/22 - 1s - loss: 2.7748 - val_loss: 3.2274
Epoch 303/2000
22/22 - 1s - loss: 2.7722 - val_loss: 3.2262
Epoch 304/2000
22/22 - 1s - loss: 2.7694 - val_loss: 3.2224
Epoch 305/2000
22/22 - 1s - loss: 2.7642 - val_loss: 3.2210
Epoch 306/2000
22/22 - 1s - loss: 2.7634 - val_loss: 3.2175
Epoch 307/2000
22/22 - 1s - loss: 2.7600 - val_loss: 3.2167
Epoch 308/2000
22/22 - 1s - loss: 2.7576 - val_loss: 3.2117
Epoch 309/2000
22/22 - 1s - loss: 2.7571 - val_loss: 3.2086
Epoch 310/2000
22/22 - 1s - loss: 2.7518 - val_loss: 3.2061
Epoch 00310: val_loss improved from 3.22975 to 3.20608, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 311/2000
22/22 - 1s - loss: 2.7523 - val_loss: 3.2030
Epoch 312/2000
22/22 - 1s - loss: 2.7489 - val_loss: 3.2018
Epoch 313/2000
22/22 - 1s - loss: 2.7441 - val_loss: 3.1989
Epoch 314/2000
22/22 - 1s - loss: 2.7409 - val_loss: 3.1957
Epoch 315/2000
22/22 - 1s - loss: 2.7394 - val_loss: 3.1920
Epoch 316/2000
22/22 - 1s - loss: 2.7365 - val_loss: 3.1905
Epoch 317/2000
22/22 - 1s - loss: 2.7310 - val_loss: 3.1883
Epoch 318/2000
22/22 - 1s - loss: 2.7319 - val_loss: 3.1862
Epoch 319/2000
22/22 - 1s - loss: 2.7278 - val_loss: 3.1830
Epoch 320/2000
22/22 - 1s - loss: 2.7236 - val_loss: 3.1811
Epoch 00320: val_loss improved from 3.20608 to 3.18108, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 321/2000
22/22 - 1s - loss: 2.7228 - val_loss: 3.1781
Epoch 322/2000
22/22 - 1s - loss: 2.7214 - val_loss: 3.1763
Epoch 323/2000
22/22 - 1s - loss: 2.7193 - val_loss: 3.1735
Epoch 324/2000
22/22 - 1s - loss: 2.7118 - val_loss: 3.1713
Epoch 325/2000
22/22 - 1s - loss: 2.7108 - val_loss: 3.1712
Epoch 326/2000
22/22 - 1s - loss: 2.7128 - val_loss: 3.1676
Epoch 327/2000
22/22 - 1s - loss: 2.7081 - val_loss: 3.1630
Epoch 328/2000
22/22 - 1s - loss: 2.7057 - val_loss: 3.1598
Epoch 329/2000
22/22 - 1s - loss: 2.7028 - val_loss: 3.1615
Epoch 330/2000
22/22 - 1s - loss: 2.6986 - val_loss: 3.1585
Epoch 00330: val_loss improved from 3.18108 to 3.15855, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 331/2000
22/22 - 1s - loss: 2.6953 - val_loss: 3.1553
Epoch 332/2000
22/22 - 1s - loss: 2.6955 - val_loss: 3.1534
Epoch 333/2000
22/22 - 1s - loss: 2.6915 - val_loss: 3.1509
Epoch 334/2000
22/22 - 1s - loss: 2.6913 - val_loss: 3.1480
Epoch 335/2000
22/22 - 1s - loss: 2.6856 - val_loss: 3.1471
Epoch 336/2000
22/22 - 1s - loss: 2.6841 - val_loss: 3.1451
Epoch 337/2000
22/22 - 1s - loss: 2.6830 - val_loss: 3.1414
Epoch 338/2000
22/22 - 1s - loss: 2.6758 - val_loss: 3.1383
Epoch 339/2000
22/22 - 1s - loss: 2.6764 - val_loss: 3.1385
Epoch 340/2000
22/22 - 1s - loss: 2.6740 - val_loss: 3.1355
Epoch 00340: val_loss improved from 3.15855 to 3.13548, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 341/2000
22/22 - 1s - loss: 2.6727 - val_loss: 3.1328
Epoch 342/2000
22/22 - 1s - loss: 2.6650 - val_loss: 3.1293
Epoch 343/2000
22/22 - 1s - loss: 2.6640 - val_loss: 3.1269
Epoch 344/2000
22/22 - 1s - loss: 2.6644 - val_loss: 3.1267
Epoch 345/2000
22/22 - 1s - loss: 2.6640 - val_loss: 3.1229
Epoch 346/2000
22/22 - 1s - loss: 2.6563 - val_loss: 3.1192
Epoch 347/2000
22/22 - 1s - loss: 2.6566 - val_loss: 3.1192
Epoch 348/2000
22/22 - 1s - loss: 2.6508 - val_loss: 3.1146
Epoch 349/2000
22/22 - 1s - loss: 2.6497 - val_loss: 3.1119
Epoch 350/2000
22/22 - 1s - loss: 2.6516 - val_loss: 3.1103
Epoch 00350: val_loss improved from 3.13548 to 3.11032, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 351/2000
22/22 - 1s - loss: 2.6455 - val_loss: 3.1086
Epoch 352/2000
22/22 - 1s - loss: 2.6430 - val_loss: 3.1081
Epoch 353/2000
22/22 - 1s - loss: 2.6433 - val_loss: 3.1042
Epoch 354/2000
22/22 - 1s - loss: 2.6377 - val_loss: 3.1013
Epoch 355/2000
22/22 - 1s - loss: 2.6357 - val_loss: 3.0984
Epoch 356/2000
22/22 - 1s - loss: 2.6324 - val_loss: 3.0963
Epoch 357/2000
22/22 - 1s - loss: 2.6310 - val_loss: 3.0930
Epoch 358/2000
22/22 - 1s - loss: 2.6318 - val_loss: 3.0906
Epoch 359/2000
22/22 - 1s - loss: 2.6251 - val_loss: 3.0874
Epoch 360/2000
22/22 - 1s - loss: 2.6260 - val_loss: 3.0864
Epoch 00360: val_loss improved from 3.11032 to 3.08636, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 361/2000
22/22 - 1s - loss: 2.6209 - val_loss: 3.0841
Epoch 362/2000
22/22 - 1s - loss: 2.6190 - val_loss: 3.0817
Epoch 363/2000
22/22 - 1s - loss: 2.6167 - val_loss: 3.0795
Epoch 364/2000
22/22 - 1s - loss: 2.6125 - val_loss: 3.0769
Epoch 365/2000
22/22 - 1s - loss: 2.6122 - val_loss: 3.0756
Epoch 366/2000
22/22 - 1s - loss: 2.6074 - val_loss: 3.0730
Epoch 367/2000
22/22 - 1s - loss: 2.6084 - val_loss: 3.0710
Epoch 368/2000
22/22 - 1s - loss: 2.6055 - val_loss: 3.0679
Epoch 369/2000
22/22 - 1s - loss: 2.6030 - val_loss: 3.0668
Epoch 370/2000
22/22 - 1s - loss: 2.5994 - val_loss: 3.0640
Epoch 00370: val_loss improved from 3.08636 to 3.06401, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 371/2000
22/22 - 1s - loss: 2.5960 - val_loss: 3.0603
Epoch 372/2000
22/22 - 1s - loss: 2.5938 - val_loss: 3.0607
Epoch 373/2000
22/22 - 1s - loss: 2.5919 - val_loss: 3.0562
Epoch 374/2000
22/22 - 1s - loss: 2.5921 - val_loss: 3.0542
Epoch 375/2000
22/22 - 1s - loss: 2.5855 - val_loss: 3.0520
Epoch 376/2000
22/22 - 1s - loss: 2.5842 - val_loss: 3.0523
Epoch 377/2000
22/22 - 1s - loss: 2.5819 - val_loss: 3.0499
Epoch 378/2000
22/22 - 1s - loss: 2.5791 - val_loss: 3.0460
Epoch 379/2000
22/22 - 1s - loss: 2.5768 - val_loss: 3.0446
Epoch 380/2000
22/22 - 1s - loss: 2.5740 - val_loss: 3.0428
Epoch 00380: val_loss improved from 3.06401 to 3.04279, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 381/2000
22/22 - 1s - loss: 2.5754 - val_loss: 3.0411
Epoch 382/2000
22/22 - 1s - loss: 2.5704 - val_loss: 3.0368
Epoch 383/2000
22/22 - 1s - loss: 2.5683 - val_loss: 3.0358
Epoch 384/2000
22/22 - 1s - loss: 2.5625 - val_loss: 3.0340
Epoch 385/2000
22/22 - 1s - loss: 2.5626 - val_loss: 3.0312
Epoch 386/2000
22/22 - 1s - loss: 2.5605 - val_loss: 3.0270
Epoch 387/2000
22/22 - 1s - loss: 2.5601 - val_loss: 3.0268
Epoch 388/2000
22/22 - 1s - loss: 2.5559 - val_loss: 3.0273
Epoch 389/2000
22/22 - 1s - loss: 2.5541 - val_loss: 3.0219
Epoch 390/2000
22/22 - 1s - loss: 2.5525 - val_loss: 3.0186
Epoch 00390: val_loss improved from 3.04279 to 3.01865, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 391/2000
22/22 - 1s - loss: 2.5535 - val_loss: 3.0172
Epoch 392/2000
22/22 - 1s - loss: 2.5480 - val_loss: 3.0160
Epoch 393/2000
22/22 - 1s - loss: 2.5440 - val_loss: 3.0123
Epoch 394/2000
22/22 - 1s - loss: 2.5429 - val_loss: 3.0104
Epoch 395/2000
22/22 - 1s - loss: 2.5400 - val_loss: 3.0092
Epoch 396/2000
22/22 - 1s - loss: 2.5363 - val_loss: 3.0078
Epoch 397/2000
22/22 - 1s - loss: 2.5370 - val_loss: 3.0048
Epoch 398/2000
22/22 - 1s - loss: 2.5316 - val_loss: 3.0026
Epoch 399/2000
22/22 - 1s - loss: 2.5308 - val_loss: 2.9994
Epoch 400/2000
22/22 - 1s - loss: 2.5255 - val_loss: 2.9970
Epoch 00400: val_loss improved from 3.01865 to 2.99697, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 401/2000
22/22 - 1s - loss: 2.5292 - val_loss: 2.9946
Epoch 402/2000
22/22 - 1s - loss: 2.5247 - val_loss: 2.9933
Epoch 403/2000
22/22 - 1s - loss: 2.5240 - val_loss: 2.9905
Epoch 404/2000
22/22 - 1s - loss: 2.5207 - val_loss: 2.9885
Epoch 405/2000
22/22 - 1s - loss: 2.5175 - val_loss: 2.9869
Epoch 406/2000
22/22 - 1s - loss: 2.5134 - val_loss: 2.9858
Epoch 407/2000
22/22 - 1s - loss: 2.5124 - val_loss: 2.9828
Epoch 408/2000
22/22 - 1s - loss: 2.5086 - val_loss: 2.9790
Epoch 409/2000
22/22 - 1s - loss: 2.5083 - val_loss: 2.9774
Epoch 410/2000
22/22 - 1s - loss: 2.5053 - val_loss: 2.9779
Epoch 00410: val_loss improved from 2.99697 to 2.97787, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 411/2000
22/22 - 1s - loss: 2.5024 - val_loss: 2.9748
Epoch 412/2000
22/22 - 1s - loss: 2.5025 - val_loss: 2.9715
Epoch 413/2000
22/22 - 1s - loss: 2.5006 - val_loss: 2.9689
Epoch 414/2000
22/22 - 1s - loss: 2.4991 - val_loss: 2.9661
Epoch 415/2000
22/22 - 1s - loss: 2.4975 - val_loss: 2.9650
Epoch 416/2000
22/22 - 1s - loss: 2.4933 - val_loss: 2.9629
Epoch 417/2000
22/22 - 1s - loss: 2.4901 - val_loss: 2.9614
Epoch 418/2000
22/22 - 1s - loss: 2.4877 - val_loss: 2.9595
Epoch 419/2000
22/22 - 1s - loss: 2.4847 - val_loss: 2.9575
Epoch 420/2000
22/22 - 1s - loss: 2.4821 - val_loss: 2.9565
Epoch 00420: val_loss improved from 2.97787 to 2.95646, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 421/2000
22/22 - 1s - loss: 2.4835 - val_loss: 2.9542
Epoch 422/2000
22/22 - 1s - loss: 2.4798 - val_loss: 2.9517
Epoch 423/2000
22/22 - 1s - loss: 2.4759 - val_loss: 2.9491
Epoch 424/2000
22/22 - 1s - loss: 2.4742 - val_loss: 2.9497
Epoch 425/2000
22/22 - 1s - loss: 2.4737 - val_loss: 2.9467
Epoch 426/2000
22/22 - 1s - loss: 2.4672 - val_loss: 2.9445
Epoch 427/2000
22/22 - 1s - loss: 2.4688 - val_loss: 2.9419
Epoch 428/2000
22/22 - 1s - loss: 2.4680 - val_loss: 2.9391
Epoch 429/2000
22/22 - 1s - loss: 2.4615 - val_loss: 2.9382
Epoch 430/2000
22/22 - 1s - loss: 2.4618 - val_loss: 2.9342
Epoch 00430: val_loss improved from 2.95646 to 2.93422, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 431/2000
22/22 - 1s - loss: 2.4597 - val_loss: 2.9326
Epoch 432/2000
22/22 - 1s - loss: 2.4558 - val_loss: 2.9310
Epoch 433/2000
22/22 - 1s - loss: 2.4526 - val_loss: 2.9286
Epoch 434/2000
22/22 - 1s - loss: 2.4516 - val_loss: 2.9252
Epoch 435/2000
22/22 - 1s - loss: 2.4510 - val_loss: 2.9240
Epoch 436/2000
22/22 - 1s - loss: 2.4468 - val_loss: 2.9211
Epoch 437/2000
22/22 - 1s - loss: 2.4449 - val_loss: 2.9194
Epoch 438/2000
22/22 - 1s - loss: 2.4430 - val_loss: 2.9165
Epoch 439/2000
22/22 - 1s - loss: 2.4434 - val_loss: 2.9159
Epoch 440/2000
22/22 - 1s - loss: 2.4392 - val_loss: 2.9133
Epoch 00440: val_loss improved from 2.93422 to 2.91335, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 441/2000
22/22 - 1s - loss: 2.4358 - val_loss: 2.9097
Epoch 442/2000
22/22 - 1s - loss: 2.4343 - val_loss: 2.9079
Epoch 443/2000
22/22 - 1s - loss: 2.4341 - val_loss: 2.9076
Epoch 444/2000
22/22 - 1s - loss: 2.4290 - val_loss: 2.9036
Epoch 445/2000
22/22 - 1s - loss: 2.4297 - val_loss: 2.9019
Epoch 446/2000
22/22 - 1s - loss: 2.4259 - val_loss: 2.9016
Epoch 447/2000
22/22 - 1s - loss: 2.4214 - val_loss: 2.8991
Epoch 448/2000
22/22 - 1s - loss: 2.4236 - val_loss: 2.8961
Epoch 449/2000
22/22 - 1s - loss: 2.4204 - val_loss: 2.8947
Epoch 450/2000
22/22 - 1s - loss: 2.4187 - val_loss: 2.8924
Epoch 00450: val_loss improved from 2.91335 to 2.89243, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 451/2000
22/22 - 1s - loss: 2.4148 - val_loss: 2.8914
Epoch 452/2000
22/22 - 1s - loss: 2.4159 - val_loss: 2.8891
Epoch 453/2000
22/22 - 1s - loss: 2.4117 - val_loss: 2.8865
Epoch 454/2000
22/22 - 1s - loss: 2.4090 - val_loss: 2.8845
Epoch 455/2000
22/22 - 1s - loss: 2.4065 - val_loss: 2.8811
Epoch 456/2000
22/22 - 1s - loss: 2.4050 - val_loss: 2.8815
Epoch 457/2000
22/22 - 1s - loss: 2.4034 - val_loss: 2.8775
Epoch 458/2000
22/22 - 1s - loss: 2.4017 - val_loss: 2.8765
Epoch 459/2000
22/22 - 1s - loss: 2.4000 - val_loss: 2.8756
Epoch 460/2000
22/22 - 1s - loss: 2.3949 - val_loss: 2.8724
Epoch 00460: val_loss improved from 2.89243 to 2.87245, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 461/2000
22/22 - 1s - loss: 2.3956 - val_loss: 2.8700
Epoch 462/2000
22/22 - 1s - loss: 2.3955 - val_loss: 2.8698
Epoch 463/2000
22/22 - 1s - loss: 2.3881 - val_loss: 2.8671
Epoch 464/2000
22/22 - 1s - loss: 2.3890 - val_loss: 2.8641
Epoch 465/2000
22/22 - 1s - loss: 2.3871 - val_loss: 2.8624
Epoch 466/2000
22/22 - 1s - loss: 2.3850 - val_loss: 2.8609
Epoch 467/2000
22/22 - 1s - loss: 2.3815 - val_loss: 2.8578
Epoch 468/2000
22/22 - 1s - loss: 2.3819 - val_loss: 2.8568
Epoch 469/2000
22/22 - 1s - loss: 2.3794 - val_loss: 2.8534
Epoch 470/2000
22/22 - 1s - loss: 2.3762 - val_loss: 2.8537
Epoch 00470: val_loss improved from 2.87245 to 2.85369, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 471/2000
22/22 - 1s - loss: 2.3718 - val_loss: 2.8495
Epoch 472/2000
22/22 - 1s - loss: 2.3718 - val_loss: 2.8468
Epoch 473/2000
22/22 - 1s - loss: 2.3707 - val_loss: 2.8466
Epoch 474/2000
22/22 - 1s - loss: 2.3681 - val_loss: 2.8466
Epoch 475/2000
22/22 - 1s - loss: 2.3686 - val_loss: 2.8489
Epoch 476/2000
22/22 - 1s - loss: 2.3667 - val_loss: 2.8433
Epoch 477/2000
22/22 - 1s - loss: 2.3626 - val_loss: 2.8391
Epoch 478/2000
22/22 - 1s - loss: 2.3592 - val_loss: 2.8363
Epoch 479/2000
22/22 - 1s - loss: 2.3552 - val_loss: 2.8335
Epoch 480/2000
22/22 - 1s - loss: 2.3557 - val_loss: 2.8315
Epoch 00480: val_loss improved from 2.85369 to 2.83154, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 481/2000
22/22 - 1s - loss: 2.3558 - val_loss: 2.8300
Epoch 482/2000
22/22 - 1s - loss: 2.3503 - val_loss: 2.8303
Epoch 483/2000
22/22 - 1s - loss: 2.3507 - val_loss: 2.8277
Epoch 484/2000
22/22 - 1s - loss: 2.3470 - val_loss: 2.8233
Epoch 485/2000
22/22 - 1s - loss: 2.3465 - val_loss: 2.8223
Epoch 486/2000
22/22 - 1s - loss: 2.3456 - val_loss: 2.8187
Epoch 487/2000
22/22 - 1s - loss: 2.3429 - val_loss: 2.8188
Epoch 488/2000
22/22 - 1s - loss: 2.3390 - val_loss: 2.8161
Epoch 489/2000
22/22 - 1s - loss: 2.3386 - val_loss: 2.8151
Epoch 490/2000
22/22 - 1s - loss: 2.3322 - val_loss: 2.8130
Epoch 00490: val_loss improved from 2.83154 to 2.81299, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 491/2000
22/22 - 1s - loss: 2.3352 - val_loss: 2.8122
Epoch 492/2000
22/22 - 1s - loss: 2.3331 - val_loss: 2.8102
Epoch 493/2000
22/22 - 1s - loss: 2.3300 - val_loss: 2.8068
Epoch 494/2000
22/22 - 1s - loss: 2.3290 - val_loss: 2.8050
Epoch 495/2000
22/22 - 1s - loss: 2.3264 - val_loss: 2.8033
Epoch 496/2000
22/22 - 1s - loss: 2.3234 - val_loss: 2.8008
Epoch 497/2000
22/22 - 1s - loss: 2.3227 - val_loss: 2.7987
Epoch 498/2000
22/22 - 1s - loss: 2.3204 - val_loss: 2.7968
Epoch 499/2000
22/22 - 1s - loss: 2.3180 - val_loss: 2.7956
Epoch 500/2000
22/22 - 1s - loss: 2.3150 - val_loss: 2.7933
Epoch 00500: val_loss improved from 2.81299 to 2.79331, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 501/2000
22/22 - 1s - loss: 2.3127 - val_loss: 2.7917
Epoch 502/2000
22/22 - 1s - loss: 2.3104 - val_loss: 2.7905
Epoch 503/2000
22/22 - 1s - loss: 2.3080 - val_loss: 2.7890
Epoch 504/2000
22/22 - 1s - loss: 2.3095 - val_loss: 2.7886
Epoch 505/2000
22/22 - 1s - loss: 2.3055 - val_loss: 2.7872
Epoch 506/2000
22/22 - 1s - loss: 2.3049 - val_loss: 2.7827
Epoch 507/2000
22/22 - 1s - loss: 2.3028 - val_loss: 2.7791
Epoch 508/2000
22/22 - 1s - loss: 2.3000 - val_loss: 2.7773
Epoch 509/2000
22/22 - 1s - loss: 2.3001 - val_loss: 2.7777
Epoch 510/2000
22/22 - 1s - loss: 2.2963 - val_loss: 2.7739
Epoch 00510: val_loss improved from 2.79331 to 2.77393, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 511/2000
22/22 - 1s - loss: 2.2953 - val_loss: 2.7729
Epoch 512/2000
22/22 - 1s - loss: 2.2937 - val_loss: 2.7694
Epoch 513/2000
22/22 - 1s - loss: 2.2903 - val_loss: 2.7668
Epoch 514/2000
22/22 - 1s - loss: 2.2887 - val_loss: 2.7660
Epoch 515/2000
22/22 - 1s - loss: 2.2850 - val_loss: 2.7651
Epoch 516/2000
22/22 - 1s - loss: 2.2833 - val_loss: 2.7639
Epoch 517/2000
22/22 - 1s - loss: 2.2818 - val_loss: 2.7609
Epoch 518/2000
22/22 - 1s - loss: 2.2797 - val_loss: 2.7602
Epoch 519/2000
22/22 - 1s - loss: 2.2810 - val_loss: 2.7589
Epoch 520/2000
22/22 - 1s - loss: 2.2755 - val_loss: 2.7570
Epoch 00520: val_loss improved from 2.77393 to 2.75699, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 521/2000
22/22 - 1s - loss: 2.2747 - val_loss: 2.7542
Epoch 522/2000
22/22 - 1s - loss: 2.2724 - val_loss: 2.7516
Epoch 523/2000
22/22 - 1s - loss: 2.2731 - val_loss: 2.7496
Epoch 524/2000
22/22 - 1s - loss: 2.2670 - val_loss: 2.7505
Epoch 525/2000
22/22 - 1s - loss: 2.2665 - val_loss: 2.7479
Epoch 526/2000
22/22 - 1s - loss: 2.2654 - val_loss: 2.7465
Epoch 527/2000
22/22 - 1s - loss: 2.2632 - val_loss: 2.7435
Epoch 528/2000
22/22 - 1s - loss: 2.2626 - val_loss: 2.7405
Epoch 529/2000
22/22 - 1s - loss: 2.2592 - val_loss: 2.7394
Epoch 530/2000
22/22 - 1s - loss: 2.2566 - val_loss: 2.7379
Epoch 00530: val_loss improved from 2.75699 to 2.73789, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 531/2000
22/22 - 1s - loss: 2.2547 - val_loss: 2.7354
Epoch 532/2000
22/22 - 1s - loss: 2.2502 - val_loss: 2.7327
Epoch 533/2000
22/22 - 1s - loss: 2.2504 - val_loss: 2.7318
Epoch 534/2000
22/22 - 1s - loss: 2.2487 - val_loss: 2.7297
Epoch 535/2000
22/22 - 1s - loss: 2.2472 - val_loss: 2.7275
Epoch 536/2000
22/22 - 1s - loss: 2.2445 - val_loss: 2.7256
Epoch 537/2000
22/22 - 1s - loss: 2.2420 - val_loss: 2.7245
Epoch 538/2000
22/22 - 1s - loss: 2.2425 - val_loss: 2.7229
Epoch 539/2000
22/22 - 1s - loss: 2.2430 - val_loss: 2.7212
Epoch 540/2000
22/22 - 1s - loss: 2.2403 - val_loss: 2.7186
Epoch 00540: val_loss improved from 2.73789 to 2.71858, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 541/2000
22/22 - 1s - loss: 2.2361 - val_loss: 2.7162
Epoch 542/2000
22/22 - 1s - loss: 2.2353 - val_loss: 2.7145
Epoch 543/2000
22/22 - 1s - loss: 2.2332 - val_loss: 2.7131
Epoch 544/2000
22/22 - 1s - loss: 2.2296 - val_loss: 2.7122
Epoch 545/2000
22/22 - 1s - loss: 2.2316 - val_loss: 2.7101
Epoch 546/2000
22/22 - 1s - loss: 2.2255 - val_loss: 2.7083
Epoch 547/2000
22/22 - 1s - loss: 2.2237 - val_loss: 2.7070
Epoch 548/2000
22/22 - 1s - loss: 2.2236 - val_loss: 2.7066
Epoch 549/2000
22/22 - 1s - loss: 2.2212 - val_loss: 2.7050
Epoch 550/2000
22/22 - 1s - loss: 2.2191 - val_loss: 2.7030
Epoch 00550: val_loss improved from 2.71858 to 2.70305, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 551/2000
22/22 - 1s - loss: 2.2176 - val_loss: 2.7000
Epoch 552/2000
22/22 - 1s - loss: 2.2161 - val_loss: 2.6965
Epoch 553/2000
22/22 - 1s - loss: 2.2147 - val_loss: 2.6951
Epoch 554/2000
22/22 - 1s - loss: 2.2130 - val_loss: 2.6962
Epoch 555/2000
22/22 - 1s - loss: 2.2087 - val_loss: 2.6954
Epoch 556/2000
22/22 - 1s - loss: 2.2103 - val_loss: 2.6942
Epoch 557/2000
22/22 - 1s - loss: 2.2082 - val_loss: 2.6898
Epoch 558/2000
22/22 - 1s - loss: 2.2059 - val_loss: 2.6888
Epoch 559/2000
22/22 - 1s - loss: 2.2045 - val_loss: 2.6850
Epoch 560/2000
22/22 - 1s - loss: 2.2009 - val_loss: 2.6824
Epoch 00560: val_loss improved from 2.70305 to 2.68237, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 561/2000
22/22 - 1s - loss: 2.2010 - val_loss: 2.6824
Epoch 562/2000
22/22 - 1s - loss: 2.1960 - val_loss: 2.6817
Epoch 563/2000
22/22 - 1s - loss: 2.1971 - val_loss: 2.6806
Epoch 564/2000
22/22 - 1s - loss: 2.1954 - val_loss: 2.6753
Epoch 565/2000
22/22 - 1s - loss: 2.1924 - val_loss: 2.6738
Epoch 566/2000
22/22 - 1s - loss: 2.1902 - val_loss: 2.6751
Epoch 567/2000
22/22 - 1s - loss: 2.1881 - val_loss: 2.6724
Epoch 568/2000
22/22 - 1s - loss: 2.1866 - val_loss: 2.6702
Epoch 569/2000
22/22 - 1s - loss: 2.1854 - val_loss: 2.6697
Epoch 570/2000
22/22 - 1s - loss: 2.1833 - val_loss: 2.6684
Epoch 00570: val_loss improved from 2.68237 to 2.66843, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 571/2000
22/22 - 1s - loss: 2.1820 - val_loss: 2.6667
Epoch 572/2000
22/22 - 1s - loss: 2.1816 - val_loss: 2.6621
Epoch 573/2000
22/22 - 1s - loss: 2.1788 - val_loss: 2.6603
Epoch 574/2000
22/22 - 1s - loss: 2.1746 - val_loss: 2.6578
Epoch 575/2000
22/22 - 1s - loss: 2.1761 - val_loss: 2.6578
Epoch 576/2000
22/22 - 1s - loss: 2.1746 - val_loss: 2.6568
Epoch 577/2000
22/22 - 1s - loss: 2.1723 - val_loss: 2.6535
Epoch 578/2000
22/22 - 1s - loss: 2.1689 - val_loss: 2.6525
Epoch 579/2000
22/22 - 1s - loss: 2.1676 - val_loss: 2.6514
Epoch 580/2000
22/22 - 1s - loss: 2.1655 - val_loss: 2.6511
Epoch 00580: val_loss improved from 2.66843 to 2.65113, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 581/2000
22/22 - 1s - loss: 2.1649 - val_loss: 2.6477
Epoch 582/2000
22/22 - 1s - loss: 2.1625 - val_loss: 2.6453
Epoch 583/2000
22/22 - 1s - loss: 2.1599 - val_loss: 2.6429
Epoch 584/2000
22/22 - 1s - loss: 2.1590 - val_loss: 2.6422
Epoch 585/2000
22/22 - 1s - loss: 2.1531 - val_loss: 2.6399
Epoch 586/2000
22/22 - 1s - loss: 2.1552 - val_loss: 2.6410
Epoch 587/2000
22/22 - 1s - loss: 2.1524 - val_loss: 2.6367
Epoch 588/2000
22/22 - 1s - loss: 2.1523 - val_loss: 2.6351
Epoch 589/2000
22/22 - 1s - loss: 2.1502 - val_loss: 2.6349
Epoch 590/2000
22/22 - 1s - loss: 2.1478 - val_loss: 2.6318
Epoch 00590: val_loss improved from 2.65113 to 2.63183, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 591/2000
22/22 - 1s - loss: 2.1461 - val_loss: 2.6305
Epoch 592/2000
22/22 - 1s - loss: 2.1443 - val_loss: 2.6287
Epoch 593/2000
22/22 - 1s - loss: 2.1434 - val_loss: 2.6291
Epoch 594/2000
22/22 - 1s - loss: 2.1417 - val_loss: 2.6256
Epoch 595/2000
22/22 - 1s - loss: 2.1390 - val_loss: 2.6241
Epoch 596/2000
22/22 - 1s - loss: 2.1380 - val_loss: 2.6239
Epoch 597/2000
22/22 - 1s - loss: 2.1374 - val_loss: 2.6215
Epoch 598/2000
22/22 - 1s - loss: 2.1352 - val_loss: 2.6195
Epoch 599/2000
22/22 - 1s - loss: 2.1318 - val_loss: 2.6169
Epoch 600/2000
22/22 - 1s - loss: 2.1278 - val_loss: 2.6143
Epoch 00600: val_loss improved from 2.63183 to 2.61430, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 601/2000
22/22 - 1s - loss: 2.1278 - val_loss: 2.6130
Epoch 602/2000
22/22 - 1s - loss: 2.1270 - val_loss: 2.6122
Epoch 603/2000
22/22 - 1s - loss: 2.1256 - val_loss: 2.6094
Epoch 604/2000
22/22 - 1s - loss: 2.1246 - val_loss: 2.6074
Epoch 605/2000
22/22 - 1s - loss: 2.1234 - val_loss: 2.6058
Epoch 606/2000
22/22 - 1s - loss: 2.1191 - val_loss: 2.6041
Epoch 607/2000
22/22 - 1s - loss: 2.1187 - val_loss: 2.6019
Epoch 608/2000
22/22 - 1s - loss: 2.1174 - val_loss: 2.6008
Epoch 609/2000
22/22 - 1s - loss: 2.1124 - val_loss: 2.5999
Epoch 610/2000
22/22 - 1s - loss: 2.1130 - val_loss: 2.5988
Epoch 00610: val_loss improved from 2.61430 to 2.59876, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 611/2000
22/22 - 1s - loss: 2.1140 - val_loss: 2.5984
Epoch 612/2000
22/22 - 1s - loss: 2.1090 - val_loss: 2.5961
Epoch 613/2000
22/22 - 1s - loss: 2.1070 - val_loss: 2.5929
Epoch 614/2000
22/22 - 1s - loss: 2.1050 - val_loss: 2.5922
Epoch 615/2000
22/22 - 1s - loss: 2.1032 - val_loss: 2.5898
Epoch 616/2000
22/22 - 1s - loss: 2.1038 - val_loss: 2.5878
Epoch 617/2000
22/22 - 1s - loss: 2.0986 - val_loss: 2.5865
Epoch 618/2000
22/22 - 1s - loss: 2.1022 - val_loss: 2.5837
Epoch 619/2000
22/22 - 1s - loss: 2.0979 - val_loss: 2.5832
Epoch 620/2000
22/22 - 1s - loss: 2.0974 - val_loss: 2.5807
Epoch 00620: val_loss improved from 2.59876 to 2.58070, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 621/2000
22/22 - 1s - loss: 2.0970 - val_loss: 2.5804
Epoch 622/2000
22/22 - 1s - loss: 2.0938 - val_loss: 2.5801
Epoch 623/2000
22/22 - 1s - loss: 2.0928 - val_loss: 2.5783
Epoch 624/2000
22/22 - 1s - loss: 2.0869 - val_loss: 2.5752
Epoch 625/2000
22/22 - 1s - loss: 2.0891 - val_loss: 2.5731
Epoch 626/2000
22/22 - 1s - loss: 2.0847 - val_loss: 2.5700
Epoch 627/2000
22/22 - 1s - loss: 2.0852 - val_loss: 2.5689
Epoch 628/2000
22/22 - 1s - loss: 2.0818 - val_loss: 2.5691
Epoch 629/2000
22/22 - 1s - loss: 2.0794 - val_loss: 2.5697
Epoch 630/2000
22/22 - 1s - loss: 2.0795 - val_loss: 2.5669
Epoch 00630: val_loss improved from 2.58070 to 2.56693, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 631/2000
22/22 - 1s - loss: 2.0793 - val_loss: 2.5672
Epoch 632/2000
22/22 - 1s - loss: 2.0738 - val_loss: 2.5636
Epoch 633/2000
22/22 - 1s - loss: 2.0752 - val_loss: 2.5600
Epoch 634/2000
22/22 - 1s - loss: 2.0694 - val_loss: 2.5599
Epoch 635/2000
22/22 - 1s - loss: 2.0682 - val_loss: 2.5572
Epoch 636/2000
22/22 - 1s - loss: 2.0687 - val_loss: 2.5536
Epoch 637/2000
22/22 - 1s - loss: 2.0677 - val_loss: 2.5530
Epoch 638/2000
22/22 - 1s - loss: 2.0640 - val_loss: 2.5515
Epoch 639/2000
22/22 - 1s - loss: 2.0639 - val_loss: 2.5504
Epoch 640/2000
22/22 - 1s - loss: 2.0614 - val_loss: 2.5485
Epoch 00640: val_loss improved from 2.56693 to 2.54852, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 641/2000
22/22 - 1s - loss: 2.0612 - val_loss: 2.5481
Epoch 642/2000
22/22 - 1s - loss: 2.0574 - val_loss: 2.5450
Epoch 643/2000
22/22 - 1s - loss: 2.0582 - val_loss: 2.5469
Epoch 644/2000
22/22 - 1s - loss: 2.0609 - val_loss: 2.5431
Epoch 645/2000
22/22 - 1s - loss: 2.0572 - val_loss: 2.5431
Epoch 646/2000
22/22 - 1s - loss: 2.0540 - val_loss: 2.5429
Epoch 647/2000
22/22 - 1s - loss: 2.0516 - val_loss: 2.5399
Epoch 648/2000
22/22 - 1s - loss: 2.0507 - val_loss: 2.5385
Epoch 649/2000
22/22 - 1s - loss: 2.0485 - val_loss: 2.5368
Epoch 650/2000
22/22 - 1s - loss: 2.0474 - val_loss: 2.5355
Epoch 00650: val_loss improved from 2.54852 to 2.53547, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 651/2000
22/22 - 1s - loss: 2.0459 - val_loss: 2.5342
Epoch 652/2000
22/22 - 1s - loss: 2.0424 - val_loss: 2.5336
Epoch 653/2000
22/22 - 1s - loss: 2.0405 - val_loss: 2.5291
Epoch 654/2000
22/22 - 1s - loss: 2.0405 - val_loss: 2.5282
Epoch 655/2000
22/22 - 1s - loss: 2.0399 - val_loss: 2.5247
Epoch 656/2000
22/22 - 1s - loss: 2.0336 - val_loss: 2.5226
Epoch 657/2000
22/22 - 1s - loss: 2.0360 - val_loss: 2.5230
Epoch 658/2000
22/22 - 1s - loss: 2.0331 - val_loss: 2.5218
Epoch 659/2000
22/22 - 1s - loss: 2.0329 - val_loss: 2.5179
Epoch 660/2000
22/22 - 1s - loss: 2.0316 - val_loss: 2.5162
Epoch 00660: val_loss improved from 2.53547 to 2.51622, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 661/2000
22/22 - 1s - loss: 2.0294 - val_loss: 2.5164
Epoch 662/2000
22/22 - 1s - loss: 2.0235 - val_loss: 2.5152
Epoch 663/2000
22/22 - 1s - loss: 2.0251 - val_loss: 2.5130
Epoch 664/2000
22/22 - 1s - loss: 2.0263 - val_loss: 2.5109
Epoch 665/2000
22/22 - 1s - loss: 2.0237 - val_loss: 2.5090
Epoch 666/2000
22/22 - 1s - loss: 2.0224 - val_loss: 2.5088
Epoch 667/2000
22/22 - 1s - loss: 2.0185 - val_loss: 2.5077
Epoch 668/2000
22/22 - 1s - loss: 2.0170 - val_loss: 2.5070
Epoch 669/2000
22/22 - 1s - loss: 2.0164 - val_loss: 2.5047
Epoch 670/2000
22/22 - 1s - loss: 2.0137 - val_loss: 2.5024
Epoch 00670: val_loss improved from 2.51622 to 2.50236, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 671/2000
22/22 - 1s - loss: 2.0126 - val_loss: 2.5008
Epoch 672/2000
22/22 - 1s - loss: 2.0105 - val_loss: 2.4986
Epoch 673/2000
22/22 - 1s - loss: 2.0099 - val_loss: 2.4995
Epoch 674/2000
22/22 - 1s - loss: 2.0047 - val_loss: 2.4981
Epoch 675/2000
22/22 - 1s - loss: 2.0070 - val_loss: 2.4952
Epoch 676/2000
22/22 - 1s - loss: 2.0068 - val_loss: 2.4937
Epoch 677/2000
22/22 - 1s - loss: 2.0015 - val_loss: 2.4913
Epoch 678/2000
22/22 - 1s - loss: 2.0020 - val_loss: 2.4885
Epoch 679/2000
22/22 - 1s - loss: 2.0012 - val_loss: 2.4895
Epoch 680/2000
22/22 - 1s - loss: 1.9981 - val_loss: 2.4872
Epoch 00680: val_loss improved from 2.50236 to 2.48718, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 681/2000
22/22 - 1s - loss: 1.9977 - val_loss: 2.4858
Epoch 682/2000
22/22 - 1s - loss: 1.9932 - val_loss: 2.4874
Epoch 683/2000
22/22 - 1s - loss: 1.9935 - val_loss: 2.4834
Epoch 684/2000
22/22 - 1s - loss: 1.9915 - val_loss: 2.4803
Epoch 685/2000
22/22 - 1s - loss: 1.9904 - val_loss: 2.4788
Epoch 686/2000
22/22 - 1s - loss: 1.9895 - val_loss: 2.4772
Epoch 687/2000
22/22 - 1s - loss: 1.9868 - val_loss: 2.4770
Epoch 688/2000
22/22 - 1s - loss: 1.9843 - val_loss: 2.4758
Epoch 689/2000
22/22 - 1s - loss: 1.9854 - val_loss: 2.4751
Epoch 690/2000
22/22 - 1s - loss: 1.9859 - val_loss: 2.4734
Epoch 00690: val_loss improved from 2.48718 to 2.47339, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 691/2000
22/22 - 1s - loss: 1.9815 - val_loss: 2.4719
Epoch 692/2000
22/22 - 1s - loss: 1.9819 - val_loss: 2.4713
Epoch 693/2000
22/22 - 1s - loss: 1.9771 - val_loss: 2.4677
Epoch 694/2000
22/22 - 1s - loss: 1.9741 - val_loss: 2.4665
Epoch 695/2000
22/22 - 1s - loss: 1.9751 - val_loss: 2.4641
Epoch 696/2000
22/22 - 1s - loss: 1.9749 - val_loss: 2.4619
Epoch 697/2000
22/22 - 1s - loss: 1.9734 - val_loss: 2.4619
Epoch 698/2000
22/22 - 1s - loss: 1.9717 - val_loss: 2.4595
Epoch 699/2000
22/22 - 1s - loss: 1.9675 - val_loss: 2.4603
Epoch 700/2000
22/22 - 1s - loss: 1.9681 - val_loss: 2.4592
Epoch 00700: val_loss improved from 2.47339 to 2.45923, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 701/2000
22/22 - 1s - loss: 1.9667 - val_loss: 2.4546
Epoch 702/2000
22/22 - 1s - loss: 1.9648 - val_loss: 2.4517
Epoch 703/2000
22/22 - 1s - loss: 1.9622 - val_loss: 2.4515
Epoch 704/2000
22/22 - 1s - loss: 1.9638 - val_loss: 2.4504
Epoch 705/2000
22/22 - 1s - loss: 1.9611 - val_loss: 2.4489
Epoch 706/2000
22/22 - 1s - loss: 1.9574 - val_loss: 2.4465
Epoch 707/2000
22/22 - 1s - loss: 1.9584 - val_loss: 2.4468
Epoch 708/2000
22/22 - 1s - loss: 1.9563 - val_loss: 2.4481
Epoch 709/2000
22/22 - 1s - loss: 1.9538 - val_loss: 2.4443
Epoch 710/2000
22/22 - 1s - loss: 1.9517 - val_loss: 2.4425
Epoch 00710: val_loss improved from 2.45923 to 2.44251, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 711/2000
22/22 - 1s - loss: 1.9493 - val_loss: 2.4410
Epoch 712/2000
22/22 - 1s - loss: 1.9477 - val_loss: 2.4402
Epoch 713/2000
22/22 - 1s - loss: 1.9483 - val_loss: 2.4392
Epoch 714/2000
22/22 - 1s - loss: 1.9484 - val_loss: 2.4369
Epoch 715/2000
22/22 - 1s - loss: 1.9426 - val_loss: 2.4355
Epoch 716/2000
22/22 - 1s - loss: 1.9414 - val_loss: 2.4348
Epoch 717/2000
22/22 - 1s - loss: 1.9418 - val_loss: 2.4314
Epoch 718/2000
22/22 - 1s - loss: 1.9382 - val_loss: 2.4303
Epoch 719/2000
22/22 - 1s - loss: 1.9387 - val_loss: 2.4279
Epoch 720/2000
22/22 - 1s - loss: 1.9379 - val_loss: 2.4265
Epoch 00720: val_loss improved from 2.44251 to 2.42648, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 721/2000
22/22 - 1s - loss: 1.9345 - val_loss: 2.4259
Epoch 722/2000
22/22 - 1s - loss: 1.9329 - val_loss: 2.4231
Epoch 723/2000
22/22 - 1s - loss: 1.9315 - val_loss: 2.4229
Epoch 724/2000
22/22 - 1s - loss: 1.9315 - val_loss: 2.4228
Epoch 725/2000
22/22 - 1s - loss: 1.9296 - val_loss: 2.4227
Epoch 726/2000
22/22 - 1s - loss: 1.9259 - val_loss: 2.4189
Epoch 727/2000
22/22 - 1s - loss: 1.9286 - val_loss: 2.4167
Epoch 728/2000
22/22 - 1s - loss: 1.9249 - val_loss: 2.4177
Epoch 729/2000
22/22 - 1s - loss: 1.9241 - val_loss: 2.4164
Epoch 730/2000
22/22 - 1s - loss: 1.9234 - val_loss: 2.4141
Epoch 00730: val_loss improved from 2.42648 to 2.41414, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 731/2000
22/22 - 1s - loss: 1.9224 - val_loss: 2.4132
Epoch 732/2000
22/22 - 1s - loss: 1.9201 - val_loss: 2.4093
Epoch 733/2000
22/22 - 1s - loss: 1.9163 - val_loss: 2.4086
Epoch 734/2000
22/22 - 1s - loss: 1.9153 - val_loss: 2.4076
Epoch 735/2000
22/22 - 1s - loss: 1.9150 - val_loss: 2.4064
Epoch 736/2000
22/22 - 1s - loss: 1.9154 - val_loss: 2.4051
Epoch 737/2000
22/22 - 1s - loss: 1.9130 - val_loss: 2.4035
Epoch 738/2000
22/22 - 1s - loss: 1.9104 - val_loss: 2.4028
Epoch 739/2000
22/22 - 1s - loss: 1.9060 - val_loss: 2.3994
Epoch 740/2000
22/22 - 1s - loss: 1.9083 - val_loss: 2.3984
Epoch 00740: val_loss improved from 2.41414 to 2.39836, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 741/2000
22/22 - 1s - loss: 1.9068 - val_loss: 2.3976
Epoch 742/2000
22/22 - 1s - loss: 1.9050 - val_loss: 2.3965
Epoch 743/2000
22/22 - 1s - loss: 1.9042 - val_loss: 2.3966
Epoch 744/2000
22/22 - 1s - loss: 1.9008 - val_loss: 2.3972
Epoch 745/2000
22/22 - 1s - loss: 1.9001 - val_loss: 2.3938
Epoch 746/2000
22/22 - 1s - loss: 1.9007 - val_loss: 2.3930
Epoch 747/2000
22/22 - 1s - loss: 1.8979 - val_loss: 2.3904
Epoch 748/2000
22/22 - 1s - loss: 1.8959 - val_loss: 2.3884
Epoch 749/2000
22/22 - 1s - loss: 1.8927 - val_loss: 2.3859
Epoch 750/2000
22/22 - 1s - loss: 1.8938 - val_loss: 2.3854
Epoch 00750: val_loss improved from 2.39836 to 2.38539, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 751/2000
22/22 - 1s - loss: 1.8902 - val_loss: 2.3842
Epoch 752/2000
22/22 - 1s - loss: 1.8890 - val_loss: 2.3811
Epoch 753/2000
22/22 - 1s - loss: 1.8903 - val_loss: 2.3799
Epoch 754/2000
22/22 - 1s - loss: 1.8889 - val_loss: 2.3789
Epoch 755/2000
22/22 - 1s - loss: 1.8854 - val_loss: 2.3794
Epoch 756/2000
22/22 - 1s - loss: 1.8848 - val_loss: 2.3784
Epoch 757/2000
22/22 - 1s - loss: 1.8829 - val_loss: 2.3777
Epoch 758/2000
22/22 - 1s - loss: 1.8810 - val_loss: 2.3746
Epoch 759/2000
22/22 - 1s - loss: 1.8795 - val_loss: 2.3713
Epoch 760/2000
22/22 - 1s - loss: 1.8774 - val_loss: 2.3703
Epoch 00760: val_loss improved from 2.38539 to 2.37029, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 761/2000
22/22 - 1s - loss: 1.8773 - val_loss: 2.3700
Epoch 762/2000
22/22 - 1s - loss: 1.8765 - val_loss: 2.3670
Epoch 763/2000
22/22 - 1s - loss: 1.8757 - val_loss: 2.3647
Epoch 764/2000
22/22 - 1s - loss: 1.8745 - val_loss: 2.3635
Epoch 765/2000
22/22 - 1s - loss: 1.8726 - val_loss: 2.3628
Epoch 766/2000
22/22 - 1s - loss: 1.8708 - val_loss: 2.3625
Epoch 767/2000
22/22 - 1s - loss: 1.8696 - val_loss: 2.3644
Epoch 768/2000
22/22 - 1s - loss: 1.8689 - val_loss: 2.3632
Epoch 769/2000
22/22 - 1s - loss: 1.8686 - val_loss: 2.3603
Epoch 770/2000
22/22 - 1s - loss: 1.8661 - val_loss: 2.3586
Epoch 00770: val_loss improved from 2.37029 to 2.35861, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 771/2000
22/22 - 1s - loss: 1.8625 - val_loss: 2.3570
Epoch 772/2000
22/22 - 1s - loss: 1.8611 - val_loss: 2.3539
Epoch 773/2000
22/22 - 1s - loss: 1.8605 - val_loss: 2.3526
Epoch 774/2000
22/22 - 1s - loss: 1.8583 - val_loss: 2.3507
Epoch 775/2000
22/22 - 1s - loss: 1.8589 - val_loss: 2.3499
Epoch 776/2000
22/22 - 1s - loss: 1.8570 - val_loss: 2.3495
Epoch 777/2000
22/22 - 1s - loss: 1.8538 - val_loss: 2.3498
Epoch 778/2000
22/22 - 1s - loss: 1.8539 - val_loss: 2.3467
Epoch 779/2000
22/22 - 1s - loss: 1.8522 - val_loss: 2.3450
Epoch 780/2000
22/22 - 1s - loss: 1.8543 - val_loss: 2.3441
Epoch 00780: val_loss improved from 2.35861 to 2.34408, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 781/2000
22/22 - 1s - loss: 1.8509 - val_loss: 2.3422
Epoch 782/2000
22/22 - 1s - loss: 1.8477 - val_loss: 2.3406
Epoch 783/2000
22/22 - 1s - loss: 1.8455 - val_loss: 2.3402
Epoch 784/2000
22/22 - 1s - loss: 1.8459 - val_loss: 2.3393
Epoch 785/2000
22/22 - 1s - loss: 1.8432 - val_loss: 2.3361
Epoch 786/2000
22/22 - 1s - loss: 1.8421 - val_loss: 2.3348
Epoch 787/2000
22/22 - 1s - loss: 1.8418 - val_loss: 2.3334
Epoch 788/2000
22/22 - 1s - loss: 1.8405 - val_loss: 2.3334
Epoch 789/2000
22/22 - 1s - loss: 1.8364 - val_loss: 2.3316
Epoch 790/2000
22/22 - 1s - loss: 1.8379 - val_loss: 2.3300
Epoch 00790: val_loss improved from 2.34408 to 2.33000, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 791/2000
22/22 - 1s - loss: 1.8359 - val_loss: 2.3272
Epoch 792/2000
22/22 - 1s - loss: 1.8361 - val_loss: 2.3253
Epoch 793/2000
22/22 - 1s - loss: 1.8303 - val_loss: 2.3251
Epoch 794/2000
22/22 - 1s - loss: 1.8317 - val_loss: 2.3252
Epoch 795/2000
22/22 - 1s - loss: 1.8281 - val_loss: 2.3222
Epoch 796/2000
22/22 - 1s - loss: 1.8310 - val_loss: 2.3213
Epoch 797/2000
22/22 - 1s - loss: 1.8258 - val_loss: 2.3193
Epoch 798/2000
22/22 - 1s - loss: 1.8255 - val_loss: 2.3174
Epoch 799/2000
22/22 - 1s - loss: 1.8245 - val_loss: 2.3164
Epoch 800/2000
22/22 - 1s - loss: 1.8235 - val_loss: 2.3156
Epoch 00800: val_loss improved from 2.33000 to 2.31557, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 801/2000
22/22 - 1s - loss: 1.8226 - val_loss: 2.3143
Epoch 802/2000
22/22 - 1s - loss: 1.8216 - val_loss: 2.3138
Epoch 803/2000
22/22 - 1s - loss: 1.8185 - val_loss: 2.3130
Epoch 804/2000
22/22 - 1s - loss: 1.8179 - val_loss: 2.3093
Epoch 805/2000
22/22 - 1s - loss: 1.8169 - val_loss: 2.3085
Epoch 806/2000
22/22 - 1s - loss: 1.8153 - val_loss: 2.3079
Epoch 807/2000
22/22 - 1s - loss: 1.8168 - val_loss: 2.3086
Epoch 808/2000
22/22 - 1s - loss: 1.8099 - val_loss: 2.3071
Epoch 809/2000
22/22 - 1s - loss: 1.8099 - val_loss: 2.3041
Epoch 810/2000
22/22 - 1s - loss: 1.8078 - val_loss: 2.3035
Epoch 00810: val_loss improved from 2.31557 to 2.30352, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 811/2000
22/22 - 1s - loss: 1.8089 - val_loss: 2.3043
Epoch 812/2000
22/22 - 1s - loss: 1.8083 - val_loss: 2.3006
Epoch 813/2000
22/22 - 1s - loss: 1.8053 - val_loss: 2.2987
Epoch 814/2000
22/22 - 1s - loss: 1.8031 - val_loss: 2.2967
Epoch 815/2000
22/22 - 1s - loss: 1.8021 - val_loss: 2.2965
Epoch 816/2000
22/22 - 1s - loss: 1.8019 - val_loss: 2.2968
Epoch 817/2000
22/22 - 1s - loss: 1.8002 - val_loss: 2.2936
Epoch 818/2000
22/22 - 1s - loss: 1.7985 - val_loss: 2.2907
Epoch 819/2000
22/22 - 1s - loss: 1.7959 - val_loss: 2.2920
Epoch 820/2000
22/22 - 1s - loss: 1.7955 - val_loss: 2.2888
Epoch 00820: val_loss improved from 2.30352 to 2.28884, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 821/2000
22/22 - 1s - loss: 1.7954 - val_loss: 2.2866
Epoch 822/2000
22/22 - 1s - loss: 1.7936 - val_loss: 2.2860
Epoch 823/2000
22/22 - 1s - loss: 1.7919 - val_loss: 2.2855
Epoch 824/2000
22/22 - 1s - loss: 1.7920 - val_loss: 2.2853
Epoch 825/2000
22/22 - 1s - loss: 1.7890 - val_loss: 2.2842
Epoch 826/2000
22/22 - 1s - loss: 1.7899 - val_loss: 2.2815
Epoch 827/2000
22/22 - 1s - loss: 1.7878 - val_loss: 2.2821
Epoch 828/2000
22/22 - 1s - loss: 1.7859 - val_loss: 2.2802
Epoch 829/2000
22/22 - 1s - loss: 1.7837 - val_loss: 2.2784
Epoch 830/2000
22/22 - 1s - loss: 1.7839 - val_loss: 2.2754
Epoch 00830: val_loss improved from 2.28884 to 2.27542, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 831/2000
22/22 - 1s - loss: 1.7822 - val_loss: 2.2752
Epoch 832/2000
22/22 - 1s - loss: 1.7809 - val_loss: 2.2745
Epoch 833/2000
22/22 - 1s - loss: 1.7812 - val_loss: 2.2741
Epoch 834/2000
22/22 - 1s - loss: 1.7772 - val_loss: 2.2708
Epoch 835/2000
22/22 - 1s - loss: 1.7772 - val_loss: 2.2700
Epoch 836/2000
22/22 - 1s - loss: 1.7752 - val_loss: 2.2683
Epoch 837/2000
22/22 - 1s - loss: 1.7738 - val_loss: 2.2663
Epoch 838/2000
22/22 - 1s - loss: 1.7718 - val_loss: 2.2652
Epoch 839/2000
22/22 - 1s - loss: 1.7709 - val_loss: 2.2655
Epoch 840/2000
22/22 - 1s - loss: 1.7702 - val_loss: 2.2627
Epoch 00840: val_loss improved from 2.27542 to 2.26272, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 841/2000
22/22 - 1s - loss: 1.7695 - val_loss: 2.2623
Epoch 842/2000
22/22 - 1s - loss: 1.7657 - val_loss: 2.2615
Epoch 843/2000
22/22 - 1s - loss: 1.7675 - val_loss: 2.2598
Epoch 844/2000
22/22 - 1s - loss: 1.7650 - val_loss: 2.2589
Epoch 845/2000
22/22 - 1s - loss: 1.7628 - val_loss: 2.2567
Epoch 846/2000
22/22 - 1s - loss: 1.7617 - val_loss: 2.2545
Epoch 847/2000
22/22 - 1s - loss: 1.7600 - val_loss: 2.2528
Epoch 848/2000
22/22 - 1s - loss: 1.7597 - val_loss: 2.2531
Epoch 849/2000
22/22 - 1s - loss: 1.7588 - val_loss: 2.2518
Epoch 850/2000
22/22 - 1s - loss: 1.7572 - val_loss: 2.2523
Epoch 00850: val_loss improved from 2.26272 to 2.25227, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 851/2000
22/22 - 1s - loss: 1.7556 - val_loss: 2.2503
Epoch 852/2000
22/22 - 1s - loss: 1.7526 - val_loss: 2.2485
Epoch 853/2000
22/22 - 1s - loss: 1.7540 - val_loss: 2.2479
Epoch 854/2000
22/22 - 1s - loss: 1.7523 - val_loss: 2.2470
Epoch 855/2000
22/22 - 1s - loss: 1.7493 - val_loss: 2.2456
Epoch 856/2000
22/22 - 1s - loss: 1.7503 - val_loss: 2.2442
Epoch 857/2000
22/22 - 1s - loss: 1.7490 - val_loss: 2.2426
Epoch 858/2000
22/22 - 1s - loss: 1.7478 - val_loss: 2.2395
Epoch 859/2000
22/22 - 1s - loss: 1.7441 - val_loss: 2.2394
Epoch 860/2000
22/22 - 1s - loss: 1.7435 - val_loss: 2.2387
Epoch 00860: val_loss improved from 2.25227 to 2.23870, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 861/2000
22/22 - 1s - loss: 1.7432 - val_loss: 2.2386
Epoch 862/2000
22/22 - 1s - loss: 1.7425 - val_loss: 2.2344
Epoch 863/2000
22/22 - 1s - loss: 1.7395 - val_loss: 2.2323
Epoch 864/2000
22/22 - 1s - loss: 1.7390 - val_loss: 2.2332
Epoch 865/2000
22/22 - 1s - loss: 1.7392 - val_loss: 2.2326
Epoch 866/2000
22/22 - 1s - loss: 1.7377 - val_loss: 2.2308
Epoch 867/2000
22/22 - 1s - loss: 1.7347 - val_loss: 2.2299
Epoch 868/2000
22/22 - 1s - loss: 1.7338 - val_loss: 2.2286
Epoch 869/2000
22/22 - 1s - loss: 1.7306 - val_loss: 2.2282
Epoch 870/2000
22/22 - 1s - loss: 1.7318 - val_loss: 2.2256
Epoch 00870: val_loss improved from 2.23870 to 2.22556, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 871/2000
22/22 - 1s - loss: 1.7307 - val_loss: 2.2251
Epoch 872/2000
22/22 - 1s - loss: 1.7305 - val_loss: 2.2230
Epoch 873/2000
22/22 - 1s - loss: 1.7278 - val_loss: 2.2223
Epoch 874/2000
22/22 - 1s - loss: 1.7239 - val_loss: 2.2208
Epoch 875/2000
22/22 - 1s - loss: 1.7239 - val_loss: 2.2193
Epoch 876/2000
22/22 - 1s - loss: 1.7236 - val_loss: 2.2184
Epoch 877/2000
22/22 - 1s - loss: 1.7228 - val_loss: 2.2167
Epoch 878/2000
22/22 - 1s - loss: 1.7216 - val_loss: 2.2152
Epoch 879/2000
22/22 - 1s - loss: 1.7221 - val_loss: 2.2154
Epoch 880/2000
22/22 - 1s - loss: 1.7213 - val_loss: 2.2123
Epoch 00880: val_loss improved from 2.22556 to 2.21231, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 881/2000
22/22 - 1s - loss: 1.7150 - val_loss: 2.2121
Epoch 882/2000
22/22 - 1s - loss: 1.7162 - val_loss: 2.2109
Epoch 883/2000
22/22 - 1s - loss: 1.7165 - val_loss: 2.2095
Epoch 884/2000
22/22 - 1s - loss: 1.7157 - val_loss: 2.2106
Epoch 885/2000
22/22 - 1s - loss: 1.7128 - val_loss: 2.2085
Epoch 886/2000
22/22 - 1s - loss: 1.7123 - val_loss: 2.2067
Epoch 887/2000
22/22 - 1s - loss: 1.7098 - val_loss: 2.2056
Epoch 888/2000
22/22 - 1s - loss: 1.7080 - val_loss: 2.2029
Epoch 889/2000
22/22 - 1s - loss: 1.7108 - val_loss: 2.2014
Epoch 890/2000
22/22 - 1s - loss: 1.7059 - val_loss: 2.2024
Epoch 00890: val_loss improved from 2.21231 to 2.20236, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 891/2000
22/22 - 1s - loss: 1.7075 - val_loss: 2.2012
Epoch 892/2000
22/22 - 1s - loss: 1.7040 - val_loss: 2.1991
Epoch 893/2000
22/22 - 1s - loss: 1.7044 - val_loss: 2.1984
Epoch 894/2000
22/22 - 1s - loss: 1.7021 - val_loss: 2.1972
Epoch 895/2000
22/22 - 1s - loss: 1.7006 - val_loss: 2.1973
Epoch 896/2000
22/22 - 1s - loss: 1.6989 - val_loss: 2.1950
Epoch 897/2000
22/22 - 1s - loss: 1.6992 - val_loss: 2.1934
Epoch 898/2000
22/22 - 1s - loss: 1.6956 - val_loss: 2.1917
Epoch 899/2000
22/22 - 1s - loss: 1.6945 - val_loss: 2.1903
Epoch 900/2000
22/22 - 1s - loss: 1.6944 - val_loss: 2.1881
Epoch 00900: val_loss improved from 2.20236 to 2.18806, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 901/2000
22/22 - 1s - loss: 1.6944 - val_loss: 2.1888
Epoch 902/2000
22/22 - 1s - loss: 1.6931 - val_loss: 2.1855
Epoch 903/2000
22/22 - 1s - loss: 1.6881 - val_loss: 2.1845
Epoch 904/2000
22/22 - 1s - loss: 1.6891 - val_loss: 2.1840
Epoch 905/2000
22/22 - 1s - loss: 1.6903 - val_loss: 2.1810
Epoch 906/2000
22/22 - 1s - loss: 1.6856 - val_loss: 2.1834
Epoch 907/2000
22/22 - 1s - loss: 1.6855 - val_loss: 2.1828
Epoch 908/2000
22/22 - 1s - loss: 1.6843 - val_loss: 2.1784
Epoch 909/2000
22/22 - 1s - loss: 1.6844 - val_loss: 2.1778
Epoch 910/2000
22/22 - 1s - loss: 1.6826 - val_loss: 2.1769
Epoch 00910: val_loss improved from 2.18806 to 2.17689, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 911/2000
22/22 - 1s - loss: 1.6806 - val_loss: 2.1763
Epoch 912/2000
22/22 - 1s - loss: 1.6799 - val_loss: 2.1745
Epoch 913/2000
22/22 - 1s - loss: 1.6799 - val_loss: 2.1734
Epoch 914/2000
22/22 - 1s - loss: 1.6768 - val_loss: 2.1727
Epoch 915/2000
22/22 - 1s - loss: 1.6787 - val_loss: 2.1728
Epoch 916/2000
22/22 - 1s - loss: 1.6745 - val_loss: 2.1709
Epoch 917/2000
22/22 - 1s - loss: 1.6742 - val_loss: 2.1688
Epoch 918/2000
22/22 - 1s - loss: 1.6732 - val_loss: 2.1686
Epoch 919/2000
22/22 - 1s - loss: 1.6714 - val_loss: 2.1675
Epoch 920/2000
22/22 - 1s - loss: 1.6700 - val_loss: 2.1662
Epoch 00920: val_loss improved from 2.17689 to 2.16622, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 921/2000
22/22 - 1s - loss: 1.6703 - val_loss: 2.1657
Epoch 922/2000
22/22 - 1s - loss: 1.6667 - val_loss: 2.1633
Epoch 923/2000
22/22 - 1s - loss: 1.6690 - val_loss: 2.1615
Epoch 924/2000
22/22 - 1s - loss: 1.6653 - val_loss: 2.1608
Epoch 925/2000
22/22 - 1s - loss: 1.6653 - val_loss: 2.1589
Epoch 926/2000
22/22 - 1s - loss: 1.6629 - val_loss: 2.1581
Epoch 927/2000
22/22 - 1s - loss: 1.6607 - val_loss: 2.1556
Epoch 928/2000
22/22 - 1s - loss: 1.6615 - val_loss: 2.1567
Epoch 929/2000
22/22 - 1s - loss: 1.6594 - val_loss: 2.1564
Epoch 930/2000
22/22 - 1s - loss: 1.6586 - val_loss: 2.1546
Epoch 00930: val_loss improved from 2.16622 to 2.15460, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 931/2000
22/22 - 1s - loss: 1.6590 - val_loss: 2.1520
Epoch 932/2000
22/22 - 1s - loss: 1.6566 - val_loss: 2.1499
Epoch 933/2000
22/22 - 1s - loss: 1.6547 - val_loss: 2.1496
Epoch 934/2000
22/22 - 1s - loss: 1.6526 - val_loss: 2.1502
Epoch 935/2000
22/22 - 1s - loss: 1.6519 - val_loss: 2.1493
Epoch 936/2000
22/22 - 1s - loss: 1.6491 - val_loss: 2.1493
Epoch 937/2000
22/22 - 1s - loss: 1.6484 - val_loss: 2.1485
Epoch 938/2000
22/22 - 1s - loss: 1.6465 - val_loss: 2.1460
Epoch 939/2000
22/22 - 1s - loss: 1.6487 - val_loss: 2.1434
Epoch 940/2000
22/22 - 1s - loss: 1.6455 - val_loss: 2.1419
Epoch 00940: val_loss improved from 2.15460 to 2.14193, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 941/2000
22/22 - 1s - loss: 1.6441 - val_loss: 2.1399
Epoch 942/2000
22/22 - 1s - loss: 1.6433 - val_loss: 2.1405
Epoch 943/2000
22/22 - 1s - loss: 1.6418 - val_loss: 2.1404
Epoch 944/2000
22/22 - 1s - loss: 1.6434 - val_loss: 2.1380
Epoch 945/2000
22/22 - 1s - loss: 1.6399 - val_loss: 2.1380
Epoch 946/2000
22/22 - 1s - loss: 1.6409 - val_loss: 2.1362
Epoch 947/2000
22/22 - 1s - loss: 1.6372 - val_loss: 2.1344
Epoch 948/2000
22/22 - 1s - loss: 1.6373 - val_loss: 2.1341
Epoch 949/2000
22/22 - 1s - loss: 1.6362 - val_loss: 2.1329
Epoch 950/2000
22/22 - 1s - loss: 1.6368 - val_loss: 2.1316
Epoch 00950: val_loss improved from 2.14193 to 2.13158, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 951/2000
22/22 - 1s - loss: 1.6333 - val_loss: 2.1300
Epoch 952/2000
22/22 - 1s - loss: 1.6328 - val_loss: 2.1281
Epoch 953/2000
22/22 - 1s - loss: 1.6323 - val_loss: 2.1291
Epoch 954/2000
22/22 - 1s - loss: 1.6316 - val_loss: 2.1267
Epoch 955/2000
22/22 - 1s - loss: 1.6284 - val_loss: 2.1268
Epoch 956/2000
22/22 - 1s - loss: 1.6286 - val_loss: 2.1235
Epoch 957/2000
22/22 - 1s - loss: 1.6284 - val_loss: 2.1218
Epoch 958/2000
22/22 - 1s - loss: 1.6259 - val_loss: 2.1209
Epoch 959/2000
22/22 - 1s - loss: 1.6260 - val_loss: 2.1195
Epoch 960/2000
22/22 - 1s - loss: 1.6240 - val_loss: 2.1190
Epoch 00960: val_loss improved from 2.13158 to 2.11904, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 961/2000
22/22 - 1s - loss: 1.6221 - val_loss: 2.1190
Epoch 962/2000
22/22 - 1s - loss: 1.6207 - val_loss: 2.1186
Epoch 963/2000
22/22 - 1s - loss: 1.6211 - val_loss: 2.1170
Epoch 964/2000
22/22 - 1s - loss: 1.6191 - val_loss: 2.1163
Epoch 965/2000
22/22 - 1s - loss: 1.6181 - val_loss: 2.1152
Epoch 966/2000
22/22 - 1s - loss: 1.6178 - val_loss: 2.1120
Epoch 967/2000
22/22 - 1s - loss: 1.6171 - val_loss: 2.1116
Epoch 968/2000
22/22 - 1s - loss: 1.6139 - val_loss: 2.1093
Epoch 969/2000
22/22 - 1s - loss: 1.6139 - val_loss: 2.1088
Epoch 970/2000
22/22 - 1s - loss: 1.6139 - val_loss: 2.1080
Epoch 00970: val_loss improved from 2.11904 to 2.10798, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 971/2000
22/22 - 1s - loss: 1.6096 - val_loss: 2.1062
Epoch 972/2000
22/22 - 1s - loss: 1.6104 - val_loss: 2.1050
Epoch 973/2000
22/22 - 1s - loss: 1.6073 - val_loss: 2.1045
Epoch 974/2000
22/22 - 1s - loss: 1.6081 - val_loss: 2.1038
Epoch 975/2000
22/22 - 1s - loss: 1.6061 - val_loss: 2.1029
Epoch 976/2000
22/22 - 1s - loss: 1.6053 - val_loss: 2.1010
Epoch 977/2000
22/22 - 1s - loss: 1.6017 - val_loss: 2.1004
Epoch 978/2000
22/22 - 1s - loss: 1.6038 - val_loss: 2.1009
Epoch 979/2000
22/22 - 1s - loss: 1.6010 - val_loss: 2.0979
Epoch 980/2000
22/22 - 1s - loss: 1.6000 - val_loss: 2.0957
Epoch 00980: val_loss improved from 2.10798 to 2.09573, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 981/2000
22/22 - 1s - loss: 1.6021 - val_loss: 2.0965
Epoch 982/2000
22/22 - 1s - loss: 1.5961 - val_loss: 2.0959
Epoch 983/2000
22/22 - 1s - loss: 1.5965 - val_loss: 2.0946
Epoch 984/2000
22/22 - 1s - loss: 1.5954 - val_loss: 2.0943
Epoch 985/2000
22/22 - 1s - loss: 1.5944 - val_loss: 2.0926
Epoch 986/2000
22/22 - 1s - loss: 1.5943 - val_loss: 2.0934
Epoch 987/2000
22/22 - 1s - loss: 1.5936 - val_loss: 2.0900
Epoch 988/2000
22/22 - 1s - loss: 1.5904 - val_loss: 2.0910
Epoch 989/2000
22/22 - 1s - loss: 1.5924 - val_loss: 2.0895
Epoch 990/2000
22/22 - 1s - loss: 1.5908 - val_loss: 2.0869
Epoch 00990: val_loss improved from 2.09573 to 2.08694, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 991/2000
22/22 - 1s - loss: 1.5904 - val_loss: 2.0851
Epoch 992/2000
22/22 - 1s - loss: 1.5870 - val_loss: 2.0847
Epoch 993/2000
22/22 - 1s - loss: 1.5855 - val_loss: 2.0841
Epoch 994/2000
22/22 - 1s - loss: 1.5862 - val_loss: 2.0834
Epoch 995/2000
22/22 - 1s - loss: 1.5854 - val_loss: 2.0824
Epoch 996/2000
22/22 - 1s - loss: 1.5837 - val_loss: 2.0790
Epoch 997/2000
22/22 - 1s - loss: 1.5825 - val_loss: 2.0767
Epoch 998/2000
22/22 - 1s - loss: 1.5820 - val_loss: 2.0765
Epoch 999/2000
22/22 - 1s - loss: 1.5811 - val_loss: 2.0762
Epoch 1000/2000
22/22 - 1s - loss: 1.5798 - val_loss: 2.0771
Epoch 01000: val_loss improved from 2.08694 to 2.07709, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1001/2000
22/22 - 1s - loss: 1.5791 - val_loss: 2.0742
Epoch 1002/2000
22/22 - 1s - loss: 1.5788 - val_loss: 2.0738
Epoch 1003/2000
22/22 - 1s - loss: 1.5747 - val_loss: 2.0718
Epoch 1004/2000
22/22 - 1s - loss: 1.5731 - val_loss: 2.0711
Epoch 1005/2000
22/22 - 1s - loss: 1.5718 - val_loss: 2.0683
Epoch 1006/2000
22/22 - 1s - loss: 1.5734 - val_loss: 2.0711
Epoch 1007/2000
22/22 - 1s - loss: 1.5689 - val_loss: 2.0681
Epoch 1008/2000
22/22 - 1s - loss: 1.5720 - val_loss: 2.0663
Epoch 1009/2000
22/22 - 1s - loss: 1.5705 - val_loss: 2.0669
Epoch 1010/2000
22/22 - 1s - loss: 1.5698 - val_loss: 2.0651
Epoch 01010: val_loss improved from 2.07709 to 2.06515, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1011/2000
22/22 - 1s - loss: 1.5680 - val_loss: 2.0651
Epoch 1012/2000
22/22 - 1s - loss: 1.5665 - val_loss: 2.0643
Epoch 1013/2000
22/22 - 1s - loss: 1.5666 - val_loss: 2.0617
Epoch 1014/2000
22/22 - 1s - loss: 1.5658 - val_loss: 2.0599
Epoch 1015/2000
22/22 - 1s - loss: 1.5625 - val_loss: 2.0591
Epoch 1016/2000
22/22 - 1s - loss: 1.5615 - val_loss: 2.0588
Epoch 1017/2000
22/22 - 1s - loss: 1.5591 - val_loss: 2.0584
Epoch 1018/2000
22/22 - 1s - loss: 1.5602 - val_loss: 2.0571
Epoch 1019/2000
22/22 - 1s - loss: 1.5571 - val_loss: 2.0548
Epoch 1020/2000
22/22 - 1s - loss: 1.5570 - val_loss: 2.0562
Epoch 01020: val_loss improved from 2.06515 to 2.05619, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1021/2000
22/22 - 1s - loss: 1.5564 - val_loss: 2.0550
Epoch 1022/2000
22/22 - 1s - loss: 1.5541 - val_loss: 2.0528
Epoch 1023/2000
22/22 - 1s - loss: 1.5540 - val_loss: 2.0502
Epoch 1024/2000
22/22 - 1s - loss: 1.5539 - val_loss: 2.0516
Epoch 1025/2000
22/22 - 1s - loss: 1.5510 - val_loss: 2.0506
Epoch 1026/2000
22/22 - 1s - loss: 1.5526 - val_loss: 2.0487
Epoch 1027/2000
22/22 - 1s - loss: 1.5483 - val_loss: 2.0461
Epoch 1028/2000
22/22 - 1s - loss: 1.5499 - val_loss: 2.0448
Epoch 1029/2000
22/22 - 1s - loss: 1.5486 - val_loss: 2.0432
Epoch 1030/2000
22/22 - 1s - loss: 1.5473 - val_loss: 2.0424
Epoch 01030: val_loss improved from 2.05619 to 2.04244, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1031/2000
22/22 - 1s - loss: 1.5460 - val_loss: 2.0399
Epoch 1032/2000
22/22 - 1s - loss: 1.5434 - val_loss: 2.0405
Epoch 1033/2000
22/22 - 1s - loss: 1.5448 - val_loss: 2.0409
Epoch 1034/2000
22/22 - 1s - loss: 1.5430 - val_loss: 2.0374
Epoch 1035/2000
22/22 - 1s - loss: 1.5427 - val_loss: 2.0380
Epoch 1036/2000
22/22 - 1s - loss: 1.5406 - val_loss: 2.0379
Epoch 1037/2000
22/22 - 1s - loss: 1.5402 - val_loss: 2.0378
Epoch 1038/2000
22/22 - 1s - loss: 1.5389 - val_loss: 2.0374
Epoch 1039/2000
22/22 - 1s - loss: 1.5370 - val_loss: 2.0362
Epoch 1040/2000
22/22 - 1s - loss: 1.5358 - val_loss: 2.0358
Epoch 01040: val_loss improved from 2.04244 to 2.03577, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1041/2000
22/22 - 1s - loss: 1.5361 - val_loss: 2.0322
Epoch 1042/2000
22/22 - 1s - loss: 1.5341 - val_loss: 2.0312
Epoch 1043/2000
22/22 - 1s - loss: 1.5353 - val_loss: 2.0295
Epoch 1044/2000
22/22 - 1s - loss: 1.5323 - val_loss: 2.0293
Epoch 1045/2000
22/22 - 1s - loss: 1.5299 - val_loss: 2.0271
Epoch 1046/2000
22/22 - 1s - loss: 1.5299 - val_loss: 2.0266
Epoch 1047/2000
22/22 - 1s - loss: 1.5286 - val_loss: 2.0293
Epoch 1048/2000
22/22 - 1s - loss: 1.5299 - val_loss: 2.0266
Epoch 1049/2000
22/22 - 1s - loss: 1.5256 - val_loss: 2.0247
Epoch 1050/2000
22/22 - 1s - loss: 1.5257 - val_loss: 2.0220
Epoch 01050: val_loss improved from 2.03577 to 2.02201, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1051/2000
22/22 - 1s - loss: 1.5246 - val_loss: 2.0200
Epoch 1052/2000
22/22 - 1s - loss: 1.5245 - val_loss: 2.0203
Epoch 1053/2000
22/22 - 1s - loss: 1.5231 - val_loss: 2.0215
Epoch 1054/2000
22/22 - 1s - loss: 1.5219 - val_loss: 2.0193
Epoch 1055/2000
22/22 - 1s - loss: 1.5213 - val_loss: 2.0178
Epoch 1056/2000
22/22 - 1s - loss: 1.5195 - val_loss: 2.0193
Epoch 1057/2000
22/22 - 1s - loss: 1.5185 - val_loss: 2.0164
Epoch 1058/2000
22/22 - 1s - loss: 1.5215 - val_loss: 2.0150
Epoch 1059/2000
22/22 - 1s - loss: 1.5193 - val_loss: 2.0162
Epoch 1060/2000
22/22 - 1s - loss: 1.5155 - val_loss: 2.0128
Epoch 01060: val_loss improved from 2.02201 to 2.01279, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1061/2000
22/22 - 1s - loss: 1.5164 - val_loss: 2.0130
Epoch 1062/2000
22/22 - 1s - loss: 1.5132 - val_loss: 2.0121
Epoch 1063/2000
22/22 - 1s - loss: 1.5127 - val_loss: 2.0098
Epoch 1064/2000
22/22 - 1s - loss: 1.5130 - val_loss: 2.0073
Epoch 1065/2000
22/22 - 1s - loss: 1.5133 - val_loss: 2.0081
Epoch 1066/2000
22/22 - 1s - loss: 1.5090 - val_loss: 2.0072
Epoch 1067/2000
22/22 - 1s - loss: 1.5069 - val_loss: 2.0067
Epoch 1068/2000
22/22 - 1s - loss: 1.5079 - val_loss: 2.0049
Epoch 1069/2000
22/22 - 1s - loss: 1.5043 - val_loss: 2.0050
Epoch 1070/2000
22/22 - 1s - loss: 1.5059 - val_loss: 2.0023
Epoch 01070: val_loss improved from 2.01279 to 2.00232, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1071/2000
22/22 - 1s - loss: 1.5033 - val_loss: 2.0027
Epoch 1072/2000
22/22 - 1s - loss: 1.5051 - val_loss: 2.0025
Epoch 1073/2000
22/22 - 1s - loss: 1.5011 - val_loss: 2.0011
Epoch 1074/2000
22/22 - 1s - loss: 1.5015 - val_loss: 1.9994
Epoch 1075/2000
22/22 - 1s - loss: 1.5004 - val_loss: 1.9985
Epoch 1076/2000
22/22 - 1s - loss: 1.4981 - val_loss: 1.9971
Epoch 1077/2000
22/22 - 1s - loss: 1.4996 - val_loss: 1.9965
Epoch 1078/2000
22/22 - 1s - loss: 1.4982 - val_loss: 1.9949
Epoch 1079/2000
22/22 - 1s - loss: 1.4965 - val_loss: 1.9943
Epoch 1080/2000
22/22 - 1s - loss: 1.4963 - val_loss: 1.9913
Epoch 01080: val_loss improved from 2.00232 to 1.99129, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1081/2000
22/22 - 1s - loss: 1.4933 - val_loss: 1.9931
Epoch 1082/2000
22/22 - 1s - loss: 1.4925 - val_loss: 1.9906
Epoch 1083/2000
22/22 - 1s - loss: 1.4936 - val_loss: 1.9885
Epoch 1084/2000
22/22 - 1s - loss: 1.4931 - val_loss: 1.9888
Epoch 1085/2000
22/22 - 1s - loss: 1.4890 - val_loss: 1.9882
Epoch 1086/2000
22/22 - 1s - loss: 1.4900 - val_loss: 1.9881
Epoch 1087/2000
22/22 - 1s - loss: 1.4884 - val_loss: 1.9878
Epoch 1088/2000
22/22 - 1s - loss: 1.4902 - val_loss: 1.9873
Epoch 1089/2000
22/22 - 1s - loss: 1.4865 - val_loss: 1.9841
Epoch 1090/2000
22/22 - 1s - loss: 1.4828 - val_loss: 1.9839
Epoch 01090: val_loss improved from 1.99129 to 1.98385, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1091/2000
22/22 - 1s - loss: 1.4840 - val_loss: 1.9832
Epoch 1092/2000
22/22 - 1s - loss: 1.4837 - val_loss: 1.9834
Epoch 1093/2000
22/22 - 1s - loss: 1.4830 - val_loss: 1.9829
Epoch 1094/2000
22/22 - 1s - loss: 1.4803 - val_loss: 1.9808
Epoch 1095/2000
22/22 - 1s - loss: 1.4807 - val_loss: 1.9781
Epoch 1096/2000
22/22 - 1s - loss: 1.4803 - val_loss: 1.9779
Epoch 1097/2000
22/22 - 1s - loss: 1.4816 - val_loss: 1.9760
Epoch 1098/2000
22/22 - 1s - loss: 1.4784 - val_loss: 1.9760
Epoch 1099/2000
22/22 - 1s - loss: 1.4752 - val_loss: 1.9748
Epoch 1100/2000
22/22 - 1s - loss: 1.4754 - val_loss: 1.9746
Epoch 01100: val_loss improved from 1.98385 to 1.97461, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1101/2000
22/22 - 1s - loss: 1.4739 - val_loss: 1.9736
Epoch 1102/2000
22/22 - 1s - loss: 1.4748 - val_loss: 1.9720
Epoch 1103/2000
22/22 - 1s - loss: 1.4729 - val_loss: 1.9709
Epoch 1104/2000
22/22 - 1s - loss: 1.4718 - val_loss: 1.9709
Epoch 1105/2000
22/22 - 1s - loss: 1.4719 - val_loss: 1.9689
Epoch 1106/2000
22/22 - 1s - loss: 1.4713 - val_loss: 1.9682
Epoch 1107/2000
22/22 - 1s - loss: 1.4681 - val_loss: 1.9675
Epoch 1108/2000
22/22 - 1s - loss: 1.4671 - val_loss: 1.9672
Epoch 1109/2000
22/22 - 1s - loss: 1.4664 - val_loss: 1.9650
Epoch 1110/2000
22/22 - 1s - loss: 1.4644 - val_loss: 1.9647
Epoch 01110: val_loss improved from 1.97461 to 1.96474, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1111/2000
22/22 - 1s - loss: 1.4651 - val_loss: 1.9622
Epoch 1112/2000
22/22 - 1s - loss: 1.4632 - val_loss: 1.9635
Epoch 1113/2000
22/22 - 1s - loss: 1.4641 - val_loss: 1.9611
Epoch 1114/2000
22/22 - 1s - loss: 1.4614 - val_loss: 1.9613
Epoch 1115/2000
22/22 - 1s - loss: 1.4615 - val_loss: 1.9590
Epoch 1116/2000
22/22 - 1s - loss: 1.4591 - val_loss: 1.9587
Epoch 1117/2000
22/22 - 1s - loss: 1.4611 - val_loss: 1.9586
Epoch 1118/2000
22/22 - 1s - loss: 1.4560 - val_loss: 1.9572
Epoch 1119/2000
22/22 - 1s - loss: 1.4577 - val_loss: 1.9562
Epoch 1120/2000
22/22 - 1s - loss: 1.4554 - val_loss: 1.9544
Epoch 01120: val_loss improved from 1.96474 to 1.95436, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1121/2000
22/22 - 1s - loss: 1.4546 - val_loss: 1.9538
Epoch 1122/2000
22/22 - 1s - loss: 1.4541 - val_loss: 1.9522
Epoch 1123/2000
22/22 - 1s - loss: 1.4548 - val_loss: 1.9513
Epoch 1124/2000
22/22 - 1s - loss: 1.4539 - val_loss: 1.9504
Epoch 1125/2000
22/22 - 1s - loss: 1.4518 - val_loss: 1.9506
Epoch 1126/2000
22/22 - 1s - loss: 1.4518 - val_loss: 1.9503
Epoch 1127/2000
22/22 - 1s - loss: 1.4494 - val_loss: 1.9481
Epoch 1128/2000
22/22 - 1s - loss: 1.4492 - val_loss: 1.9463
Epoch 1129/2000
22/22 - 1s - loss: 1.4479 - val_loss: 1.9441
Epoch 1130/2000
22/22 - 1s - loss: 1.4464 - val_loss: 1.9436
Epoch 01130: val_loss improved from 1.95436 to 1.94362, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1131/2000
22/22 - 1s - loss: 1.4455 - val_loss: 1.9420
Epoch 1132/2000
22/22 - 1s - loss: 1.4454 - val_loss: 1.9426
Epoch 1133/2000
22/22 - 1s - loss: 1.4414 - val_loss: 1.9432
Epoch 1134/2000
22/22 - 1s - loss: 1.4438 - val_loss: 1.9409
Epoch 1135/2000
22/22 - 1s - loss: 1.4417 - val_loss: 1.9393
Epoch 1136/2000
22/22 - 1s - loss: 1.4421 - val_loss: 1.9399
Epoch 1137/2000
22/22 - 1s - loss: 1.4405 - val_loss: 1.9373
Epoch 1138/2000
22/22 - 1s - loss: 1.4396 - val_loss: 1.9364
Epoch 1139/2000
22/22 - 1s - loss: 1.4398 - val_loss: 1.9371
Epoch 1140/2000
22/22 - 1s - loss: 1.4357 - val_loss: 1.9352
Epoch 01140: val_loss improved from 1.94362 to 1.93522, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1141/2000
22/22 - 1s - loss: 1.4363 - val_loss: 1.9356
Epoch 1142/2000
22/22 - 1s - loss: 1.4354 - val_loss: 1.9341
Epoch 1143/2000
22/22 - 1s - loss: 1.4338 - val_loss: 1.9336
Epoch 1144/2000
22/22 - 1s - loss: 1.4333 - val_loss: 1.9317
Epoch 1145/2000
22/22 - 1s - loss: 1.4337 - val_loss: 1.9300
Epoch 1146/2000
22/22 - 1s - loss: 1.4305 - val_loss: 1.9306
Epoch 1147/2000
22/22 - 1s - loss: 1.4311 - val_loss: 1.9291
Epoch 1148/2000
22/22 - 1s - loss: 1.4299 - val_loss: 1.9300
Epoch 1149/2000
22/22 - 1s - loss: 1.4286 - val_loss: 1.9257
Epoch 1150/2000
22/22 - 1s - loss: 1.4278 - val_loss: 1.9252
Epoch 01150: val_loss improved from 1.93522 to 1.92524, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1151/2000
22/22 - 1s - loss: 1.4285 - val_loss: 1.9258
Epoch 1152/2000
22/22 - 1s - loss: 1.4262 - val_loss: 1.9247
Epoch 1153/2000
22/22 - 1s - loss: 1.4241 - val_loss: 1.9233
Epoch 1154/2000
22/22 - 1s - loss: 1.4273 - val_loss: 1.9225
Epoch 1155/2000
22/22 - 1s - loss: 1.4236 - val_loss: 1.9211
Epoch 1156/2000
22/22 - 1s - loss: 1.4212 - val_loss: 1.9204
Epoch 1157/2000
22/22 - 1s - loss: 1.4185 - val_loss: 1.9192
Epoch 1158/2000
22/22 - 1s - loss: 1.4228 - val_loss: 1.9204
Epoch 1159/2000
22/22 - 1s - loss: 1.4206 - val_loss: 1.9185
Epoch 1160/2000
22/22 - 1s - loss: 1.4186 - val_loss: 1.9177
Epoch 01160: val_loss improved from 1.92524 to 1.91772, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1161/2000
22/22 - 1s - loss: 1.4182 - val_loss: 1.9153
Epoch 1162/2000
22/22 - 1s - loss: 1.4162 - val_loss: 1.9148
Epoch 1163/2000
22/22 - 1s - loss: 1.4146 - val_loss: 1.9131
Epoch 1164/2000
22/22 - 1s - loss: 1.4188 - val_loss: 1.9127
Epoch 1165/2000
22/22 - 1s - loss: 1.4117 - val_loss: 1.9135
Epoch 1166/2000
22/22 - 1s - loss: 1.4132 - val_loss: 1.9117
Epoch 1167/2000
22/22 - 1s - loss: 1.4114 - val_loss: 1.9115
Epoch 1168/2000
22/22 - 1s - loss: 1.4103 - val_loss: 1.9108
Epoch 1169/2000
22/22 - 1s - loss: 1.4106 - val_loss: 1.9093
Epoch 1170/2000
22/22 - 1s - loss: 1.4066 - val_loss: 1.9074
Epoch 01170: val_loss improved from 1.91772 to 1.90743, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1171/2000
22/22 - 1s - loss: 1.4074 - val_loss: 1.9056
Epoch 1172/2000
22/22 - 1s - loss: 1.4055 - val_loss: 1.9047
Epoch 1173/2000
22/22 - 1s - loss: 1.4057 - val_loss: 1.9038
Epoch 1174/2000
22/22 - 1s - loss: 1.4063 - val_loss: 1.9022
Epoch 1175/2000
22/22 - 1s - loss: 1.4041 - val_loss: 1.9042
Epoch 1176/2000
22/22 - 1s - loss: 1.4056 - val_loss: 1.9024
Epoch 1177/2000
22/22 - 1s - loss: 1.4039 - val_loss: 1.9037
Epoch 1178/2000
22/22 - 1s - loss: 1.4010 - val_loss: 1.9013
Epoch 1179/2000
22/22 - 1s - loss: 1.4024 - val_loss: 1.8980
Epoch 1180/2000
22/22 - 1s - loss: 1.3990 - val_loss: 1.8978
Epoch 01180: val_loss improved from 1.90743 to 1.89783, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1181/2000
22/22 - 1s - loss: 1.3981 - val_loss: 1.8980
Epoch 1182/2000
22/22 - 1s - loss: 1.3974 - val_loss: 1.8986
Epoch 1183/2000
22/22 - 1s - loss: 1.3984 - val_loss: 1.8975
Epoch 1184/2000
22/22 - 1s - loss: 1.3970 - val_loss: 1.8949
Epoch 1185/2000
22/22 - 1s - loss: 1.3956 - val_loss: 1.8952
Epoch 1186/2000
22/22 - 1s - loss: 1.3927 - val_loss: 1.8947
Epoch 1187/2000
22/22 - 1s - loss: 1.3925 - val_loss: 1.8922
Epoch 1188/2000
22/22 - 1s - loss: 1.3937 - val_loss: 1.8925
Epoch 1189/2000
22/22 - 1s - loss: 1.3921 - val_loss: 1.8916
Epoch 1190/2000
22/22 - 1s - loss: 1.3897 - val_loss: 1.8914
Epoch 01190: val_loss improved from 1.89783 to 1.89144, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1191/2000
22/22 - 1s - loss: 1.3892 - val_loss: 1.8901
Epoch 1192/2000
22/22 - 1s - loss: 1.3889 - val_loss: 1.8888
Epoch 1193/2000
22/22 - 1s - loss: 1.3889 - val_loss: 1.8877
Epoch 1194/2000
22/22 - 1s - loss: 1.3867 - val_loss: 1.8857
Epoch 1195/2000
22/22 - 1s - loss: 1.3870 - val_loss: 1.8851
Epoch 1196/2000
22/22 - 1s - loss: 1.3844 - val_loss: 1.8850
Epoch 1197/2000
22/22 - 1s - loss: 1.3839 - val_loss: 1.8854
Epoch 1198/2000
22/22 - 1s - loss: 1.3835 - val_loss: 1.8845
Epoch 1199/2000
22/22 - 1s - loss: 1.3826 - val_loss: 1.8820
Epoch 1200/2000
22/22 - 1s - loss: 1.3824 - val_loss: 1.8796
Epoch 01200: val_loss improved from 1.89144 to 1.87957, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1201/2000
22/22 - 1s - loss: 1.3829 - val_loss: 1.8805
Epoch 1202/2000
22/22 - 1s - loss: 1.3803 - val_loss: 1.8783
Epoch 1203/2000
22/22 - 1s - loss: 1.3798 - val_loss: 1.8802
Epoch 1204/2000
22/22 - 1s - loss: 1.3769 - val_loss: 1.8781
Epoch 1205/2000
22/22 - 1s - loss: 1.3793 - val_loss: 1.8767
Epoch 1206/2000
22/22 - 1s - loss: 1.3764 - val_loss: 1.8754
Epoch 1207/2000
22/22 - 1s - loss: 1.3750 - val_loss: 1.8751
Epoch 1208/2000
22/22 - 1s - loss: 1.3751 - val_loss: 1.8743
Epoch 1209/2000
22/22 - 1s - loss: 1.3754 - val_loss: 1.8730
Epoch 1210/2000
22/22 - 1s - loss: 1.3719 - val_loss: 1.8724
Epoch 01210: val_loss improved from 1.87957 to 1.87242, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1211/2000
22/22 - 1s - loss: 1.3712 - val_loss: 1.8735
Epoch 1212/2000
22/22 - 1s - loss: 1.3702 - val_loss: 1.8719
Epoch 1213/2000
22/22 - 1s - loss: 1.3712 - val_loss: 1.8712
Epoch 1214/2000
22/22 - 1s - loss: 1.3690 - val_loss: 1.8695
Epoch 1215/2000
22/22 - 1s - loss: 1.3692 - val_loss: 1.8678
Epoch 1216/2000
22/22 - 1s - loss: 1.3698 - val_loss: 1.8681
Epoch 1217/2000
22/22 - 1s - loss: 1.3694 - val_loss: 1.8682
Epoch 1218/2000
22/22 - 1s - loss: 1.3670 - val_loss: 1.8664
Epoch 1219/2000
22/22 - 1s - loss: 1.3652 - val_loss: 1.8654
Epoch 1220/2000
22/22 - 1s - loss: 1.3671 - val_loss: 1.8659
Epoch 01220: val_loss improved from 1.87242 to 1.86590, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1221/2000
22/22 - 1s - loss: 1.3631 - val_loss: 1.8657
Epoch 1222/2000
22/22 - 1s - loss: 1.3630 - val_loss: 1.8624
Epoch 1223/2000
22/22 - 1s - loss: 1.3625 - val_loss: 1.8627
Epoch 1224/2000
22/22 - 1s - loss: 1.3603 - val_loss: 1.8600
Epoch 1225/2000
22/22 - 1s - loss: 1.3612 - val_loss: 1.8594
Epoch 1226/2000
22/22 - 1s - loss: 1.3596 - val_loss: 1.8600
Epoch 1227/2000
22/22 - 1s - loss: 1.3571 - val_loss: 1.8576
Epoch 1228/2000
22/22 - 1s - loss: 1.3566 - val_loss: 1.8555
Epoch 1229/2000
22/22 - 1s - loss: 1.3576 - val_loss: 1.8539
Epoch 1230/2000
22/22 - 1s - loss: 1.3579 - val_loss: 1.8555
Epoch 01230: val_loss improved from 1.86590 to 1.85548, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1231/2000
22/22 - 1s - loss: 1.3548 - val_loss: 1.8544
Epoch 1232/2000
22/22 - 1s - loss: 1.3555 - val_loss: 1.8529
Epoch 1233/2000
22/22 - 1s - loss: 1.3523 - val_loss: 1.8525
Epoch 1234/2000
22/22 - 1s - loss: 1.3525 - val_loss: 1.8507
Epoch 1235/2000
22/22 - 1s - loss: 1.3506 - val_loss: 1.8512
Epoch 1236/2000
22/22 - 1s - loss: 1.3523 - val_loss: 1.8479
Epoch 1237/2000
22/22 - 1s - loss: 1.3518 - val_loss: 1.8497
Epoch 1238/2000
22/22 - 1s - loss: 1.3481 - val_loss: 1.8493
Epoch 1239/2000
22/22 - 1s - loss: 1.3499 - val_loss: 1.8480
Epoch 1240/2000
22/22 - 1s - loss: 1.3480 - val_loss: 1.8465
Epoch 01240: val_loss improved from 1.85548 to 1.84647, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1241/2000
22/22 - 1s - loss: 1.3463 - val_loss: 1.8473
Epoch 1242/2000
22/22 - 1s - loss: 1.3450 - val_loss: 1.8453
Epoch 1243/2000
22/22 - 1s - loss: 1.3452 - val_loss: 1.8450
Epoch 1244/2000
22/22 - 1s - loss: 1.3442 - val_loss: 1.8434
Epoch 1245/2000
22/22 - 1s - loss: 1.3439 - val_loss: 1.8422
Epoch 1246/2000
22/22 - 1s - loss: 1.3444 - val_loss: 1.8428
Epoch 1247/2000
22/22 - 1s - loss: 1.3405 - val_loss: 1.8409
Epoch 1248/2000
22/22 - 1s - loss: 1.3406 - val_loss: 1.8412
Epoch 1249/2000
22/22 - 1s - loss: 1.3406 - val_loss: 1.8401
Epoch 1250/2000
22/22 - 1s - loss: 1.3401 - val_loss: 1.8386
Epoch 01250: val_loss improved from 1.84647 to 1.83861, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1251/2000
22/22 - 1s - loss: 1.3378 - val_loss: 1.8376
Epoch 1252/2000
22/22 - 1s - loss: 1.3372 - val_loss: 1.8349
Epoch 1253/2000
22/22 - 1s - loss: 1.3354 - val_loss: 1.8361
Epoch 1254/2000
22/22 - 1s - loss: 1.3345 - val_loss: 1.8351
Epoch 1255/2000
22/22 - 1s - loss: 1.3352 - val_loss: 1.8347
Epoch 1256/2000
22/22 - 1s - loss: 1.3325 - val_loss: 1.8342
Epoch 1257/2000
22/22 - 1s - loss: 1.3344 - val_loss: 1.8332
Epoch 1258/2000
22/22 - 1s - loss: 1.3323 - val_loss: 1.8331
Epoch 1259/2000
22/22 - 1s - loss: 1.3323 - val_loss: 1.8305
Epoch 1260/2000
22/22 - 1s - loss: 1.3302 - val_loss: 1.8323
Epoch 01260: val_loss improved from 1.83861 to 1.83231, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1261/2000
22/22 - 1s - loss: 1.3284 - val_loss: 1.8297
Epoch 1262/2000
22/22 - 1s - loss: 1.3291 - val_loss: 1.8298
Epoch 1263/2000
22/22 - 1s - loss: 1.3276 - val_loss: 1.8288
Epoch 1264/2000
22/22 - 1s - loss: 1.3257 - val_loss: 1.8280
Epoch 1265/2000
22/22 - 1s - loss: 1.3261 - val_loss: 1.8259
Epoch 1266/2000
22/22 - 1s - loss: 1.3260 - val_loss: 1.8256
Epoch 1267/2000
22/22 - 1s - loss: 1.3233 - val_loss: 1.8252
Epoch 1268/2000
22/22 - 1s - loss: 1.3243 - val_loss: 1.8240
Epoch 1269/2000
22/22 - 1s - loss: 1.3231 - val_loss: 1.8231
Epoch 1270/2000
22/22 - 1s - loss: 1.3205 - val_loss: 1.8233
Epoch 01270: val_loss improved from 1.83231 to 1.82334, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1271/2000
22/22 - 1s - loss: 1.3215 - val_loss: 1.8213
Epoch 1272/2000
22/22 - 1s - loss: 1.3200 - val_loss: 1.8211
Epoch 1273/2000
22/22 - 1s - loss: 1.3181 - val_loss: 1.8211
Epoch 1274/2000
22/22 - 1s - loss: 1.3187 - val_loss: 1.8196
Epoch 1275/2000
22/22 - 1s - loss: 1.3181 - val_loss: 1.8188
Epoch 1276/2000
22/22 - 1s - loss: 1.3163 - val_loss: 1.8181
Epoch 1277/2000
22/22 - 1s - loss: 1.3170 - val_loss: 1.8178
Epoch 1278/2000
22/22 - 1s - loss: 1.3144 - val_loss: 1.8159
Epoch 1279/2000
22/22 - 1s - loss: 1.3136 - val_loss: 1.8147
Epoch 1280/2000
22/22 - 1s - loss: 1.3121 - val_loss: 1.8141
Epoch 01280: val_loss improved from 1.82334 to 1.81406, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1281/2000
22/22 - 1s - loss: 1.3131 - val_loss: 1.8141
Epoch 1282/2000
22/22 - 1s - loss: 1.3113 - val_loss: 1.8120
Epoch 1283/2000
22/22 - 1s - loss: 1.3117 - val_loss: 1.8138
Epoch 1284/2000
22/22 - 1s - loss: 1.3117 - val_loss: 1.8125
Epoch 1285/2000
22/22 - 1s - loss: 1.3092 - val_loss: 1.8115
Epoch 1286/2000
22/22 - 1s - loss: 1.3092 - val_loss: 1.8098
Epoch 1287/2000
22/22 - 1s - loss: 1.3059 - val_loss: 1.8103
Epoch 1288/2000
22/22 - 1s - loss: 1.3077 - val_loss: 1.8091
Epoch 1289/2000
22/22 - 1s - loss: 1.3065 - val_loss: 1.8078
Epoch 1290/2000
22/22 - 1s - loss: 1.3049 - val_loss: 1.8081
Epoch 01290: val_loss improved from 1.81406 to 1.80815, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1291/2000
22/22 - 1s - loss: 1.3037 - val_loss: 1.8073
Epoch 1292/2000
22/22 - 1s - loss: 1.3039 - val_loss: 1.8063
Epoch 1293/2000
22/22 - 1s - loss: 1.3021 - val_loss: 1.8051
Epoch 1294/2000
22/22 - 1s - loss: 1.3008 - val_loss: 1.8045
Epoch 1295/2000
22/22 - 1s - loss: 1.3002 - val_loss: 1.8017
Epoch 1296/2000
22/22 - 1s - loss: 1.3006 - val_loss: 1.8021
Epoch 1297/2000
22/22 - 1s - loss: 1.3005 - val_loss: 1.8019
Epoch 1298/2000
22/22 - 1s - loss: 1.2991 - val_loss: 1.7993
Epoch 1299/2000
22/22 - 1s - loss: 1.2981 - val_loss: 1.8005
Epoch 1300/2000
22/22 - 1s - loss: 1.2974 - val_loss: 1.7990
Epoch 01300: val_loss improved from 1.80815 to 1.79901, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1301/2000
22/22 - 1s - loss: 1.2971 - val_loss: 1.7982
Epoch 1302/2000
22/22 - 1s - loss: 1.2970 - val_loss: 1.7952
Epoch 1303/2000
22/22 - 1s - loss: 1.2937 - val_loss: 1.7959
Epoch 1304/2000
22/22 - 1s - loss: 1.2948 - val_loss: 1.7951
Epoch 1305/2000
22/22 - 1s - loss: 1.2934 - val_loss: 1.7949
Epoch 1306/2000
22/22 - 1s - loss: 1.2941 - val_loss: 1.7940
Epoch 1307/2000
22/22 - 1s - loss: 1.2913 - val_loss: 1.7913
Epoch 1308/2000
22/22 - 1s - loss: 1.2912 - val_loss: 1.7922
Epoch 1309/2000
22/22 - 1s - loss: 1.2896 - val_loss: 1.7911
Epoch 1310/2000
22/22 - 1s - loss: 1.2922 - val_loss: 1.7898
Epoch 01310: val_loss improved from 1.79901 to 1.78980, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1311/2000
22/22 - 1s - loss: 1.2885 - val_loss: 1.7899
Epoch 1312/2000
22/22 - 1s - loss: 1.2869 - val_loss: 1.7895
Epoch 1313/2000
22/22 - 1s - loss: 1.2883 - val_loss: 1.7902
Epoch 1314/2000
22/22 - 1s - loss: 1.2867 - val_loss: 1.7863
Epoch 1315/2000
22/22 - 1s - loss: 1.2849 - val_loss: 1.7866
Epoch 1316/2000
22/22 - 1s - loss: 1.2851 - val_loss: 1.7849
Epoch 1317/2000
22/22 - 1s - loss: 1.2849 - val_loss: 1.7846
Epoch 1318/2000
22/22 - 1s - loss: 1.2847 - val_loss: 1.7843
Epoch 1319/2000
22/22 - 1s - loss: 1.2830 - val_loss: 1.7837
Epoch 1320/2000
22/22 - 1s - loss: 1.2805 - val_loss: 1.7830
Epoch 01320: val_loss improved from 1.78980 to 1.78297, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1321/2000
22/22 - 1s - loss: 1.2803 - val_loss: 1.7835
Epoch 1322/2000
22/22 - 1s - loss: 1.2799 - val_loss: 1.7824
Epoch 1323/2000
22/22 - 1s - loss: 1.2809 - val_loss: 1.7815
Epoch 1324/2000
22/22 - 1s - loss: 1.2787 - val_loss: 1.7807
Epoch 1325/2000
22/22 - 1s - loss: 1.2771 - val_loss: 1.7808
Epoch 1326/2000
22/22 - 1s - loss: 1.2762 - val_loss: 1.7794
Epoch 1327/2000
22/22 - 1s - loss: 1.2772 - val_loss: 1.7765
Epoch 1328/2000
22/22 - 1s - loss: 1.2771 - val_loss: 1.7775
Epoch 1329/2000
22/22 - 1s - loss: 1.2744 - val_loss: 1.7744
Epoch 1330/2000
22/22 - 1s - loss: 1.2734 - val_loss: 1.7759
Epoch 01330: val_loss improved from 1.78297 to 1.77591, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1331/2000
22/22 - 1s - loss: 1.2738 - val_loss: 1.7749
Epoch 1332/2000
22/22 - 1s - loss: 1.2733 - val_loss: 1.7730
Epoch 1333/2000
22/22 - 1s - loss: 1.2711 - val_loss: 1.7719
Epoch 1334/2000
22/22 - 1s - loss: 1.2701 - val_loss: 1.7720
Epoch 1335/2000
22/22 - 1s - loss: 1.2707 - val_loss: 1.7722
Epoch 1336/2000
22/22 - 1s - loss: 1.2698 - val_loss: 1.7702
Epoch 1337/2000
22/22 - 1s - loss: 1.2688 - val_loss: 1.7710
Epoch 1338/2000
22/22 - 1s - loss: 1.2679 - val_loss: 1.7692
Epoch 1339/2000
22/22 - 1s - loss: 1.2680 - val_loss: 1.7687
Epoch 1340/2000
22/22 - 1s - loss: 1.2691 - val_loss: 1.7675
Epoch 01340: val_loss improved from 1.77591 to 1.76748, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1341/2000
22/22 - 1s - loss: 1.2632 - val_loss: 1.7661
Epoch 1342/2000
22/22 - 1s - loss: 1.2660 - val_loss: 1.7656
Epoch 1343/2000
22/22 - 1s - loss: 1.2646 - val_loss: 1.7640
Epoch 1344/2000
22/22 - 1s - loss: 1.2632 - val_loss: 1.7677
Epoch 1345/2000
22/22 - 1s - loss: 1.2627 - val_loss: 1.7648
Epoch 1346/2000
22/22 - 1s - loss: 1.2613 - val_loss: 1.7637
Epoch 1347/2000
22/22 - 1s - loss: 1.2629 - val_loss: 1.7643
Epoch 1348/2000
22/22 - 1s - loss: 1.2604 - val_loss: 1.7633
Epoch 1349/2000
22/22 - 1s - loss: 1.2600 - val_loss: 1.7617
Epoch 1350/2000
22/22 - 1s - loss: 1.2596 - val_loss: 1.7602
Epoch 01350: val_loss improved from 1.76748 to 1.76021, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1351/2000
22/22 - 1s - loss: 1.2587 - val_loss: 1.7592
Epoch 1352/2000
22/22 - 1s - loss: 1.2559 - val_loss: 1.7564
Epoch 1353/2000
22/22 - 1s - loss: 1.2576 - val_loss: 1.7559
Epoch 1354/2000
22/22 - 1s - loss: 1.2549 - val_loss: 1.7582
Epoch 1355/2000
22/22 - 1s - loss: 1.2556 - val_loss: 1.7564
Epoch 1356/2000
22/22 - 1s - loss: 1.2548 - val_loss: 1.7552
Epoch 1357/2000
22/22 - 1s - loss: 1.2542 - val_loss: 1.7548
Epoch 1358/2000
22/22 - 1s - loss: 1.2528 - val_loss: 1.7543
Epoch 1359/2000
22/22 - 1s - loss: 1.2527 - val_loss: 1.7534
Epoch 1360/2000
22/22 - 1s - loss: 1.2511 - val_loss: 1.7531
Epoch 01360: val_loss improved from 1.76021 to 1.75310, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1361/2000
22/22 - 1s - loss: 1.2501 - val_loss: 1.7512
Epoch 1362/2000
22/22 - 1s - loss: 1.2479 - val_loss: 1.7509
Epoch 1363/2000
22/22 - 1s - loss: 1.2464 - val_loss: 1.7508
Epoch 1364/2000
22/22 - 1s - loss: 1.2483 - val_loss: 1.7492
Epoch 1365/2000
22/22 - 1s - loss: 1.2468 - val_loss: 1.7475
Epoch 1366/2000
22/22 - 1s - loss: 1.2445 - val_loss: 1.7481
Epoch 1367/2000
22/22 - 1s - loss: 1.2449 - val_loss: 1.7476
Epoch 1368/2000
22/22 - 1s - loss: 1.2467 - val_loss: 1.7473
Epoch 1369/2000
22/22 - 1s - loss: 1.2451 - val_loss: 1.7451
Epoch 1370/2000
22/22 - 1s - loss: 1.2445 - val_loss: 1.7447
Epoch 01370: val_loss improved from 1.75310 to 1.74466, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1371/2000
22/22 - 1s - loss: 1.2429 - val_loss: 1.7471
Epoch 1372/2000
22/22 - 1s - loss: 1.2388 - val_loss: 1.7439
Epoch 1373/2000
22/22 - 1s - loss: 1.2422 - val_loss: 1.7423
Epoch 1374/2000
22/22 - 1s - loss: 1.2396 - val_loss: 1.7424
Epoch 1375/2000
22/22 - 1s - loss: 1.2402 - val_loss: 1.7414
Epoch 1376/2000
22/22 - 1s - loss: 1.2386 - val_loss: 1.7406
Epoch 1377/2000
22/22 - 1s - loss: 1.2380 - val_loss: 1.7393
Epoch 1378/2000
22/22 - 1s - loss: 1.2382 - val_loss: 1.7376
Epoch 1379/2000
22/22 - 1s - loss: 1.2367 - val_loss: 1.7377
Epoch 1380/2000
22/22 - 1s - loss: 1.2353 - val_loss: 1.7377
Epoch 01380: val_loss improved from 1.74466 to 1.73767, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1381/2000
22/22 - 1s - loss: 1.2350 - val_loss: 1.7377
Epoch 1382/2000
22/22 - 1s - loss: 1.2344 - val_loss: 1.7353
Epoch 1383/2000
22/22 - 1s - loss: 1.2343 - val_loss: 1.7363
Epoch 1384/2000
22/22 - 1s - loss: 1.2322 - val_loss: 1.7348
Epoch 1385/2000
22/22 - 1s - loss: 1.2302 - val_loss: 1.7333
Epoch 1386/2000
22/22 - 1s - loss: 1.2318 - val_loss: 1.7334
Epoch 1387/2000
22/22 - 1s - loss: 1.2318 - val_loss: 1.7327
Epoch 1388/2000
22/22 - 1s - loss: 1.2304 - val_loss: 1.7309
Epoch 1389/2000
22/22 - 1s - loss: 1.2287 - val_loss: 1.7299
Epoch 1390/2000
22/22 - 1s - loss: 1.2294 - val_loss: 1.7289
Epoch 01390: val_loss improved from 1.73767 to 1.72888, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1391/2000
22/22 - 1s - loss: 1.2256 - val_loss: 1.7288
Epoch 1392/2000
22/22 - 1s - loss: 1.2264 - val_loss: 1.7283
Epoch 1393/2000
22/22 - 1s - loss: 1.2292 - val_loss: 1.7281
Epoch 1394/2000
22/22 - 1s - loss: 1.2263 - val_loss: 1.7273
Epoch 1395/2000
22/22 - 1s - loss: 1.2246 - val_loss: 1.7261
Epoch 1396/2000
22/22 - 1s - loss: 1.2239 - val_loss: 1.7258
Epoch 1397/2000
22/22 - 1s - loss: 1.2227 - val_loss: 1.7249
Epoch 1398/2000
22/22 - 1s - loss: 1.2197 - val_loss: 1.7241
Epoch 1399/2000
22/22 - 1s - loss: 1.2220 - val_loss: 1.7225
Epoch 1400/2000
22/22 - 1s - loss: 1.2210 - val_loss: 1.7233
Epoch 01400: val_loss improved from 1.72888 to 1.72329, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1401/2000
22/22 - 1s - loss: 1.2215 - val_loss: 1.7237
Epoch 1402/2000
22/22 - 1s - loss: 1.2210 - val_loss: 1.7219
Epoch 1403/2000
22/22 - 1s - loss: 1.2192 - val_loss: 1.7213
Epoch 1404/2000
22/22 - 1s - loss: 1.2190 - val_loss: 1.7209
Epoch 1405/2000
22/22 - 1s - loss: 1.2175 - val_loss: 1.7190
Epoch 1406/2000
22/22 - 1s - loss: 1.2180 - val_loss: 1.7212
Epoch 1407/2000
22/22 - 1s - loss: 1.2149 - val_loss: 1.7181
Epoch 1408/2000
22/22 - 1s - loss: 1.2159 - val_loss: 1.7179
Epoch 1409/2000
22/22 - 1s - loss: 1.2159 - val_loss: 1.7171
Epoch 1410/2000
22/22 - 1s - loss: 1.2148 - val_loss: 1.7156
Epoch 01410: val_loss improved from 1.72329 to 1.71560, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1411/2000
22/22 - 1s - loss: 1.2128 - val_loss: 1.7164
Epoch 1412/2000
22/22 - 1s - loss: 1.2104 - val_loss: 1.7126
Epoch 1413/2000
22/22 - 1s - loss: 1.2119 - val_loss: 1.7123
Epoch 1414/2000
22/22 - 1s - loss: 1.2115 - val_loss: 1.7109
Epoch 1415/2000
22/22 - 1s - loss: 1.2089 - val_loss: 1.7134
Epoch 1416/2000
22/22 - 1s - loss: 1.2106 - val_loss: 1.7108
Epoch 1417/2000
22/22 - 1s - loss: 1.2089 - val_loss: 1.7111
Epoch 1418/2000
22/22 - 1s - loss: 1.2089 - val_loss: 1.7117
Epoch 1419/2000
22/22 - 1s - loss: 1.2071 - val_loss: 1.7099
Epoch 1420/2000
22/22 - 1s - loss: 1.2072 - val_loss: 1.7081
Epoch 01420: val_loss improved from 1.71560 to 1.70813, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1421/2000
22/22 - 1s - loss: 1.2065 - val_loss: 1.7073
Epoch 1422/2000
22/22 - 1s - loss: 1.2050 - val_loss: 1.7077
Epoch 1423/2000
22/22 - 1s - loss: 1.2056 - val_loss: 1.7075
Epoch 1424/2000
22/22 - 1s - loss: 1.2040 - val_loss: 1.7055
Epoch 1425/2000
22/22 - 1s - loss: 1.2045 - val_loss: 1.7054
Epoch 1426/2000
22/22 - 1s - loss: 1.2012 - val_loss: 1.7056
Epoch 1427/2000
22/22 - 1s - loss: 1.2026 - val_loss: 1.7046
Epoch 1428/2000
22/22 - 1s - loss: 1.2024 - val_loss: 1.7025
Epoch 1429/2000
22/22 - 1s - loss: 1.1990 - val_loss: 1.7010
Epoch 1430/2000
22/22 - 1s - loss: 1.2005 - val_loss: 1.7013
Epoch 01430: val_loss improved from 1.70813 to 1.70133, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1431/2000
22/22 - 1s - loss: 1.1987 - val_loss: 1.7010
Epoch 1432/2000
22/22 - 1s - loss: 1.1992 - val_loss: 1.7007
Epoch 1433/2000
22/22 - 1s - loss: 1.1964 - val_loss: 1.7015
Epoch 1434/2000
22/22 - 1s - loss: 1.1990 - val_loss: 1.7024
Epoch 1435/2000
22/22 - 1s - loss: 1.1966 - val_loss: 1.6991
Epoch 1436/2000
22/22 - 1s - loss: 1.1956 - val_loss: 1.6976
Epoch 1437/2000
22/22 - 1s - loss: 1.1947 - val_loss: 1.6968
Epoch 1438/2000
22/22 - 1s - loss: 1.1949 - val_loss: 1.6961
Epoch 1439/2000
22/22 - 1s - loss: 1.1928 - val_loss: 1.6948
Epoch 1440/2000
22/22 - 1s - loss: 1.1934 - val_loss: 1.6949
Epoch 01440: val_loss improved from 1.70133 to 1.69486, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1441/2000
22/22 - 1s - loss: 1.1926 - val_loss: 1.6954
Epoch 1442/2000
22/22 - 1s - loss: 1.1916 - val_loss: 1.6931
Epoch 1443/2000
22/22 - 1s - loss: 1.1893 - val_loss: 1.6932
Epoch 1444/2000
22/22 - 1s - loss: 1.1936 - val_loss: 1.6909
Epoch 1445/2000
22/22 - 1s - loss: 1.1880 - val_loss: 1.6920
Epoch 1446/2000
22/22 - 1s - loss: 1.1898 - val_loss: 1.6915
Epoch 1447/2000
22/22 - 1s - loss: 1.1893 - val_loss: 1.6903
Epoch 1448/2000
22/22 - 1s - loss: 1.1858 - val_loss: 1.6898
Epoch 1449/2000
22/22 - 1s - loss: 1.1894 - val_loss: 1.6895
Epoch 1450/2000
22/22 - 1s - loss: 1.1869 - val_loss: 1.6877
Epoch 01450: val_loss improved from 1.69486 to 1.68768, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1451/2000
22/22 - 1s - loss: 1.1842 - val_loss: 1.6869
Epoch 1452/2000
22/22 - 1s - loss: 1.1832 - val_loss: 1.6857
Epoch 1453/2000
22/22 - 1s - loss: 1.1838 - val_loss: 1.6857
Epoch 1454/2000
22/22 - 1s - loss: 1.1825 - val_loss: 1.6876
Epoch 1455/2000
22/22 - 1s - loss: 1.1824 - val_loss: 1.6856
Epoch 1456/2000
22/22 - 1s - loss: 1.1814 - val_loss: 1.6854
Epoch 1457/2000
22/22 - 1s - loss: 1.1813 - val_loss: 1.6832
Epoch 1458/2000
22/22 - 1s - loss: 1.1814 - val_loss: 1.6834
Epoch 1459/2000
22/22 - 1s - loss: 1.1786 - val_loss: 1.6832
Epoch 1460/2000
22/22 - 1s - loss: 1.1816 - val_loss: 1.6805
Epoch 01460: val_loss improved from 1.68768 to 1.68050, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1461/2000
22/22 - 1s - loss: 1.1760 - val_loss: 1.6807
Epoch 1462/2000
22/22 - 1s - loss: 1.1766 - val_loss: 1.6800
Epoch 1463/2000
22/22 - 1s - loss: 1.1745 - val_loss: 1.6791
Epoch 1464/2000
22/22 - 1s - loss: 1.1770 - val_loss: 1.6774
Epoch 1465/2000
22/22 - 1s - loss: 1.1742 - val_loss: 1.6769
Epoch 1466/2000
22/22 - 1s - loss: 1.1758 - val_loss: 1.6775
Epoch 1467/2000
22/22 - 1s - loss: 1.1752 - val_loss: 1.6758
Epoch 1468/2000
22/22 - 1s - loss: 1.1731 - val_loss: 1.6765
Epoch 1469/2000
22/22 - 1s - loss: 1.1734 - val_loss: 1.6746
Epoch 1470/2000
22/22 - 1s - loss: 1.1734 - val_loss: 1.6744
Epoch 01470: val_loss improved from 1.68050 to 1.67439, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1471/2000
22/22 - 1s - loss: 1.1712 - val_loss: 1.6736
Epoch 1472/2000
22/22 - 1s - loss: 1.1712 - val_loss: 1.6725
Epoch 1473/2000
22/22 - 1s - loss: 1.1695 - val_loss: 1.6721
Epoch 1474/2000
22/22 - 1s - loss: 1.1695 - val_loss: 1.6712
Epoch 1475/2000
22/22 - 1s - loss: 1.1676 - val_loss: 1.6706
Epoch 1476/2000
22/22 - 1s - loss: 1.1684 - val_loss: 1.6699
Epoch 1477/2000
22/22 - 1s - loss: 1.1680 - val_loss: 1.6705
Epoch 1478/2000
22/22 - 1s - loss: 1.1652 - val_loss: 1.6709
Epoch 1479/2000
22/22 - 1s - loss: 1.1666 - val_loss: 1.6684
Epoch 1480/2000
22/22 - 1s - loss: 1.1660 - val_loss: 1.6676
Epoch 01480: val_loss improved from 1.67439 to 1.66759, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1481/2000
22/22 - 1s - loss: 1.1663 - val_loss: 1.6664
Epoch 1482/2000
22/22 - 1s - loss: 1.1649 - val_loss: 1.6676
Epoch 1483/2000
22/22 - 1s - loss: 1.1633 - val_loss: 1.6668
Epoch 1484/2000
22/22 - 1s - loss: 1.1628 - val_loss: 1.6658
Epoch 1485/2000
22/22 - 1s - loss: 1.1623 - val_loss: 1.6649
Epoch 1486/2000
22/22 - 1s - loss: 1.1642 - val_loss: 1.6664
Epoch 1487/2000
22/22 - 1s - loss: 1.1605 - val_loss: 1.6639
Epoch 1488/2000
22/22 - 1s - loss: 1.1601 - val_loss: 1.6622
Epoch 1489/2000
22/22 - 1s - loss: 1.1596 - val_loss: 1.6615
Epoch 1490/2000
22/22 - 1s - loss: 1.1590 - val_loss: 1.6603
Epoch 01490: val_loss improved from 1.66759 to 1.66030, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1491/2000
22/22 - 1s - loss: 1.1554 - val_loss: 1.6615
Epoch 1492/2000
22/22 - 1s - loss: 1.1573 - val_loss: 1.6603
Epoch 1493/2000
22/22 - 1s - loss: 1.1563 - val_loss: 1.6590
Epoch 1494/2000
22/22 - 1s - loss: 1.1556 - val_loss: 1.6583
Epoch 1495/2000
22/22 - 1s - loss: 1.1553 - val_loss: 1.6581
Epoch 1496/2000
22/22 - 1s - loss: 1.1554 - val_loss: 1.6543
Epoch 1497/2000
22/22 - 1s - loss: 1.1543 - val_loss: 1.6545
Epoch 1498/2000
22/22 - 1s - loss: 1.1527 - val_loss: 1.6555
Epoch 1499/2000
22/22 - 1s - loss: 1.1511 - val_loss: 1.6538
Epoch 1500/2000
22/22 - 1s - loss: 1.1511 - val_loss: 1.6539
Epoch 01500: val_loss improved from 1.66030 to 1.65387, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1501/2000
22/22 - 1s - loss: 1.1520 - val_loss: 1.6543
Epoch 1502/2000
22/22 - 1s - loss: 1.1508 - val_loss: 1.6523
Epoch 1503/2000
22/22 - 1s - loss: 1.1478 - val_loss: 1.6545
Epoch 1504/2000
22/22 - 1s - loss: 1.1482 - val_loss: 1.6515
Epoch 1505/2000
22/22 - 1s - loss: 1.1476 - val_loss: 1.6506
Epoch 1506/2000
22/22 - 1s - loss: 1.1479 - val_loss: 1.6491
Epoch 1507/2000
22/22 - 1s - loss: 1.1454 - val_loss: 1.6495
Epoch 1508/2000
22/22 - 1s - loss: 1.1472 - val_loss: 1.6511
Epoch 1509/2000
22/22 - 1s - loss: 1.1459 - val_loss: 1.6493
Epoch 1510/2000
22/22 - 1s - loss: 1.1466 - val_loss: 1.6483
Epoch 01510: val_loss improved from 1.65387 to 1.64831, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1511/2000
22/22 - 1s - loss: 1.1461 - val_loss: 1.6469
Epoch 1512/2000
22/22 - 1s - loss: 1.1446 - val_loss: 1.6465
Epoch 1513/2000
22/22 - 1s - loss: 1.1437 - val_loss: 1.6464
Epoch 1514/2000
22/22 - 1s - loss: 1.1418 - val_loss: 1.6449
Epoch 1515/2000
22/22 - 1s - loss: 1.1439 - val_loss: 1.6451
Epoch 1516/2000
22/22 - 1s - loss: 1.1410 - val_loss: 1.6443
Epoch 1517/2000
22/22 - 1s - loss: 1.1410 - val_loss: 1.6446
Epoch 1518/2000
22/22 - 1s - loss: 1.1405 - val_loss: 1.6448
Epoch 1519/2000
22/22 - 1s - loss: 1.1397 - val_loss: 1.6432
Epoch 1520/2000
22/22 - 2s - loss: 1.1407 - val_loss: 1.6424
Epoch 01520: val_loss improved from 1.64831 to 1.64237, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1521/2000
22/22 - 1s - loss: 1.1366 - val_loss: 1.6398
Epoch 1522/2000
22/22 - 1s - loss: 1.1365 - val_loss: 1.6396
Epoch 1523/2000
22/22 - 1s - loss: 1.1358 - val_loss: 1.6385
Epoch 1524/2000
22/22 - 1s - loss: 1.1359 - val_loss: 1.6382
Epoch 1525/2000
22/22 - 1s - loss: 1.1348 - val_loss: 1.6389
Epoch 1526/2000
22/22 - 1s - loss: 1.1365 - val_loss: 1.6369
Epoch 1527/2000
22/22 - 1s - loss: 1.1350 - val_loss: 1.6356
Epoch 1528/2000
22/22 - 1s - loss: 1.1342 - val_loss: 1.6359
Epoch 1529/2000
22/22 - 1s - loss: 1.1331 - val_loss: 1.6354
Epoch 1530/2000
22/22 - 1s - loss: 1.1333 - val_loss: 1.6349
Epoch 01530: val_loss improved from 1.64237 to 1.63486, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1531/2000
22/22 - 1s - loss: 1.1313 - val_loss: 1.6361
Epoch 1532/2000
22/22 - 1s - loss: 1.1323 - val_loss: 1.6329
Epoch 1533/2000
22/22 - 1s - loss: 1.1316 - val_loss: 1.6350
Epoch 1534/2000
22/22 - 1s - loss: 1.1303 - val_loss: 1.6332
Epoch 1535/2000
22/22 - 1s - loss: 1.1299 - val_loss: 1.6324
Epoch 1536/2000
22/22 - 1s - loss: 1.1288 - val_loss: 1.6300
Epoch 1537/2000
22/22 - 1s - loss: 1.1265 - val_loss: 1.6318
Epoch 1538/2000
22/22 - 1s - loss: 1.1275 - val_loss: 1.6309
Epoch 1539/2000
22/22 - 1s - loss: 1.1256 - val_loss: 1.6288
Epoch 1540/2000
22/22 - 1s - loss: 1.1257 - val_loss: 1.6288
Epoch 01540: val_loss improved from 1.63486 to 1.62881, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1541/2000
22/22 - 1s - loss: 1.1237 - val_loss: 1.6266
Epoch 1542/2000
22/22 - 1s - loss: 1.1249 - val_loss: 1.6266
Epoch 1543/2000
22/22 - 1s - loss: 1.1251 - val_loss: 1.6248
Epoch 1544/2000
22/22 - 1s - loss: 1.1239 - val_loss: 1.6267
Epoch 1545/2000
22/22 - 1s - loss: 1.1227 - val_loss: 1.6254
Epoch 1546/2000
22/22 - 1s - loss: 1.1225 - val_loss: 1.6271
Epoch 1547/2000
22/22 - 1s - loss: 1.1228 - val_loss: 1.6256
Epoch 1548/2000
22/22 - 1s - loss: 1.1208 - val_loss: 1.6241
Epoch 1549/2000
22/22 - 1s - loss: 1.1209 - val_loss: 1.6230
Epoch 1550/2000
22/22 - 1s - loss: 1.1195 - val_loss: 1.6235
Epoch 01550: val_loss improved from 1.62881 to 1.62352, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1551/2000
22/22 - 1s - loss: 1.1201 - val_loss: 1.6223
Epoch 1552/2000
22/22 - 1s - loss: 1.1192 - val_loss: 1.6236
Epoch 1553/2000
22/22 - 1s - loss: 1.1160 - val_loss: 1.6234
Epoch 1554/2000
22/22 - 1s - loss: 1.1168 - val_loss: 1.6230
Epoch 1555/2000
22/22 - 1s - loss: 1.1185 - val_loss: 1.6228
Epoch 1556/2000
22/22 - 1s - loss: 1.1155 - val_loss: 1.6199
Epoch 1557/2000
22/22 - 1s - loss: 1.1159 - val_loss: 1.6194
Epoch 1558/2000
22/22 - 1s - loss: 1.1136 - val_loss: 1.6203
Epoch 1559/2000
22/22 - 1s - loss: 1.1149 - val_loss: 1.6200
Epoch 1560/2000
22/22 - 1s - loss: 1.1126 - val_loss: 1.6179
Epoch 01560: val_loss improved from 1.62352 to 1.61794, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1561/2000
22/22 - 1s - loss: 1.1134 - val_loss: 1.6166
Epoch 1562/2000
22/22 - 1s - loss: 1.1117 - val_loss: 1.6129
Epoch 1563/2000
22/22 - 1s - loss: 1.1148 - val_loss: 1.6143
Epoch 1564/2000
22/22 - 1s - loss: 1.1100 - val_loss: 1.6131
Epoch 1565/2000
22/22 - 1s - loss: 1.1092 - val_loss: 1.6134
Epoch 1566/2000
22/22 - 1s - loss: 1.1118 - val_loss: 1.6143
Epoch 1567/2000
22/22 - 1s - loss: 1.1111 - val_loss: 1.6140
Epoch 1568/2000
22/22 - 1s - loss: 1.1089 - val_loss: 1.6106
Epoch 1569/2000
22/22 - 1s - loss: 1.1077 - val_loss: 1.6100
Epoch 1570/2000
22/22 - 1s - loss: 1.1082 - val_loss: 1.6087
Epoch 01570: val_loss improved from 1.61794 to 1.60868, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1571/2000
22/22 - 1s - loss: 1.1058 - val_loss: 1.6076
Epoch 1572/2000
22/22 - 1s - loss: 1.1056 - val_loss: 1.6078
Epoch 1573/2000
22/22 - 1s - loss: 1.1050 - val_loss: 1.6081
Epoch 1574/2000
22/22 - 1s - loss: 1.1044 - val_loss: 1.6092
Epoch 1575/2000
22/22 - 1s - loss: 1.1033 - val_loss: 1.6085
Epoch 1576/2000
22/22 - 1s - loss: 1.1041 - val_loss: 1.6078
Epoch 1577/2000
22/22 - 1s - loss: 1.1017 - val_loss: 1.6082
Epoch 1578/2000
22/22 - 1s - loss: 1.1015 - val_loss: 1.6072
Epoch 1579/2000
22/22 - 1s - loss: 1.1022 - val_loss: 1.6050
Epoch 1580/2000
22/22 - 1s - loss: 1.1013 - val_loss: 1.6038
Epoch 01580: val_loss improved from 1.60868 to 1.60376, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1581/2000
22/22 - 1s - loss: 1.1004 - val_loss: 1.6044
Epoch 1582/2000
22/22 - 1s - loss: 1.0993 - val_loss: 1.6035
Epoch 1583/2000
22/22 - 1s - loss: 1.1004 - val_loss: 1.6035
Epoch 1584/2000
22/22 - 1s - loss: 1.0983 - val_loss: 1.6020
Epoch 1585/2000
22/22 - 1s - loss: 1.0968 - val_loss: 1.6025
Epoch 1586/2000
22/22 - 1s - loss: 1.0967 - val_loss: 1.6021
Epoch 1587/2000
22/22 - 1s - loss: 1.0977 - val_loss: 1.5996
Epoch 1588/2000
22/22 - 1s - loss: 1.0946 - val_loss: 1.6004
Epoch 1589/2000
22/22 - 1s - loss: 1.0941 - val_loss: 1.5997
Epoch 1590/2000
22/22 - 1s - loss: 1.0947 - val_loss: 1.5973
Epoch 01590: val_loss improved from 1.60376 to 1.59730, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1591/2000
22/22 - 1s - loss: 1.0945 - val_loss: 1.5975
Epoch 1592/2000
22/22 - 1s - loss: 1.0944 - val_loss: 1.6003
Epoch 1593/2000
22/22 - 1s - loss: 1.0931 - val_loss: 1.5981
Epoch 1594/2000
22/22 - 1s - loss: 1.0943 - val_loss: 1.5968
Epoch 1595/2000
22/22 - 1s - loss: 1.0917 - val_loss: 1.5947
Epoch 1596/2000
22/22 - 1s - loss: 1.0923 - val_loss: 1.5947
Epoch 1597/2000
22/22 - 1s - loss: 1.0902 - val_loss: 1.5958
Epoch 1598/2000
22/22 - 1s - loss: 1.0901 - val_loss: 1.5939
Epoch 1599/2000
22/22 - 1s - loss: 1.0904 - val_loss: 1.5942
Epoch 1600/2000
22/22 - 1s - loss: 1.0883 - val_loss: 1.5926
Epoch 01600: val_loss improved from 1.59730 to 1.59262, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1601/2000
22/22 - 1s - loss: 1.0902 - val_loss: 1.5932
Epoch 1602/2000
22/22 - 1s - loss: 1.0883 - val_loss: 1.5925
Epoch 1603/2000
22/22 - 1s - loss: 1.0883 - val_loss: 1.5883
Epoch 1604/2000
22/22 - 1s - loss: 1.0878 - val_loss: 1.5889
Epoch 1605/2000
22/22 - 1s - loss: 1.0843 - val_loss: 1.5897
Epoch 1606/2000
22/22 - 1s - loss: 1.0847 - val_loss: 1.5883
Epoch 1607/2000
22/22 - 1s - loss: 1.0868 - val_loss: 1.5878
Epoch 1608/2000
22/22 - 1s - loss: 1.0834 - val_loss: 1.5894
Epoch 1609/2000
22/22 - 1s - loss: 1.0865 - val_loss: 1.5879
Epoch 1610/2000
22/22 - 1s - loss: 1.0836 - val_loss: 1.5865
Epoch 01610: val_loss improved from 1.59262 to 1.58652, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1611/2000
22/22 - 1s - loss: 1.0821 - val_loss: 1.5862
Epoch 1612/2000
22/22 - 1s - loss: 1.0818 - val_loss: 1.5871
Epoch 1613/2000
22/22 - 1s - loss: 1.0833 - val_loss: 1.5858
Epoch 1614/2000
22/22 - 1s - loss: 1.0805 - val_loss: 1.5841
Epoch 1615/2000
22/22 - 1s - loss: 1.0801 - val_loss: 1.5847
Epoch 1616/2000
22/22 - 1s - loss: 1.0805 - val_loss: 1.5828
Epoch 1617/2000
22/22 - 1s - loss: 1.0794 - val_loss: 1.5835
Epoch 1618/2000
22/22 - 1s - loss: 1.0769 - val_loss: 1.5816
Epoch 1619/2000
22/22 - 1s - loss: 1.0792 - val_loss: 1.5829
Epoch 1620/2000
22/22 - 1s - loss: 1.0783 - val_loss: 1.5821
Epoch 01620: val_loss improved from 1.58652 to 1.58212, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1621/2000
22/22 - 1s - loss: 1.0783 - val_loss: 1.5822
Epoch 1622/2000
22/22 - 1s - loss: 1.0765 - val_loss: 1.5828
Epoch 1623/2000
22/22 - 1s - loss: 1.0775 - val_loss: 1.5800
Epoch 1624/2000
22/22 - 1s - loss: 1.0757 - val_loss: 1.5806
Epoch 1625/2000
22/22 - 1s - loss: 1.0755 - val_loss: 1.5800
Epoch 1626/2000
22/22 - 1s - loss: 1.0717 - val_loss: 1.5782
Epoch 1627/2000
22/22 - 1s - loss: 1.0732 - val_loss: 1.5765
Epoch 1628/2000
22/22 - 1s - loss: 1.0726 - val_loss: 1.5764
Epoch 1629/2000
22/22 - 1s - loss: 1.0731 - val_loss: 1.5747
Epoch 1630/2000
22/22 - 1s - loss: 1.0730 - val_loss: 1.5786
Epoch 01630: val_loss improved from 1.58212 to 1.57856, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1631/2000
22/22 - 1s - loss: 1.0718 - val_loss: 1.5760
Epoch 1632/2000
22/22 - 1s - loss: 1.0700 - val_loss: 1.5748
Epoch 1633/2000
22/22 - 1s - loss: 1.0715 - val_loss: 1.5775
Epoch 1634/2000
22/22 - 1s - loss: 1.0676 - val_loss: 1.5726
Epoch 1635/2000
22/22 - 1s - loss: 1.0688 - val_loss: 1.5725
Epoch 1636/2000
22/22 - 1s - loss: 1.0686 - val_loss: 1.5711
Epoch 1637/2000
22/22 - 1s - loss: 1.0673 - val_loss: 1.5719
Epoch 1638/2000
22/22 - 1s - loss: 1.0667 - val_loss: 1.5690
Epoch 1639/2000
22/22 - 1s - loss: 1.0672 - val_loss: 1.5721
Epoch 1640/2000
22/22 - 1s - loss: 1.0677 - val_loss: 1.5702
Epoch 01640: val_loss improved from 1.57856 to 1.57020, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1641/2000
22/22 - 1s - loss: 1.0634 - val_loss: 1.5684
Epoch 1642/2000
22/22 - 1s - loss: 1.0646 - val_loss: 1.5666
Epoch 1643/2000
22/22 - 1s - loss: 1.0645 - val_loss: 1.5675
Epoch 1644/2000
22/22 - 1s - loss: 1.0620 - val_loss: 1.5660
Epoch 1645/2000
22/22 - 1s - loss: 1.0645 - val_loss: 1.5674
Epoch 1646/2000
22/22 - 1s - loss: 1.0619 - val_loss: 1.5660
Epoch 1647/2000
22/22 - 1s - loss: 1.0612 - val_loss: 1.5661
Epoch 1648/2000
22/22 - 1s - loss: 1.0592 - val_loss: 1.5670
Epoch 1649/2000
22/22 - 1s - loss: 1.0615 - val_loss: 1.5658
Epoch 1650/2000
22/22 - 1s - loss: 1.0590 - val_loss: 1.5635
Epoch 01650: val_loss improved from 1.57020 to 1.56352, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1651/2000
22/22 - 1s - loss: 1.0586 - val_loss: 1.5630
Epoch 1652/2000
22/22 - 1s - loss: 1.0587 - val_loss: 1.5624
Epoch 1653/2000
22/22 - 1s - loss: 1.0582 - val_loss: 1.5620
Epoch 1654/2000
22/22 - 1s - loss: 1.0574 - val_loss: 1.5617
Epoch 1655/2000
22/22 - 1s - loss: 1.0573 - val_loss: 1.5631
Epoch 1656/2000
22/22 - 1s - loss: 1.0575 - val_loss: 1.5616
Epoch 1657/2000
22/22 - 1s - loss: 1.0550 - val_loss: 1.5597
Epoch 1658/2000
22/22 - 1s - loss: 1.0553 - val_loss: 1.5607
Epoch 1659/2000
22/22 - 1s - loss: 1.0546 - val_loss: 1.5589
Epoch 1660/2000
22/22 - 1s - loss: 1.0546 - val_loss: 1.5590
Epoch 01660: val_loss improved from 1.56352 to 1.55900, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1661/2000
22/22 - 1s - loss: 1.0533 - val_loss: 1.5581
Epoch 1662/2000
22/22 - 1s - loss: 1.0540 - val_loss: 1.5583
Epoch 1663/2000
22/22 - 1s - loss: 1.0520 - val_loss: 1.5563
Epoch 1664/2000
22/22 - 1s - loss: 1.0538 - val_loss: 1.5558
Epoch 1665/2000
22/22 - 1s - loss: 1.0513 - val_loss: 1.5564
Epoch 1666/2000
22/22 - 1s - loss: 1.0514 - val_loss: 1.5566
Epoch 1667/2000
22/22 - 1s - loss: 1.0489 - val_loss: 1.5553
Epoch 1668/2000
22/22 - 1s - loss: 1.0485 - val_loss: 1.5531
Epoch 1669/2000
22/22 - 1s - loss: 1.0490 - val_loss: 1.5537
Epoch 1670/2000
22/22 - 1s - loss: 1.0485 - val_loss: 1.5516
Epoch 01670: val_loss improved from 1.55900 to 1.55165, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1671/2000
22/22 - 1s - loss: 1.0477 - val_loss: 1.5525
Epoch 1672/2000
22/22 - 1s - loss: 1.0475 - val_loss: 1.5515
Epoch 1673/2000
22/22 - 1s - loss: 1.0485 - val_loss: 1.5526
Epoch 1674/2000
22/22 - 1s - loss: 1.0457 - val_loss: 1.5528
Epoch 1675/2000
22/22 - 1s - loss: 1.0450 - val_loss: 1.5505
Epoch 1676/2000
22/22 - 1s - loss: 1.0450 - val_loss: 1.5498
Epoch 1677/2000
22/22 - 1s - loss: 1.0441 - val_loss: 1.5507
Epoch 1678/2000
22/22 - 1s - loss: 1.0444 - val_loss: 1.5480
Epoch 1679/2000
22/22 - 1s - loss: 1.0440 - val_loss: 1.5469
Epoch 1680/2000
22/22 - 1s - loss: 1.0433 - val_loss: 1.5487
Epoch 01680: val_loss improved from 1.55165 to 1.54874, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1681/2000
22/22 - 1s - loss: 1.0420 - val_loss: 1.5465
Epoch 1682/2000
22/22 - 1s - loss: 1.0439 - val_loss: 1.5474
Epoch 1683/2000
22/22 - 1s - loss: 1.0405 - val_loss: 1.5455
Epoch 1684/2000
22/22 - 1s - loss: 1.0393 - val_loss: 1.5464
Epoch 1685/2000
22/22 - 1s - loss: 1.0391 - val_loss: 1.5446
Epoch 1686/2000
22/22 - 1s - loss: 1.0413 - val_loss: 1.5442
Epoch 1687/2000
22/22 - 1s - loss: 1.0406 - val_loss: 1.5446
Epoch 1688/2000
22/22 - 1s - loss: 1.0394 - val_loss: 1.5432
Epoch 1689/2000
22/22 - 1s - loss: 1.0382 - val_loss: 1.5435
Epoch 1690/2000
22/22 - 1s - loss: 1.0385 - val_loss: 1.5432
Epoch 01690: val_loss improved from 1.54874 to 1.54319, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1691/2000
22/22 - 1s - loss: 1.0368 - val_loss: 1.5408
Epoch 1692/2000
22/22 - 1s - loss: 1.0371 - val_loss: 1.5408
Epoch 1693/2000
22/22 - 1s - loss: 1.0365 - val_loss: 1.5406
Epoch 1694/2000
22/22 - 1s - loss: 1.0353 - val_loss: 1.5403
Epoch 1695/2000
22/22 - 1s - loss: 1.0355 - val_loss: 1.5404
Epoch 1696/2000
22/22 - 1s - loss: 1.0349 - val_loss: 1.5398
Epoch 1697/2000
22/22 - 1s - loss: 1.0328 - val_loss: 1.5384
Epoch 1698/2000
22/22 - 1s - loss: 1.0324 - val_loss: 1.5363
Epoch 1699/2000
22/22 - 1s - loss: 1.0340 - val_loss: 1.5359
Epoch 1700/2000
22/22 - 1s - loss: 1.0310 - val_loss: 1.5375
Epoch 01700: val_loss improved from 1.54319 to 1.53749, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1701/2000
22/22 - 1s - loss: 1.0314 - val_loss: 1.5379
Epoch 1702/2000
22/22 - 1s - loss: 1.0321 - val_loss: 1.5363
Epoch 1703/2000
22/22 - 1s - loss: 1.0290 - val_loss: 1.5362
Epoch 1704/2000
22/22 - 1s - loss: 1.0300 - val_loss: 1.5338
Epoch 1705/2000
22/22 - 1s - loss: 1.0317 - val_loss: 1.5337
Epoch 1706/2000
22/22 - 1s - loss: 1.0302 - val_loss: 1.5340
Epoch 1707/2000
22/22 - 1s - loss: 1.0299 - val_loss: 1.5330
Epoch 1708/2000
22/22 - 1s - loss: 1.0265 - val_loss: 1.5317
Epoch 1709/2000
22/22 - 1s - loss: 1.0290 - val_loss: 1.5299
Epoch 1710/2000
22/22 - 1s - loss: 1.0261 - val_loss: 1.5308
Epoch 01710: val_loss improved from 1.53749 to 1.53078, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1711/2000
22/22 - 1s - loss: 1.0270 - val_loss: 1.5319
Epoch 1712/2000
22/22 - 1s - loss: 1.0256 - val_loss: 1.5306
Epoch 1713/2000
22/22 - 1s - loss: 1.0250 - val_loss: 1.5283
Epoch 1714/2000
22/22 - 1s - loss: 1.0235 - val_loss: 1.5277
Epoch 1715/2000
22/22 - 1s - loss: 1.0239 - val_loss: 1.5283
Epoch 1716/2000
22/22 - 1s - loss: 1.0224 - val_loss: 1.5281
Epoch 1717/2000
22/22 - 1s - loss: 1.0227 - val_loss: 1.5273
Epoch 1718/2000
22/22 - 1s - loss: 1.0216 - val_loss: 1.5264
Epoch 1719/2000
22/22 - 1s - loss: 1.0214 - val_loss: 1.5266
Epoch 1720/2000
22/22 - 1s - loss: 1.0204 - val_loss: 1.5256
Epoch 01720: val_loss improved from 1.53078 to 1.52563, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1721/2000
22/22 - 1s - loss: 1.0218 - val_loss: 1.5249
Epoch 1722/2000
22/22 - 1s - loss: 1.0198 - val_loss: 1.5244
Epoch 1723/2000
22/22 - 1s - loss: 1.0187 - val_loss: 1.5239
Epoch 1724/2000
22/22 - 1s - loss: 1.0185 - val_loss: 1.5234
Epoch 1725/2000
22/22 - 1s - loss: 1.0189 - val_loss: 1.5247
Epoch 1726/2000
22/22 - 1s - loss: 1.0176 - val_loss: 1.5238
Epoch 1727/2000
22/22 - 1s - loss: 1.0187 - val_loss: 1.5223
Epoch 1728/2000
22/22 - 1s - loss: 1.0156 - val_loss: 1.5204
Epoch 1729/2000
22/22 - 1s - loss: 1.0176 - val_loss: 1.5236
Epoch 1730/2000
22/22 - 1s - loss: 1.0176 - val_loss: 1.5222
Epoch 01730: val_loss improved from 1.52563 to 1.52223, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1731/2000
22/22 - 1s - loss: 1.0156 - val_loss: 1.5209
Epoch 1732/2000
22/22 - 1s - loss: 1.0153 - val_loss: 1.5203
Epoch 1733/2000
22/22 - 1s - loss: 1.0136 - val_loss: 1.5209
Epoch 1734/2000
22/22 - 1s - loss: 1.0137 - val_loss: 1.5173
Epoch 1735/2000
22/22 - 1s - loss: 1.0141 - val_loss: 1.5172
Epoch 1736/2000
22/22 - 1s - loss: 1.0118 - val_loss: 1.5164
Epoch 1737/2000
22/22 - 1s - loss: 1.0140 - val_loss: 1.5171
Epoch 1738/2000
22/22 - 1s - loss: 1.0123 - val_loss: 1.5159
Epoch 1739/2000
22/22 - 1s - loss: 1.0104 - val_loss: 1.5164
Epoch 1740/2000
22/22 - 1s - loss: 1.0108 - val_loss: 1.5167
Epoch 01740: val_loss improved from 1.52223 to 1.51673, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1741/2000
22/22 - 1s - loss: 1.0094 - val_loss: 1.5159
Epoch 1742/2000
22/22 - 1s - loss: 1.0083 - val_loss: 1.5144
Epoch 1743/2000
22/22 - 1s - loss: 1.0093 - val_loss: 1.5145
Epoch 1744/2000
22/22 - 1s - loss: 1.0082 - val_loss: 1.5141
Epoch 1745/2000
22/22 - 1s - loss: 1.0072 - val_loss: 1.5115
Epoch 1746/2000
22/22 - 1s - loss: 1.0091 - val_loss: 1.5125
Epoch 1747/2000
22/22 - 1s - loss: 1.0080 - val_loss: 1.5117
Epoch 1748/2000
22/22 - 1s - loss: 1.0066 - val_loss: 1.5105
Epoch 1749/2000
22/22 - 1s - loss: 1.0062 - val_loss: 1.5127
Epoch 1750/2000
22/22 - 1s - loss: 1.0072 - val_loss: 1.5118
Epoch 01750: val_loss improved from 1.51673 to 1.51183, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1751/2000
22/22 - 1s - loss: 1.0057 - val_loss: 1.5117
Epoch 1752/2000
22/22 - 1s - loss: 1.0075 - val_loss: 1.5107
Epoch 1753/2000
22/22 - 1s - loss: 1.0037 - val_loss: 1.5085
Epoch 1754/2000
22/22 - 1s - loss: 1.0032 - val_loss: 1.5095
Epoch 1755/2000
22/22 - 1s - loss: 1.0029 - val_loss: 1.5079
Epoch 1756/2000
22/22 - 1s - loss: 1.0018 - val_loss: 1.5070
Epoch 1757/2000
22/22 - 1s - loss: 1.0000 - val_loss: 1.5080
Epoch 1758/2000
22/22 - 1s - loss: 1.0036 - val_loss: 1.5060
Epoch 1759/2000
22/22 - 1s - loss: 1.0015 - val_loss: 1.5059
Epoch 1760/2000
22/22 - 1s - loss: 1.0003 - val_loss: 1.5060
Epoch 01760: val_loss improved from 1.51183 to 1.50596, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1761/2000
22/22 - 1s - loss: 1.0020 - val_loss: 1.5042
Epoch 1762/2000
22/22 - 1s - loss: 1.0005 - val_loss: 1.5037
Epoch 1763/2000
22/22 - 1s - loss: 1.0006 - val_loss: 1.5035
Epoch 1764/2000
22/22 - 1s - loss: 0.9985 - val_loss: 1.5023
Epoch 1765/2000
22/22 - 1s - loss: 0.9958 - val_loss: 1.5029
Epoch 1766/2000
22/22 - 1s - loss: 0.9991 - val_loss: 1.5016
Epoch 1767/2000
22/22 - 1s - loss: 0.9979 - val_loss: 1.5013
Epoch 1768/2000
22/22 - 1s - loss: 0.9973 - val_loss: 1.5010
Epoch 1769/2000
22/22 - 1s - loss: 0.9961 - val_loss: 1.4999
Epoch 1770/2000
22/22 - 1s - loss: 0.9958 - val_loss: 1.5013
Epoch 01770: val_loss improved from 1.50596 to 1.50129, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1771/2000
22/22 - 1s - loss: 0.9933 - val_loss: 1.4996
Epoch 1772/2000
22/22 - 1s - loss: 0.9949 - val_loss: 1.4998
Epoch 1773/2000
22/22 - 1s - loss: 0.9955 - val_loss: 1.5004
Epoch 1774/2000
22/22 - 1s - loss: 0.9937 - val_loss: 1.4992
Epoch 1775/2000
22/22 - 1s - loss: 0.9934 - val_loss: 1.4970
Epoch 1776/2000
22/22 - 1s - loss: 0.9926 - val_loss: 1.4973
Epoch 1777/2000
22/22 - 1s - loss: 0.9917 - val_loss: 1.4959
Epoch 1778/2000
22/22 - 1s - loss: 0.9917 - val_loss: 1.4971
Epoch 1779/2000
22/22 - 1s - loss: 0.9922 - val_loss: 1.4972
Epoch 1780/2000
22/22 - 1s - loss: 0.9910 - val_loss: 1.4983
Epoch 01780: val_loss improved from 1.50129 to 1.49833, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1781/2000
22/22 - 1s - loss: 0.9904 - val_loss: 1.4950
Epoch 1782/2000
22/22 - 1s - loss: 0.9895 - val_loss: 1.4932
Epoch 1783/2000
22/22 - 1s - loss: 0.9895 - val_loss: 1.4938
Epoch 1784/2000
22/22 - 1s - loss: 0.9874 - val_loss: 1.4945
Epoch 1785/2000
22/22 - 1s - loss: 0.9889 - val_loss: 1.4923
Epoch 1786/2000
22/22 - 1s - loss: 0.9855 - val_loss: 1.4945
Epoch 1787/2000
22/22 - 1s - loss: 0.9885 - val_loss: 1.4918
Epoch 1788/2000
22/22 - 1s - loss: 0.9890 - val_loss: 1.4921
Epoch 1789/2000
22/22 - 1s - loss: 0.9853 - val_loss: 1.4911
Epoch 1790/2000
22/22 - 1s - loss: 0.9864 - val_loss: 1.4915
Epoch 01790: val_loss improved from 1.49833 to 1.49146, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1791/2000
22/22 - 1s - loss: 0.9863 - val_loss: 1.4896
Epoch 1792/2000
22/22 - 1s - loss: 0.9838 - val_loss: 1.4899
Epoch 1793/2000
22/22 - 1s - loss: 0.9831 - val_loss: 1.4901
Epoch 1794/2000
22/22 - 1s - loss: 0.9818 - val_loss: 1.4880
Epoch 1795/2000
22/22 - 1s - loss: 0.9825 - val_loss: 1.4884
Epoch 1796/2000
22/22 - 1s - loss: 0.9808 - val_loss: 1.4876
Epoch 1797/2000
22/22 - 1s - loss: 0.9820 - val_loss: 1.4887
Epoch 1798/2000
22/22 - 1s - loss: 0.9819 - val_loss: 1.4855
Epoch 1799/2000
22/22 - 1s - loss: 0.9793 - val_loss: 1.4845
Epoch 1800/2000
22/22 - 1s - loss: 0.9819 - val_loss: 1.4868
Epoch 01800: val_loss improved from 1.49146 to 1.48681, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1801/2000
22/22 - 1s - loss: 0.9801 - val_loss: 1.4855
Epoch 1802/2000
22/22 - 1s - loss: 0.9804 - val_loss: 1.4867
Epoch 1803/2000
22/22 - 1s - loss: 0.9792 - val_loss: 1.4867
Epoch 1804/2000
22/22 - 1s - loss: 0.9778 - val_loss: 1.4846
Epoch 1805/2000
22/22 - 1s - loss: 0.9789 - val_loss: 1.4841
Epoch 1806/2000
22/22 - 1s - loss: 0.9768 - val_loss: 1.4833
Epoch 1807/2000
22/22 - 1s - loss: 0.9755 - val_loss: 1.4843
Epoch 1808/2000
22/22 - 1s - loss: 0.9755 - val_loss: 1.4834
Epoch 1809/2000
22/22 - 1s - loss: 0.9773 - val_loss: 1.4804
Epoch 1810/2000
22/22 - 1s - loss: 0.9757 - val_loss: 1.4799
Epoch 01810: val_loss improved from 1.48681 to 1.47994, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1811/2000
22/22 - 1s - loss: 0.9746 - val_loss: 1.4815
Epoch 1812/2000
22/22 - 1s - loss: 0.9755 - val_loss: 1.4809
Epoch 1813/2000
22/22 - 1s - loss: 0.9734 - val_loss: 1.4791
Epoch 1814/2000
22/22 - 1s - loss: 0.9751 - val_loss: 1.4777
Epoch 1815/2000
22/22 - 1s - loss: 0.9719 - val_loss: 1.4790
Epoch 1816/2000
22/22 - 1s - loss: 0.9735 - val_loss: 1.4764
Epoch 1817/2000
22/22 - 1s - loss: 0.9728 - val_loss: 1.4787
Epoch 1818/2000
22/22 - 1s - loss: 0.9734 - val_loss: 1.4757
Epoch 1819/2000
22/22 - 1s - loss: 0.9712 - val_loss: 1.4767
Epoch 1820/2000
22/22 - 1s - loss: 0.9707 - val_loss: 1.4771
Epoch 01820: val_loss improved from 1.47994 to 1.47707, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1821/2000
22/22 - 1s - loss: 0.9678 - val_loss: 1.4746
Epoch 1822/2000
22/22 - 1s - loss: 0.9704 - val_loss: 1.4730
Epoch 1823/2000
22/22 - 1s - loss: 0.9689 - val_loss: 1.4750
Epoch 1824/2000
22/22 - 1s - loss: 0.9675 - val_loss: 1.4736
Epoch 1825/2000
22/22 - 1s - loss: 0.9685 - val_loss: 1.4736
Epoch 1826/2000
22/22 - 1s - loss: 0.9688 - val_loss: 1.4728
Epoch 1827/2000
22/22 - 1s - loss: 0.9686 - val_loss: 1.4735
Epoch 1828/2000
22/22 - 1s - loss: 0.9671 - val_loss: 1.4741
Epoch 1829/2000
22/22 - 1s - loss: 0.9650 - val_loss: 1.4706
Epoch 1830/2000
22/22 - 1s - loss: 0.9659 - val_loss: 1.4705
Epoch 01830: val_loss improved from 1.47707 to 1.47046, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1831/2000
22/22 - 1s - loss: 0.9668 - val_loss: 1.4695
Epoch 1832/2000
22/22 - 1s - loss: 0.9647 - val_loss: 1.4701
Epoch 1833/2000
22/22 - 1s - loss: 0.9651 - val_loss: 1.4710
Epoch 1834/2000
22/22 - 1s - loss: 0.9639 - val_loss: 1.4687
Epoch 1835/2000
22/22 - 1s - loss: 0.9657 - val_loss: 1.4681
Epoch 1836/2000
22/22 - 1s - loss: 0.9621 - val_loss: 1.4675
Epoch 1837/2000
22/22 - 1s - loss: 0.9647 - val_loss: 1.4676
Epoch 1838/2000
22/22 - 1s - loss: 0.9616 - val_loss: 1.4680
Epoch 1839/2000
22/22 - 1s - loss: 0.9618 - val_loss: 1.4674
Epoch 1840/2000
22/22 - 1s - loss: 0.9616 - val_loss: 1.4676
Epoch 01840: val_loss improved from 1.47046 to 1.46764, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1841/2000
22/22 - 1s - loss: 0.9596 - val_loss: 1.4656
Epoch 1842/2000
22/22 - 1s - loss: 0.9607 - val_loss: 1.4649
Epoch 1843/2000
22/22 - 1s - loss: 0.9600 - val_loss: 1.4648
Epoch 1844/2000
22/22 - 1s - loss: 0.9579 - val_loss: 1.4660
Epoch 1845/2000
22/22 - 1s - loss: 0.9592 - val_loss: 1.4642
Epoch 1846/2000
22/22 - 1s - loss: 0.9582 - val_loss: 1.4624
Epoch 1847/2000
22/22 - 1s - loss: 0.9575 - val_loss: 1.4620
Epoch 1848/2000
22/22 - 1s - loss: 0.9572 - val_loss: 1.4615
Epoch 1849/2000
22/22 - 1s - loss: 0.9554 - val_loss: 1.4635
Epoch 1850/2000
22/22 - 1s - loss: 0.9573 - val_loss: 1.4606
Epoch 01850: val_loss improved from 1.46764 to 1.46059, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1851/2000
22/22 - 1s - loss: 0.9563 - val_loss: 1.4606
Epoch 1852/2000
22/22 - 1s - loss: 0.9550 - val_loss: 1.4616
Epoch 1853/2000
22/22 - 1s - loss: 0.9552 - val_loss: 1.4594
Epoch 1854/2000
22/22 - 1s - loss: 0.9554 - val_loss: 1.4574
Epoch 1855/2000
22/22 - 1s - loss: 0.9529 - val_loss: 1.4578
Epoch 1856/2000
22/22 - 1s - loss: 0.9528 - val_loss: 1.4588
Epoch 1857/2000
22/22 - 1s - loss: 0.9527 - val_loss: 1.4570
Epoch 1858/2000
22/22 - 1s - loss: 0.9509 - val_loss: 1.4562
Epoch 1859/2000
22/22 - 1s - loss: 0.9525 - val_loss: 1.4575
Epoch 1860/2000
22/22 - 1s - loss: 0.9542 - val_loss: 1.4558
Epoch 01860: val_loss improved from 1.46059 to 1.45578, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1861/2000
22/22 - 1s - loss: 0.9516 - val_loss: 1.4559
Epoch 1862/2000
22/22 - 1s - loss: 0.9512 - val_loss: 1.4553
Epoch 1863/2000
22/22 - 1s - loss: 0.9493 - val_loss: 1.4553
Epoch 1864/2000
22/22 - 1s - loss: 0.9484 - val_loss: 1.4548
Epoch 1865/2000
22/22 - 1s - loss: 0.9496 - val_loss: 1.4549
Epoch 1866/2000
22/22 - 1s - loss: 0.9476 - val_loss: 1.4559
Epoch 1867/2000
22/22 - 1s - loss: 0.9496 - val_loss: 1.4530
Epoch 1868/2000
22/22 - 1s - loss: 0.9477 - val_loss: 1.4531
Epoch 1869/2000
22/22 - 1s - loss: 0.9481 - val_loss: 1.4536
Epoch 1870/2000
22/22 - 1s - loss: 0.9465 - val_loss: 1.4517
Epoch 01870: val_loss improved from 1.45578 to 1.45174, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1871/2000
22/22 - 1s - loss: 0.9468 - val_loss: 1.4516
Epoch 1872/2000
22/22 - 1s - loss: 0.9443 - val_loss: 1.4532
Epoch 1873/2000
22/22 - 1s - loss: 0.9469 - val_loss: 1.4519
Epoch 1874/2000
22/22 - 1s - loss: 0.9457 - val_loss: 1.4510
Epoch 1875/2000
22/22 - 1s - loss: 0.9444 - val_loss: 1.4495
Epoch 1876/2000
22/22 - 1s - loss: 0.9430 - val_loss: 1.4518
Epoch 1877/2000
22/22 - 1s - loss: 0.9435 - val_loss: 1.4494
Epoch 1878/2000
22/22 - 1s - loss: 0.9430 - val_loss: 1.4506
Epoch 1879/2000
22/22 - 1s - loss: 0.9435 - val_loss: 1.4494
Epoch 1880/2000
22/22 - 1s - loss: 0.9422 - val_loss: 1.4477
Epoch 01880: val_loss improved from 1.45174 to 1.44775, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1881/2000
22/22 - 1s - loss: 0.9418 - val_loss: 1.4466
Epoch 1882/2000
22/22 - 1s - loss: 0.9422 - val_loss: 1.4481
Epoch 1883/2000
22/22 - 1s - loss: 0.9412 - val_loss: 1.4466
Epoch 1884/2000
22/22 - 1s - loss: 0.9408 - val_loss: 1.4474
Epoch 1885/2000
22/22 - 1s - loss: 0.9391 - val_loss: 1.4446
Epoch 1886/2000
22/22 - 1s - loss: 0.9383 - val_loss: 1.4426
Epoch 1887/2000
22/22 - 1s - loss: 0.9386 - val_loss: 1.4443
Epoch 1888/2000
22/22 - 1s - loss: 0.9394 - val_loss: 1.4431
Epoch 1889/2000
22/22 - 1s - loss: 0.9401 - val_loss: 1.4451
Epoch 1890/2000
22/22 - 1s - loss: 0.9369 - val_loss: 1.4430
Epoch 01890: val_loss improved from 1.44775 to 1.44297, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1891/2000
22/22 - 1s - loss: 0.9398 - val_loss: 1.4422
Epoch 1892/2000
22/22 - 1s - loss: 0.9378 - val_loss: 1.4409
Epoch 1893/2000
22/22 - 1s - loss: 0.9369 - val_loss: 1.4412
Epoch 1894/2000
22/22 - 1s - loss: 0.9360 - val_loss: 1.4432
Epoch 1895/2000
22/22 - 1s - loss: 0.9363 - val_loss: 1.4426
Epoch 1896/2000
22/22 - 1s - loss: 0.9360 - val_loss: 1.4421
Epoch 1897/2000
22/22 - 1s - loss: 0.9351 - val_loss: 1.4415
Epoch 1898/2000
22/22 - 1s - loss: 0.9335 - val_loss: 1.4399
Epoch 1899/2000
22/22 - 1s - loss: 0.9340 - val_loss: 1.4401
Epoch 1900/2000
22/22 - 1s - loss: 0.9336 - val_loss: 1.4382
Epoch 01900: val_loss improved from 1.44297 to 1.43822, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1901/2000
22/22 - 1s - loss: 0.9337 - val_loss: 1.4369
Epoch 1902/2000
22/22 - 1s - loss: 0.9309 - val_loss: 1.4376
Epoch 1903/2000
22/22 - 1s - loss: 0.9316 - val_loss: 1.4377
Epoch 1904/2000
22/22 - 1s - loss: 0.9311 - val_loss: 1.4366
Epoch 1905/2000
22/22 - 1s - loss: 0.9297 - val_loss: 1.4371
Epoch 1906/2000
22/22 - 1s - loss: 0.9309 - val_loss: 1.4361
Epoch 1907/2000
22/22 - 1s - loss: 0.9296 - val_loss: 1.4349
Epoch 1908/2000
22/22 - 1s - loss: 0.9288 - val_loss: 1.4352
Epoch 1909/2000
22/22 - 1s - loss: 0.9279 - val_loss: 1.4341
Epoch 1910/2000
22/22 - 1s - loss: 0.9276 - val_loss: 1.4332
Epoch 01910: val_loss improved from 1.43822 to 1.43317, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1911/2000
22/22 - 1s - loss: 0.9277 - val_loss: 1.4328
Epoch 1912/2000
22/22 - 1s - loss: 0.9264 - val_loss: 1.4330
Epoch 1913/2000
22/22 - 1s - loss: 0.9269 - val_loss: 1.4323
Epoch 1914/2000
22/22 - 1s - loss: 0.9270 - val_loss: 1.4348
Epoch 1915/2000
22/22 - 1s - loss: 0.9265 - val_loss: 1.4328
Epoch 1916/2000
22/22 - 1s - loss: 0.9259 - val_loss: 1.4329
Epoch 1917/2000
22/22 - 1s - loss: 0.9252 - val_loss: 1.4323
Epoch 1918/2000
22/22 - 1s - loss: 0.9246 - val_loss: 1.4331
Epoch 1919/2000
22/22 - 1s - loss: 0.9225 - val_loss: 1.4316
Epoch 1920/2000
22/22 - 1s - loss: 0.9233 - val_loss: 1.4297
Epoch 01920: val_loss improved from 1.43317 to 1.42971, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1921/2000
22/22 - 1s - loss: 0.9245 - val_loss: 1.4307
Epoch 1922/2000
22/22 - 1s - loss: 0.9232 - val_loss: 1.4306
Epoch 1923/2000
22/22 - 1s - loss: 0.9229 - val_loss: 1.4305
Epoch 1924/2000
22/22 - 1s - loss: 0.9229 - val_loss: 1.4286
Epoch 1925/2000
22/22 - 1s - loss: 0.9227 - val_loss: 1.4272
Epoch 1926/2000
22/22 - 1s - loss: 0.9224 - val_loss: 1.4263
Epoch 1927/2000
22/22 - 1s - loss: 0.9213 - val_loss: 1.4292
Epoch 1928/2000
22/22 - 1s - loss: 0.9214 - val_loss: 1.4262
Epoch 1929/2000
22/22 - 1s - loss: 0.9187 - val_loss: 1.4273
Epoch 1930/2000
22/22 - 1s - loss: 0.9180 - val_loss: 1.4282
Epoch 01930: val_loss improved from 1.42971 to 1.42822, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1931/2000
22/22 - 1s - loss: 0.9220 - val_loss: 1.4281
Epoch 1932/2000
22/22 - 1s - loss: 0.9192 - val_loss: 1.4249
Epoch 1933/2000
22/22 - 1s - loss: 0.9189 - val_loss: 1.4243
Epoch 1934/2000
22/22 - 1s - loss: 0.9194 - val_loss: 1.4236
Epoch 1935/2000
22/22 - 1s - loss: 0.9188 - val_loss: 1.4233
Epoch 1936/2000
22/22 - 1s - loss: 0.9191 - val_loss: 1.4226
Epoch 1937/2000
22/22 - 1s - loss: 0.9166 - val_loss: 1.4222
Epoch 1938/2000
22/22 - 1s - loss: 0.9147 - val_loss: 1.4224
Epoch 1939/2000
22/22 - 1s - loss: 0.9167 - val_loss: 1.4240
Epoch 1940/2000
22/22 - 1s - loss: 0.9170 - val_loss: 1.4220
Epoch 01940: val_loss improved from 1.42822 to 1.42196, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1941/2000
22/22 - 1s - loss: 0.9150 - val_loss: 1.4210
Epoch 1942/2000
22/22 - 1s - loss: 0.9141 - val_loss: 1.4206
Epoch 1943/2000
22/22 - 1s - loss: 0.9123 - val_loss: 1.4196
Epoch 1944/2000
22/22 - 1s - loss: 0.9162 - val_loss: 1.4198
Epoch 1945/2000
22/22 - 1s - loss: 0.9137 - val_loss: 1.4204
Epoch 1946/2000
22/22 - 1s - loss: 0.9133 - val_loss: 1.4183
Epoch 1947/2000
22/22 - 1s - loss: 0.9141 - val_loss: 1.4194
Epoch 1948/2000
22/22 - 1s - loss: 0.9138 - val_loss: 1.4186
Epoch 1949/2000
22/22 - 1s - loss: 0.9111 - val_loss: 1.4171
Epoch 1950/2000
22/22 - 1s - loss: 0.9090 - val_loss: 1.4178
Epoch 01950: val_loss improved from 1.42196 to 1.41776, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1951/2000
22/22 - 1s - loss: 0.9109 - val_loss: 1.4168
Epoch 1952/2000
22/22 - 1s - loss: 0.9106 - val_loss: 1.4180
Epoch 1953/2000
22/22 - 1s - loss: 0.9094 - val_loss: 1.4183
Epoch 1954/2000
22/22 - 1s - loss: 0.9091 - val_loss: 1.4169
Epoch 1955/2000
22/22 - 1s - loss: 0.9099 - val_loss: 1.4167
Epoch 1956/2000
22/22 - 1s - loss: 0.9094 - val_loss: 1.4158
Epoch 1957/2000
22/22 - 1s - loss: 0.9095 - val_loss: 1.4146
Epoch 1958/2000
22/22 - 1s - loss: 0.9089 - val_loss: 1.4138
Epoch 1959/2000
22/22 - 1s - loss: 0.9076 - val_loss: 1.4144
Epoch 1960/2000
22/22 - 1s - loss: 0.9076 - val_loss: 1.4151
Epoch 01960: val_loss improved from 1.41776 to 1.41511, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1961/2000
22/22 - 1s - loss: 0.9054 - val_loss: 1.4143
Epoch 1962/2000
22/22 - 1s - loss: 0.9049 - val_loss: 1.4130
Epoch 1963/2000
22/22 - 1s - loss: 0.9060 - val_loss: 1.4115
Epoch 1964/2000
22/22 - 1s - loss: 0.9067 - val_loss: 1.4131
Epoch 1965/2000
22/22 - 1s - loss: 0.9057 - val_loss: 1.4113
Epoch 1966/2000
22/22 - 1s - loss: 0.9032 - val_loss: 1.4114
Epoch 1967/2000
22/22 - 1s - loss: 0.9027 - val_loss: 1.4110
Epoch 1968/2000
22/22 - 1s - loss: 0.9044 - val_loss: 1.4099
Epoch 1969/2000
22/22 - 1s - loss: 0.9030 - val_loss: 1.4096
Epoch 1970/2000
22/22 - 1s - loss: 0.9022 - val_loss: 1.4100
Epoch 01970: val_loss improved from 1.41511 to 1.41000, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1971/2000
22/22 - 1s - loss: 0.9006 - val_loss: 1.4083
Epoch 1972/2000
22/22 - 1s - loss: 0.9033 - val_loss: 1.4081
Epoch 1973/2000
22/22 - 1s - loss: 0.9002 - val_loss: 1.4067
Epoch 1974/2000
22/22 - 1s - loss: 0.9021 - val_loss: 1.4049
Epoch 1975/2000
22/22 - 1s - loss: 0.9011 - val_loss: 1.4085
Epoch 1976/2000
22/22 - 1s - loss: 0.9011 - val_loss: 1.4084
Epoch 1977/2000
22/22 - 1s - loss: 0.9000 - val_loss: 1.4083
Epoch 1978/2000
22/22 - 1s - loss: 0.8982 - val_loss: 1.4075
Epoch 1979/2000
22/22 - 1s - loss: 0.8999 - val_loss: 1.4065
Epoch 1980/2000
22/22 - 1s - loss: 0.9003 - val_loss: 1.4068
Epoch 01980: val_loss improved from 1.41000 to 1.40684, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1981/2000
22/22 - 1s - loss: 0.8971 - val_loss: 1.4058
Epoch 1982/2000
22/22 - 1s - loss: 0.8981 - val_loss: 1.4047
Epoch 1983/2000
22/22 - 1s - loss: 0.8985 - val_loss: 1.4066
Epoch 1984/2000
22/22 - 1s - loss: 0.8983 - val_loss: 1.4053
Epoch 1985/2000
22/22 - 1s - loss: 0.8956 - val_loss: 1.4033
Epoch 1986/2000
22/22 - 1s - loss: 0.8943 - val_loss: 1.4038
Epoch 1987/2000
22/22 - 1s - loss: 0.8957 - val_loss: 1.4032
Epoch 1988/2000
22/22 - 1s - loss: 0.8945 - val_loss: 1.4025
Epoch 1989/2000
22/22 - 1s - loss: 0.8952 - val_loss: 1.4024
Epoch 1990/2000
22/22 - 1s - loss: 0.8944 - val_loss: 1.4028
Epoch 01990: val_loss improved from 1.40684 to 1.40284, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1991/2000
22/22 - 1s - loss: 0.8945 - val_loss: 1.4011
Epoch 1992/2000
22/22 - 1s - loss: 0.8933 - val_loss: 1.3994
Epoch 1993/2000
22/22 - 1s - loss: 0.8937 - val_loss: 1.3993
Epoch 1994/2000
22/22 - 1s - loss: 0.8919 - val_loss: 1.3976
Epoch 1995/2000
22/22 - 1s - loss: 0.8923 - val_loss: 1.3982
Epoch 1996/2000
22/22 - 1s - loss: 0.8909 - val_loss: 1.3963
Epoch 1997/2000
22/22 - 1s - loss: 0.8936 - val_loss: 1.3989
Epoch 1998/2000
22/22 - 1s - loss: 0.8921 - val_loss: 1.3974
Epoch 1999/2000
22/22 - 1s - loss: 0.8892 - val_loss: 1.3969
Epoch 2000/2000
INFO     Computation time for training the single-label model for AR: 41.7 min
INFO     Network type: REG
INFO     Model: "sequential"
INFO     _________________________________________________________________
INFO     Layer (type)                 Output Shape              Param #
INFO     =================================================================
INFO     dense (Dense)                (None, 1024)              2098176
INFO     _________________________________________________________________
INFO     dropout (Dropout)            (None, 1024)              0
INFO     _________________________________________________________________
INFO     dense_1 (Dense)              (None, 512)               524800
INFO     _________________________________________________________________
INFO     dropout_1 (Dropout)          (None, 512)               0
INFO     _________________________________________________________________
INFO     dense_2 (Dense)              (None, 256)               131328
INFO     _________________________________________________________________
INFO     dropout_2 (Dropout)          (None, 256)               0
INFO     _________________________________________________________________
INFO     dense_3 (Dense)              (None, 128)               32896
INFO     _________________________________________________________________
INFO     dropout_3 (Dropout)          (None, 128)               0
INFO     _________________________________________________________________
INFO     dense_4 (Dense)              (None, 1)                 129
INFO     =================================================================
INFO     Total params: 2,787,329
INFO     Trainable params: 2,787,329
INFO     Non-trainable params: 0
INFO     _________________________________________________________________
INFO     Evaluating trained model 'AR single-labeled Fold-1' on test data
22/22 - 1s - loss: 0.8900 - val_loss: 1.3971
Epoch 02000: val_loss improved from 1.40284 to 1.39712, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
INFO     Training of fold number: 2
INFO     Training sample distribution: train data: {-1.2016383409500122: 4, -1.2016366720199585: 4, -1.20163094997406: 3, -1.2016355991363525: 3, -1.201636552810669: 3, -1.2016369104385376: 3, -1.2016327381134033: 3, -1.2016363143920898: 3, -1.2016351222991943: 2, -1.2016353607177734: 2, -1.2016345262527466: 2, -1.201621651649475: 2, -1.2015819549560547: 2, -1.201629877090454: 2, -1.2016304731369019: 2, -1.2016377449035645: 2, -1.201636791229248: 2, -1.2016303539276123: 2, -1.2016375064849854: 2, -1.2016384601593018: 2, -1.2016324996948242: 2, -1.201635479927063: 2, -1.2016295194625854: 2, -1.2009780406951904: 2, -1.2016326189041138: 2, -1.2016315460205078: 2, -1.2016159296035767: 2, -1.2016041278839111: 2, -1.2016254663467407: 2, -1.201637625694275: 2, 1.6950387954711914: 1, 1.4458893537521362: 1, 0.2713952362537384: 1, 0.5299685597419739: 1, -0.16027171909809113: 1, 0.002237366745248437: 1, -0.37317758798599243: 1, -0.5348793864250183: 1, 0.25592291355133057: 1, -0.2558962106704712: 1, 0.02149348333477974: 1, -0.2516374886035919: 1, 0.31096193194389343: 1, -1.0873279571533203: 1, 1.5720155239105225: 1, 0.7778733968734741: 1, 0.20332399010658264: 1, 1.2952117919921875: 1, 0.6442342400550842: 1, -0.9010018706321716: 1, 1.4237861633300781: 1, 1.4535586833953857: 1, 1.0667099952697754: 1, 0.6158161163330078: 1, 0.2900018095970154: 1, 1.1870161294937134: 1, 1.6106586456298828: 1, 0.5732383131980896: 1, -0.16630633175373077: 1, 0.06667295098304749: 1, -0.4970245659351349: 1, 0.2669691741466522: 1, -0.29657527804374695: 1, 0.21768306195735931: 1, 0.21878471970558167: 1, 1.1950836181640625: 1, 0.007953275926411152: 1, -0.5027830004692078: 1, -0.24468590319156647: 1, 1.478103518486023: 1, -0.399366557598114: 1, -1.1840753555297852: 1, 1.621954083442688: 1, 1.5633379220962524: 1, 1.593016266822815: 1, 0.32263097167015076: 1, 0.9703378081321716: 1, -1.2016290426254272: 1, 1.5667779445648193: 1, 1.1970174312591553: 1, -1.201623558998108: 1, 0.7218993902206421: 1, 1.5455868244171143: 1, 0.3489625155925751: 1, -0.3774075210094452: 1, -0.16336557269096375: 1, 1.8254425525665283: 1, 0.8313724994659424: 1, -0.7750424146652222: 1, -0.41311657428741455: 1, -0.3203604817390442: 1, 0.0860099047422409: 1, 0.6573249697685242: 1, -0.5330178141593933: 1, -0.008235386572778225: 1, 0.2470504194498062: 1, -0.297276109457016: 1, 1.5978947877883911: 1, 0.8825770616531372: 1, 0.32629087567329407: 1, 0.8396581411361694: 1, -1.160044550895691: 1, 0.16810350120067596: 1, 1.2379846572875977: 1, 0.0010552277090027928: 1, 0.6820655465126038: 1, 0.8028292059898376: 1, 0.6465133428573608: 1, 1.4755654335021973: 1, 1.166581153869629: 1, 1.457643747329712: 1, 1.2997812032699585: 1, 1.2994223833084106: 1, 0.5403873920440674: 1, 0.10213274508714676: 1, -0.17338967323303223: 1, 0.9650787115097046: 1, 0.31522658467292786: 1, -0.48075738549232483: 1, -0.2711203992366791: 1, 0.2743425965309143: 1, -1.1746125221252441: 1, 1.090896725654602: 1, -0.7282882928848267: 1, -1.1950759887695312: 1, 0.9149655103683472: 1, -0.993471086025238: 1, 0.17100460827350616: 1, -0.040320102125406265: 1, 1.286658525466919: 1, 0.8274267315864563: 1, 0.7171676754951477: 1, 0.6386443972587585: 1, 1.4500402212142944: 1, -0.24653010070323944: 1, 1.5124294757843018: 1, -0.22351212799549103: 1, 0.5170465111732483: 1, 0.21625953912734985: 1, -0.19042591750621796: 1, 0.5247108340263367: 1, -0.18418414890766144: 1, 0.1385408341884613: 1, -0.17784874141216278: 1, 1.595760464668274: 1, 1.6063342094421387: 1, 1.605971336364746: 1, 0.2661615014076233: 1, 0.2954026460647583: 1, 0.8664619326591492: 1, 0.04832748696208: 1, 1.6047093868255615: 1, 1.6089627742767334: 1, 1.5149681568145752: 1, 0.9998847246170044: 1, 0.19487899541854858: 1, -0.06838630884885788: 1, 0.31109386682510376: 1, 0.2610865831375122: 1, 0.17204155027866364: 1, -0.31671836972236633: 1, 0.643775999546051: 1, 0.29451489448547363: 1, 0.15692748129367828: 1, 0.36155056953430176: 1, -0.4993174076080322: 1, 0.015656888484954834: 1, -0.3344474732875824: 1, 0.34191834926605225: 1, 1.4653488397598267: 1, 0.3422311544418335: 1, 1.3042255640029907: 1, 1.571059226989746: 1, 1.2753889560699463: 1, 0.663663923740387: 1, 0.15129715204238892: 1, -1.194837212562561: 1, -0.30124133825302124: 1, -0.1421377956867218: 1, -0.6546655297279358: 1, 1.5627902746200562: 1, 1.1079081296920776: 1, -1.1962995529174805: 1, -1.1956098079681396: 1, -1.1269394159317017: 1, -1.1961579322814941: 1, -0.15980762243270874: 1, -0.8842195272445679: 1, 1.7539664506912231: 1, -0.005905755329877138: 1, -1.1402051448822021: 1, -1.1482487916946411: 1, -0.07565759867429733: 1, 0.8374537825584412: 1, 1.4295574426651: 1, -0.00951747503131628: 1, 1.4342314004898071: 1, -0.09802207350730896: 1, 0.28549933433532715: 1, -0.4268537759780884: 1, -1.2008297443389893: 1, -1.2009947299957275: 1, -1.1927686929702759: 1, -1.1962217092514038: 1, -1.1323087215423584: 1, -1.1476235389709473: 1, 0.5436715483665466: 1, -1.1639471054077148: 1, -1.2007761001586914: 1, -1.2015410661697388: 1, -1.045175313949585: 1, -1.1976145505905151: 1, -0.7745821475982666: 1, -1.1987890005111694: 1, -1.196489930152893: 1, -1.1852269172668457: 1, -1.1929398775100708: 1, -0.34237241744995117: 1, -1.1871466636657715: 1, -1.1237342357635498: 1, -1.1972938776016235: 1, 0.16395387053489685: 1, -0.29486238956451416: 1, -0.5042855143547058: 1, 1.8768579959869385: 1, 1.8398889303207397: 1, 1.8953936100006104: 1, -1.009895920753479: 1, -0.30134135484695435: 1, 0.7517246007919312: 1, -1.2016119956970215: 1, 1.5550175905227661: 1, -0.8897131681442261: 1, -0.116549052298069: 1, -0.47023993730545044: 1, 1.0860453844070435: 1, 1.0165932178497314: 1, 1.477781891822815: 1, -0.03796708956360817: 1, -0.408608615398407: 1, -1.1897622346878052: 1, -0.5717604756355286: 1, 1.7107861042022705: 1, 0.6560998558998108: 1, 1.4029184579849243: 1, 0.2407364845275879: 1, -0.3579169511795044: 1, -1.177890419960022: 1, 1.603068470954895: 1, -0.2865978181362152: 1, 1.4387927055358887: 1, 0.18236172199249268: 1, 1.2996296882629395: 1, 1.3062021732330322: 1, -0.37773171067237854: 1, -0.3334684669971466: 1, -0.9769929647445679: 1, 0.7675377726554871: 1, -0.4147615134716034: 1, -0.883184015750885: 1, -0.19289743900299072: 1, 0.0290671493858099: 1, -0.6085047125816345: 1, 1.520749807357788: 1, -0.37911587953567505: 1, 0.670230507850647: 1, -0.5828713178634644: 1, -1.2015936374664307: 1, -0.6465518474578857: 1, -1.1526696681976318: 1, -1.2014310359954834: 1, -1.0348321199417114: 1, 0.603252649307251: 1, -0.4781683385372162: 1, -1.2015955448150635: 1, -1.201597809791565: 1, 0.034827083349227905: 1, -0.9153497219085693: 1, -0.1883729249238968: 1, -1.2015149593353271: 1, -1.055851936340332: 1, -0.5025617480278015: 1, -1.2004907131195068: 1, -0.6686071157455444: 1, -1.1959139108657837: 1, -0.3525408208370209: 1, -0.506174623966217: 1, -0.4260459542274475: 1, 0.031881630420684814: 1, 0.007752139586955309: 1, -1.1937233209609985: 1, -1.1647279262542725: 1, 0.20390741527080536: 1, -1.2013626098632812: 1, -0.46576231718063354: 1, -0.916256844997406: 1, 0.339458167552948: 1, -1.137963891029358: 1, -0.9888867139816284: 1, -0.9931707978248596: 1, -1.2013576030731201: 1, -1.092740535736084: 1, -1.1730265617370605: 1, -0.8519163131713867: 1, 0.6741006970405579: 1, 0.9611315131187439: 1, -1.2007629871368408: 1, -1.2015974521636963: 1, -1.2015589475631714: 1, -1.1205617189407349: 1, -1.2015609741210938: 1, -0.2831217646598816: 1, 0.7523799538612366: 1, -0.8503693342208862: 1, -1.2014657258987427: 1, -1.186226725578308: 1, -1.2014594078063965: 1, 0.5888639092445374: 1, -1.1972260475158691: 1, -1.1996524333953857: 1, 0.39954620599746704: 1, -1.1674489974975586: 1, 1.14208984375: 1, -1.1830931901931763: 1, -0.6864959597587585: 1, -0.33821895718574524: 1, 0.4001854956150055: 1, -0.09781666100025177: 1, -0.4154701232910156: 1, -0.7713357210159302: 1, 0.1561044156551361: 1, 0.21984633803367615: 1, -1.1999688148498535: 1, -1.2006030082702637: 1, -1.2015992403030396: 1, 0.6647039651870728: 1, -1.201633095741272: 1, 0.46835482120513916: 1, -1.2016280889511108: 1, -1.201627254486084: 1, -1.1506352424621582: 1, -1.2014697790145874: 1, -1.201564908027649: 1, -1.2014739513397217: 1, -0.7784246206283569: 1, -1.0070439577102661: 1, -1.201596975326538: 1, -1.201622486114502: 1, -0.8657602667808533: 1, 1.7721431255340576: 1, 1.5277636051177979: 1, 0.6965684294700623: 1, -1.1412583589553833: 1, -1.0422825813293457: 1, -0.6316778063774109: 1, 0.7494425773620605: 1, -0.556195080280304: 1, -0.896551251411438: 1, 1.8447388410568237: 1, -1.1907743215560913: 1, -1.1941906213760376: 1, -1.111975073814392: 1, -0.26542410254478455: 1, -1.0090559720993042: 1, 1.3791615962982178: 1, 1.577986240386963: 1, 1.5516252517700195: 1, 0.7257186770439148: 1, -0.3594827950000763: 1, -0.41735291481018066: 1, -0.9047161340713501: 1, 0.10187211632728577: 1, 0.7908697128295898: 1, -0.5884833335876465: 1, -0.1989370882511139: 1, -1.0411947965621948: 1, -0.923246443271637: 1, -0.8935588002204895: 1, -1.0519613027572632: 1, -1.0768063068389893: 1, -1.0751264095306396: 1, -0.7096079587936401: 1, -0.35407769680023193: 1, 0.2903974652290344: 1, -1.193503737449646: 1, -0.930094301700592: 1, -0.6980454921722412: 1, -0.6311256289482117: 1, 1.192413568496704: 1, -0.9288858771324158: 1, -0.18136551976203918: 1, -0.2154475301504135: 1, -1.0634658336639404: 1, 1.6304298639297485: 1, -0.43377333879470825: 1, 0.006390336435288191: 1, -1.1358855962753296: 1, -0.5620161294937134: 1, -0.9981153011322021: 1, -0.9736310243606567: 1, 0.9790754318237305: 1, 0.20924636721611023: 1, 0.05307941138744354: 1, 0.46832725405693054: 1, -0.8993450403213501: 1, -1.1569617986679077: 1, 0.8691079020500183: 1, 1.3982725143432617: 1, -1.0213873386383057: 1, -1.1798655986785889: 1, -0.5144169330596924: 1, 1.2280430793762207: 1, 0.16367900371551514: 1, -1.1800106763839722: 1, 1.5657020807266235: 1, 1.9090871810913086: 1, 0.06941241025924683: 1, -1.1138004064559937: 1, -1.1072322130203247: 1, -1.1329883337020874: 1, 1.8358889818191528: 1, -0.07864277809858322: 1, -0.04164140671491623: 1, 0.4990178942680359: 1, -0.4610443115234375: 1, -1.0622771978378296: 1, -1.1442792415618896: 1, 0.2268732339143753: 1, -0.5990430116653442: 1, 0.15106460452079773: 1, 0.19889964163303375: 1, -0.12463472783565521: 1, -1.0407896041870117: 1, -1.1999518871307373: 1, 1.6431117057800293: 1, -1.0393953323364258: 1, -0.8472578525543213: 1, -1.024053692817688: 1, -0.9803085327148438: 1, 0.39953649044036865: 1, 0.4876076281070709: 1, 1.0163378715515137: 1, -0.233070969581604: 1, 1.4876383543014526: 1, 0.5211433172225952: 1, -0.9951556921005249: 1, 0.6749281883239746: 1, -1.1212905645370483: 1, -0.3465023636817932: 1, 1.3890480995178223: 1, -1.0539400577545166: 1, -0.7244925498962402: 1, -1.1898142099380493: 1, -0.8890491724014282: 1, -0.8917420506477356: 1, 1.1045739650726318: 1, 0.6570013761520386: 1, -1.0776159763336182: 1, -0.8936908841133118: 1, 0.638069212436676: 1, 0.7275790572166443: 1, 0.3124358654022217: 1, 1.556864857673645: 1, -0.20395949482917786: 1, -0.13872799277305603: 1, 0.3074534237384796: 1, 0.8999701142311096: 1, -1.1716605424880981: 1, -1.1999285221099854: 1, 1.0112850666046143: 1, -0.49170398712158203: 1, -0.974824070930481: 1, -0.9460977911949158: 1, -0.7166287899017334: 1, -0.67027348279953: 1, 1.8432141542434692: 1, 0.49092090129852295: 1, 0.04404761642217636: 1, 0.0663938894867897: 1, -0.9940837025642395: 1, -0.9299888610839844: 1, -0.608140766620636: 1, -1.0076677799224854: 1, 1.0116826295852661: 1, -0.9982122778892517: 1, -1.2001420259475708: 1, -1.2009141445159912: 1, -1.1325860023498535: 1, -0.36594024300575256: 1, -1.1659823656082153: 1, 0.17207567393779755: 1, -0.4354245066642761: 1, -1.1712636947631836: 1, -1.1646332740783691: 1, -1.194373369216919: 1, -0.9642779231071472: 1, -1.2012841701507568: 1, -0.19042661786079407: 1, -0.2147151529788971: 1, 0.30057549476623535: 1, -1.199107050895691: 1, 3.259727716445923: 1, 0.3200169503688812: 1, 1.4842547178268433: 1, -0.981871485710144: 1, -1.1791499853134155: 1, 0.2662027180194855: 1, -1.078048825263977: 1, 1.536348581314087: 1, 1.8480533361434937: 1, 1.0025136470794678: 1, 1.6904795169830322: 1, -1.0221163034439087: 1, -0.1391475349664688: 1, -1.094030499458313: 1, 0.5474697947502136: 1, -0.7628646492958069: 1, 1.114488959312439: 1, -0.8923998475074768: 1, 1.0554637908935547: 1, -0.700495719909668: 1, -0.9755853414535522: 1, 0.5357815027236938: 1, 0.6780659556388855: 1, 0.5222756862640381: 1, 0.9129876494407654: 1, 1.2882025241851807: 1, 0.11351441591978073: 1, 1.0355088710784912: 1, 0.27488669753074646: 1, 0.7110782265663147: 1, 0.1011820137500763: 1, 1.2984728813171387: 1, -0.5288078188896179: 1, -0.6162902116775513: 1, 1.4811328649520874: 1, -0.8986467123031616: 1, -1.1914112567901611: 1, 1.5811641216278076: 1, -0.25767362117767334: 1, -0.027011625468730927: 1, 1.3250715732574463: 1, 0.4746771454811096: 1, 0.8892757296562195: 1, 0.8741052150726318: 1, 0.8417104482650757: 1, -0.5022063255310059: 1, 0.003300704760476947: 1, 0.2808535397052765: 1, 1.3437533378601074: 1, -0.26631277799606323: 1, 0.9666041731834412: 1, 1.4656307697296143: 1, 0.16298139095306396: 1, 0.9499619007110596: 1, 0.6718612909317017: 1, -0.9881011247634888: 1, -1.2016340494155884: 1, 1.7417840957641602: 1, -0.7180438041687012: 1, -0.6611031889915466: 1, -0.7170498371124268: 1, 1.5755736827850342: 1, 1.9331234693527222: 1, 1.224327802658081: 1, -1.1970343589782715: 1, -0.030014334246516228: 1, 0.7825468182563782: 1, 1.4694101810455322: 1, 0.3604428768157959: 1, 1.427628755569458: 1, 0.699573814868927: 1, 0.3411409258842468: 1, 0.394859254360199: 1, -0.004194003064185381: 1, -0.4297850430011749: 1, -0.9218448400497437: 1, -0.059458885341882706: 1, -0.22176611423492432: 1, 0.13578376173973083: 1, 0.2726168930530548: 1, 0.03207547590136528: 1, 1.5805106163024902: 1, 0.4048600494861603: 1, 1.4308977127075195: 1, 1.573736548423767: 1, 1.2712597846984863: 1, 1.2730098962783813: 1, 1.3784377574920654: 1, 0.2308363914489746: 1, 1.1008855104446411: 1, 1.4938230514526367: 1, 1.1383615732192993: 1, 1.2588475942611694: 1, -0.24819114804267883: 1, 1.427193522453308: 1, -0.39828142523765564: 1, 1.3024239540100098: 1, 1.2427196502685547: 1, 0.71575528383255: 1, 1.6178103685379028: 1, -1.2016338109970093: 1, 0.15925046801567078: 1, -0.37734130024909973: 1, 1.0202414989471436: 1, 0.22981515526771545: 1, 0.10751932114362717: 1, -0.016411839053034782: 1, 0.46373531222343445: 1, -0.0960833728313446: 1, -0.8622487187385559: 1, 1.2659807205200195: 1, -0.8956496715545654: 1, -1.0024443864822388: 1, -0.38419657945632935: 1, -0.6876721978187561: 1, -0.27164560556411743: 1, 1.6284458637237549: 1, -0.48030614852905273: 1, 0.3286954462528229: 1, 1.9067574739456177: 1, 0.07571198046207428: 1, -0.8877851366996765: 1, -0.9158876538276672: 1, 0.342952162027359: 1, -0.9859256744384766: 1, 1.425097107887268: 1, 1.287703037261963: 1, 1.7851945161819458: 1, -0.9035985469818115: 1, 0.40835040807724: 1, -1.1544640064239502: 1, -0.03840658441185951: 1, -0.9793020486831665: 1, 1.324471354484558: 1, -1.0523524284362793: 1, -0.7811231017112732: 1, 0.4252239763736725: 1, 0.784543514251709: 1, -1.199570655822754: 1, -0.5242934823036194: 1, 0.9346560835838318: 1, -0.8747767210006714: 1, 1.4774726629257202: 1, -0.731924295425415: 1, -0.05734042823314667: 1, -0.45086827874183655: 1, -0.9131625890731812: 1, 1.1575602293014526: 1, 0.35307395458221436: 1, 1.4413039684295654: 1, -0.8234555125236511: 1, -0.8486149311065674: 1, -0.514695405960083: 1, 0.9138351678848267: 1, 0.36046868562698364: 1, -0.16177639365196228: 1, 0.3255096673965454: 1, 0.3698660731315613: 1, -0.539240837097168: 1, 0.2775695323944092: 1, 0.28860634565353394: 1, 0.2634084224700928: 1, -0.589444637298584: 1, -0.32526895403862: 1, 1.4644434452056885: 1, 1.053705096244812: 1, 1.001185417175293: 1, -1.031872034072876: 1, 0.1467430293560028: 1, 1.0683897733688354: 1, 0.32160520553588867: 1, -1.1940946578979492: 1, 0.1897212415933609: 1, -1.1507015228271484: 1, -0.9339156150817871: 1, -1.1607003211975098: 1, 1.1217374801635742: 1, -0.13197508454322815: 1, -1.07969331741333: 1, -0.14438967406749725: 1, -0.4185396134853363: 1, 0.7527873516082764: 1, 0.739153265953064: 1, 0.002877143444493413: 1, -0.047685928642749786: 1, 0.7724436521530151: 1, -0.11414719372987747: 1, -0.18094922602176666: 1, 0.9568199515342712: 1, -0.1799832135438919: 1, -0.07945768535137177: 1, -0.3424704372882843: 1, 1.5701237916946411: 1, -0.7447215914726257: 1, -0.807515025138855: 1, 0.28526046872138977: 1, -1.2016263008117676: 1, 1.3301595449447632: 1, 0.4860861599445343: 1, 1.6493014097213745: 1, -1.1416743993759155: 1, -0.9635469913482666: 1, -0.35984915494918823: 1, 1.6441566944122314: 1, -0.9704957008361816: 1, -1.102870225906372: 1, 1.5992790460586548: 1, -0.9806917309761047: 1, 0.454349547624588: 1, 1.2604477405548096: 1, 0.35712626576423645: 1, 1.9024626016616821: 1, -1.2014974355697632: 1, 0.11243647336959839: 1, -1.201619267463684: 1, 1.7208062410354614: 1, -0.962668240070343: 1, -1.2016352415084839: 1, -0.2495342493057251: 1, -0.26848453283309937: 1, 0.3117649555206299: 1, 1.76934015750885: 1, -1.2016271352767944: 1, 0.9834553003311157: 1, -1.2006548643112183: 1, -1.0099024772644043: 1, -1.1663979291915894: 1, 1.8360271453857422: 1, -1.2016189098358154: 1, 1.5793328285217285: 1, 0.42848649621009827: 1, -0.41689032316207886: 1, 1.8881990909576416: 1, 1.6134487390518188: 1, 1.5396013259887695: 1, -1.0555497407913208: 1, 1.0374422073364258: 1, 1.5698747634887695: 1, 1.9196945428848267: 1, -1.0791629552841187: 1, 1.1523829698562622: 1, 1.6013163328170776: 1, 0.3571586012840271: 1, 1.8800233602523804: 1, -0.7931265234947205: 1, 0.5326334834098816: 1, -0.9877029061317444: 1, 0.5422061085700989: 1, 1.5719680786132812: 1, -1.2016080617904663: 1, 0.23050570487976074: 1, 0.8521577715873718: 1, -0.009660118259489536: 1, -0.7990305423736572: 1, 1.901644229888916: 1, -1.0994056463241577: 1, 1.633592128753662: 1, 0.5419895052909851: 1, 1.3466922044754028: 1, 1.5722275972366333: 1, -0.8662946820259094: 1, 0.5880253314971924: 1, -1.088794231414795: 1, -0.7490406036376953: 1, -1.042400598526001: 1, 0.10260368138551712: 1, 0.6003371477127075: 1, 0.9758270978927612: 1, -0.253825306892395: 1, 0.14208731055259705: 1, 1.8529928922653198: 1, -0.10222127288579941: 1, 1.7350994348526: 1, -0.44384631514549255: 1, 1.5665374994277954: 1, -0.7435175180435181: 1, 0.2675780653953552: 1, 1.3883335590362549: 1, -0.180640310049057: 1, 1.590468406677246: 1, 0.5755087733268738: 1, 0.39559683203697205: 1, -0.1754150390625: 1, -0.4791189730167389: 1, 0.9761800765991211: 1, -0.2709258496761322: 1, 1.0254307985305786: 1, -0.6814844608306885: 1, 0.523788332939148: 1, 0.4076603651046753: 1, 1.8846700191497803: 1, 0.6905592679977417: 1, 1.1628241539001465: 1, -0.22097758948802948: 1, 1.4221618175506592: 1, -1.054260492324829: 1, -0.10395042598247528: 1, 0.7045637369155884: 1, -0.22210338711738586: 1, 0.2889866232872009: 1, 1.4337563514709473: 1, 1.6179100275039673: 1, 0.11675674468278885: 1, 0.3740043342113495: 1, -0.22299611568450928: 1, 0.056893277913331985: 1, 1.6735926866531372: 1, 0.0023630079813301563: 1, 0.7833142876625061: 1, -0.7948459982872009: 1, -0.7293344736099243: 1, 1.1236602067947388: 1, 0.26557838916778564: 1, 0.43645578622817993: 1, -0.2772659659385681: 1, 0.7481110692024231: 1, 0.04922454059123993: 1, -0.08275699615478516: 1, -0.0515512116253376: 1, 1.3716888427734375: 1, -0.5665901303291321: 1, 0.18436919152736664: 1, 1.5315228700637817: 1, 0.27008119225502014: 1, 0.7021965980529785: 1, -0.025178229436278343: 1, 1.7298698425292969: 1, 1.4182209968566895: 1, -0.2232077419757843: 1, 1.0101338624954224: 1, 0.9832457304000854: 1, 0.014584558084607124: 1, 0.19680047035217285: 1, 0.8425688147544861: 1, 0.5497986674308777: 1, 0.08808748424053192: 1, 1.6464146375656128: 1, -1.0590986013412476: 1, 1.6939563751220703: 1, 0.9952530264854431: 1, 0.26564425230026245: 1, 0.949316143989563: 1, 0.521833598613739: 1, 1.6153100728988647: 1, 1.865704894065857: 1, -0.31705647706985474: 1, 0.9889004230499268: 1, 0.8423707485198975: 1, 0.5330069065093994: 1, -0.2079317569732666: 1, 1.612230896949768: 1, -0.3399538993835449: 1, 0.723430871963501: 1, 0.9554869532585144: 1, 0.7741647958755493: 1, 1.6500415802001953: 1, 1.8263726234436035: 1, 1.7850080728530884: 1, 0.9454357028007507: 1, 0.8505056500434875: 1, -1.174880027770996: 1, -1.201361894607544: 1, 1.7429335117340088: 1, 1.2539737224578857: 1, -1.2015928030014038: 1, -1.2015763521194458: 1, 0.6071187853813171: 1, 1.3225599527359009: 1, 1.4854387044906616: 1, 0.9784976243972778: 1, 1.7376213073730469: 1, 1.0590068101882935: 1, 1.610649824142456: 1, 1.92928946018219: 1, -1.0813920497894287: 1, -1.0571757555007935: 1, 0.8005363941192627: 1, 0.10180643945932388: 1, -1.012891411781311: 1, -1.2016215324401855: 1, 0.963599681854248: 1, 0.6376197934150696: 1, 0.562964141368866: 1, -0.0861414223909378: 1, 1.271135687828064: 1, -0.43114355206489563: 1, -0.20698606967926025: 1, 0.20394694805145264: 1, 0.780617356300354: 1, 1.8291547298431396: 1, 1.6759966611862183: 1, 1.0316096544265747: 1, -1.2015867233276367: 1, -0.538144052028656: 1, 1.7389863729476929: 1, 1.896134853363037: 1, 1.519827961921692: 1, 1.3174397945404053: 1, 0.8646785616874695: 1, -0.7696746587753296: 1, 1.2727433443069458: 1, 0.5681759119033813: 1, 0.261216938495636: 1, 0.6801310777664185: 1, 0.7080846428871155: 1, -1.2015223503112793: 1, 0.4620521366596222: 1, -0.34224218130111694: 1, -0.06900987029075623: 1, -0.40272125601768494: 1, -1.130736231803894: 1, -0.24564200639724731: 1, -1.2007230520248413: 1, 0.9158507585525513: 1, -0.6896651983261108: 1, 1.3129916191101074: 1, 0.8895251154899597: 1, 1.5496872663497925: 1, -1.2016278505325317: 1, 0.4616207182407379: 1, -1.2016236782073975: 1, 0.8969712257385254: 1, -0.8653848767280579: 1, -0.7542659640312195: 1, -1.0611162185668945: 1, 0.5805153250694275: 1, 0.9156394600868225: 1, -0.9954119920730591: 1, -1.2016057968139648: 1, -0.2255825698375702: 1, 1.3490053415298462: 1, 1.0934252738952637: 1, -0.9789384603500366: 1, -1.14544677734375: 1, 0.9750880599021912: 1, 1.3510494232177734: 1, -0.6586742401123047: 1, -1.2011404037475586: 1, 1.339892864227295: 1, 1.1129239797592163: 1, -1.1302224397659302: 1, 1.3847272396087646: 1, 1.110692024230957: 1, 1.7776927947998047: 1, 1.2369016408920288: 1, 0.3816104531288147: 1, -1.201613187789917: 1, 1.127722144126892: 1, 1.6221997737884521: 1, -0.22918304800987244: 1, 1.446770191192627: 1, -0.2059621661901474: 1, 1.7338974475860596: 1, -1.1562846899032593: 1, 1.8416829109191895: 1, 1.7511584758758545: 1, 1.788615107536316: 1, 0.8217967748641968: 1, 0.63859623670578: 1, 0.8861773610115051: 1, -0.010684509761631489: 1, 1.887890100479126: 1, 0.09368380159139633: 1, 1.4813575744628906: 1, -0.907404899597168: 1, 1.2843722105026245: 1, 1.8285820484161377: 1, 1.4475048780441284: 1, -0.8602240085601807: 1, 0.05316608399152756: 1, 1.7397531270980835: 1, 1.2302463054656982: 1, 0.8335886001586914: 1, -0.8916471600532532: 1, 1.4848576784133911: 1, 1.8896540403366089: 1, -1.2014458179473877: 1, 0.6508381366729736: 1, 0.9216548800468445: 1, 0.8007993698120117: 1, -0.4143541157245636: 1, 1.6676998138427734: 1, -1.2014168500900269: 1, 0.9160022139549255: 1, -0.9175146222114563: 1, 0.10331395268440247: 1, 1.0329307317733765: 1, -0.796614408493042: 1, 1.473099708557129: 1, 0.46223318576812744: 1, -1.156775951385498: 1, 0.35647323727607727: 1, 1.2849032878875732: 1, 1.5142070055007935: 1, -0.420527845621109: 1, -0.5466570258140564: 1, 1.6787821054458618: 1, 1.8180814981460571: 1, 1.4667671918869019: 1, 1.455495834350586: 1, -0.21755054593086243: 1, -1.1579643487930298: 1, 0.8198267817497253: 1, 1.8067305088043213: 1, 0.12914451956748962: 1, -0.24759358167648315: 1, -0.655949056148529: 1, 1.8303353786468506: 1, 1.5742621421813965: 1, 1.0331618785858154: 1, 1.703546404838562: 1, -0.11489463597536087: 1, 1.746630072593689: 1, 1.5016316175460815: 1, 1.6772441864013672: 1, 5.270293235778809: 1, 0.7037346363067627: 1, -0.5458275079727173: 1, 1.0119178295135498: 1, 1.8225407600402832: 1, -0.1335328370332718: 1, 1.7602899074554443: 1, 1.8266348838806152: 1, 0.32922476530075073: 1, 1.7661612033843994: 1, 1.7499927282333374: 1, -0.7876200675964355: 1, 0.13984030485153198: 1, 1.9223994016647339: 1, 1.6278821229934692: 1, 1.7392624616622925: 1, 1.9158005714416504: 1, -0.7031784653663635: 1, 1.2115764617919922: 1, -0.558110237121582: 1, -0.47733497619628906: 1, -0.014680324122309685: 1, -0.24040797352790833: 1, -1.1957098245620728: 1, 0.3244344890117645: 1, -0.4622204601764679: 1, -0.37577909231185913: 1, 1.7544053792953491: 1, 0.2143784463405609: 1, 1.9232004880905151: 1, -0.3439752459526062: 1, -0.6786049008369446: 1, -0.07709828019142151: 1, -0.4369107186794281: 1, -0.48933231830596924: 1, -0.7822737693786621: 1, -0.032225385308265686: 1, -0.29665860533714294: 1, -0.9143604040145874: 1, -1.0387167930603027: 1, 1.2964091300964355: 1, 1.5233625173568726: 1, -0.466978520154953: 1, -0.9003047943115234: 1, 1.569981575012207: 1, -0.92970210313797: 1, 0.9121710062026978: 1, -0.23732632398605347: 1, -1.093284010887146: 1, 1.8227369785308838: 1, -0.16766004264354706: 1, 0.7785534858703613: 1, 1.456301212310791: 1, 1.542358636856079: 1, -0.6440175771713257: 1, -0.5345551371574402: 1, 1.2142442464828491: 1, -0.28641819953918457: 1, -0.9299617409706116: 1, -1.1991149187088013: 1, -0.2761753499507904: 1, -0.21501778066158295: 1, -0.06596078723669052: 1, 1.6570682525634766: 1, 1.8792171478271484: 1, -0.42767956852912903: 1, -0.0695866122841835: 1, 0.16830426454544067: 1, 0.3824501037597656: 1, 0.1589841991662979: 1, 0.7185215353965759: 1, 1.8601784706115723: 1, -0.3237372636795044: 1, 0.13262629508972168: 1, 1.629822015762329: 1, -0.8671026229858398: 1, 1.8596769571304321: 1, 1.2151012420654297: 1, -0.5378462672233582: 1, 1.6555308103561401: 1, 1.8707987070083618: 1, 0.43118155002593994: 1, 0.6026607155799866: 1, 1.6845048666000366: 1, -1.0817426443099976: 1, -1.2014148235321045: 1, -0.31910526752471924: 1, -1.193153738975525: 1, -0.5629591941833496: 1, -0.3858093321323395: 1, 1.3953920602798462: 1, 1.878305196762085: 1, -0.46113380789756775: 1, 1.5919677019119263: 1, 0.6444322466850281: 1, -0.3169466555118561: 1, -0.8443267941474915: 1, -1.0356733798980713: 1, -1.1333893537521362: 1, 1.052090048789978: 1, 1.4788439273834229: 1, 1.483720302581787: 1, 0.8612715601921082: 1, 0.16428887844085693: 1, -0.35491982102394104: 1, -0.5378177762031555: 1, -0.960394024848938: 1, -0.29989245533943176: 1, -1.1109116077423096: 1, 0.06790906190872192: 1, -0.2670007050037384: 1, 3.323413610458374: 1, 1.9010276794433594: 1, -0.007546336855739355: 1, 1.187366247177124: 1, -0.766767144203186: 1, -0.5713714957237244: 1, -1.0160380601882935: 1, -0.4835919141769409: 1, -0.13618627190589905: 1, 0.47734835743904114: 1, 0.9746946096420288: 1, -0.5030357241630554: 1, -0.5843047499656677: 1, -0.47762712836265564: 1, -0.9396678805351257: 1, 1.0024393796920776: 1, -0.1308683305978775: 1, -0.7269378900527954: 1, 1.6493046283721924: 1, 0.6463801264762878: 1, -0.5880576372146606: 1, -0.7417653203010559: 1, 0.4303920269012451: 1, -0.3907565474510193: 1, 1.491416096687317: 1, -1.1868388652801514: 1, -0.9987406134605408: 1, 1.4534815549850464: 1, -0.30685290694236755: 1, -0.46731990575790405: 1, 0.9851498603820801: 1, 0.672914445400238: 1, 0.2551988363265991: 1, 1.3037792444229126: 1, 1.6146701574325562: 1, 0.31867286562919617: 1, 1.7650138139724731: 1, -0.29448574781417847: 1, -0.8708242177963257: 1, -0.023513898253440857: 1, -1.0031712055206299: 1, -0.6234576106071472: 1, -0.014453819021582603: 1, -0.3702247440814972: 1, -0.8769359588623047: 1, 0.7159720659255981: 1, -0.0832226425409317: 1, 0.012689988128840923: 1, 1.5722923278808594: 1, -0.0211151335388422: 1, 0.19413244724273682: 1, 0.17652635276317596: 1, -0.8521113991737366: 1, 1.5933094024658203: 1, -0.2992294430732727: 1, 1.8017815351486206: 1, -0.4548023045063019: 1, 1.6648616790771484: 1, 0.7212937474250793: 1, -0.5381679534912109: 1, 0.5300026535987854: 1, -0.6655375957489014: 1, 0.3571058511734009: 1, 0.3796524703502655: 1, 0.04851381108164787: 1, -0.8232764601707458: 1, -0.3349834084510803: 1, 1.6888245344161987: 1, 0.6790488958358765: 1, 0.37810415029525757: 1, -1.0713788270950317: 1, -0.49563685059547424: 1, -0.43086937069892883: 1, -0.5209143161773682: 1, 1.5299123525619507: 1, -0.24634599685668945: 1, -1.1569743156433105: 1, -0.28438881039619446: 1, 0.5530001521110535: 1, 1.6552691459655762: 1, 1.0318180322647095: 1, 0.2092854529619217: 1, 0.15644948184490204: 1, 1.8505216836929321: 1, 0.3915187418460846: 1, -1.1991721391677856: 1, 1.5747997760772705: 1, 0.49887171387672424: 1, -0.01187801081687212: 1, 0.10666077584028244: 1, 1.4984081983566284: 1, -1.2015986442565918: 1, 0.956155002117157: 1, -0.9726563692092896: 1, 1.7555813789367676: 1, -1.1350090503692627: 1, -0.9834045767784119: 1, -0.5802597403526306: 1, -0.9971945881843567: 1, 0.2097160518169403: 1, -0.46826034784317017: 1, -0.9147067666053772: 1, 0.8109627962112427: 1, 2.0270323753356934: 1, -0.39972129464149475: 1, -0.5899453163146973: 1, -1.0319366455078125: 1, -1.1006834506988525: 1, -0.03495830297470093: 1, 1.6034691333770752: 1, 0.3640252351760864: 1, -0.9720814824104309: 1, -0.18857857584953308: 1, 1.5428107976913452: 1, 0.3213244378566742: 1, -1.0813374519348145: 1, 1.525660514831543: 1, -0.6562255024909973: 1, -0.9310538172721863: 1, 1.8530430793762207: 1, 0.46222802996635437: 1, 1.2401331663131714: 1, -1.2016234397888184: 1, 0.7334970831871033: 1, -0.2089931219816208: 1, -0.2558027505874634: 1, -0.024080123752355576: 1, -0.4433523118495941: 1, -0.6431723237037659: 1, -0.23398016393184662: 1, -0.4275071322917938: 1, 1.231010913848877: 1, -0.22727444767951965: 1, 1.2042120695114136: 1, -0.15924076735973358: 1, -0.8965012431144714: 1, 1.4624748229980469: 1, -1.2016310691833496: 1, 0.21643884479999542: 1, -0.27935805916786194: 1, -0.3255411982536316: 1, -0.061833277344703674: 1, 0.21555371582508087: 1, 0.3141234815120697: 1, -0.22874142229557037: 1, -1.2016154527664185: 1, -1.2016232013702393: 1, 0.04079057276248932: 1, -0.3426608741283417: 1, 0.6405824422836304: 1, -0.09393322467803955: 1, -0.8678878545761108: 1, 1.905008316040039: 1, -0.14373785257339478: 1, 0.611980676651001: 1, 0.5500550270080566: 1, 0.11937177926301956: 1, -0.13741447031497955: 1, -1.1205850839614868: 1, -1.0625981092453003: 1, 0.4937743842601776: 1, -0.7983001470565796: 1, -0.7234905958175659: 1, -0.9730278849601746: 1, 1.571593999862671: 1, -0.26545509696006775: 1, -0.9465891718864441: 1, -0.6922621726989746: 1, -0.26783594489097595: 1, -0.8816695213317871: 1, -0.7939702272415161: 1, -0.5884281992912292: 1, -1.1422733068466187: 1, 0.4647309482097626: 1, -0.29477736353874207: 1, -0.6015652418136597: 1, 0.012895430438220501: 1, -0.36108481884002686: 1, -0.539626955986023: 1, -0.27475300431251526: 1, -1.2016171216964722: 1, -0.30786851048469543: 1, -0.4168057143688202: 1, -0.906280517578125: 1, -0.7995052933692932: 1, -0.22242674231529236: 1, 1.7963730096817017: 1, -1.1617506742477417: 1, 1.5116616487503052: 1, -0.24450848996639252: 1, -0.3431845009326935: 1, -0.563433825969696: 1, -1.201271653175354: 1, 0.18035785853862762: 1, -0.4653182625770569: 1, -1.1008306741714478: 1, -0.37186357378959656: 1, -0.8952922821044922: 1, -0.9930511116981506: 1, 0.7230984568595886: 1, -1.1925454139709473: 1, -0.7790790796279907: 1, -1.1515345573425293: 1, -1.1207038164138794: 1, 3.341240167617798: 1, -1.1624175310134888: 1, -1.2015869617462158: 1, -1.1236215829849243: 1, -0.8602992296218872: 1, -1.201583981513977: 1, -0.3992172181606293: 1, -1.2016146183013916: 1, -1.1971925497055054: 1, 0.5123543739318848: 1, 0.5790113210678101: 1, -1.1232612133026123: 1, -0.8350648880004883: 1, -1.188301682472229: 1, -1.1760764122009277: 1, -0.2970311641693115: 1, -1.1694631576538086: 1, -0.7289202809333801: 1, -1.2002716064453125: 1, -1.199577808380127: 1, -1.201569676399231: 1, -1.1498054265975952: 1, -1.1406184434890747: 1, -0.9465845823287964: 1, 4.282702445983887: 1, -1.1953339576721191: 1, -1.1108695268630981: 1, -1.0460647344589233: 1, -1.1475187540054321: 1, 0.9462845921516418: 1, -1.2015608549118042: 1, -1.1969398260116577: 1, -1.201536774635315: 1, -1.1448140144348145: 1, 0.8056516647338867: 1, -1.201555848121643: 1, -0.9751223921775818: 1, 0.31239357590675354: 1, 0.8865175843238831: 1, -1.086830973625183: 1, -1.169080138206482: 1, 0.264765202999115: 1, -0.8984062075614929: 1, 0.7242860198020935: 1, -1.2016023397445679: 1, 0.5111210942268372: 1, 0.7875488996505737: 1, 0.8666924238204956: 1, -0.16389766335487366: 1, 1.488932728767395: 1, 0.8550933599472046: 1, -1.2011059522628784: 1, -0.05180063098669052: 1, -0.8982016444206238: 1, -0.35893186926841736: 1, -1.2001534700393677: 1, -0.12521179020404816: 1, -0.5657321810722351: 1, -0.9202075004577637: 1, -1.186597228050232: 1, -0.5369265675544739: 1, -1.193078875541687: 1, -1.192414402961731: 1, 0.19746625423431396: 1, 1.1413462162017822: 1, -0.6031686067581177: 1, 1.25115966796875: 1, 0.3299405574798584: 1, 0.9907589554786682: 1, -1.2016347646713257: 1, -0.861803412437439: 1, -1.187011957168579: 1, -0.671938955783844: 1, -0.7864221334457397: 1, -1.1379677057266235: 1, -0.7683218121528625: 1, 0.8018452525138855: 1, 0.1461368203163147: 1, 1.2778337001800537: 1, -1.051085352897644: 1, -0.03033183142542839: 1, 0.3790889084339142: 1, -1.1906999349594116: 1, -0.40984249114990234: 1, -0.04764068126678467: 1, -0.6981508731842041: 1, -0.01552529539912939: 1, -0.9249427914619446: 1, -0.17132940888404846: 1, -1.20045006275177: 1, 0.944926917552948: 1, 0.5761882066726685: 1, -1.2010724544525146: 1, 0.013616573065519333: 1, -1.1908975839614868: 1, -0.5374748706817627: 1, -0.171332448720932: 1, -0.8429122567176819: 1, 0.040903303772211075: 1, 1.257509708404541: 1, 0.6273993253707886: 1, 0.8944936394691467: 1, -0.7482910752296448: 1, -1.1091188192367554: 1, -1.15325927734375: 1, -1.0079307556152344: 1, -1.1992580890655518: 1, 0.9220188856124878: 1, -0.4707425832748413: 1, -1.0164505243301392: 1, -0.1598413586616516: 1, -1.1632294654846191: 1, -1.1747305393218994: 1, -0.20172715187072754: 1, 0.4001394212245941: 1, -0.7597726583480835: 1, -0.4824763238430023: 1, -0.4976504147052765: 1, -1.1774768829345703: 1, 0.3949566185474396: 1, -1.1074978113174438: 1, 0.6585915088653564: 1, -1.1735186576843262: 1, -0.15097910165786743: 1, 0.6907184720039368: 1, 1.282943606376648: 1, -0.3503449857234955: 1, -1.2002339363098145: 1, -0.2299073189496994: 1, 0.9924389719963074: 1, 0.9692907333374023: 1, -1.0224525928497314: 1, -0.21156159043312073: 1, -1.144519329071045: 1, -1.1276121139526367: 1, 0.004596139770001173: 1, -0.042717207223176956: 1, -0.6867349743843079: 1, -1.2015101909637451: 1, -0.39157962799072266: 1, -0.09844925999641418: 1, -1.1992988586425781: 1, -0.4774172008037567: 1, 1.0670119524002075: 1, -1.1760457754135132: 1, 1.2671806812286377: 1, 1.229008674621582: 1, 0.5329916477203369: 1, -0.3316475749015808: 1, 0.21256738901138306: 1, -0.8594576716423035: 1, -0.35862404108047485: 1, -1.1689379215240479: 1, -0.8211961388587952: 1, 0.369517058134079: 1, 1.2405850887298584: 1, 0.5858985185623169: 1, 0.48456287384033203: 1, -0.30494359135627747: 1, 0.7670885920524597: 1, -0.4656817615032196: 1, -1.0180860757827759: 1, -0.5976389050483704: 1, -1.1941806077957153: 1, 1.2313082218170166: 1, -0.29541367292404175: 1, -0.7601802349090576: 1, -0.5920382142066956: 1, 0.669349730014801: 1, 0.5067287683486938: 1, 1.0160198211669922: 1, 0.5890273451805115: 1, 0.8621184229850769: 1, -0.8395327925682068: 1, 0.050834186375141144: 1, -1.192929744720459: 1, -0.7811260223388672: 1, -0.7514510154724121: 1, -1.09720778465271: 1, -0.06760650873184204: 1, -1.083876609802246: 1, -0.5272284746170044: 1, -0.32589226961135864: 1, -1.0825824737548828: 1, -1.1549781560897827: 1, 1.177069067955017: 1, -0.8366453051567078: 1, 1.1708914041519165: 1, 0.8209275603294373: 1, -0.41031748056411743: 1, -0.3642917275428772: 1, -0.5142630338668823: 1, -0.005674127489328384: 1, 0.7907249331474304: 1, -0.13443662226200104: 1, 0.03998654708266258: 1, -0.002722974168136716: 1, -1.1580485105514526: 1, 0.8628989458084106: 1, -1.1866058111190796: 1, -0.5910298824310303: 1, -0.5807573795318604: 1, 1.0215860605239868: 1, 1.1708778142929077: 1, 0.3778545558452606: 1, 0.9156388640403748: 1, -0.14771254360675812: 1, -1.1707801818847656: 1, -0.2519146203994751: 1, 0.6744810342788696: 1, 0.9348665475845337: 1, 0.6152291297912598: 1, -1.0131398439407349: 1, -0.003650385420769453: 1, -0.3348834216594696: 1, 1.182131290435791: 1, -0.15388239920139313: 1, 0.05971863865852356: 1, -0.12709279358386993: 1, 1.1462621688842773: 1, -1.1966403722763062: 1, -0.8245752453804016: 1, 0.2748369872570038: 1, -0.21517714858055115: 1, 0.9726781249046326: 1, -0.12876513600349426: 1, 1.30207359790802: 1, -0.32544630765914917: 1, -0.2588912844657898: 1, -0.024461327120661736: 1, -0.01721060648560524: 1, -0.2265346348285675: 1, -0.9494366645812988: 1, -0.18796367943286896: 1, 1.043671727180481: 1, -0.3034926950931549: 1, -0.06601618975400925: 1, 0.7746310830116272: 1, -1.1586220264434814: 1, -0.3917173445224762: 1, 0.2549906373023987: 1, 0.6609118580818176: 1, -0.6155209541320801: 1, -0.24131079018115997: 1, -0.048064157366752625: 1, -0.06410756707191467: 1, -0.42906203866004944: 1, 0.008013189770281315: 1, 1.3108209371566772: 1, -0.016240552067756653: 1, -0.1383906453847885: 1, 0.7786849141120911: 1, -0.19042982161045074: 1, -0.257107138633728: 1, -0.5744653940200806: 1, -0.27063482999801636: 1, 0.5745441317558289: 1, 0.2699461281299591: 1, -0.024909501895308495: 1, 0.5615567564964294: 1, 0.09289468824863434: 1, 1.2464758157730103: 1, 0.8297379016876221: 1, -0.27802959084510803: 1, 0.05186900869011879: 1, -0.5160067081451416: 1, -1.0983095169067383: 1, 1.2668349742889404: 1, 1.2882169485092163: 1, 0.2788977324962616: 1, -0.1045512706041336: 1, -0.4809620976448059: 1, -0.01294313371181488: 1, 1.0150052309036255: 1, -0.0404001921415329: 1, 0.6649335622787476: 1, -0.11206144839525223: 1, -0.7266088128089905: 1, 0.022439440712332726: 1, -0.3780987560749054: 1, 1.1997345685958862: 1, -0.36745402216911316: 1, -0.26567548513412476: 1, -0.19212104380130768: 1, 0.41225412487983704: 1, 0.15262554585933685: 1, -0.09314227104187012: 1, 1.274375557899475: 1, 1.0399528741836548: 1, 0.7346874475479126: 1, -0.6019781231880188: 1, -0.7642948627471924: 1, 0.0014178809942677617: 1, -0.7650251388549805: 1, -1.1914066076278687: 1, 1.1363762617111206: 1, -1.2016372680664062: 1, -1.1513574123382568: 1, 0.8519235253334045: 1, -0.03992554917931557: 1, -0.07969757169485092: 1, -1.0076838731765747: 1, -0.2553582787513733: 1, -0.2878003716468811: 1, -1.1991511583328247: 1, -0.16514527797698975: 1, 1.023429274559021: 1, -0.7420557737350464: 1, -0.3634156584739685: 1, -1.0054869651794434: 1, -0.9430715441703796: 1, -0.3076569437980652: 1, -1.1654343605041504: 1, -0.27813780307769775: 1, -0.9743238091468811: 1, -0.10418325662612915: 1, 1.1114397048950195: 1, 0.68455970287323: 1, -0.05808640271425247: 1, 0.03431294485926628: 1, -1.1873303651809692: 1, -1.200080394744873: 1, -0.35624369978904724: 1, -0.3301823139190674: 1, -1.1322532892227173: 1, -1.192280888557434: 1, -0.25587567687034607: 1, 1.0084635019302368: 1, -0.0674244612455368: 1, -1.006115436553955: 1, 1.0866621732711792: 1, -0.11618001013994217: 1, -0.26975223422050476: 1, -0.9567206501960754: 1, -0.29025569558143616: 1, -0.07907160371541977: 1, -1.0697368383407593: 1, 0.12906195223331451: 1, -0.08915898948907852: 1, 0.021945519372820854: 1, 0.004652172327041626: 1, -0.32367783784866333: 1, -0.23161835968494415: 1, 0.9278587102890015: 1, -0.19999556243419647: 1, 0.26051849126815796: 1, 0.053155358880758286: 1, 1.0157313346862793: 1, -0.056251220405101776: 1, 1.1558300256729126: 1, -1.1260875463485718: 1, 0.048983871936798096: 1, -0.15591250360012054: 1, -0.5908447504043579: 1, 1.2993144989013672: 1, 0.5070648193359375: 1, 0.05442709103226662: 1, -1.1338447332382202: 1, -0.5088394284248352: 1, 1.2035483121871948: 1, 1.0808086395263672: 1, -1.2008949518203735: 1, -0.455705851316452: 1, -0.9566242694854736: 1, 0.033837273716926575: 1, -0.5931430459022522: 1, 1.0766181945800781: 1, -0.22990889847278595: 1, 1.2829649448394775: 1, -0.23146884143352509: 1, 0.03461417928338051: 1, 0.7100340127944946: 1, 0.9881972074508667: 1, 1.2256038188934326: 1, -0.44190311431884766: 1, 1.2598285675048828: 1, -1.193282127380371: 1, -0.6325321793556213: 1, -1.194840431213379: 1, -1.0467379093170166: 1, -1.0309724807739258: 1, -0.8834558129310608: 1, -0.6588079929351807: 1, -1.1501390933990479: 1, -1.201562523841858: 1, -0.6635865569114685: 1, -0.5772318840026855: 1, -0.2236318439245224: 1, -1.200749397277832: 1, -1.17979097366333: 1, -0.4475323557853699: 1, -1.0135008096694946: 1, -0.63347989320755: 1, -0.5631559491157532: 1, -0.7443411946296692: 1, -0.5115338563919067: 1, -1.167817234992981: 1, -1.1509727239608765: 1, -0.5520062446594238: 1, -1.0223268270492554: 1, -0.7736660242080688: 1, -0.41631850600242615: 1, -1.0399388074874878: 1, -0.8366922736167908: 1, -0.7228063344955444: 1, -0.9656466245651245: 1, -1.0281414985656738: 1, -0.694696843624115: 1, -1.05448579788208: 1, -0.6411266922950745: 1, -0.4774998426437378: 1, -1.2015254497528076: 1, -1.1344302892684937: 1, -0.2650972902774811: 1, -0.4338090121746063: 1, -1.1520825624465942: 1, -0.9714952111244202: 1, -0.712040364742279: 1, -1.180148720741272: 1, -1.2008585929870605: 1, -0.5804309248924255: 1, -0.9933805465698242: 1, -1.2016000747680664: 1, -1.1563661098480225: 1, -0.9732658267021179: 1, -1.1349726915359497: 1, -1.1859160661697388: 1, -1.1816785335540771: 1, -0.17838822305202484: 1, -0.5632508397102356: 1, -1.1857632398605347: 1, -1.198327660560608: 1, -0.40758535265922546: 1, -1.091170310974121: 1, -1.175873041152954: 1, -1.1691778898239136: 1, -1.201621413230896: 1, -0.862193763256073: 1, -1.0810678005218506: 1, -1.126016616821289: 1, -0.6395750641822815: 1, -1.1800310611724854: 1, -1.194927453994751: 1, -1.1923733949661255: 1, -1.162119746208191: 1, -1.2006478309631348: 1, -1.1858601570129395: 1, -0.8829452991485596: 1, -1.1954984664916992: 1, -0.6137503981590271: 1, -0.788809597492218: 1, -1.0831377506256104: 1, -1.1470420360565186: 1, -1.161582589149475: 1, -1.0068711042404175: 1, -0.7897263169288635: 1, -1.1350377798080444: 1, -0.5385211110115051: 1, -1.0538722276687622: 1, -1.2014809846878052: 1, -0.4824954569339752: 1, -1.1754673719406128: 1, 5.037554740905762: 1, 1.7253838777542114: 1, -0.4444347023963928: 1, -0.7420345544815063: 1, -0.21827441453933716: 1, -0.7001152634620667: 1, -0.9021695256233215: 1, -1.0589720010757446: 1, -1.1036909818649292: 1, -0.25539615750312805: 1, -1.1933567523956299: 1, -0.86516273021698: 1, -1.0501618385314941: 1, -1.1981457471847534: 1, -0.9225814342498779: 1, -0.9485399723052979: 1, 5.012721538543701: 1, 5.006033897399902: 1, -0.3156169652938843: 1, -1.1136521100997925: 1, 0.057195477187633514: 1, 4.863577365875244: 1, -0.9046985507011414: 1, -0.622269868850708: 1, 5.001932144165039: 1, 4.900933742523193: 1, 4.722275257110596: 1, 0.939198911190033: 1, -1.1445417404174805: 1, -0.7939543128013611: 1, 4.58308219909668: 1, -0.07361169159412384: 1, -1.1480247974395752: 1, 5.066896438598633: 1, 5.05051851272583: 1, 3.830434560775757: 1, -1.1785725355148315: 1, -1.087907314300537: 1, -1.1977088451385498: 1, -0.5409148931503296: 1, -0.5638068318367004: 1, -0.6967374682426453: 1, -1.0416630506515503: 1, 0.216363787651062: 1, -0.38882899284362793: 1, -0.9539603590965271: 1, -0.5497206449508667: 1, -1.1790684461593628: 1, -1.167614459991455: 1, -0.6818874478340149: 1, -1.1940333843231201: 1, -0.8758900761604309: 1, -0.75212162733078: 1, 0.29684698581695557: 1, -1.1212965250015259: 1, -0.9272821545600891: 1, -1.02364981174469: 1, -1.0699774026870728: 1, -1.0434380769729614: 1, -1.189503788948059: 1, -1.1703253984451294: 1, -1.1503796577453613: 1, -0.9726890325546265: 1, -0.9449849724769592: 1, -1.1986464262008667: 1, -1.0933367013931274: 1, -1.0186684131622314: 1, -1.1181161403656006: 1, -0.4794164001941681: 1, -1.1434556245803833: 1, -1.1468044519424438: 1, -0.4437340795993805: 1, -0.967963457107544: 1, -1.0246316194534302: 1, -0.1397843062877655: 1, -0.8599510192871094: 1, -0.9453350901603699: 1, -0.8791208267211914: 1, -1.0074002742767334: 1, -1.0323843955993652: 1, -1.1855055093765259: 1, 1.0147548913955688: 1, -0.6900844573974609: 1, -1.1882411241531372: 1, 1.5164122581481934: 1, -1.2010202407836914: 1, -0.23251420259475708: 1, -0.15451399981975555: 1, 1.094826102256775: 1, -0.1682627946138382: 1, 0.7936035394668579: 1, 0.6101611256599426: 1, 0.9798484444618225: 1, -0.6111128330230713: 1, -0.433001846075058: 1, -1.0877141952514648: 1, 0.4647965729236603: 1, 0.9053057432174683: 1, 1.6532078981399536: 1, 1.5002496242523193: 1, -0.726171612739563: 1, 0.5599294900894165: 1, -0.8635501265525818: 1, 0.8697875142097473: 1, -0.31994664669036865: 1, -0.7167530059814453: 1, 0.18603742122650146: 1, -0.9458178281784058: 1, 0.6880602836608887: 1, 0.6052579283714294: 1, 0.8645955324172974: 1, 0.7324214577674866: 1, 1.6113003492355347: 1, 0.29118791222572327: 1, -1.1721446514129639: 1, -0.23552395403385162: 1, -0.7970835566520691: 1, 0.5490202307701111: 1, 1.6066374778747559: 1, 0.8136993050575256: 1, -0.16204535961151123: 1, -1.1626108884811401: 1, 0.5944019556045532: 1, 0.3594037592411041: 1, -1.2016305923461914: 1, -0.7776852250099182: 1, -1.1028145551681519: 1, -0.4003660976886749: 1, 0.6591589450836182: 1, -0.1892606019973755: 1, -0.8111313581466675: 1, -0.8244988322257996: 1, -0.5816406607627869: 1, -0.4005826711654663: 1, -0.6496835947036743: 1, -0.7019102573394775: 1, -1.1959316730499268: 1, -0.9352316856384277: 1, -0.8736492395401001: 1, 0.6975425481796265: 1, 0.9127581715583801: 1, 0.4960012137889862: 1, -1.2016229629516602: 1, 0.6509128212928772: 1, -0.44575440883636475: 1, -0.45371508598327637: 1, 1.2443398237228394: 1, 1.7609484195709229: 1, -1.201631784439087: 1, -0.1808653175830841: 1, -0.17489729821681976: 1, 0.05797763168811798: 1, 0.5601547360420227: 1, 1.0962333679199219: 1, 0.6412140727043152: 1, 1.3093386888504028: 1, -0.8054187893867493: 1, 0.5863446593284607: 1, 1.1463862657546997: 1, -1.154268741607666: 1, -1.201583743095398: 1, -1.199397087097168: 1, 0.009732205420732498: 1, -0.4477367699146271: 1, -0.17054051160812378: 1, -1.1832531690597534: 1, -0.7456731200218201: 1, -1.1821529865264893: 1, -1.076475739479065: 1, -1.2009553909301758: 1, -1.0944702625274658: 1, -1.1784441471099854: 1, -1.1624016761779785: 1, -0.7622874975204468: 1, -1.18364417552948: 1, -1.1482324600219727: 1, -0.5079992413520813: 1, -1.0951330661773682: 1, -1.1970044374465942: 1, -0.6870049834251404: 1, -1.1676386594772339: 1, -0.6259949207305908: 1, -1.1963162422180176: 1, -1.201351284980774: 1, -1.1595861911773682: 1, -1.1066040992736816: 1, -1.1365838050842285: 1, -1.0986745357513428: 1, -1.2012261152267456: 1, -0.6185922622680664: 1, -0.9966712594032288: 1, -0.7500604391098022: 1, -1.2013036012649536: 1, -0.6249680519104004: 1, -1.170555830001831: 1, -0.957834780216217: 1, -1.0740134716033936: 1, -1.1981604099273682: 1, -1.1905823945999146: 1, -1.2016135454177856: 1, -1.1405699253082275: 1, -1.0521938800811768: 1, -1.1961623430252075: 1, -1.1871360540390015: 1, -1.0014375448226929: 1, -1.0963668823242188: 1, -1.196357011795044: 1, -1.1971783638000488: 1, -0.40274444222450256: 1, -1.1819733381271362: 1, -1.1612766981124878: 1, -1.1862393617630005: 1, -1.1950762271881104: 1, -1.1970906257629395: 1, 1.7719837427139282: 1, -1.1995155811309814: 1, -1.1924408674240112: 1, -0.6460915207862854: 1, -1.1948115825653076: 1, -0.6645460724830627: 1, 0.05473056063055992: 1, -1.0081939697265625: 1, -0.04312499612569809: 1, -1.1948130130767822: 1, -1.201563835144043: 1, -1.1990872621536255: 1, -1.1986582279205322: 1, -0.6273359060287476: 1, 5.645717620849609: 1, -0.7029581665992737: 1, -0.7359880805015564: 1, -0.459964781999588: 1, -1.1054575443267822: 1, 1.8886953592300415: 1, 0.19575154781341553: 1, -1.2002415657043457: 1, -1.164048194885254: 1, -1.2014538049697876: 1, -1.194710373878479: 1, 0.6086026430130005: 1, -1.171040415763855: 1, -1.1539435386657715: 1, -1.1912822723388672: 1, -1.1973918676376343: 1, -0.4641222357749939: 1, -1.1811505556106567: 1, 1.6846264600753784: 1, 0.21761490404605865: 1, 0.43160757422447205: 1, -0.809281051158905: 1, -0.22873257100582123: 1, 0.22853031754493713: 1, -0.6209532618522644: 1, -0.0526600144803524: 1, 0.33931559324264526: 1, 0.3338538706302643: 1, 1.914624810218811: 1, 0.6983891129493713: 1, 1.431997537612915: 1, 0.06706182658672333: 1, 0.18826737999916077: 1, -0.11514480412006378: 1, 1.6200270652770996: 1, 0.05425111949443817: 1, 0.4515918791294098: 1, 0.20913533866405487: 1, -0.20442236959934235: 1, 1.4727312326431274: 1, 1.379611611366272: 1, -0.1031346470117569: 1, 1.481839656829834: 1, 1.1887938976287842: 1, 0.7778939008712769: 1, 1.5112932920455933: 1, -1.1865193843841553: 1, 0.9693878293037415: 1, -1.0057076215744019: 1, 0.35881760716438293: 1, 0.8502970337867737: 1, -0.6454114317893982: 1, 0.753506600856781: 1, 0.2539007067680359: 1, 0.002965901279821992: 1, 1.3110779523849487: 1, 1.55558443069458: 1, -0.16230207681655884: 1, 1.4751214981079102: 1, 0.3319496512413025: 1, 0.22794750332832336: 1, 0.7984791398048401: 1, 0.5713070034980774: 1, 0.7623692750930786: 1, 0.6647680401802063: 1, 1.1509000062942505: 1, 1.1246896982192993: 1, 0.7825908064842224: 1, 1.375723958015442: 1, -0.2681446373462677: 1, 0.3024345636367798: 1, 0.276736855506897: 1, 1.4752575159072876: 1, -0.10448039323091507: 1, -0.2946179509162903: 1, 0.07517638802528381: 1, 0.2801852524280548: 1, 1.5357881784439087: 1, 0.31122344732284546: 1, 1.3920340538024902: 1, 1.0482161045074463: 1, 0.44933900237083435: 1, -0.10372047126293182: 1, -0.6031805276870728: 1, 1.5503476858139038: 1, 1.556430697441101: 1, 0.24058645963668823: 1, 0.3425867557525635: 1, 0.35902467370033264: 1, 0.1928955614566803: 1, 0.14948004484176636: 1, 0.11391282081604004: 1, 1.2999117374420166: 1, -0.44012218713760376: 1, -0.812441349029541: 1, 0.3157300651073456: 1, 0.6854439377784729: 1, 1.5044559240341187: 1, 1.4972656965255737: 1, 1.0554699897766113: 1, -0.31073465943336487: 1, 0.21012525260448456: 1, 1.3231751918792725: 1, -1.1871042251586914: 1, -0.11516252905130386: 1, -0.5775644183158875: 1, 0.9252344369888306: 1, -1.140577793121338: 1, 0.4845307767391205: 1, 0.021630888804793358: 1, 1.0225938558578491: 1, -0.22581757605075836: 1, 0.8458276391029358: 1, 0.29691436886787415: 1, 1.0407052040100098: 1, -1.1736985445022583: 1, 0.8794386982917786: 1, 0.26258811354637146: 1, 1.244690179824829: 1, 1.5059622526168823: 1, -0.4757806956768036: 1, 0.021963730454444885: 1, 0.3591007590293884: 1, -0.06609977036714554: 1, 0.31476086378097534: 1, 0.3153885304927826: 1, 0.7346283793449402: 1, -0.0922759622335434: 1, -1.2015061378479004: 1, -0.41403406858444214: 1, -1.2015372514724731: 1, -1.180970549583435: 1, 0.20136801898479462: 1, -0.416104793548584: 1, -0.11705746501684189: 1, -1.201634168624878: 1, -1.150158166885376: 1, -1.1623584032058716: 1, -1.201610803604126: 1, -1.1100589036941528: 1, -0.6426911950111389: 1, -0.9999420642852783: 1, -0.8209242224693298: 1, 0.34270647168159485: 1, -0.5093275308609009: 1, 1.5724915266036987: 1, 1.466652274131775: 1, -0.1758507341146469: 1, 1.5651975870132446: 1, -0.11230547726154327: 1, 0.19260334968566895: 1, 0.36868804693222046: 1, -0.5517370104789734: 1, 0.21266861259937286: 1, -0.03138621151447296: 1, 1.466456651687622: 1, 0.9224774241447449: 1, 1.256608247756958: 1, 1.533963680267334: 1, 1.3359390497207642: 1, 1.4527193307876587: 1, 1.5988372564315796: 1, 1.3192821741104126: 1, 1.533202886581421: 1, 0.19674475491046906: 1, -0.3147757053375244: 1, 1.4282907247543335: 1, 0.11682117730379105: 1, 0.11165464669466019: 1, 0.7675086855888367: 1, 1.111115574836731: 1, 1.5909359455108643: 1, 0.017385760322213173: 1, 1.3268651962280273: 1, 0.853833794593811: 1, 0.8655160665512085: 1, 0.8642333745956421: 1, 0.3231067657470703: 1, -0.5888111591339111: 1, 1.7802213430404663: 1, 0.2615222632884979: 1, 0.5367603302001953: 1, -0.06798223406076431: 1, 1.3918044567108154: 1, 1.3780083656311035: 1, 1.1870133876800537: 1, 1.2197668552398682: 1, 0.5573470592498779: 1, 0.81052166223526: 1, -1.1104997396469116: 1, 0.8847638368606567: 1, 1.547389030456543: 1, 1.471611738204956: 1, 1.4821670055389404: 1, 0.11520501971244812: 1, 1.2778698205947876: 1, -0.6971253156661987: 1, 0.24872112274169922: 1, 1.0432312488555908: 1, 1.4069277048110962: 1, -1.1837621927261353: 1, 1.5033998489379883: 1, 0.2389289289712906: 1, 1.1072840690612793: 1, 0.22126778960227966: 1, 1.5512200593948364: 1, -0.16245944797992706: 1, 0.29415011405944824: 1, 1.3578754663467407: 1, 1.133412480354309: 1, 0.8618729114532471: 1, 0.8533394932746887: 1, 0.8744557499885559: 1, 0.5000193119049072: 1, 0.9953116178512573: 1, 1.5040947198867798: 1, 1.0842758417129517: 1, 0.9064841866493225: 1, 1.3597513437271118: 1, 1.368375539779663: 1, 0.0863155722618103: 1, -0.11868320405483246: 1, 0.3126457929611206: 1, 1.5916389226913452: 1, 0.19180983304977417: 1, 0.8997297883033752: 1, 0.9983642101287842: 1, 0.6865427494049072: 1, 1.5009726285934448: 1, 1.6108869314193726: 1, 1.5557059049606323: 1, 1.4529069662094116: 1, -0.56696617603302: 1, 0.7161386013031006: 1, 0.7092652916908264: 1, 0.27865079045295715: 1, 0.7332795858383179: 1, 1.4580349922180176: 1, 1.3759030103683472: 1, -1.2015975713729858: 1, 0.7280606031417847: 1, 0.6448225975036621: 1, 0.2692132890224457: 1, -0.870884895324707: 1, 1.5217971801757812: 1, -0.30212122201919556: 1, -0.19083698093891144: 1, -0.091583751142025: 1, 0.2824403941631317: 1, 0.3697844445705414: 1, 1.1921144723892212: 1, 1.1847658157348633: 1, 0.2642950117588043: 1, -0.004799619782716036: 1, 0.9053120613098145: 1, 0.6183062195777893: 1, 1.3463716506958008: 1, -0.9639919996261597: 1, -0.025239035487174988: 1, 0.041503969579935074: 1, 1.4127790927886963: 1, 0.8287729024887085: 1, -1.1504056453704834: 1, 1.5179330110549927: 1, -0.20943275094032288: 1, 0.3431006968021393: 1, 0.35639217495918274: 1, 1.405086636543274: 1, 0.7253832817077637: 1, 0.80833899974823: 1, 0.9344565272331238: 1, 1.4793431758880615: 1, 0.8943637013435364: 1, 1.233654499053955: 1, 0.339995801448822: 1, 0.5472216606140137: 1, 0.2568971812725067: 1, 0.5611000061035156: 1, 0.8098611235618591: 1, 0.4243623912334442: 1, 0.6104775071144104: 1, 0.39806419610977173: 1, 1.473071813583374: 1, 1.069445013999939: 1, 1.5127235651016235: 1, -0.30856987833976746: 1, 1.2513844966888428: 1, 0.1911333203315735: 1, 0.39437854290008545: 1, 1.3862555027008057: 1, 1.5425565242767334: 1, -0.41700541973114014: 1, 0.11328306794166565: 1, -0.10053509473800659: 1, 0.881543755531311: 1, 0.2816910445690155: 1, 1.3900312185287476: 1, 0.3554361164569855: 1, 1.3589857816696167: 1, -0.3440307080745697: 1, 0.3602719008922577: 1, 0.04298178479075432: 1, -0.11455459892749786: 1, 0.30982303619384766: 1, 0.340713769197464: 1, 0.822748601436615: 1, 1.4639532566070557: 1, 1.1571227312088013: 1, -0.30866673588752747: 1, 1.540098786354065: 1, 1.4051384925842285: 1, 1.0418422222137451: 1, 1.2591054439544678: 1, 1.5978120565414429: 1, 1.2155990600585938: 1, -1.024586796760559: 1, 1.7404301166534424: 1, 0.2925977110862732: 1, 0.31707215309143066: 1, 0.7008569240570068: 1, 0.5241292119026184: 1, 0.8512904644012451: 1, 0.5769325494766235: 1, 1.4151798486709595: 1, -0.11862857639789581: 1, -0.1884830892086029: 1, -1.0716415643692017: 1, -0.2168547362089157: 1, 0.6513680219650269: 1, -0.040548212826251984: 1, -0.4737872779369354: 1, -0.2417261004447937: 1, 0.6668532490730286: 1, 1.0762102603912354: 1, -0.08143828064203262: 1, -1.0314160585403442: 1, 0.3661552369594574: 1, 0.8222253322601318: 1, 1.5401815176010132: 1, 0.8500742316246033: 1, 0.02118554152548313: 1, 0.8342987895011902: 1, 0.98076331615448: 1, 1.007346510887146: 1, -0.6390724778175354: 1, 0.23464715480804443: 1, -1.1742432117462158: 1, 0.9422100782394409: 1, -0.6223658919334412: 1, 1.4056357145309448: 1, 2.4050354957580566: 1, 0.29443448781967163: 1, -0.20047886669635773: 1, 0.3499395549297333: 1, -0.1565595418214798: 1, -1.2016048431396484: 1, -1.2016196250915527: 1, -1.2016316652297974: 1, 0.7264453172683716: 1, 1.2736730575561523: 1, -0.16393840312957764: 1, 0.840314507484436: 1, -0.24846623837947845: 1, -1.2004797458648682: 1, -1.1211071014404297: 1, 0.5979693531990051: 1, 0.8318881988525391: 1, 1.0863690376281738: 1, 0.5795131325721741: 1, 0.7288377285003662: 1, 1.3004077672958374: 1, -0.012521528638899326: 1, -0.4453357756137848: 1, 1.009254813194275: 1, -0.24570585787296295: 1, -1.1972460746765137: 1, 0.23339834809303284: 1, 0.654328465461731: 1, -0.28629931807518005: 1, -0.41674312949180603: 1, 1.1942393779754639: 1, 1.179612159729004: 1, 1.2479115724563599: 1, -0.22866979241371155: 1, -1.1798354387283325: 1, 0.6535213589668274: 1, 1.2529374361038208: 1, -0.37873539328575134: 1, 1.2188843488693237: 1, 0.9235488772392273: 1, -0.5479841232299805: 1, 0.4542813301086426: 1, 0.8476697206497192: 1, -0.22304323315620422: 1, -0.12660875916481018: 1, -0.18254651129245758: 1, 0.838370680809021: 1, 0.055386364459991455: 1, -0.2671576142311096: 1, 1.1683013439178467: 1, -0.2164342701435089: 1, 1.2822530269622803: 1, -1.1879688501358032: 1, 1.2413678169250488: 1, 0.008748067542910576: 1, 0.6657525897026062: 1, -0.9053927063941956: 1, -0.37779542803764343: 1, -0.5749701261520386: 1, 1.150696039199829: 1, 0.9305616617202759: 1, 0.6777730584144592: 1, 1.0972230434417725: 1, -0.25068432092666626: 1, 0.8119110465049744: 1, 0.4251300096511841: 1, -0.2664187550544739: 1, -0.33581042289733887: 1, -0.39580973982810974: 1, 0.9840258359909058: 1, 1.256977915763855: 1, -0.1916339248418808: 1, 0.24701066315174103: 1, 1.1922990083694458: 1, -0.8705235719680786: 1, -1.1247143745422363: 1, 0.8834699392318726: 1, -0.007039392367005348: 1, -0.621107280254364: 1, 1.0945255756378174: 1, -0.9168434739112854: 1, -0.8882886171340942: 1, -0.09275590628385544: 1, -0.06864805519580841: 1, 0.008864369243383408: 1, 0.8532567620277405: 1, 0.23268936574459076: 1, 0.6013088822364807: 1, 1.185150146484375: 1, -1.1928194761276245: 1, 1.0187321901321411: 1, 0.15111742913722992: 1, -0.7624772787094116: 1, -0.6921688914299011: 1, -0.019019143655896187: 1, -1.1852235794067383: 1, -0.16533638536930084: 1, 1.104429006576538: 1, 0.525627076625824: 1, 0.31131237745285034: 1, 1.2513697147369385: 1, -0.6482374668121338: 1, -1.1878859996795654: 1, -0.8494775891304016: 1, -0.12005668878555298: 1, 1.1650327444076538: 1, 1.1985303163528442: 1, -0.13838951289653778: 1, -0.11709770560264587: 1, 0.0796559751033783: 1, 0.6605957746505737: 1, 0.32448264956474304: 1, 0.004906347021460533: 1, -0.17582924664020538: 1, 0.34432777762413025: 1, 1.1265980005264282: 1, 0.017293494194746017: 1, -0.32669803500175476: 1, -0.8852211833000183: 1, -0.12913857400417328: 1, 1.2238743305206299: 1, -1.194933295249939: 1, 1.1819933652877808: 1, -0.5622422695159912: 1, 1.2926610708236694: 1, -1.194725751876831: 1, -0.34817975759506226: 1, 1.0467203855514526: 1, 0.029728559777140617: 1, -0.35746997594833374: 1, -0.025104349479079247: 1, -0.20500345528125763: 1, -0.23531334102153778: 1, -0.1855221837759018: 1, 0.29174771904945374: 1, 1.0578463077545166: 1, -0.0449078269302845: 1, -0.24153171479701996: 1, -1.2016297578811646: 1, 0.02958042360842228: 1, -0.04175446555018425: 1, -0.2098180055618286: 1, -0.1759326308965683: 1, 0.7244752645492554: 1, -0.29299479722976685: 1, 0.4987215995788574: 1, -0.6826592683792114: 1, -0.816495954990387: 1, 0.4205377995967865: 1, 0.033690646290779114: 1, -0.20062661170959473: 1, -1.1656997203826904: 1, 1.039072036743164: 1, 0.1316474825143814: 1, -0.15267345309257507: 1, -1.1981785297393799: 1, -0.3641962707042694: 1, -0.23942992091178894: 1, -1.095828652381897: 1, 1.2373515367507935: 1, -0.3953478932380676: 1, 0.7884150743484497: 1, -0.4121406674385071: 1, -1.0349785089492798: 1, 0.02054119110107422: 1, -0.35524412989616394: 1, -0.39703771471977234: 1, -0.1827705055475235: 1, -1.0583271980285645: 1, 0.042822714895009995: 1, 1.1279875040054321: 1, 0.7561059594154358: 1, -0.11954380571842194: 1, 0.4259852468967438: 1, 0.6872115731239319: 1, -0.1517976075410843: 1, -0.848010778427124: 1, -1.1948002576828003: 1, 4.363720893859863: 1, -1.1943039894104004: 1, -0.11716806888580322: 1, -0.14289136230945587: 1, 0.469992071390152: 1, 0.36895880103111267: 1, -1.2016255855560303: 1, -1.2015341520309448: 1, -1.1318936347961426: 1, -1.2013877630233765: 1, -1.1967262029647827: 1, 0.1497194468975067: 1, -1.2014774084091187: 1, -1.201634407043457: 1, -1.2016342878341675: 1, -1.201493740081787: 1, -0.3404213488101959: 1, 2.26233172416687: 1, -0.7989376187324524: 1, 0.19668835401535034: 1, -0.3213060796260834: 1, -1.1992485523223877: 1, 0.2079305797815323: 1, -1.1586899757385254: 1, -1.0591267347335815: 1, -1.2015695571899414: 1, -1.2016273736953735: 1, -1.2015702724456787: 1, -1.201588749885559: 1, -1.201620101928711: 1, 0.1331777125597: 1, -1.2016350030899048: 1, 0.0891273021697998: 1, -1.201523780822754: 1, -1.2015215158462524: 1, -1.2015316486358643: 1, -1.201473593711853: 1, -1.2016335725784302: 1, -0.8910970091819763: 1, -0.16826894879341125: 1, -1.2013384103775024: 1, 0.9015107750892639: 1, -0.28742867708206177: 1, -0.518195390701294: 1, -0.5834662318229675: 1, -1.2015115022659302: 1, -1.1257261037826538: 1, -0.006573406048119068: 1, -0.5612114071846008: 1, 0.07849415391683578: 1, 0.86496901512146: 1, -0.21048426628112793: 1, 0.8498935103416443: 1, -1.201079249382019: 1, -1.2016206979751587: 1, -1.2003253698349: 1, 0.5647678971290588: 1, -1.1779894828796387: 1, -1.1490250825881958: 1, -1.1954847574234009: 1, 0.3015405535697937: 1, -0.6758065819740295: 1, -1.1923046112060547: 1, -1.1979888677597046: 1, -1.1363192796707153: 1, -1.0203382968902588: 1, 0.2464127391576767: 1, -1.2016191482543945: 1, -1.2016382217407227: 1, -1.201630711555481: 1, -0.4132004976272583: 1, -1.2015154361724854: 1, -0.4687884449958801: 1, -1.1849600076675415: 1, -1.2015478610992432: 1, 0.1322988122701645: 1, -1.2016332149505615: 1, -1.0628424882888794: 1, -0.6676098704338074: 1, 1.018097996711731: 1, -0.2126571536064148: 1, 1.4819786548614502: 1, 0.8218473196029663: 1, -0.1427413374185562: 1, -0.8740671277046204: 1, -0.0332360677421093: 1, 1.1960792541503906: 1, 0.08744630217552185: 1, 0.01664043217897415: 1, -0.6421114802360535: 1, -0.3469093143939972: 1, -1.1963841915130615: 1, -0.14633353054523468: 1, -1.1634924411773682: 1, 0.27843114733695984: 1, 0.6842910647392273: 1, -0.3879204988479614: 1, -0.14631414413452148: 1, -0.4360010027885437: 1, -0.0580214224755764: 1, 0.9394524693489075: 1, -1.1812138557434082: 1, -0.1688508540391922: 1, -1.2005233764648438: 1, -0.9032037854194641: 1, -1.2016253471374512: 1, -0.09905305504798889: 1, 0.6233600974082947: 1, 1.2749443054199219: 1, 1.113309621810913: 1, -1.1755220890045166: 1, -1.0965174436569214: 1, 0.015011060051620007: 1, -1.1905653476715088: 1, -0.6103007793426514: 1, 0.029840881004929543: 1, 0.09383262693881989: 1, 0.36468705534935: 1, 0.5362502932548523: 1, 0.5997416377067566: 1, 0.6232529878616333: 1, 0.8548043966293335: 1, 0.18457916378974915: 1, 1.200035810470581: 1, 0.01943967677652836: 1, -0.11945565789937973: 1, 0.8965467214584351: 1, -1.0558182001113892: 1, -0.33075013756752014: 1, -0.24580752849578857: 1, -1.0237786769866943: 1, 0.9712859988212585: 1, 1.176633596420288: 1, -0.04390183836221695: 1, 1.1995047330856323: 1, 1.0674824714660645: 1, 0.8222996592521667: 1, -0.47594985365867615: 1, 0.5694330334663391: 1, -0.040736664086580276: 1, -0.2904500663280487: 1, -0.16829612851142883: 1, 0.29349300265312195: 1, 1.28579580783844: 1, 1.0769490003585815: 1, 0.9066123366355896: 1, 0.04932570457458496: 1, -0.9972583055496216: 1, -0.30726683139801025: 1, 1.2879470586776733: 1, -0.8751402497291565: 1, -1.1107620000839233: 1, 1.2916063070297241: 1, -1.193730115890503: 1, 0.8610304594039917: 1, -0.015705665573477745: 1, -0.3390061855316162: 1, 0.3886134922504425: 1, -1.1947468519210815: 1, 3.9672465324401855: 1, -0.463926762342453: 1, -0.37206733226776123: 1, -1.2003904581069946: 1} test data: {-1.2016366720199585: 5, -1.2016353607177734: 2, -1.2016302347183228: 2, -1.2016288042068481: 2, -1.2016342878341675: 2, -1.2016339302062988: 2, -1.201635479927063: 2, -1.201635718345642: 2, -1.2016324996948242: 2, -1.2016377449035645: 2, 0.26908448338508606: 1, 0.06883639097213745: 1, 0.22961212694644928: 1, 0.3664189279079437: 1, -0.4307803809642792: 1, -0.5706648826599121: 1, 1.415509581565857: 1, 0.21748410165309906: 1, 0.8440163731575012: 1, 0.36003923416137695: 1, 1.3083291053771973: 1, -0.3040960133075714: 1, 0.5601941347122192: 1, 1.168498158454895: 1, 1.5088152885437012: 1, 0.10525540262460709: 1, 1.4836735725402832: 1, -0.4459679424762726: 1, 0.5256680846214294: 1, -0.6442912220954895: 1, 0.3488617241382599: 1, 0.4933412969112396: 1, 0.28807583451271057: 1, -1.201627492904663: 1, 0.22109845280647278: 1, -0.35442060232162476: 1, 0.08119866997003555: 1, -0.31709083914756775: 1, -0.425843745470047: 1, -1.2016195058822632: 1, -1.1963087320327759: 1, 0.6829988956451416: 1, -0.6966411471366882: 1, 0.831580638885498: 1, 1.3986883163452148: 1, -1.0201867818832397: 1, 0.2609155476093292: 1, -1.193596363067627: 1, 1.4251669645309448: 1, 1.4424492120742798: 1, -0.10035426914691925: 1, 0.23716896772384644: 1, 0.49536043405532837: 1, -0.13643299043178558: 1, 0.3245508372783661: 1, -1.1701372861862183: 1, 0.9285130500793457: 1, 1.088638186454773: 1, 0.7614589929580688: 1, 1.5370275974273682: 1, 0.3443826735019684: 1, 0.34950539469718933: 1, 1.0995265245437622: 1, 1.1508636474609375: 1, -1.115770697593689: 1, 0.7471779584884644: 1, 0.912930965423584: 1, -1.189130187034607: 1, 0.31334978342056274: 1, 1.4158756732940674: 1, -0.20045150816440582: 1, 0.25783771276474: 1, 1.5035943984985352: 1, 0.7412875294685364: 1, 0.7279888391494751: 1, -0.026365874335169792: 1, 1.9054079055786133: 1, 0.2500914931297302: 1, 0.695344090461731: 1, 1.1683435440063477: 1, -0.9669303297996521: 1, 1.4923255443572998: 1, 0.10738043487071991: 1, 0.540539026260376: 1, -1.0873202085494995: 1, 0.7402693033218384: 1, -0.2393776774406433: 1, 0.8569538593292236: 1, -0.10739652067422867: 1, -0.09881063550710678: 1, -0.9223102331161499: 1, -1.198028802871704: 1, -1.2016290426254272: 1, -1.1939657926559448: 1, 0.005114047322422266: 1, -1.1664562225341797: 1, -1.1866750717163086: 1, -1.197901725769043: 1, -1.1945018768310547: 1, -1.1964974403381348: 1, -0.9927355051040649: 1, -1.197718858718872: 1, -1.2007057666778564: 1, -1.2000701427459717: 1, -0.512914776802063: 1, -1.1987498998641968: 1, -0.9657180905342102: 1, -1.201310396194458: 1, -0.27123570442199707: 1, -1.1980621814727783: 1, -1.1962871551513672: 1, 1.0551327466964722: 1, -1.201625108718872: 1, -0.30772721767425537: 1, -0.683555543422699: 1, 0.2787120044231415: 1, -0.8235211968421936: 1, 0.2115481197834015: 1, -0.32787415385246277: 1, -0.27864202857017517: 1, -0.5222632884979248: 1, 0.5479292273521423: 1, -0.39423879981040955: 1, -0.5639210939407349: 1, 0.9686956405639648: 1, -0.26343750953674316: 1, -0.5718616843223572: 1, -0.2302703857421875: 1, -0.7196366786956787: 1, -0.7543148398399353: 1, -1.201615810394287: 1, -1.2016253471374512: 1, -1.201596975326538: 1, -0.06465810537338257: 1, -1.201349139213562: 1, -0.6204319000244141: 1, 0.16982176899909973: 1, 0.37413451075553894: 1, -1.2015366554260254: 1, -1.2012096643447876: 1, 1.4907145500183105: 1, 0.44327667355537415: 1, 1.1153340339660645: 1, 0.12603989243507385: 1, 1.3625078201293945: 1, -0.5620038509368896: 1, 0.11128426343202591: 1, 0.34516385197639465: 1, 1.3501269817352295: 1, 1.3818247318267822: 1, 1.5410983562469482: 1, -1.201637625694275: 1, -1.2016162872314453: 1, -0.3419588804244995: 1, -1.201629400253296: 1, -1.2002341747283936: 1, -1.201614260673523: 1, -0.8027163147926331: 1, -1.1382324695587158: 1, -1.201348900794983: 1, -1.201606273651123: 1, -0.21383443474769592: 1, 0.812082827091217: 1, -1.2016046047210693: 1, -1.2016221284866333: 1, -1.201509714126587: 1, 0.1308048814535141: 1, 0.10008653253316879: 1, -1.2016127109527588: 1, 0.8243502378463745: 1, -1.180529236793518: 1, -0.21738800406455994: 1, 1.3996703624725342: 1, 1.6281145811080933: 1, -0.8634552955627441: 1, 0.5685755014419556: 1, 0.6492028832435608: 1, -0.5120261311531067: 1, 0.292474627494812: 1, -0.3605387806892395: 1, -1.18631112575531: 1, 1.7476170063018799: 1, -0.9751180410385132: 1, -1.1919200420379639: 1, -1.0893759727478027: 1, -1.0799649953842163: 1, -0.8241826295852661: 1, 1.5167564153671265: 1, -1.2013144493103027: 1, -1.1312958002090454: 1, -0.24214474856853485: 1, -1.098894715309143: 1, -1.1573199033737183: 1, 0.20812031626701355: 1, -1.1004241704940796: 1, 0.637361466884613: 1, -1.17500901222229: 1, -0.5784834027290344: 1, -0.9050359129905701: 1, -1.0084080696105957: 1, -1.1873440742492676: 1, 0.22341269254684448: 1, -0.748826265335083: 1, -0.5724524259567261: 1, -1.2006902694702148: 1, -1.1196062564849854: 1, 0.5786248445510864: 1, -0.5850008726119995: 1, -1.1570571660995483: 1, -0.7376867532730103: 1, -1.1983946561813354: 1, -0.8407037854194641: 1, -0.4367877244949341: 1, 1.281778335571289: 1, -0.26597675681114197: 1, 1.3899286985397339: 1, -0.9432769417762756: 1, -0.3497014045715332: 1, 1.4690353870391846: 1, -1.1845873594284058: 1, -0.9627416729927063: 1, 0.8790757060050964: 1, 0.9532740116119385: 1, -0.8962797522544861: 1, -0.09378552436828613: 1, 0.025435535237193108: 1, -1.1946264505386353: 1, 0.6293842196464539: 1, -1.0395952463150024: 1, -0.8580590486526489: 1, -1.1605626344680786: 1, -0.3573164939880371: 1, -0.5982488989830017: 1, -0.7701697945594788: 1, -0.90742427110672: 1, -1.0858170986175537: 1, -0.8108161091804504: 1, -1.0507465600967407: 1, 1.8212419748306274: 1, -1.0714704990386963: 1, -1.1041064262390137: 1, 0.43905025720596313: 1, 1.788353681564331: 1, -0.8920286297798157: 1, 1.4887964725494385: 1, -0.5754680633544922: 1, -1.1560314893722534: 1, -1.188598394393921: 1, -1.1848548650741577: 1, -0.45158419013023376: 1, -0.8680421113967896: 1, -1.1370965242385864: 1, -0.4400210678577423: 1, -1.2003082036972046: 1, -0.2589719295501709: 1, -1.197619080543518: 1, -1.18907630443573: 1, 1.3862724304199219: 1, -0.8754668831825256: 1, -0.2101057469844818: 1, -0.09434226900339127: 1, -0.15746326744556427: 1, 0.36215391755104065: 1, -0.06996402144432068: 1, -0.08030800521373749: 1, 0.4723914861679077: 1, 0.8504006266593933: 1, 0.2403424084186554: 1, 0.465168297290802: 1, 0.12876805663108826: 1, 1.3002121448516846: 1, -0.16252407431602478: 1, 0.6696186065673828: 1, 1.0170906782150269: 1, 1.3494455814361572: 1, 1.495969295501709: 1, -0.16512484848499298: 1, 0.5456136465072632: 1, 1.210314393043518: 1, 1.2950940132141113: 1, -0.5825971961021423: 1, -0.16857680678367615: 1, 0.7533062696456909: 1, 1.5823100805282593: 1, -0.02133699133992195: 1, -0.4253336787223816: 1, 0.7950616478919983: 1, -0.20539666712284088: 1, 1.6003168821334839: 1, 0.23328566551208496: 1, -1.201627254486084: 1, 1.4536134004592896: 1, 0.35480692982673645: 1, 0.4633817672729492: 1, -1.113705039024353: 1, 0.7611046433448792: 1, 1.1998642683029175: 1, -0.930620014667511: 1, -0.8510425090789795: 1, -0.5881722569465637: 1, 0.9470359086990356: 1, 0.9657272696495056: 1, -1.107023000717163: 1, 1.1112544536590576: 1, 0.2964378595352173: 1, -0.10225572437047958: 1, 0.3607413172721863: 1, 1.3262649774551392: 1, 0.3313300311565399: 1, -0.19720852375030518: 1, -0.2782626748085022: 1, 1.4062000513076782: 1, 0.03332117572426796: 1, -0.23679472506046295: 1, 1.5870535373687744: 1, 0.1708119511604309: 1, -1.1969212293624878: 1, 1.9238320589065552: 1, -1.0719594955444336: 1, -1.1913617849349976: 1, -1.1895103454589844: 1, 1.6006011962890625: 1, 0.7951827049255371: 1, 1.366266131401062: 1, -0.16261765360832214: 1, 0.900846004486084: 1, 0.12815426290035248: 1, 1.0303751230239868: 1, -0.01062939316034317: 1, 1.0541952848434448: 1, 0.6606081128120422: 1, 1.0402084589004517: 1, 0.8268551230430603: 1, 1.3615977764129639: 1, 0.02830575592815876: 1, 0.2640135586261749: 1, 0.16384904086589813: 1, 0.1881372481584549: 1, 0.31799715757369995: 1, 0.6039040088653564: 1, -0.021469445899128914: 1, -0.18478137254714966: 1, 0.9005318284034729: 1, 1.7614071369171143: 1, 0.5139665007591248: 1, 0.6321052312850952: 1, 1.0926192998886108: 1, 1.9098440408706665: 1, -0.2686515748500824: 1, 1.0540943145751953: 1, -1.1264077425003052: 1, 1.5100682973861694: 1, 0.5131713151931763: 1, -0.4567924439907074: 1, 1.7558945417404175: 1, 1.4320223331451416: 1, 1.891126036643982: 1, 0.6886505484580994: 1, 1.1086541414260864: 1, 0.27857378125190735: 1, 0.24442099034786224: 1, 0.591416597366333: 1, 0.5438616275787354: 1, 0.9327585697174072: 1, -0.2531147599220276: 1, -0.04758370667695999: 1, -1.061113953590393: 1, 1.2202951908111572: 1, -0.4105451703071594: 1, 1.280470848083496: 1, 0.5713223814964294: 1, 1.7166484594345093: 1, -1.2016383409500122: 1, 0.804813027381897: 1, 1.2709132432937622: 1, -0.5811882019042969: 1, -0.2810556888580322: 1, 1.4280599355697632: 1, -1.2002416849136353: 1, -1.2016136646270752: 1, -1.1658029556274414: 1, -0.864342451095581: 1, 1.2889325618743896: 1, -0.5300003886222839: 1, -1.2016212940216064: 1, 1.1998889446258545: 1, -1.2013123035430908: 1, 0.5059344172477722: 1, 0.1083393469452858: 1, 0.21325090527534485: 1, 0.766596257686615: 1, 1.8893579244613647: 1, 0.19334031641483307: 1, 1.3033519983291626: 1, 1.545008897781372: 1, 0.03225273638963699: 1, -0.03429622948169708: 1, 0.14515815675258636: 1, 0.9173278212547302: 1, 1.6143009662628174: 1, -0.19816404581069946: 1, 0.81379234790802: 1, -0.03557446971535683: 1, -0.7589280009269714: 1, 0.24924464523792267: 1, 0.8739101886749268: 1, 0.16001862287521362: 1, 0.4917255938053131: 1, 1.8799982070922852: 1, -1.0097246170043945: 1, -0.03317539766430855: 1, 1.7539278268814087: 1, 1.0263571739196777: 1, 1.0885628461837769: 1, 1.0135881900787354: 1, 1.7395148277282715: 1, 1.7069745063781738: 1, 1.4504951238632202: 1, 0.6905925273895264: 1, 1.9327584505081177: 1, 0.6320856809616089: 1, 0.34220972657203674: 1, 1.7223035097122192: 1, 1.5815212726593018: 1, 1.6906383037567139: 1, -0.12864308059215546: 1, 0.48093563318252563: 1, 0.7860816717147827: 1, 0.29674577713012695: 1, -0.4220041036605835: 1, 1.3496158123016357: 1, 1.9020731449127197: 1, -1.2016111612319946: 1, 0.2591017186641693: 1, 1.2595564126968384: 1, -0.9450681209564209: 1, 1.9389169216156006: 1, 0.38025134801864624: 1, 0.8071705102920532: 1, 1.6581584215164185: 1, -0.7838892936706543: 1, -1.201594352722168: 1, 1.0987391471862793: 1, -0.5369133949279785: 1, -1.085170865058899: 1, 1.16111421585083: 1, -0.8136187195777893: 1, -1.040601134300232: 1, -1.002498984336853: 1, 1.8749902248382568: 1, 1.7461694478988647: 1, 1.5291143655776978: 1, -0.13542917370796204: 1, 0.4253986179828644: 1, 1.5920872688293457: 1, 1.7425659894943237: 1, 1.8480027914047241: 1, -0.7448302507400513: 1, -1.1936933994293213: 1, -0.20700129866600037: 1, -0.3587249517440796: 1, 1.8261455297470093: 1, -0.8793452382087708: 1, 1.3335411548614502: 1, -0.5476839542388916: 1, -0.7379482984542847: 1, -0.24342182278633118: 1, -0.13886263966560364: 1, -1.200726866722107: 1, 0.1718989461660385: 1, 1.6034055948257446: 1, -0.6306192278862: 1, -1.2016384601593018: 1, 1.8354456424713135: 1, 1.0356202125549316: 1, 0.1478143036365509: 1, 1.8706549406051636: 1, 1.9172451496124268: 1, -1.2016191482543945: 1, 1.0343732833862305: 1, 0.9006003737449646: 1, 1.636014699935913: 1, 0.25442907214164734: 1, 0.04224063828587532: 1, -0.08804576843976974: 1, -0.8557604551315308: 1, 1.114802360534668: 1, -0.7923315763473511: 1, -0.4422439932823181: 1, -0.5870760679244995: 1, -0.94952791929245: 1, 1.796111822128296: 1, 0.9807740449905396: 1, -0.6532332301139832: 1, 0.43482455611228943: 1, 1.760259985923767: 1, -0.4194781482219696: 1, -0.11584310233592987: 1, 1.058468222618103: 1, 0.019096076488494873: 1, 1.0276689529418945: 1, -0.7692103385925293: 1, 1.7267848253250122: 1, 0.9968640804290771: 1, 0.9797016382217407: 1, 1.0176738500595093: 1, 0.9059203267097473: 1, 1.597861409187317: 1, 0.9352988600730896: 1, 1.7114263772964478: 1, 1.7614209651947021: 1, 1.5984208583831787: 1, 0.8788592219352722: 1, 0.08071509003639221: 1, -0.9069592952728271: 1, 1.692647099494934: 1, -0.3100615441799164: 1, 0.7566526532173157: 1, -0.9877877235412598: 1, 1.3759360313415527: 1, 0.017721591517329216: 1, -0.7505127787590027: 1, -0.6007823944091797: 1, 0.9276106357574463: 1, -0.8232591152191162: 1, -0.4729682505130768: 1, -0.3422335684299469: 1, -0.4592617452144623: 1, 0.22667939960956573: 1, -0.7915438413619995: 1, -0.46590861678123474: 1, -0.8751227855682373: 1, 0.006228437647223473: 1, -0.3081098794937134: 1, -0.47152507305145264: 1, -0.34284013509750366: 1, 1.3913298845291138: 1, 1.1735796928405762: 1, -0.03591597080230713: 1, -0.14395083487033844: 1, 0.5354000329971313: 1, -0.9762966632843018: 1, 1.3363116979599: 1, 1.7606215476989746: 1, -0.24207068979740143: 1, -1.1479105949401855: 1, -0.5857658982276917: 1, 0.15484686195850372: 1, -0.44544142484664917: 1, 1.3399251699447632: 1, -1.1891201734542847: 1, -1.185569167137146: 1, 0.29917436838150024: 1, 0.6785101294517517: 1, -0.33291712403297424: 1, 1.4956955909729004: 1, -0.34326422214508057: 1, 0.171223446726799: 1, -0.06991042196750641: 1, -0.3753896951675415: 1, 0.025362450629472733: 1, -0.4605187475681305: 1, -0.200442373752594: 1, -0.5582360029220581: 1, -0.1967451572418213: 1, -0.17455770075321198: 1, -0.13842415809631348: 1, 0.15043634176254272: 1, -0.2937408983707428: 1, -0.5413272380828857: 1, 0.6789939403533936: 1, -1.1767117977142334: 1, -0.46823152899742126: 1, 1.5324621200561523: 1, -0.6538054943084717: 1, 0.7365891337394714: 1, -0.4007585644721985: 1, 1.3204209804534912: 1, -0.9919153451919556: 1, 1.7616217136383057: 1, 0.3248298168182373: 1, -0.36514076590538025: 1, 1.4998488426208496: 1, -1.179208755493164: 1, -0.5210259556770325: 1, 1.2261123657226562: 1, -0.21868036687374115: 1, -0.5174428224563599: 1, 0.5128249526023865: 1, 0.48826223611831665: 1, 0.9130324721336365: 1, 1.376474142074585: 1, -1.1766016483306885: 1, 0.273947536945343: 1, -0.5338919758796692: 1, -0.238590806722641: 1, 0.6150393486022949: 1, -0.8833669424057007: 1, -0.288830429315567: 1, -0.879592776298523: 1, -0.8456059098243713: 1, 0.6444940567016602: 1, -0.6276612877845764: 1, -0.04817575961351395: 1, -1.0406304597854614: 1, 0.5874748826026917: 1, -0.21730461716651917: 1, 0.2422785758972168: 1, 0.2126206010580063: 1, 1.7796648740768433: 1, 0.04068145155906677: 1, 1.2531977891921997: 1, -1.1448076963424683: 1, -0.0690179392695427: 1, 1.552185297012329: 1, 0.3162490129470825: 1, 1.9273109436035156: 1, -0.6904935240745544: 1, -0.8279891610145569: 1, -1.103760838508606: 1, -0.392406165599823: 1, -1.155177116394043: 1, -0.1501469612121582: 1, -0.3135724663734436: 1, -0.6055639982223511: 1, 0.12007596343755722: 1, -0.4171464443206787: 1, 0.1769435554742813: 1, 0.8171444535255432: 1, -0.2874172031879425: 1, -0.5307965278625488: 1, 1.6683679819107056: 1, 1.7097703218460083: 1, -0.26246213912963867: 1, 0.7304840087890625: 1, 1.6697852611541748: 1, 0.7721536755561829: 1, 1.852098822593689: 1, 0.3744315505027771: 1, 0.6704513430595398: 1, -0.25207433104515076: 1, 1.0491001605987549: 1, -0.4845651388168335: 1, -1.1336802244186401: 1, -0.5809049010276794: 1, -1.1680830717086792: 1, -0.8523722290992737: 1, -1.1831696033477783: 1, -0.44949105381965637: 1, -1.1689732074737549: 1, 1.5011951923370361: 1, -0.25122812390327454: 1, -0.3189155161380768: 1, -0.364163339138031: 1, -0.3123464286327362: 1, 1.896323323249817: 1, -1.061142921447754: 1, -0.9981749057769775: 1, -0.2183121144771576: 1, -0.608808159828186: 1, 1.93668532371521: 1, 1.4193427562713623: 1, 1.1691573858261108: 1, 1.058951735496521: 1, -0.3383774757385254: 1, -0.7296833395957947: 1, -1.2013746500015259: 1, -1.0483030080795288: 1, -0.5361114740371704: 1, 0.7078472375869751: 1, -0.2223142683506012: 1, -1.1994960308074951: 1, -0.13113076984882355: 1, 4.5118513107299805: 1, 4.112330913543701: 1, -1.1829675436019897: 1, -1.2015472650527954: 1, -1.1635701656341553: 1, -1.2015953063964844: 1, -1.201560139656067: 1, -1.1494790315628052: 1, -1.1945738792419434: 1, -0.6403390765190125: 1, -1.110012173652649: 1, 0.5143935680389404: 1, -1.1343045234680176: 1, -0.04211708903312683: 1, 0.5575955510139465: 1, 1.3031054735183716: 1, 1.2888545989990234: 1, -1.2016228437423706: 1, -0.3311309218406677: 1, -1.0657732486724854: 1, -1.1993433237075806: 1, 0.032617583870887756: 1, -1.1756606101989746: 1, -1.150406837463379: 1, -0.07614605128765106: 1, 0.5382747054100037: 1, -0.10903797298669815: 1, -0.39080610871315: 1, -0.6006320714950562: 1, -0.6715388298034668: 1, -1.1981743574142456: 1, -0.7557051777839661: 1, 1.2057219743728638: 1, -0.11124473065137863: 1, -0.012192374095320702: 1, -1.1882494688034058: 1, -0.0996609777212143: 1, -0.14265145361423492: 1, -0.7893494963645935: 1, 0.49005118012428284: 1, -0.8901136517524719: 1, -0.03235474228858948: 1, -0.04230939969420433: 1, 0.17733901739120483: 1, -0.5050346255302429: 1, -0.881252646446228: 1, -0.7673166990280151: 1, 1.0700626373291016: 1, -0.24605818092823029: 1, -0.8764730095863342: 1, 0.4138732850551605: 1, 0.00948107335716486: 1, -1.1809542179107666: 1, -0.03975825384259224: 1, -1.1139642000198364: 1, -1.2008600234985352: 1, -0.3552163541316986: 1, 0.02189079485833645: 1, -0.08644621819257736: 1, 0.8677871227264404: 1, -0.15606260299682617: 1, 1.1807068586349487: 1, -0.07435453683137894: 1, 1.258391261100769: 1, 1.0952398777008057: 1, 1.1094199419021606: 1, -1.20154869556427: 1, 0.6366953253746033: 1, -0.13237591087818146: 1, 0.4334481656551361: 1, 0.9452794194221497: 1, -0.24963954091072083: 1, -1.1028809547424316: 1, -0.3122004270553589: 1, -0.3055903911590576: 1, 1.0410280227661133: 1, -0.5897277593612671: 1, -0.7184330821037292: 1, -1.1992239952087402: 1, -1.201612949371338: 1, -1.1420694589614868: 1, -0.17077305912971497: 1, -0.136034294962883: 1, 0.8696494698524475: 1, 1.143233299255371: 1, 0.7848809361457825: 1, 0.7787987589836121: 1, -0.4258005619049072: 1, 0.043348729610443115: 1, -1.2009185552597046: 1, -0.16215069591999054: 1, 0.7587617635726929: 1, -0.3075839579105377: 1, -1.201613426208496: 1, -0.19534148275852203: 1, -1.1375747919082642: 1, -1.0178923606872559: 1, -1.2016328573226929: 1, 0.810942530632019: 1, -0.08619289845228195: 1, -0.7380070686340332: 1, -1.2016348838806152: 1, 0.5923774242401123: 1, 1.0350605249404907: 1, -0.618209183216095: 1, 1.1690500974655151: 1, -0.11562295258045197: 1, -0.028692159801721573: 1, 0.8076177835464478: 1, -0.1227283924818039: 1, -1.2016351222991943: 1, -0.21699510514736176: 1, -0.5649014711380005: 1, -1.1697360277175903: 1, 1.1562855243682861: 1, 1.1278204917907715: 1, -1.198868989944458: 1, -0.5774872899055481: 1, -0.13060888648033142: 1, 0.9105262160301208: 1, 0.9495259523391724: 1, 0.6296795010566711: 1, -1.1068792343139648: 1, 1.0087279081344604: 1, 0.5908839702606201: 1, -0.253052294254303: 1, 0.02654222585260868: 1, -0.4923703670501709: 1, 0.666642963886261: 1, 0.028185075148940086: 1, -1.182140827178955: 1, 0.606712281703949: 1, 1.0692790746688843: 1, -1.200330376625061: 1, -1.1826090812683105: 1, -0.17342792451381683: 1, -0.09158548712730408: 1, 1.196900725364685: 1, 0.07662157714366913: 1, -0.6869497895240784: 1, -0.7992537021636963: 1, -0.06836508214473724: 1, 0.01674770377576351: 1, 0.5583640336990356: 1, 0.762050449848175: 1, 0.9011586904525757: 1, -0.05626079440116882: 1, -0.999508798122406: 1, -0.3548189401626587: 1, -0.6266202926635742: 1, -1.2016363143920898: 1, -0.002401667181402445: 1, -1.1934514045715332: 1, 1.1214849948883057: 1, -1.155191421508789: 1, 1.0595874786376953: 1, 1.3037681579589844: 1, -1.2016304731369019: 1, -0.16474246978759766: 1, -1.0579359531402588: 1, 0.6161129474639893: 1, 1.030333161354065: 1, -1.109034538269043: 1, -0.15946216881275177: 1, -0.019765598699450493: 1, -1.193916916847229: 1, 1.00320303440094: 1, -0.6903147101402283: 1, 0.7206960916519165: 1, 0.2524677515029907: 1, -0.02432066947221756: 1, -1.2016310691833496: 1, -0.21185417473316193: 1, -0.34825557470321655: 1, -0.15634003281593323: 1, -0.7010860443115234: 1, 0.6174435615539551: 1, 1.1092147827148438: 1, -0.07029284536838531: 1, -1.0963338613510132: 1, -0.5897085666656494: 1, -0.0026253368705511093: 1, -0.5935716032981873: 1, -0.5342384576797485: 1, -1.0416687726974487: 1, -1.1558103561401367: 1, -1.2015275955200195: 1, -1.1936330795288086: 1, -1.1552096605300903: 1, -0.8899372816085815: 1, -1.1568188667297363: 1, -1.1333073377609253: 1, -1.1969208717346191: 1, -0.832706868648529: 1, -1.1685314178466797: 1, -0.39197561144828796: 1, -1.1339654922485352: 1, -1.0433918237686157: 1, -1.1721937656402588: 1, -1.0160771608352661: 1, -1.0668615102767944: 1, -1.191677212715149: 1, -1.1582452058792114: 1, -1.1722731590270996: 1, -1.1300313472747803: 1, -0.6205415725708008: 1, -1.2015451192855835: 1, -1.1019858121871948: 1, -1.1993547677993774: 1, 0.741217315196991: 1, -1.195172905921936: 1, -1.1910632848739624: 1, -1.1999870538711548: 1, -0.5026354193687439: 1, -0.9010990262031555: 1, -1.1594713926315308: 1, -1.186979055404663: 1, -1.188031554222107: 1, -1.1610828638076782: 1, -0.40432149171829224: 1, -1.1643073558807373: 1, -1.0865159034729004: 1, -1.1485586166381836: 1, -1.1857080459594727: 1, -1.1929867267608643: 1, -1.1744723320007324: 1, -0.19914157688617706: 1, -1.194190263748169: 1, -0.669135332107544: 1, -1.00275719165802: 1, -0.4648326635360718: 1, -0.9824236631393433: 1, -0.8666650652885437: 1, -0.47614291310310364: 1, -1.144382119178772: 1, -1.1891672611236572: 1, -1.2006280422210693: 1, -1.200010895729065: 1, -1.1486388444900513: 1, -0.8267379403114319: 1, -1.0461647510528564: 1, -0.6635236740112305: 1, 0.4937030076980591: 1, 5.072229385375977: 1, 4.875144004821777: 1, -1.1364892721176147: 1, -0.6867276430130005: 1, -1.2014120817184448: 1, -1.1989647150039673: 1, -1.1946135759353638: 1, -1.010536789894104: 1, -1.1729844808578491: 1, -0.8867827653884888: 1, -1.157931923866272: 1, -0.9300684928894043: 1, -1.026742935180664: 1, -1.1463063955307007: 1, -1.1062737703323364: 1, -0.9129625558853149: 1, -0.8772833347320557: 1, -0.9141108393669128: 1, -1.2005667686462402: 1, -1.145255446434021: 1, -0.7968862652778625: 1, 0.18425099551677704: 1, -1.1150031089782715: 1, -1.0620733499526978: 1, -1.1929975748062134: 1, 0.3444092869758606: 1, 0.7184975147247314: 1, 1.0577486753463745: 1, -0.7252834439277649: 1, -0.32108673453330994: 1, 0.5780434608459473: 1, 0.8030492067337036: 1, -0.411021888256073: 1, 1.123262643814087: 1, -1.1621276140213013: 1, -1.171905517578125: 1, -1.2015637159347534: 1, 1.585684895515442: 1, -0.34684932231903076: 1, -0.3561877906322479: 1, 1.5706232786178589: 1, 1.0823159217834473: 1, -0.7198786735534668: 1, -0.750208854675293: 1, 1.2748090028762817: 1, -0.5334532856941223: 1, -0.5885310769081116: 1, 1.036679744720459: 1, 1.2156920433044434: 1, -1.0666824579238892: 1, 0.6389100551605225: 1, -0.7073397040367126: 1, -1.1690752506256104: 1, 0.821479320526123: 1, -1.195853590965271: 1, -0.576421856880188: 1, -1.2016373872756958: 1, -0.5739816427230835: 1, -1.2016358375549316: 1, -0.9499993324279785: 1, 0.842113196849823: 1, 0.605282723903656: 1, -1.084214448928833: 1, 0.880368709564209: 1, 0.9517895579338074: 1, -0.35478705167770386: 1, 0.2817704975605011: 1, 0.2436400204896927: 1, -1.157366394996643: 1, -1.173497200012207: 1, -1.1685289144515991: 1, -1.1728081703186035: 1, 0.4647902548313141: 1, -1.0542218685150146: 1, -0.9958106875419617: 1, -1.0349972248077393: 1, -1.2008846998214722: 1, -0.9986592531204224: 1, -1.196737289428711: 1, -1.198758602142334: 1, -1.1966667175292969: 1, -1.2010711431503296: 1, -1.0972120761871338: 1, -1.1865227222442627: 1, -0.23024950921535492: 1, 0.275499552488327: 1, -1.175083041191101: 1, -1.1994209289550781: 1, -0.638097882270813: 1, 1.8255465030670166: 1, -1.2015430927276611: 1, 1.6096405982971191: 1, 1.0043728351593018: 1, 1.593440055847168: 1, -0.9092623591423035: 1, 1.1564749479293823: 1, 0.8852934837341309: 1, 0.7067909836769104: 1, 0.7108749151229858: 1, -0.15851348638534546: 1, -0.2950673997402191: 1, -1.201418161392212: 1, -0.39169713854789734: 1, -1.016434669494629: 1, -1.2015913724899292: 1, -1.1964715719223022: 1, -1.188680648803711: 1, 0.23683039844036102: 1, 0.167218878865242: 1, 0.12959904968738556: 1, -1.1653438806533813: 1, 0.23767612874507904: 1, 0.7352478504180908: 1, -0.9236016869544983: 1, 1.1449819803237915: 1, -0.6113037467002869: 1, -0.1709771454334259: 1, 1.4899855852127075: 1, -0.1196289211511612: 1, -0.13019442558288574: 1, 0.23876602947711945: 1, -1.2015849351882935: 1, 1.561331033706665: 1, 1.3110328912734985: 1, -0.10369612276554108: 1, -0.40100592374801636: 1, 1.622856616973877: 1, 0.3626178801059723: 1, 0.7718502283096313: 1, 0.12041562795639038: 1, 1.0590577125549316: 1, 0.592692494392395: 1, -0.9130086302757263: 1, 1.260263442993164: 1, -0.20360040664672852: 1, 0.1144905686378479: 1, 0.2596873939037323: 1, 0.2826254069805145: 1, 0.9935461282730103: 1, 1.215183138847351: 1, -0.05793027952313423: 1, 1.4599251747131348: 1, -0.48821160197257996: 1, -1.1556631326675415: 1, -0.19414900243282318: 1, 1.6643083095550537: 1, 0.35236239433288574: 1, 0.2222539335489273: 1, 0.18295887112617493: 1, 0.7438257932662964: 1, 1.4145740270614624: 1, -0.10326965153217316: 1, 0.3140702247619629: 1, -0.632728099822998: 1, 0.01178812701255083: 1, 1.0209075212478638: 1, 0.7520656585693359: 1, 0.004675476811826229: 1, 1.126368761062622: 1, -0.3219013214111328: 1, -0.014552570879459381: 1, -0.0920228511095047: 1, 1.059990644454956: 1, -0.22985504567623138: 1, -0.20083148777484894: 1, -0.4890022277832031: 1, -0.16844011843204498: 1, 0.9222752451896667: 1, -1.0867854356765747: 1, 1.189407229423523: 1, 0.05546194687485695: 1, 0.10832516103982925: 1, 0.7781831622123718: 1, -0.5199218392372131: 1, 0.30889958143234253: 1, 0.5634517669677734: 1, 0.2561040222644806: 1, 0.06557659059762955: 1, 0.11681299656629562: 1, 1.5148382186889648: 1, -0.21601253747940063: 1, 0.9434877038002014: 1, 0.987504780292511: 1, 1.0659617185592651: 1, 0.7782679200172424: 1, 1.282376766204834: 1, 0.858607292175293: 1, 0.18870316445827484: 1, 1.4786081314086914: 1, 1.4364150762557983: 1, -0.1960129290819168: 1, 0.31844547390937805: 1, -0.03743843734264374: 1, -0.15476621687412262: 1, 1.5731940269470215: 1, 1.4644227027893066: 1, 1.5027039051055908: 1, 1.4103199243545532: 1, 0.009009350091218948: 1, -0.5194405913352966: 1, 0.0384686179459095: 1, -1.1040070056915283: 1, -0.2038322389125824: 1, 2.0702569484710693: 1, 1.5550216436386108: 1, -0.11932859569787979: 1, 0.22540217638015747: 1, 0.7684015035629272: 1, 1.3844947814941406: 1, 0.263934850692749: 1, -0.17545948922634125: 1, -0.16789886355400085: 1, 0.8805737495422363: 1, -0.7541334629058838: 1, 0.7922017574310303: 1, 1.4735376834869385: 1, 0.05755604803562164: 1, 0.008224710822105408: 1, 1.4807751178741455: 1, -0.8671839237213135: 1, -1.19014310836792: 1, 4.302996635437012: 1, -0.8270519375801086: 1, -0.9134752750396729: 1, 1.4680920839309692: 1, 0.46070119738578796: 1, -1.1579349040985107: 1, 0.9760450720787048: 1, -0.04237562045454979: 1, 1.4938876628875732: 1, -0.6757361888885498: 1, -1.1851680278778076: 1, 0.3276282548904419: 1, -0.009973025880753994: 1, 0.48444247245788574: 1, 0.9195832014083862: 1, -0.19494549930095673: 1, 0.2846507132053375: 1, -0.4644731879234314: 1, 1.916335940361023: 1, 0.8433452844619751: 1, -0.3739321231842041: 1, 0.9266675710678101: 1, -0.2210041582584381: 1, 1.5686708688735962: 1, 1.5599995851516724: 1, -0.28925973176956177: 1, 0.9952307939529419: 1, 0.06307864934206009: 1, -0.09898030012845993: 1, 1.0041277408599854: 1, 0.968934953212738: 1, 1.1854524612426758: 1, 1.622040867805481: 1, 1.4411144256591797: 1, -1.1900941133499146: 1, 1.5063830614089966: 1, -0.11317183822393417: 1, -0.8828660249710083: 1, 0.3683989942073822: 1, 0.09744428098201752: 1, 1.5597952604293823: 1, -1.1632437705993652: 1, 1.3494865894317627: 1, -0.07051629573106766: 1, -0.43688738346099854: 1, 1.3772739171981812: 1, 0.5069756507873535: 1, 1.5239653587341309: 1, 0.8933335542678833: 1, 0.7159395813941956: 1, 1.302850365638733: 1, -0.20986565947532654: 1, 1.1731380224227905: 1, 0.30258285999298096: 1, 0.35628634691238403: 1, 0.1563034951686859: 1, -0.8935246467590332: 1, -0.5888482928276062: 1, 1.0611008405685425: 1, -1.201277732849121: 1, 0.7325264811515808: 1, -0.4684484004974365: 1, 0.8952587246894836: 1, 0.8562594652175903: 1, 0.9413108229637146: 1, -0.2299586683511734: 1, 1.2350752353668213: 1, -0.27776581048965454: 1, 0.008470889180898666: 1, -0.06736226379871368: 1, 0.938378632068634: 1, 0.9848727583885193: 1, -1.126828908920288: 1, -0.35842111706733704: 1, -0.20541705191135406: 1, 0.8631362915039062: 1, 0.23272329568862915: 1, 0.23997803032398224: 1, 1.2978951930999756: 1, 0.729964017868042: 1, -0.8322789072990417: 1, -0.2597183287143707: 1, 0.2588636577129364: 1, -0.28555259108543396: 1, -0.05695538595318794: 1, 0.637002170085907: 1, 0.5468651056289673: 1, -0.14243346452713013: 1, -0.5104274749755859: 1, 1.310013771057129: 1, -0.08886344730854034: 1, -1.0537559986114502: 1, 1.3061707019805908: 1, -0.7430113554000854: 1, -1.2015588283538818: 1, 1.2229245901107788: 1, 1.0347987413406372: 1, 1.196738839149475: 1, 1.035197138786316: 1, 0.009196557104587555: 1, -0.674019455909729: 1, 0.889022946357727: 1, -0.5372467637062073: 1, -1.20163094997406: 1, 0.41254743933677673: 1, -1.2004859447479248: 1, -0.9307324290275574: 1, -0.9308528304100037: 1, 1.0013794898986816: 1, 0.44458866119384766: 1, 1.0134633779525757: 1, -0.586733877658844: 1, -0.11078709363937378: 1, -0.02250954695045948: 1, 1.0062413215637207: 1, -0.7914682626724243: 1, -1.2016282081604004: 1, -0.19495585560798645: 1, -1.1774789094924927: 1, 1.2197721004486084: 1, 0.15707539021968842: 1, -0.8713244199752808: 1, 0.9730016589164734: 1, -0.14272816479206085: 1, -0.8759819269180298: 1, 1.2192449569702148: 1, 1.150854468345642: 1, -0.12074612081050873: 1, -1.0489486455917358: 1, -0.002975589595735073: 1, -0.42487016320228577: 1, -0.0411478653550148: 1, -1.2016277313232422: 1, 1.1694233417510986: 1, -0.24156887829303741: 1, -0.9900954961776733: 1, 0.8838387131690979: 1, -0.42109400033950806: 1, -0.19223442673683167: 1, 1.2327803373336792: 1, 1.1865397691726685: 1, -0.5884501934051514: 1, -1.201636552810669: 1, -0.34523066878318787: 1, -0.08719541132450104: 1, -1.2011888027191162: 1, -0.053435858339071274: 1, -1.193373441696167: 1, -1.201407790184021: 1, -1.2015197277069092: 1, -1.2016361951828003: 1, -0.07099064439535141: 1, -0.6771114468574524: 1, -1.1987295150756836: 1, -1.201633095741272: 1, -1.2015831470489502: 1, -1.2016292810440063: 1, -1.2016286849975586: 1, 0.9705496430397034: 1, -1.179785132408142: 1, -1.2015916109085083: 1, -1.2016299962997437: 1, -0.4057319462299347: 1, -1.2016379833221436: 1, -1.2016347646713257: 1, -1.194821834564209: 1, -1.2014681100845337: 1, -1.201622486114502: 1, 0.3900337219238281: 1, -1.171350359916687: 1, -1.1903178691864014: 1, -1.2012838125228882: 1, -1.1383882761001587: 1, -1.2014092206954956: 1, -0.5475073456764221: 1, -0.009843221865594387: 1, -0.9857455492019653: 1, -0.9951181411743164: 1, -1.2014601230621338: 1, -0.9676279425621033: 1, 0.1581522524356842: 1, -0.11703558266162872: 1, -0.045158740133047104: 1, -0.5937166213989258: 1, -1.2015864849090576: 1, -0.029267514124512672: 1, 0.047311048954725266: 1, 0.3951069414615631: 1, 1.2526917457580566: 1, 0.7298784255981445: 1, -1.175829291343689: 1, -0.011501064524054527: 1, 0.8673886060714722: 1, -0.19123347103595734: 1, -0.9734629392623901: 1, 0.9265954494476318: 1, -0.9601404666900635: 1, 0.9529565572738647: 1, -0.84548020362854: 1, 1.2064505815505981: 1, 0.5252094268798828: 1, 0.04192548990249634: 1, 0.33690160512924194: 1, -0.21881107985973358: 1, 1.0069524049758911: 1, 1.1649004220962524: 1, -1.201595425605774: 1, -0.34107181429862976: 1, -1.1484540700912476: 1, -0.16572129726409912: 1, -0.10643515735864639: 1, -1.201636791229248: 1, 0.6481048464775085: 1, -0.10290281474590302: 1, 0.9786769151687622: 1, 1.0520529747009277: 1, -0.21433545649051666: 1, 1.0808240175247192: 1, 0.09893729537725449: 1, 0.005373222753405571: 1, -0.10037212073802948: 1, 0.30879271030426025: 1, 1.2726079225540161: 1, 1.1717408895492554: 1, 1.295539140701294: 1, -1.1320221424102783: 1}
INFO     Network type: REG
INFO     Model: "sequential_1"
INFO     _________________________________________________________________
INFO     Layer (type)                 Output Shape              Param #
INFO     =================================================================
INFO     dense_5 (Dense)              (None, 1024)              2098176
INFO     _________________________________________________________________
INFO     dropout_4 (Dropout)          (None, 1024)              0
INFO     _________________________________________________________________
INFO     dense_6 (Dense)              (None, 512)               524800
INFO     _________________________________________________________________
INFO     dropout_5 (Dropout)          (None, 512)               0
INFO     _________________________________________________________________
INFO     dense_7 (Dense)              (None, 256)               131328
INFO     _________________________________________________________________
INFO     dropout_6 (Dropout)          (None, 256)               0
INFO     _________________________________________________________________
INFO     dense_8 (Dense)              (None, 128)               32896
INFO     _________________________________________________________________
INFO     dropout_7 (Dropout)          (None, 128)               0
INFO     _________________________________________________________________
INFO     dense_9 (Dense)              (None, 1)                 129
INFO     =================================================================
INFO     Total params: 2,787,329
INFO     Trainable params: 2,787,329
INFO     Non-trainable params: 0
INFO     _________________________________________________________________
WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.
WARNING  `period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.
2024-07-15 15:43:22.413594: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0
2024-07-15 15:43:22.413872: I tensorflow/core/grappler/clusters/single_machine.cc:357] Starting new session
2024-07-15 15:43:22.417264: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:1137] Optimization results for grappler item: graph_to_optimize
  function_optimizer: function_optimizer did nothing. time = 0.022ms.
  function_optimizer: function_optimizer did nothing. time = 0.004ms.
Epoch 1/2000
22/22 - 2s - loss: 5.1000 - val_loss: 5.0221
Epoch 2/2000
22/22 - 1s - loss: 4.9799 - val_loss: 4.9154
Epoch 3/2000
22/22 - 1s - loss: 4.8777 - val_loss: 4.8229
Epoch 4/2000
22/22 - 1s - loss: 4.7875 - val_loss: 4.7411
Epoch 5/2000
22/22 - 1s - loss: 4.7090 - val_loss: 4.6698
Epoch 6/2000
22/22 - 1s - loss: 4.6431 - val_loss: 4.6159
Epoch 7/2000
22/22 - 1s - loss: 4.6009 - val_loss: 4.5797
Epoch 8/2000
22/22 - 1s - loss: 4.5699 - val_loss: 4.5550
Epoch 9/2000
22/22 - 1s - loss: 4.5421 - val_loss: 4.5366
Epoch 10/2000
22/22 - 1s - loss: 4.5206 - val_loss: 4.5189
Epoch 00010: val_loss improved from inf to 4.51894, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 11/2000
22/22 - 1s - loss: 4.4991 - val_loss: 4.5025
Epoch 12/2000
22/22 - 1s - loss: 4.4800 - val_loss: 4.4874
Epoch 13/2000
22/22 - 1s - loss: 4.4595 - val_loss: 4.4733
Epoch 14/2000
22/22 - 1s - loss: 4.4416 - val_loss: 4.4601
Epoch 15/2000
22/22 - 1s - loss: 4.4262 - val_loss: 4.4469
Epoch 16/2000
22/22 - 1s - loss: 4.4084 - val_loss: 4.4344
Epoch 17/2000
22/22 - 1s - loss: 4.3941 - val_loss: 4.4219
Epoch 18/2000
22/22 - 1s - loss: 4.3778 - val_loss: 4.4097
Epoch 19/2000
22/22 - 1s - loss: 4.3634 - val_loss: 4.3976
Epoch 20/2000
22/22 - 1s - loss: 4.3500 - val_loss: 4.3861
Epoch 00020: val_loss improved from 4.51894 to 4.38607, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 21/2000
22/22 - 1s - loss: 4.3329 - val_loss: 4.3747
Epoch 22/2000
22/22 - 1s - loss: 4.3214 - val_loss: 4.3634
Epoch 23/2000
22/22 - 1s - loss: 4.3077 - val_loss: 4.3521
Epoch 24/2000
22/22 - 1s - loss: 4.2936 - val_loss: 4.3413
Epoch 25/2000
22/22 - 1s - loss: 4.2820 - val_loss: 4.3302
Epoch 26/2000
22/22 - 1s - loss: 4.2649 - val_loss: 4.3195
Epoch 27/2000
22/22 - 1s - loss: 4.2523 - val_loss: 4.3091
Epoch 28/2000
22/22 - 1s - loss: 4.2392 - val_loss: 4.2984
Epoch 29/2000
22/22 - 1s - loss: 4.2253 - val_loss: 4.2879
Epoch 30/2000
22/22 - 1s - loss: 4.2125 - val_loss: 4.2779
Epoch 00030: val_loss improved from 4.38607 to 4.27790, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 31/2000
22/22 - 1s - loss: 4.2009 - val_loss: 4.2678
Epoch 32/2000
22/22 - 1s - loss: 4.1871 - val_loss: 4.2575
Epoch 33/2000
22/22 - 1s - loss: 4.1739 - val_loss: 4.2478
Epoch 34/2000
22/22 - 1s - loss: 4.1627 - val_loss: 4.2379
Epoch 35/2000
22/22 - 1s - loss: 4.1476 - val_loss: 4.2281
Epoch 36/2000
22/22 - 1s - loss: 4.1361 - val_loss: 4.2185
Epoch 37/2000
22/22 - 1s - loss: 4.1227 - val_loss: 4.2088
Epoch 38/2000
22/22 - 1s - loss: 4.1118 - val_loss: 4.1993
Epoch 39/2000
22/22 - 1s - loss: 4.0970 - val_loss: 4.1898
Epoch 40/2000
22/22 - 1s - loss: 4.0863 - val_loss: 4.1805
Epoch 00040: val_loss improved from 4.27790 to 4.18052, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 41/2000
22/22 - 1s - loss: 4.0727 - val_loss: 4.1710
Epoch 42/2000
22/22 - 1s - loss: 4.0603 - val_loss: 4.1623
Epoch 43/2000
22/22 - 1s - loss: 4.0473 - val_loss: 4.1532
Epoch 44/2000
22/22 - 1s - loss: 4.0385 - val_loss: 4.1445
Epoch 45/2000
22/22 - 1s - loss: 4.0266 - val_loss: 4.1356
Epoch 46/2000
22/22 - 1s - loss: 4.0132 - val_loss: 4.1268
Epoch 47/2000
22/22 - 1s - loss: 4.0005 - val_loss: 4.1186
Epoch 48/2000
22/22 - 1s - loss: 3.9874 - val_loss: 4.1110
Epoch 49/2000
22/22 - 1s - loss: 3.9771 - val_loss: 4.1021
Epoch 50/2000
22/22 - 1s - loss: 3.9658 - val_loss: 4.0941
Epoch 00050: val_loss improved from 4.18052 to 4.09412, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 51/2000
22/22 - 1s - loss: 3.9546 - val_loss: 4.0861
Epoch 52/2000
22/22 - 1s - loss: 3.9437 - val_loss: 4.0775
Epoch 53/2000
22/22 - 1s - loss: 3.9318 - val_loss: 4.0704
Epoch 54/2000
22/22 - 1s - loss: 3.9211 - val_loss: 4.0627
Epoch 55/2000
22/22 - 1s - loss: 3.9092 - val_loss: 4.0550
Epoch 56/2000
22/22 - 1s - loss: 3.8972 - val_loss: 4.0471
Epoch 57/2000
22/22 - 1s - loss: 3.8879 - val_loss: 4.0400
Epoch 58/2000
22/22 - 1s - loss: 3.8765 - val_loss: 4.0330
Epoch 59/2000
22/22 - 1s - loss: 3.8665 - val_loss: 4.0262
Epoch 60/2000
22/22 - 1s - loss: 3.8566 - val_loss: 4.0190
Epoch 00060: val_loss improved from 4.09412 to 4.01898, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 61/2000
22/22 - 1s - loss: 3.8466 - val_loss: 4.0118
Epoch 62/2000
22/22 - 1s - loss: 3.8346 - val_loss: 4.0046
Epoch 63/2000
22/22 - 1s - loss: 3.8263 - val_loss: 3.9979
Epoch 64/2000
22/22 - 1s - loss: 3.8148 - val_loss: 3.9915
Epoch 65/2000
22/22 - 1s - loss: 3.8043 - val_loss: 3.9855
Epoch 66/2000
22/22 - 1s - loss: 3.7940 - val_loss: 3.9781
Epoch 67/2000
22/22 - 1s - loss: 3.7854 - val_loss: 3.9719
Epoch 68/2000
22/22 - 1s - loss: 3.7758 - val_loss: 3.9659
Epoch 69/2000
22/22 - 1s - loss: 3.7668 - val_loss: 3.9596
Epoch 70/2000
22/22 - 1s - loss: 3.7568 - val_loss: 3.9536
Epoch 00070: val_loss improved from 4.01898 to 3.95360, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 71/2000
22/22 - 1s - loss: 3.7465 - val_loss: 3.9473
Epoch 72/2000
22/22 - 1s - loss: 3.7403 - val_loss: 3.9407
Epoch 73/2000
22/22 - 1s - loss: 3.7296 - val_loss: 3.9361
Epoch 74/2000
22/22 - 1s - loss: 3.7208 - val_loss: 3.9295
Epoch 75/2000
22/22 - 1s - loss: 3.7105 - val_loss: 3.9243
Epoch 76/2000
22/22 - 1s - loss: 3.7030 - val_loss: 3.9178
Epoch 77/2000
22/22 - 1s - loss: 3.6935 - val_loss: 3.9125
Epoch 78/2000
22/22 - 1s - loss: 3.6849 - val_loss: 3.9067
Epoch 79/2000
22/22 - 1s - loss: 3.6745 - val_loss: 3.9007
Epoch 80/2000
22/22 - 1s - loss: 3.6691 - val_loss: 3.8958
Epoch 00080: val_loss improved from 3.95360 to 3.89581, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 81/2000
22/22 - 1s - loss: 3.6587 - val_loss: 3.8900
Epoch 82/2000
22/22 - 1s - loss: 3.6534 - val_loss: 3.8841
Epoch 83/2000
22/22 - 1s - loss: 3.6434 - val_loss: 3.8785
Epoch 84/2000
22/22 - 1s - loss: 3.6346 - val_loss: 3.8738
Epoch 85/2000
22/22 - 1s - loss: 3.6266 - val_loss: 3.8678
Epoch 86/2000
22/22 - 1s - loss: 3.6194 - val_loss: 3.8622
Epoch 87/2000
22/22 - 1s - loss: 3.6138 - val_loss: 3.8570
Epoch 88/2000
22/22 - 1s - loss: 3.6045 - val_loss: 3.8514
Epoch 89/2000
22/22 - 1s - loss: 3.5944 - val_loss: 3.8466
Epoch 90/2000
22/22 - 1s - loss: 3.5909 - val_loss: 3.8432
Epoch 00090: val_loss improved from 3.89581 to 3.84316, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 91/2000
22/22 - 1s - loss: 3.5827 - val_loss: 3.8374
Epoch 92/2000
22/22 - 1s - loss: 3.5729 - val_loss: 3.8319
Epoch 93/2000
22/22 - 1s - loss: 3.5655 - val_loss: 3.8274
Epoch 94/2000
22/22 - 1s - loss: 3.5597 - val_loss: 3.8226
Epoch 95/2000
22/22 - 1s - loss: 3.5523 - val_loss: 3.8180
Epoch 96/2000
22/22 - 1s - loss: 3.5442 - val_loss: 3.8122
Epoch 97/2000
22/22 - 1s - loss: 3.5398 - val_loss: 3.8071
Epoch 98/2000
22/22 - 1s - loss: 3.5282 - val_loss: 3.8033
Epoch 99/2000
22/22 - 1s - loss: 3.5240 - val_loss: 3.7984
Epoch 100/2000
22/22 - 1s - loss: 3.5170 - val_loss: 3.7933
Epoch 00100: val_loss improved from 3.84316 to 3.79330, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 101/2000
22/22 - 1s - loss: 3.5079 - val_loss: 3.7892
Epoch 102/2000
22/22 - 1s - loss: 3.5037 - val_loss: 3.7858
Epoch 103/2000
22/22 - 1s - loss: 3.4946 - val_loss: 3.7788
Epoch 104/2000
22/22 - 1s - loss: 3.4872 - val_loss: 3.7755
Epoch 105/2000
22/22 - 1s - loss: 3.4823 - val_loss: 3.7696
Epoch 106/2000
22/22 - 1s - loss: 3.4741 - val_loss: 3.7653
Epoch 107/2000
22/22 - 1s - loss: 3.4709 - val_loss: 3.7611
Epoch 108/2000
22/22 - 1s - loss: 3.4619 - val_loss: 3.7557
Epoch 109/2000
22/22 - 1s - loss: 3.4559 - val_loss: 3.7507
Epoch 110/2000
22/22 - 1s - loss: 3.4517 - val_loss: 3.7459
Epoch 00110: val_loss improved from 3.79330 to 3.74595, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 111/2000
22/22 - 1s - loss: 3.4423 - val_loss: 3.7424
Epoch 112/2000
22/22 - 1s - loss: 3.4380 - val_loss: 3.7378
Epoch 113/2000
22/22 - 1s - loss: 3.4344 - val_loss: 3.7336
Epoch 114/2000
22/22 - 1s - loss: 3.4260 - val_loss: 3.7297
Epoch 115/2000
22/22 - 1s - loss: 3.4180 - val_loss: 3.7244
Epoch 116/2000
22/22 - 1s - loss: 3.4133 - val_loss: 3.7192
Epoch 117/2000
22/22 - 1s - loss: 3.4062 - val_loss: 3.7148
Epoch 118/2000
22/22 - 1s - loss: 3.4011 - val_loss: 3.7104
Epoch 119/2000
22/22 - 1s - loss: 3.3973 - val_loss: 3.7062
Epoch 120/2000
22/22 - 1s - loss: 3.3909 - val_loss: 3.7024
Epoch 00120: val_loss improved from 3.74595 to 3.70236, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 121/2000
22/22 - 1s - loss: 3.3848 - val_loss: 3.6981
Epoch 122/2000
22/22 - 1s - loss: 3.3781 - val_loss: 3.6931
Epoch 123/2000
22/22 - 1s - loss: 3.3716 - val_loss: 3.6901
Epoch 124/2000
22/22 - 1s - loss: 3.3669 - val_loss: 3.6863
Epoch 125/2000
22/22 - 1s - loss: 3.3597 - val_loss: 3.6815
Epoch 126/2000
22/22 - 1s - loss: 3.3585 - val_loss: 3.6770
Epoch 127/2000
22/22 - 1s - loss: 3.3499 - val_loss: 3.6721
Epoch 128/2000
22/22 - 1s - loss: 3.3448 - val_loss: 3.6685
Epoch 129/2000
22/22 - 1s - loss: 3.3417 - val_loss: 3.6653
Epoch 130/2000
22/22 - 1s - loss: 3.3355 - val_loss: 3.6616
Epoch 00130: val_loss improved from 3.70236 to 3.66165, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 131/2000
22/22 - 1s - loss: 3.3310 - val_loss: 3.6555
Epoch 132/2000
22/22 - 1s - loss: 3.3227 - val_loss: 3.6513
Epoch 133/2000
22/22 - 1s - loss: 3.3188 - val_loss: 3.6479
Epoch 134/2000
22/22 - 1s - loss: 3.3114 - val_loss: 3.6426
Epoch 135/2000
22/22 - 1s - loss: 3.3074 - val_loss: 3.6387
Epoch 136/2000
22/22 - 1s - loss: 3.3053 - val_loss: 3.6358
Epoch 137/2000
22/22 - 1s - loss: 3.2947 - val_loss: 3.6323
Epoch 138/2000
22/22 - 1s - loss: 3.2905 - val_loss: 3.6279
Epoch 139/2000
22/22 - 1s - loss: 3.2880 - val_loss: 3.6237
Epoch 140/2000
22/22 - 1s - loss: 3.2821 - val_loss: 3.6200
Epoch 00140: val_loss improved from 3.66165 to 3.62000, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 141/2000
22/22 - 1s - loss: 3.2804 - val_loss: 3.6164
Epoch 142/2000
22/22 - 1s - loss: 3.2737 - val_loss: 3.6128
Epoch 143/2000
22/22 - 1s - loss: 3.2686 - val_loss: 3.6093
Epoch 144/2000
22/22 - 1s - loss: 3.2629 - val_loss: 3.6057
Epoch 145/2000
22/22 - 1s - loss: 3.2599 - val_loss: 3.5985
Epoch 146/2000
22/22 - 1s - loss: 3.2549 - val_loss: 3.5946
Epoch 147/2000
22/22 - 1s - loss: 3.2465 - val_loss: 3.5911
Epoch 148/2000
22/22 - 1s - loss: 3.2399 - val_loss: 3.5878
Epoch 149/2000
22/22 - 1s - loss: 3.2360 - val_loss: 3.5848
Epoch 150/2000
22/22 - 1s - loss: 3.2341 - val_loss: 3.5803
Epoch 00150: val_loss improved from 3.62000 to 3.58026, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 151/2000
22/22 - 1s - loss: 3.2296 - val_loss: 3.5766
Epoch 152/2000
22/22 - 1s - loss: 3.2232 - val_loss: 3.5730
Epoch 153/2000
22/22 - 1s - loss: 3.2206 - val_loss: 3.5684
Epoch 154/2000
22/22 - 1s - loss: 3.2150 - val_loss: 3.5644
Epoch 155/2000
22/22 - 1s - loss: 3.2112 - val_loss: 3.5610
Epoch 156/2000
22/22 - 1s - loss: 3.2063 - val_loss: 3.5577
Epoch 157/2000
22/22 - 1s - loss: 3.2057 - val_loss: 3.5530
Epoch 158/2000
22/22 - 1s - loss: 3.1984 - val_loss: 3.5483
Epoch 159/2000
22/22 - 1s - loss: 3.1901 - val_loss: 3.5455
Epoch 160/2000
22/22 - 1s - loss: 3.1896 - val_loss: 3.5419
Epoch 00160: val_loss improved from 3.58026 to 3.54195, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 161/2000
22/22 - 1s - loss: 3.1856 - val_loss: 3.5374
Epoch 162/2000
22/22 - 1s - loss: 3.1778 - val_loss: 3.5349
Epoch 163/2000
22/22 - 1s - loss: 3.1757 - val_loss: 3.5325
Epoch 164/2000
22/22 - 1s - loss: 3.1739 - val_loss: 3.5297
Epoch 165/2000
22/22 - 1s - loss: 3.1666 - val_loss: 3.5225
Epoch 166/2000
22/22 - 1s - loss: 3.1615 - val_loss: 3.5189
Epoch 167/2000
22/22 - 1s - loss: 3.1584 - val_loss: 3.5164
Epoch 168/2000
22/22 - 1s - loss: 3.1505 - val_loss: 3.5136
Epoch 169/2000
22/22 - 1s - loss: 3.1474 - val_loss: 3.5083
Epoch 170/2000
22/22 - 1s - loss: 3.1435 - val_loss: 3.5053
Epoch 00170: val_loss improved from 3.54195 to 3.50528, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 171/2000
22/22 - 1s - loss: 3.1401 - val_loss: 3.5022
Epoch 172/2000
22/22 - 1s - loss: 3.1366 - val_loss: 3.4982
Epoch 173/2000
22/22 - 1s - loss: 3.1329 - val_loss: 3.4956
Epoch 174/2000
22/22 - 1s - loss: 3.1282 - val_loss: 3.4925
Epoch 175/2000
22/22 - 1s - loss: 3.1250 - val_loss: 3.4892
Epoch 176/2000
22/22 - 1s - loss: 3.1177 - val_loss: 3.4864
Epoch 177/2000
22/22 - 1s - loss: 3.1132 - val_loss: 3.4813
Epoch 178/2000
22/22 - 1s - loss: 3.1109 - val_loss: 3.4771
Epoch 179/2000
22/22 - 1s - loss: 3.1055 - val_loss: 3.4741
Epoch 180/2000
22/22 - 1s - loss: 3.1032 - val_loss: 3.4711
Epoch 00180: val_loss improved from 3.50528 to 3.47107, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 181/2000
22/22 - 1s - loss: 3.0970 - val_loss: 3.4675
Epoch 182/2000
22/22 - 1s - loss: 3.0976 - val_loss: 3.4640
Epoch 183/2000
22/22 - 1s - loss: 3.0910 - val_loss: 3.4603
Epoch 184/2000
22/22 - 1s - loss: 3.0903 - val_loss: 3.4571
Epoch 185/2000
22/22 - 1s - loss: 3.0840 - val_loss: 3.4543
Epoch 186/2000
22/22 - 1s - loss: 3.0802 - val_loss: 3.4501
Epoch 187/2000
22/22 - 1s - loss: 3.0733 - val_loss: 3.4478
Epoch 188/2000
22/22 - 1s - loss: 3.0707 - val_loss: 3.4441
Epoch 189/2000
22/22 - 1s - loss: 3.0668 - val_loss: 3.4399
Epoch 190/2000
22/22 - 1s - loss: 3.0630 - val_loss: 3.4362
Epoch 00190: val_loss improved from 3.47107 to 3.43624, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 191/2000
22/22 - 1s - loss: 3.0591 - val_loss: 3.4335
Epoch 192/2000
22/22 - 1s - loss: 3.0572 - val_loss: 3.4305
Epoch 193/2000
22/22 - 1s - loss: 3.0497 - val_loss: 3.4278
Epoch 194/2000
22/22 - 1s - loss: 3.0511 - val_loss: 3.4234
Epoch 195/2000
22/22 - 1s - loss: 3.0447 - val_loss: 3.4207
Epoch 196/2000
22/22 - 1s - loss: 3.0411 - val_loss: 3.4168
Epoch 197/2000
22/22 - 1s - loss: 3.0349 - val_loss: 3.4146
Epoch 198/2000
22/22 - 1s - loss: 3.0328 - val_loss: 3.4095
Epoch 199/2000
22/22 - 1s - loss: 3.0282 - val_loss: 3.4065
Epoch 200/2000
22/22 - 1s - loss: 3.0277 - val_loss: 3.4033
Epoch 00200: val_loss improved from 3.43624 to 3.40331, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 201/2000
22/22 - 1s - loss: 3.0206 - val_loss: 3.4008
Epoch 202/2000
22/22 - 1s - loss: 3.0162 - val_loss: 3.3973
Epoch 203/2000
22/22 - 1s - loss: 3.0170 - val_loss: 3.3941
Epoch 204/2000
22/22 - 1s - loss: 3.0127 - val_loss: 3.3911
Epoch 205/2000
22/22 - 1s - loss: 3.0053 - val_loss: 3.3882
Epoch 206/2000
22/22 - 1s - loss: 3.0021 - val_loss: 3.3849
Epoch 207/2000
22/22 - 1s - loss: 2.9984 - val_loss: 3.3816
Epoch 208/2000
22/22 - 1s - loss: 2.9977 - val_loss: 3.3788
Epoch 209/2000
22/22 - 1s - loss: 2.9911 - val_loss: 3.3760
Epoch 210/2000
22/22 - 1s - loss: 2.9882 - val_loss: 3.3715
Epoch 00210: val_loss improved from 3.40331 to 3.37151, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 211/2000
22/22 - 1s - loss: 2.9850 - val_loss: 3.3691
Epoch 212/2000
22/22 - 1s - loss: 2.9807 - val_loss: 3.3656
Epoch 213/2000
22/22 - 1s - loss: 2.9796 - val_loss: 3.3626
Epoch 214/2000
22/22 - 1s - loss: 2.9726 - val_loss: 3.3588
Epoch 215/2000
22/22 - 1s - loss: 2.9740 - val_loss: 3.3550
Epoch 216/2000
22/22 - 1s - loss: 2.9664 - val_loss: 3.3541
Epoch 217/2000
22/22 - 1s - loss: 2.9636 - val_loss: 3.3500
Epoch 218/2000
22/22 - 1s - loss: 2.9596 - val_loss: 3.3460
Epoch 219/2000
22/22 - 1s - loss: 2.9596 - val_loss: 3.3436
Epoch 220/2000
22/22 - 1s - loss: 2.9554 - val_loss: 3.3407
Epoch 00220: val_loss improved from 3.37151 to 3.34070, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 221/2000
22/22 - 1s - loss: 2.9495 - val_loss: 3.3372
Epoch 222/2000
22/22 - 1s - loss: 2.9435 - val_loss: 3.3347
Epoch 223/2000
22/22 - 1s - loss: 2.9390 - val_loss: 3.3308
Epoch 224/2000
22/22 - 1s - loss: 2.9405 - val_loss: 3.3286
Epoch 225/2000
22/22 - 1s - loss: 2.9353 - val_loss: 3.3255
Epoch 226/2000
22/22 - 1s - loss: 2.9338 - val_loss: 3.3243
Epoch 227/2000
22/22 - 1s - loss: 2.9307 - val_loss: 3.3193
Epoch 228/2000
22/22 - 1s - loss: 2.9269 - val_loss: 3.3158
Epoch 229/2000
22/22 - 1s - loss: 2.9221 - val_loss: 3.3128
Epoch 230/2000
22/22 - 1s - loss: 2.9202 - val_loss: 3.3094
Epoch 00230: val_loss improved from 3.34070 to 3.30944, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 231/2000
22/22 - 1s - loss: 2.9125 - val_loss: 3.3067
Epoch 232/2000
22/22 - 1s - loss: 2.9113 - val_loss: 3.3030
Epoch 233/2000
22/22 - 1s - loss: 2.9085 - val_loss: 3.3001
Epoch 234/2000
22/22 - 1s - loss: 2.9070 - val_loss: 3.2976
Epoch 235/2000
22/22 - 1s - loss: 2.9012 - val_loss: 3.2953
Epoch 236/2000
22/22 - 1s - loss: 2.8994 - val_loss: 3.2927
Epoch 237/2000
22/22 - 1s - loss: 2.8970 - val_loss: 3.2901
Epoch 238/2000
22/22 - 1s - loss: 2.8924 - val_loss: 3.2869
Epoch 239/2000
22/22 - 1s - loss: 2.8886 - val_loss: 3.2837
Epoch 240/2000
22/22 - 1s - loss: 2.8869 - val_loss: 3.2809
Epoch 00240: val_loss improved from 3.30944 to 3.28093, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 241/2000
22/22 - 1s - loss: 2.8819 - val_loss: 3.2783
Epoch 242/2000
22/22 - 1s - loss: 2.8776 - val_loss: 3.2749
Epoch 243/2000
22/22 - 1s - loss: 2.8766 - val_loss: 3.2714
Epoch 244/2000
22/22 - 1s - loss: 2.8734 - val_loss: 3.2680
Epoch 245/2000
22/22 - 1s - loss: 2.8695 - val_loss: 3.2647
Epoch 246/2000
22/22 - 1s - loss: 2.8679 - val_loss: 3.2627
Epoch 247/2000
22/22 - 1s - loss: 2.8654 - val_loss: 3.2605
Epoch 248/2000
22/22 - 1s - loss: 2.8648 - val_loss: 3.2552
Epoch 249/2000
22/22 - 1s - loss: 2.8555 - val_loss: 3.2535
Epoch 250/2000
22/22 - 1s - loss: 2.8546 - val_loss: 3.2526
Epoch 00250: val_loss improved from 3.28093 to 3.25258, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 251/2000
22/22 - 1s - loss: 2.8515 - val_loss: 3.2486
Epoch 252/2000
22/22 - 1s - loss: 2.8505 - val_loss: 3.2452
Epoch 253/2000
22/22 - 1s - loss: 2.8422 - val_loss: 3.2427
Epoch 254/2000
22/22 - 1s - loss: 2.8390 - val_loss: 3.2393
Epoch 255/2000
22/22 - 1s - loss: 2.8367 - val_loss: 3.2358
Epoch 256/2000
22/22 - 1s - loss: 2.8335 - val_loss: 3.2349
Epoch 257/2000
22/22 - 1s - loss: 2.8325 - val_loss: 3.2318
Epoch 258/2000
22/22 - 1s - loss: 2.8296 - val_loss: 3.2289
Epoch 259/2000
22/22 - 1s - loss: 2.8282 - val_loss: 3.2256
Epoch 260/2000
22/22 - 1s - loss: 2.8243 - val_loss: 3.2235
Epoch 00260: val_loss improved from 3.25258 to 3.22354, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 261/2000
22/22 - 1s - loss: 2.8154 - val_loss: 3.2200
Epoch 262/2000
22/22 - 1s - loss: 2.8150 - val_loss: 3.2171
Epoch 263/2000
22/22 - 1s - loss: 2.8126 - val_loss: 3.2138
Epoch 264/2000
22/22 - 1s - loss: 2.8092 - val_loss: 3.2105
Epoch 265/2000
22/22 - 1s - loss: 2.8075 - val_loss: 3.2092
Epoch 266/2000
22/22 - 1s - loss: 2.8037 - val_loss: 3.2062
Epoch 267/2000
22/22 - 1s - loss: 2.8029 - val_loss: 3.2021
Epoch 268/2000
22/22 - 1s - loss: 2.8005 - val_loss: 3.2003
Epoch 269/2000
22/22 - 1s - loss: 2.7945 - val_loss: 3.1977
Epoch 270/2000
22/22 - 1s - loss: 2.7919 - val_loss: 3.1948
Epoch 00270: val_loss improved from 3.22354 to 3.19483, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 271/2000
22/22 - 1s - loss: 2.7884 - val_loss: 3.1916
Epoch 272/2000
22/22 - 1s - loss: 2.7842 - val_loss: 3.1902
Epoch 273/2000
22/22 - 1s - loss: 2.7838 - val_loss: 3.1860
Epoch 274/2000
22/22 - 1s - loss: 2.7811 - val_loss: 3.1829
Epoch 275/2000
22/22 - 1s - loss: 2.7766 - val_loss: 3.1810
Epoch 276/2000
22/22 - 1s - loss: 2.7695 - val_loss: 3.1771
Epoch 277/2000
22/22 - 1s - loss: 2.7693 - val_loss: 3.1744
Epoch 278/2000
22/22 - 1s - loss: 2.7657 - val_loss: 3.1719
Epoch 279/2000
22/22 - 1s - loss: 2.7668 - val_loss: 3.1688
Epoch 280/2000
22/22 - 1s - loss: 2.7641 - val_loss: 3.1668
Epoch 00280: val_loss improved from 3.19483 to 3.16680, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 281/2000
22/22 - 1s - loss: 2.7591 - val_loss: 3.1654
Epoch 282/2000
22/22 - 1s - loss: 2.7534 - val_loss: 3.1607
Epoch 283/2000
22/22 - 1s - loss: 2.7545 - val_loss: 3.1575
Epoch 284/2000
22/22 - 1s - loss: 2.7506 - val_loss: 3.1558
Epoch 285/2000
22/22 - 1s - loss: 2.7503 - val_loss: 3.1534
Epoch 286/2000
22/22 - 1s - loss: 2.7466 - val_loss: 3.1501
Epoch 287/2000
22/22 - 1s - loss: 2.7415 - val_loss: 3.1471
Epoch 288/2000
22/22 - 1s - loss: 2.7379 - val_loss: 3.1461
Epoch 289/2000
22/22 - 1s - loss: 2.7372 - val_loss: 3.1446
Epoch 290/2000
22/22 - 1s - loss: 2.7332 - val_loss: 3.1401
Epoch 00290: val_loss improved from 3.16680 to 3.14010, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 291/2000
22/22 - 1s - loss: 2.7307 - val_loss: 3.1380
Epoch 292/2000
22/22 - 1s - loss: 2.7273 - val_loss: 3.1362
Epoch 293/2000
22/22 - 1s - loss: 2.7252 - val_loss: 3.1331
Epoch 294/2000
22/22 - 1s - loss: 2.7241 - val_loss: 3.1304
Epoch 295/2000
22/22 - 1s - loss: 2.7218 - val_loss: 3.1270
Epoch 296/2000
22/22 - 1s - loss: 2.7148 - val_loss: 3.1243
Epoch 297/2000
22/22 - 1s - loss: 2.7128 - val_loss: 3.1225
Epoch 298/2000
22/22 - 1s - loss: 2.7122 - val_loss: 3.1197
Epoch 299/2000
22/22 - 1s - loss: 2.7078 - val_loss: 3.1171
Epoch 300/2000
22/22 - 1s - loss: 2.7038 - val_loss: 3.1151
Epoch 00300: val_loss improved from 3.14010 to 3.11513, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 301/2000
22/22 - 1s - loss: 2.7020 - val_loss: 3.1129
Epoch 302/2000
22/22 - 1s - loss: 2.6953 - val_loss: 3.1095
Epoch 303/2000
22/22 - 1s - loss: 2.6980 - val_loss: 3.1061
Epoch 304/2000
22/22 - 1s - loss: 2.6927 - val_loss: 3.1046
Epoch 305/2000
22/22 - 1s - loss: 2.6898 - val_loss: 3.1011
Epoch 306/2000
22/22 - 1s - loss: 2.6895 - val_loss: 3.0987
Epoch 307/2000
22/22 - 1s - loss: 2.6868 - val_loss: 3.0948
Epoch 308/2000
22/22 - 1s - loss: 2.6805 - val_loss: 3.0933
Epoch 309/2000
22/22 - 1s - loss: 2.6818 - val_loss: 3.0915
Epoch 310/2000
22/22 - 1s - loss: 2.6782 - val_loss: 3.0883
Epoch 00310: val_loss improved from 3.11513 to 3.08832, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 311/2000
22/22 - 1s - loss: 2.6727 - val_loss: 3.0854
Epoch 312/2000
22/22 - 1s - loss: 2.6722 - val_loss: 3.0835
Epoch 313/2000
22/22 - 1s - loss: 2.6694 - val_loss: 3.0802
Epoch 314/2000
22/22 - 1s - loss: 2.6690 - val_loss: 3.0785
Epoch 315/2000
22/22 - 1s - loss: 2.6631 - val_loss: 3.0765
Epoch 316/2000
22/22 - 1s - loss: 2.6610 - val_loss: 3.0742
Epoch 317/2000
22/22 - 1s - loss: 2.6587 - val_loss: 3.0724
Epoch 318/2000
22/22 - 1s - loss: 2.6562 - val_loss: 3.0683
Epoch 319/2000
22/22 - 1s - loss: 2.6522 - val_loss: 3.0654
Epoch 320/2000
22/22 - 1s - loss: 2.6510 - val_loss: 3.0637
Epoch 00320: val_loss improved from 3.08832 to 3.06367, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 321/2000
22/22 - 1s - loss: 2.6454 - val_loss: 3.0617
Epoch 322/2000
22/22 - 1s - loss: 2.6472 - val_loss: 3.0588
Epoch 323/2000
22/22 - 1s - loss: 2.6430 - val_loss: 3.0559
Epoch 324/2000
22/22 - 1s - loss: 2.6394 - val_loss: 3.0538
Epoch 325/2000
22/22 - 1s - loss: 2.6398 - val_loss: 3.0506
Epoch 326/2000
22/22 - 1s - loss: 2.6357 - val_loss: 3.0494
Epoch 327/2000
22/22 - 1s - loss: 2.6339 - val_loss: 3.0455
Epoch 328/2000
22/22 - 1s - loss: 2.6285 - val_loss: 3.0429
Epoch 329/2000
22/22 - 1s - loss: 2.6254 - val_loss: 3.0407
Epoch 330/2000
22/22 - 1s - loss: 2.6222 - val_loss: 3.0386
Epoch 00330: val_loss improved from 3.06367 to 3.03862, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 331/2000
22/22 - 1s - loss: 2.6223 - val_loss: 3.0357
Epoch 332/2000
22/22 - 1s - loss: 2.6211 - val_loss: 3.0329
Epoch 333/2000
22/22 - 1s - loss: 2.6170 - val_loss: 3.0314
Epoch 334/2000
22/22 - 1s - loss: 2.6137 - val_loss: 3.0286
Epoch 335/2000
22/22 - 1s - loss: 2.6097 - val_loss: 3.0259
Epoch 336/2000
22/22 - 1s - loss: 2.6088 - val_loss: 3.0238
Epoch 337/2000
22/22 - 1s - loss: 2.6032 - val_loss: 3.0218
Epoch 338/2000
22/22 - 1s - loss: 2.6031 - val_loss: 3.0204
Epoch 339/2000
22/22 - 1s - loss: 2.6034 - val_loss: 3.0162
Epoch 340/2000
22/22 - 1s - loss: 2.5992 - val_loss: 3.0142
Epoch 00340: val_loss improved from 3.03862 to 3.01417, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 341/2000
22/22 - 1s - loss: 2.5957 - val_loss: 3.0118
Epoch 342/2000
22/22 - 1s - loss: 2.5923 - val_loss: 3.0084
Epoch 343/2000
22/22 - 1s - loss: 2.5919 - val_loss: 3.0068
Epoch 344/2000
22/22 - 1s - loss: 2.5905 - val_loss: 3.0032
Epoch 345/2000
22/22 - 1s - loss: 2.5862 - val_loss: 3.0014
Epoch 346/2000
22/22 - 1s - loss: 2.5807 - val_loss: 3.0004
Epoch 347/2000
22/22 - 1s - loss: 2.5781 - val_loss: 2.9989
Epoch 348/2000
22/22 - 1s - loss: 2.5769 - val_loss: 2.9962
Epoch 349/2000
22/22 - 1s - loss: 2.5765 - val_loss: 2.9925
Epoch 350/2000
22/22 - 1s - loss: 2.5723 - val_loss: 2.9899
Epoch 00350: val_loss improved from 3.01417 to 2.98987, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 351/2000
22/22 - 1s - loss: 2.5704 - val_loss: 2.9869
Epoch 352/2000
22/22 - 1s - loss: 2.5673 - val_loss: 2.9848
Epoch 353/2000
22/22 - 1s - loss: 2.5682 - val_loss: 2.9821
Epoch 354/2000
22/22 - 1s - loss: 2.5635 - val_loss: 2.9800
Epoch 355/2000
22/22 - 1s - loss: 2.5587 - val_loss: 2.9778
Epoch 356/2000
22/22 - 1s - loss: 2.5552 - val_loss: 2.9758
Epoch 357/2000
22/22 - 1s - loss: 2.5588 - val_loss: 2.9736
Epoch 358/2000
22/22 - 1s - loss: 2.5530 - val_loss: 2.9718
Epoch 359/2000
22/22 - 1s - loss: 2.5502 - val_loss: 2.9688
Epoch 360/2000
22/22 - 1s - loss: 2.5466 - val_loss: 2.9670
Epoch 00360: val_loss improved from 2.98987 to 2.96699, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 361/2000
22/22 - 1s - loss: 2.5457 - val_loss: 2.9634
Epoch 362/2000
22/22 - 1s - loss: 2.5446 - val_loss: 2.9609
Epoch 363/2000
22/22 - 1s - loss: 2.5389 - val_loss: 2.9590
Epoch 364/2000
22/22 - 1s - loss: 2.5401 - val_loss: 2.9576
Epoch 365/2000
22/22 - 1s - loss: 2.5399 - val_loss: 2.9532
Epoch 366/2000
22/22 - 1s - loss: 2.5326 - val_loss: 2.9524
Epoch 367/2000
22/22 - 1s - loss: 2.5312 - val_loss: 2.9505
Epoch 368/2000
22/22 - 1s - loss: 2.5292 - val_loss: 2.9480
Epoch 369/2000
22/22 - 1s - loss: 2.5262 - val_loss: 2.9451
Epoch 370/2000
22/22 - 1s - loss: 2.5235 - val_loss: 2.9431
Epoch 00370: val_loss improved from 2.96699 to 2.94313, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 371/2000
22/22 - 1s - loss: 2.5225 - val_loss: 2.9406
Epoch 372/2000
22/22 - 1s - loss: 2.5220 - val_loss: 2.9395
Epoch 373/2000
22/22 - 1s - loss: 2.5153 - val_loss: 2.9365
Epoch 374/2000
22/22 - 1s - loss: 2.5146 - val_loss: 2.9353
Epoch 375/2000
22/22 - 1s - loss: 2.5126 - val_loss: 2.9336
Epoch 376/2000
22/22 - 1s - loss: 2.5109 - val_loss: 2.9298
Epoch 377/2000
22/22 - 1s - loss: 2.5072 - val_loss: 2.9272
Epoch 378/2000
22/22 - 1s - loss: 2.5057 - val_loss: 2.9252
Epoch 379/2000
22/22 - 1s - loss: 2.4995 - val_loss: 2.9230
Epoch 380/2000
22/22 - 1s - loss: 2.5042 - val_loss: 2.9201
Epoch 00380: val_loss improved from 2.94313 to 2.92006, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 381/2000
22/22 - 1s - loss: 2.4971 - val_loss: 2.9176
Epoch 382/2000
22/22 - 1s - loss: 2.4945 - val_loss: 2.9152
Epoch 383/2000
22/22 - 1s - loss: 2.4937 - val_loss: 2.9130
Epoch 384/2000
22/22 - 1s - loss: 2.4922 - val_loss: 2.9110
Epoch 385/2000
22/22 - 1s - loss: 2.4858 - val_loss: 2.9090
Epoch 386/2000
22/22 - 1s - loss: 2.4859 - val_loss: 2.9062
Epoch 387/2000
22/22 - 1s - loss: 2.4818 - val_loss: 2.9033
Epoch 388/2000
22/22 - 1s - loss: 2.4823 - val_loss: 2.9011
Epoch 389/2000
22/22 - 1s - loss: 2.4777 - val_loss: 2.9010
Epoch 390/2000
22/22 - 1s - loss: 2.4752 - val_loss: 2.8997
Epoch 00390: val_loss improved from 2.92006 to 2.89967, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 391/2000
22/22 - 1s - loss: 2.4753 - val_loss: 2.8972
Epoch 392/2000
22/22 - 1s - loss: 2.4746 - val_loss: 2.8942
Epoch 393/2000
22/22 - 1s - loss: 2.4673 - val_loss: 2.8907
Epoch 394/2000
22/22 - 1s - loss: 2.4704 - val_loss: 2.8880
Epoch 395/2000
22/22 - 1s - loss: 2.4640 - val_loss: 2.8848
Epoch 396/2000
22/22 - 1s - loss: 2.4627 - val_loss: 2.8844
Epoch 397/2000
22/22 - 1s - loss: 2.4582 - val_loss: 2.8838
Epoch 398/2000
22/22 - 1s - loss: 2.4589 - val_loss: 2.8806
Epoch 399/2000
22/22 - 1s - loss: 2.4581 - val_loss: 2.8774
Epoch 400/2000
22/22 - 1s - loss: 2.4502 - val_loss: 2.8762
Epoch 00400: val_loss improved from 2.89967 to 2.87619, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 401/2000
22/22 - 1s - loss: 2.4526 - val_loss: 2.8737
Epoch 402/2000
22/22 - 1s - loss: 2.4476 - val_loss: 2.8717
Epoch 403/2000
22/22 - 1s - loss: 2.4468 - val_loss: 2.8691
Epoch 404/2000
22/22 - 1s - loss: 2.4468 - val_loss: 2.8660
Epoch 405/2000
22/22 - 1s - loss: 2.4425 - val_loss: 2.8639
Epoch 406/2000
22/22 - 1s - loss: 2.4384 - val_loss: 2.8628
Epoch 407/2000
22/22 - 1s - loss: 2.4357 - val_loss: 2.8614
Epoch 408/2000
22/22 - 1s - loss: 2.4373 - val_loss: 2.8596
Epoch 409/2000
22/22 - 1s - loss: 2.4314 - val_loss: 2.8566
Epoch 410/2000
22/22 - 1s - loss: 2.4307 - val_loss: 2.8542
Epoch 00410: val_loss improved from 2.87619 to 2.85418, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 411/2000
22/22 - 1s - loss: 2.4275 - val_loss: 2.8514
Epoch 412/2000
22/22 - 1s - loss: 2.4259 - val_loss: 2.8506
Epoch 413/2000
22/22 - 1s - loss: 2.4226 - val_loss: 2.8483
Epoch 414/2000
22/22 - 1s - loss: 2.4207 - val_loss: 2.8457
Epoch 415/2000
22/22 - 1s - loss: 2.4189 - val_loss: 2.8432
Epoch 416/2000
22/22 - 1s - loss: 2.4180 - val_loss: 2.8406
Epoch 417/2000
22/22 - 1s - loss: 2.4146 - val_loss: 2.8385
Epoch 418/2000
22/22 - 1s - loss: 2.4114 - val_loss: 2.8371
Epoch 419/2000
22/22 - 1s - loss: 2.4102 - val_loss: 2.8341
Epoch 420/2000
22/22 - 1s - loss: 2.4083 - val_loss: 2.8318
Epoch 00420: val_loss improved from 2.85418 to 2.83176, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 421/2000
22/22 - 1s - loss: 2.4051 - val_loss: 2.8307
Epoch 422/2000
22/22 - 1s - loss: 2.4055 - val_loss: 2.8274
Epoch 423/2000
22/22 - 1s - loss: 2.4001 - val_loss: 2.8247
Epoch 424/2000
22/22 - 1s - loss: 2.4001 - val_loss: 2.8255
Epoch 425/2000
22/22 - 1s - loss: 2.3994 - val_loss: 2.8217
Epoch 426/2000
22/22 - 1s - loss: 2.3933 - val_loss: 2.8192
Epoch 427/2000
22/22 - 1s - loss: 2.3952 - val_loss: 2.8179
Epoch 428/2000
22/22 - 1s - loss: 2.3911 - val_loss: 2.8159
Epoch 429/2000
22/22 - 1s - loss: 2.3884 - val_loss: 2.8142
Epoch 430/2000
22/22 - 1s - loss: 2.3860 - val_loss: 2.8111
Epoch 00430: val_loss improved from 2.83176 to 2.81108, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 431/2000
22/22 - 1s - loss: 2.3838 - val_loss: 2.8092
Epoch 432/2000
22/22 - 1s - loss: 2.3801 - val_loss: 2.8074
Epoch 433/2000
22/22 - 1s - loss: 2.3810 - val_loss: 2.8048
Epoch 434/2000
22/22 - 1s - loss: 2.3784 - val_loss: 2.8032
Epoch 435/2000
22/22 - 1s - loss: 2.3746 - val_loss: 2.8015
Epoch 436/2000
22/22 - 1s - loss: 2.3730 - val_loss: 2.8002
Epoch 437/2000
22/22 - 1s - loss: 2.3707 - val_loss: 2.7966
Epoch 438/2000
22/22 - 1s - loss: 2.3688 - val_loss: 2.7943
Epoch 439/2000
22/22 - 1s - loss: 2.3680 - val_loss: 2.7922
Epoch 440/2000
22/22 - 1s - loss: 2.3642 - val_loss: 2.7917
Epoch 00440: val_loss improved from 2.81108 to 2.79166, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 441/2000
22/22 - 1s - loss: 2.3636 - val_loss: 2.7888
Epoch 442/2000
22/22 - 1s - loss: 2.3590 - val_loss: 2.7863
Epoch 443/2000
22/22 - 1s - loss: 2.3590 - val_loss: 2.7837
Epoch 444/2000
22/22 - 1s - loss: 2.3570 - val_loss: 2.7820
Epoch 445/2000
22/22 - 1s - loss: 2.3543 - val_loss: 2.7807
Epoch 446/2000
22/22 - 1s - loss: 2.3521 - val_loss: 2.7781
Epoch 447/2000
22/22 - 1s - loss: 2.3498 - val_loss: 2.7763
Epoch 448/2000
22/22 - 1s - loss: 2.3484 - val_loss: 2.7762
Epoch 449/2000
22/22 - 1s - loss: 2.3456 - val_loss: 2.7728
Epoch 450/2000
22/22 - 1s - loss: 2.3425 - val_loss: 2.7697
Epoch 00450: val_loss improved from 2.79166 to 2.76971, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 451/2000
22/22 - 1s - loss: 2.3414 - val_loss: 2.7680
Epoch 452/2000
22/22 - 1s - loss: 2.3393 - val_loss: 2.7659
Epoch 453/2000
22/22 - 1s - loss: 2.3386 - val_loss: 2.7643
Epoch 454/2000
22/22 - 1s - loss: 2.3351 - val_loss: 2.7623
Epoch 455/2000
22/22 - 1s - loss: 2.3326 - val_loss: 2.7599
Epoch 456/2000
22/22 - 1s - loss: 2.3323 - val_loss: 2.7582
Epoch 457/2000
22/22 - 1s - loss: 2.3263 - val_loss: 2.7559
Epoch 458/2000
22/22 - 1s - loss: 2.3276 - val_loss: 2.7542
Epoch 459/2000
22/22 - 1s - loss: 2.3238 - val_loss: 2.7520
Epoch 460/2000
22/22 - 1s - loss: 2.3216 - val_loss: 2.7498
Epoch 00460: val_loss improved from 2.76971 to 2.74984, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 461/2000
22/22 - 1s - loss: 2.3190 - val_loss: 2.7468
Epoch 462/2000
22/22 - 1s - loss: 2.3165 - val_loss: 2.7448
Epoch 463/2000
22/22 - 1s - loss: 2.3165 - val_loss: 2.7440
Epoch 464/2000
22/22 - 1s - loss: 2.3128 - val_loss: 2.7432
Epoch 465/2000
22/22 - 1s - loss: 2.3120 - val_loss: 2.7396
Epoch 466/2000
22/22 - 1s - loss: 2.3095 - val_loss: 2.7382
Epoch 467/2000
22/22 - 1s - loss: 2.3094 - val_loss: 2.7344
Epoch 468/2000
22/22 - 1s - loss: 2.3065 - val_loss: 2.7344
Epoch 469/2000
22/22 - 1s - loss: 2.3030 - val_loss: 2.7335
Epoch 470/2000
22/22 - 1s - loss: 2.3030 - val_loss: 2.7289
Epoch 00470: val_loss improved from 2.74984 to 2.72892, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 471/2000
22/22 - 1s - loss: 2.3005 - val_loss: 2.7278
Epoch 472/2000
22/22 - 1s - loss: 2.2972 - val_loss: 2.7252
Epoch 473/2000
22/22 - 1s - loss: 2.2963 - val_loss: 2.7226
Epoch 474/2000
22/22 - 1s - loss: 2.2937 - val_loss: 2.7208
Epoch 475/2000
22/22 - 1s - loss: 2.2935 - val_loss: 2.7195
Epoch 476/2000
22/22 - 1s - loss: 2.2874 - val_loss: 2.7179
Epoch 477/2000
22/22 - 1s - loss: 2.2871 - val_loss: 2.7159
Epoch 478/2000
22/22 - 1s - loss: 2.2846 - val_loss: 2.7149
Epoch 479/2000
22/22 - 1s - loss: 2.2838 - val_loss: 2.7112
Epoch 480/2000
22/22 - 1s - loss: 2.2808 - val_loss: 2.7108
Epoch 00480: val_loss improved from 2.72892 to 2.71083, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 481/2000
22/22 - 1s - loss: 2.2777 - val_loss: 2.7070
Epoch 482/2000
22/22 - 1s - loss: 2.2764 - val_loss: 2.7068
Epoch 483/2000
22/22 - 1s - loss: 2.2740 - val_loss: 2.7050
Epoch 484/2000
22/22 - 1s - loss: 2.2747 - val_loss: 2.7040
Epoch 485/2000
22/22 - 1s - loss: 2.2717 - val_loss: 2.7005
Epoch 486/2000
22/22 - 1s - loss: 2.2701 - val_loss: 2.6992
Epoch 487/2000
22/22 - 1s - loss: 2.2670 - val_loss: 2.6970
Epoch 488/2000
22/22 - 1s - loss: 2.2652 - val_loss: 2.6945
Epoch 489/2000
22/22 - 1s - loss: 2.2632 - val_loss: 2.6923
Epoch 490/2000
22/22 - 1s - loss: 2.2609 - val_loss: 2.6905
Epoch 00490: val_loss improved from 2.71083 to 2.69049, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 491/2000
22/22 - 1s - loss: 2.2610 - val_loss: 2.6892
Epoch 492/2000
22/22 - 1s - loss: 2.2582 - val_loss: 2.6860
Epoch 493/2000
22/22 - 1s - loss: 2.2562 - val_loss: 2.6837
Epoch 494/2000
22/22 - 1s - loss: 2.2534 - val_loss: 2.6822
Epoch 495/2000
22/22 - 1s - loss: 2.2541 - val_loss: 2.6821
Epoch 496/2000
22/22 - 1s - loss: 2.2478 - val_loss: 2.6783
Epoch 497/2000
22/22 - 1s - loss: 2.2470 - val_loss: 2.6773
Epoch 498/2000
22/22 - 1s - loss: 2.2445 - val_loss: 2.6756
Epoch 499/2000
22/22 - 1s - loss: 2.2441 - val_loss: 2.6748
Epoch 500/2000
22/22 - 1s - loss: 2.2438 - val_loss: 2.6728
Epoch 00500: val_loss improved from 2.69049 to 2.67276, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 501/2000
22/22 - 1s - loss: 2.2391 - val_loss: 2.6706
Epoch 502/2000
22/22 - 1s - loss: 2.2384 - val_loss: 2.6685
Epoch 503/2000
22/22 - 1s - loss: 2.2341 - val_loss: 2.6666
Epoch 504/2000
22/22 - 1s - loss: 2.2305 - val_loss: 2.6630
Epoch 505/2000
22/22 - 1s - loss: 2.2331 - val_loss: 2.6630
Epoch 506/2000
22/22 - 1s - loss: 2.2315 - val_loss: 2.6607
Epoch 507/2000
22/22 - 1s - loss: 2.2286 - val_loss: 2.6571
Epoch 508/2000
22/22 - 1s - loss: 2.2260 - val_loss: 2.6564
Epoch 509/2000
22/22 - 1s - loss: 2.2239 - val_loss: 2.6555
Epoch 510/2000
22/22 - 1s - loss: 2.2226 - val_loss: 2.6538
Epoch 00510: val_loss improved from 2.67276 to 2.65376, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 511/2000
22/22 - 1s - loss: 2.2210 - val_loss: 2.6506
Epoch 512/2000
22/22 - 1s - loss: 2.2178 - val_loss: 2.6483
Epoch 513/2000
22/22 - 1s - loss: 2.2141 - val_loss: 2.6469
Epoch 514/2000
22/22 - 1s - loss: 2.2156 - val_loss: 2.6452
Epoch 515/2000
22/22 - 1s - loss: 2.2118 - val_loss: 2.6438
Epoch 516/2000
22/22 - 1s - loss: 2.2086 - val_loss: 2.6415
Epoch 517/2000
22/22 - 1s - loss: 2.2110 - val_loss: 2.6390
Epoch 518/2000
22/22 - 1s - loss: 2.2061 - val_loss: 2.6382
Epoch 519/2000
22/22 - 1s - loss: 2.2070 - val_loss: 2.6365
Epoch 520/2000
22/22 - 1s - loss: 2.2001 - val_loss: 2.6355
Epoch 00520: val_loss improved from 2.65376 to 2.63547, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 521/2000
22/22 - 1s - loss: 2.2011 - val_loss: 2.6328
Epoch 522/2000
22/22 - 1s - loss: 2.1996 - val_loss: 2.6310
Epoch 523/2000
22/22 - 1s - loss: 2.1973 - val_loss: 2.6291
Epoch 524/2000
22/22 - 1s - loss: 2.1963 - val_loss: 2.6289
Epoch 525/2000
22/22 - 1s - loss: 2.1935 - val_loss: 2.6265
Epoch 526/2000
22/22 - 1s - loss: 2.1923 - val_loss: 2.6242
Epoch 527/2000
22/22 - 1s - loss: 2.1876 - val_loss: 2.6227
Epoch 528/2000
22/22 - 1s - loss: 2.1873 - val_loss: 2.6211
Epoch 529/2000
22/22 - 1s - loss: 2.1862 - val_loss: 2.6174
Epoch 530/2000
22/22 - 1s - loss: 2.1863 - val_loss: 2.6153
Epoch 00530: val_loss improved from 2.63547 to 2.61532, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 531/2000
22/22 - 1s - loss: 2.1820 - val_loss: 2.6136
Epoch 532/2000
22/22 - 1s - loss: 2.1804 - val_loss: 2.6123
Epoch 533/2000
22/22 - 1s - loss: 2.1780 - val_loss: 2.6100
Epoch 534/2000
22/22 - 1s - loss: 2.1771 - val_loss: 2.6085
Epoch 535/2000
22/22 - 1s - loss: 2.1761 - val_loss: 2.6068
Epoch 536/2000
22/22 - 1s - loss: 2.1717 - val_loss: 2.6060
Epoch 537/2000
22/22 - 1s - loss: 2.1713 - val_loss: 2.6040
Epoch 538/2000
22/22 - 1s - loss: 2.1663 - val_loss: 2.6016
Epoch 539/2000
22/22 - 1s - loss: 2.1657 - val_loss: 2.5987
Epoch 540/2000
22/22 - 1s - loss: 2.1650 - val_loss: 2.5980
Epoch 00540: val_loss improved from 2.61532 to 2.59797, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 541/2000
22/22 - 1s - loss: 2.1611 - val_loss: 2.5951
Epoch 542/2000
22/22 - 1s - loss: 2.1614 - val_loss: 2.5951
Epoch 543/2000
22/22 - 1s - loss: 2.1587 - val_loss: 2.5929
Epoch 544/2000
22/22 - 1s - loss: 2.1560 - val_loss: 2.5900
Epoch 545/2000
22/22 - 1s - loss: 2.1574 - val_loss: 2.5889
Epoch 546/2000
22/22 - 1s - loss: 2.1550 - val_loss: 2.5862
Epoch 547/2000
22/22 - 1s - loss: 2.1538 - val_loss: 2.5858
Epoch 548/2000
22/22 - 1s - loss: 2.1529 - val_loss: 2.5850
Epoch 549/2000
22/22 - 1s - loss: 2.1489 - val_loss: 2.5822
Epoch 550/2000
22/22 - 1s - loss: 2.1471 - val_loss: 2.5806
Epoch 00550: val_loss improved from 2.59797 to 2.58057, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 551/2000
22/22 - 1s - loss: 2.1447 - val_loss: 2.5778
Epoch 552/2000
22/22 - 1s - loss: 2.1421 - val_loss: 2.5762
Epoch 553/2000
22/22 - 1s - loss: 2.1426 - val_loss: 2.5745
Epoch 554/2000
22/22 - 1s - loss: 2.1384 - val_loss: 2.5735
Epoch 555/2000
22/22 - 1s - loss: 2.1366 - val_loss: 2.5725
Epoch 556/2000
22/22 - 1s - loss: 2.1382 - val_loss: 2.5696
Epoch 557/2000
22/22 - 1s - loss: 2.1356 - val_loss: 2.5684
Epoch 558/2000
22/22 - 1s - loss: 2.1321 - val_loss: 2.5666
Epoch 559/2000
22/22 - 1s - loss: 2.1316 - val_loss: 2.5655
Epoch 560/2000
22/22 - 1s - loss: 2.1303 - val_loss: 2.5627
Epoch 00560: val_loss improved from 2.58057 to 2.56267, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 561/2000
22/22 - 1s - loss: 2.1276 - val_loss: 2.5612
Epoch 562/2000
22/22 - 1s - loss: 2.1259 - val_loss: 2.5599
Epoch 563/2000
22/22 - 1s - loss: 2.1233 - val_loss: 2.5561
Epoch 564/2000
22/22 - 1s - loss: 2.1215 - val_loss: 2.5546
Epoch 565/2000
22/22 - 1s - loss: 2.1196 - val_loss: 2.5541
Epoch 566/2000
22/22 - 1s - loss: 2.1152 - val_loss: 2.5514
Epoch 567/2000
22/22 - 1s - loss: 2.1158 - val_loss: 2.5494
Epoch 568/2000
22/22 - 1s - loss: 2.1138 - val_loss: 2.5468
Epoch 569/2000
22/22 - 1s - loss: 2.1135 - val_loss: 2.5445
Epoch 570/2000
22/22 - 1s - loss: 2.1113 - val_loss: 2.5431
Epoch 00570: val_loss improved from 2.56267 to 2.54310, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 571/2000
22/22 - 1s - loss: 2.1081 - val_loss: 2.5412
Epoch 572/2000
22/22 - 1s - loss: 2.1063 - val_loss: 2.5412
Epoch 573/2000
22/22 - 1s - loss: 2.1050 - val_loss: 2.5403
Epoch 574/2000
22/22 - 1s - loss: 2.1028 - val_loss: 2.5379
Epoch 575/2000
22/22 - 1s - loss: 2.1028 - val_loss: 2.5362
Epoch 576/2000
22/22 - 1s - loss: 2.1000 - val_loss: 2.5335
Epoch 577/2000
22/22 - 1s - loss: 2.0997 - val_loss: 2.5324
Epoch 578/2000
22/22 - 1s - loss: 2.0957 - val_loss: 2.5289
Epoch 579/2000
22/22 - 1s - loss: 2.0953 - val_loss: 2.5271
Epoch 580/2000
22/22 - 1s - loss: 2.0914 - val_loss: 2.5270
Epoch 00580: val_loss improved from 2.54310 to 2.52704, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 581/2000
22/22 - 1s - loss: 2.0917 - val_loss: 2.5258
Epoch 582/2000
22/22 - 1s - loss: 2.0899 - val_loss: 2.5245
Epoch 583/2000
22/22 - 1s - loss: 2.0900 - val_loss: 2.5222
Epoch 584/2000
22/22 - 1s - loss: 2.0865 - val_loss: 2.5205
Epoch 585/2000
22/22 - 1s - loss: 2.0857 - val_loss: 2.5190
Epoch 586/2000
22/22 - 1s - loss: 2.0835 - val_loss: 2.5168
Epoch 587/2000
22/22 - 1s - loss: 2.0822 - val_loss: 2.5152
Epoch 588/2000
22/22 - 1s - loss: 2.0782 - val_loss: 2.5134
Epoch 589/2000
22/22 - 1s - loss: 2.0766 - val_loss: 2.5116
Epoch 590/2000
22/22 - 1s - loss: 2.0741 - val_loss: 2.5102
Epoch 00590: val_loss improved from 2.52704 to 2.51021, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 591/2000
22/22 - 1s - loss: 2.0730 - val_loss: 2.5078
Epoch 592/2000
22/22 - 1s - loss: 2.0725 - val_loss: 2.5068
Epoch 593/2000
22/22 - 1s - loss: 2.0706 - val_loss: 2.5048
Epoch 594/2000
22/22 - 1s - loss: 2.0673 - val_loss: 2.5032
Epoch 595/2000
22/22 - 1s - loss: 2.0681 - val_loss: 2.5019
Epoch 596/2000
22/22 - 1s - loss: 2.0663 - val_loss: 2.5003
Epoch 597/2000
22/22 - 1s - loss: 2.0644 - val_loss: 2.4968
Epoch 598/2000
22/22 - 1s - loss: 2.0604 - val_loss: 2.4961
Epoch 599/2000
22/22 - 1s - loss: 2.0594 - val_loss: 2.4959
Epoch 600/2000
22/22 - 1s - loss: 2.0596 - val_loss: 2.4943
Epoch 00600: val_loss improved from 2.51021 to 2.49428, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 601/2000
22/22 - 1s - loss: 2.0569 - val_loss: 2.4912
Epoch 602/2000
22/22 - 1s - loss: 2.0537 - val_loss: 2.4917
Epoch 603/2000
22/22 - 1s - loss: 2.0532 - val_loss: 2.4894
Epoch 604/2000
22/22 - 1s - loss: 2.0501 - val_loss: 2.4866
Epoch 605/2000
22/22 - 1s - loss: 2.0526 - val_loss: 2.4865
Epoch 606/2000
22/22 - 1s - loss: 2.0469 - val_loss: 2.4843
Epoch 607/2000
22/22 - 1s - loss: 2.0447 - val_loss: 2.4825
Epoch 608/2000
22/22 - 1s - loss: 2.0454 - val_loss: 2.4817
Epoch 609/2000
22/22 - 1s - loss: 2.0420 - val_loss: 2.4812
Epoch 610/2000
22/22 - 1s - loss: 2.0411 - val_loss: 2.4787
Epoch 00610: val_loss improved from 2.49428 to 2.47872, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 611/2000
22/22 - 1s - loss: 2.0387 - val_loss: 2.4786
Epoch 612/2000
22/22 - 1s - loss: 2.0375 - val_loss: 2.4749
Epoch 613/2000
22/22 - 1s - loss: 2.0365 - val_loss: 2.4755
Epoch 614/2000
22/22 - 1s - loss: 2.0323 - val_loss: 2.4722
Epoch 615/2000
22/22 - 1s - loss: 2.0322 - val_loss: 2.4702
Epoch 616/2000
22/22 - 1s - loss: 2.0304 - val_loss: 2.4680
Epoch 617/2000
22/22 - 1s - loss: 2.0304 - val_loss: 2.4664
Epoch 618/2000
22/22 - 1s - loss: 2.0287 - val_loss: 2.4641
Epoch 619/2000
22/22 - 1s - loss: 2.0255 - val_loss: 2.4632
Epoch 620/2000
22/22 - 1s - loss: 2.0254 - val_loss: 2.4622
Epoch 00620: val_loss improved from 2.47872 to 2.46220, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 621/2000
22/22 - 1s - loss: 2.0246 - val_loss: 2.4595
Epoch 622/2000
22/22 - 1s - loss: 2.0190 - val_loss: 2.4587
Epoch 623/2000
22/22 - 1s - loss: 2.0203 - val_loss: 2.4560
Epoch 624/2000
22/22 - 1s - loss: 2.0179 - val_loss: 2.4533
Epoch 625/2000
22/22 - 1s - loss: 2.0168 - val_loss: 2.4532
Epoch 626/2000
22/22 - 1s - loss: 2.0147 - val_loss: 2.4523
Epoch 627/2000
22/22 - 1s - loss: 2.0124 - val_loss: 2.4500
Epoch 628/2000
22/22 - 1s - loss: 2.0103 - val_loss: 2.4479
Epoch 629/2000
22/22 - 1s - loss: 2.0093 - val_loss: 2.4479
Epoch 630/2000
22/22 - 1s - loss: 2.0085 - val_loss: 2.4469
Epoch 00630: val_loss improved from 2.46220 to 2.44694, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 631/2000
22/22 - 1s - loss: 2.0055 - val_loss: 2.4431
Epoch 632/2000
22/22 - 1s - loss: 2.0037 - val_loss: 2.4410
Epoch 633/2000
22/22 - 1s - loss: 2.0046 - val_loss: 2.4393
Epoch 634/2000
22/22 - 1s - loss: 2.0012 - val_loss: 2.4385
Epoch 635/2000
22/22 - 1s - loss: 2.0007 - val_loss: 2.4365
Epoch 636/2000
22/22 - 1s - loss: 2.0018 - val_loss: 2.4363
Epoch 637/2000
22/22 - 1s - loss: 1.9981 - val_loss: 2.4335
Epoch 638/2000
22/22 - 1s - loss: 1.9947 - val_loss: 2.4326
Epoch 639/2000
22/22 - 1s - loss: 1.9921 - val_loss: 2.4297
Epoch 640/2000
22/22 - 1s - loss: 1.9916 - val_loss: 2.4287
Epoch 00640: val_loss improved from 2.44694 to 2.42868, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 641/2000
22/22 - 1s - loss: 1.9918 - val_loss: 2.4285
Epoch 642/2000
22/22 - 1s - loss: 1.9893 - val_loss: 2.4256
Epoch 643/2000
22/22 - 1s - loss: 1.9866 - val_loss: 2.4235
Epoch 644/2000
22/22 - 1s - loss: 1.9839 - val_loss: 2.4219
Epoch 645/2000
22/22 - 1s - loss: 1.9848 - val_loss: 2.4209
Epoch 646/2000
22/22 - 1s - loss: 1.9810 - val_loss: 2.4186
Epoch 647/2000
22/22 - 1s - loss: 1.9801 - val_loss: 2.4180
Epoch 648/2000
22/22 - 1s - loss: 1.9791 - val_loss: 2.4156
Epoch 649/2000
22/22 - 1s - loss: 1.9771 - val_loss: 2.4123
Epoch 650/2000
22/22 - 1s - loss: 1.9768 - val_loss: 2.4125
Epoch 00650: val_loss improved from 2.42868 to 2.41254, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 651/2000
22/22 - 1s - loss: 1.9749 - val_loss: 2.4122
Epoch 652/2000
22/22 - 1s - loss: 1.9725 - val_loss: 2.4124
Epoch 653/2000
22/22 - 1s - loss: 1.9711 - val_loss: 2.4104
Epoch 654/2000
22/22 - 1s - loss: 1.9687 - val_loss: 2.4068
Epoch 655/2000
22/22 - 1s - loss: 1.9669 - val_loss: 2.4066
Epoch 656/2000
22/22 - 1s - loss: 1.9680 - val_loss: 2.4043
Epoch 657/2000
22/22 - 1s - loss: 1.9646 - val_loss: 2.4023
Epoch 658/2000
22/22 - 1s - loss: 1.9630 - val_loss: 2.4004
Epoch 659/2000
22/22 - 1s - loss: 1.9612 - val_loss: 2.3989
Epoch 660/2000
22/22 - 1s - loss: 1.9608 - val_loss: 2.4001
Epoch 00660: val_loss improved from 2.41254 to 2.40006, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 661/2000
22/22 - 1s - loss: 1.9584 - val_loss: 2.3975
Epoch 662/2000
22/22 - 1s - loss: 1.9560 - val_loss: 2.3952
Epoch 663/2000
22/22 - 1s - loss: 1.9550 - val_loss: 2.3949
Epoch 664/2000
22/22 - 1s - loss: 1.9513 - val_loss: 2.3921
Epoch 665/2000
22/22 - 1s - loss: 1.9513 - val_loss: 2.3923
Epoch 666/2000
22/22 - 1s - loss: 1.9490 - val_loss: 2.3914
Epoch 667/2000
22/22 - 1s - loss: 1.9508 - val_loss: 2.3884
Epoch 668/2000
22/22 - 1s - loss: 1.9445 - val_loss: 2.3843
Epoch 669/2000
22/22 - 1s - loss: 1.9466 - val_loss: 2.3829
Epoch 670/2000
22/22 - 1s - loss: 1.9442 - val_loss: 2.3824
Epoch 00670: val_loss improved from 2.40006 to 2.38244, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 671/2000
22/22 - 1s - loss: 1.9426 - val_loss: 2.3819
Epoch 672/2000
22/22 - 1s - loss: 1.9402 - val_loss: 2.3786
Epoch 673/2000
22/22 - 1s - loss: 1.9379 - val_loss: 2.3762
Epoch 674/2000
22/22 - 1s - loss: 1.9388 - val_loss: 2.3735
Epoch 675/2000
22/22 - 1s - loss: 1.9365 - val_loss: 2.3739
Epoch 676/2000
22/22 - 1s - loss: 1.9350 - val_loss: 2.3725
Epoch 677/2000
22/22 - 1s - loss: 1.9318 - val_loss: 2.3709
Epoch 678/2000
22/22 - 1s - loss: 1.9341 - val_loss: 2.3694
Epoch 679/2000
22/22 - 1s - loss: 1.9291 - val_loss: 2.3665
Epoch 680/2000
22/22 - 1s - loss: 1.9270 - val_loss: 2.3668
Epoch 00680: val_loss improved from 2.38244 to 2.36683, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 681/2000
22/22 - 1s - loss: 1.9298 - val_loss: 2.3659
Epoch 682/2000
22/22 - 1s - loss: 1.9261 - val_loss: 2.3632
Epoch 683/2000
22/22 - 1s - loss: 1.9255 - val_loss: 2.3617
Epoch 684/2000
22/22 - 1s - loss: 1.9248 - val_loss: 2.3617
Epoch 685/2000
22/22 - 1s - loss: 1.9214 - val_loss: 2.3597
Epoch 686/2000
22/22 - 1s - loss: 1.9204 - val_loss: 2.3586
Epoch 687/2000
22/22 - 1s - loss: 1.9178 - val_loss: 2.3582
Epoch 688/2000
22/22 - 1s - loss: 1.9140 - val_loss: 2.3544
Epoch 689/2000
22/22 - 1s - loss: 1.9144 - val_loss: 2.3536
Epoch 690/2000
22/22 - 1s - loss: 1.9122 - val_loss: 2.3535
Epoch 00690: val_loss improved from 2.36683 to 2.35347, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 691/2000
22/22 - 1s - loss: 1.9138 - val_loss: 2.3517
Epoch 692/2000
22/22 - 1s - loss: 1.9085 - val_loss: 2.3507
Epoch 693/2000
22/22 - 1s - loss: 1.9107 - val_loss: 2.3473
Epoch 694/2000
22/22 - 1s - loss: 1.9052 - val_loss: 2.3464
Epoch 695/2000
22/22 - 1s - loss: 1.9062 - val_loss: 2.3448
Epoch 696/2000
22/22 - 1s - loss: 1.9031 - val_loss: 2.3431
Epoch 697/2000
22/22 - 1s - loss: 1.9050 - val_loss: 2.3424
Epoch 698/2000
22/22 - 1s - loss: 1.9014 - val_loss: 2.3416
Epoch 699/2000
22/22 - 1s - loss: 1.8983 - val_loss: 2.3387
Epoch 700/2000
22/22 - 1s - loss: 1.8987 - val_loss: 2.3394
Epoch 00700: val_loss improved from 2.35347 to 2.33936, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 701/2000
22/22 - 1s - loss: 1.8991 - val_loss: 2.3376
Epoch 702/2000
22/22 - 1s - loss: 1.8963 - val_loss: 2.3352
Epoch 703/2000
22/22 - 1s - loss: 1.8920 - val_loss: 2.3336
Epoch 704/2000
22/22 - 1s - loss: 1.8925 - val_loss: 2.3317
Epoch 705/2000
22/22 - 1s - loss: 1.8945 - val_loss: 2.3308
Epoch 706/2000
22/22 - 1s - loss: 1.8909 - val_loss: 2.3294
Epoch 707/2000
22/22 - 1s - loss: 1.8890 - val_loss: 2.3275
Epoch 708/2000
22/22 - 1s - loss: 1.8855 - val_loss: 2.3265
Epoch 709/2000
22/22 - 1s - loss: 1.8840 - val_loss: 2.3247
Epoch 710/2000
22/22 - 1s - loss: 1.8820 - val_loss: 2.3222
Epoch 00710: val_loss improved from 2.33936 to 2.32222, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 711/2000
22/22 - 1s - loss: 1.8835 - val_loss: 2.3218
Epoch 712/2000
22/22 - 1s - loss: 1.8795 - val_loss: 2.3218
Epoch 713/2000
22/22 - 1s - loss: 1.8789 - val_loss: 2.3197
Epoch 714/2000
22/22 - 1s - loss: 1.8795 - val_loss: 2.3161
Epoch 715/2000
22/22 - 1s - loss: 1.8752 - val_loss: 2.3148
Epoch 716/2000
22/22 - 1s - loss: 1.8739 - val_loss: 2.3155
Epoch 717/2000
22/22 - 1s - loss: 1.8742 - val_loss: 2.3155
Epoch 718/2000
22/22 - 1s - loss: 1.8728 - val_loss: 2.3129
Epoch 719/2000
22/22 - 1s - loss: 1.8705 - val_loss: 2.3107
Epoch 720/2000
22/22 - 1s - loss: 1.8693 - val_loss: 2.3082
Epoch 00720: val_loss improved from 2.32222 to 2.30819, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 721/2000
22/22 - 1s - loss: 1.8674 - val_loss: 2.3050
Epoch 722/2000
22/22 - 1s - loss: 1.8684 - val_loss: 2.3044
Epoch 723/2000
22/22 - 1s - loss: 1.8641 - val_loss: 2.3054
Epoch 724/2000
22/22 - 1s - loss: 1.8658 - val_loss: 2.3020
Epoch 725/2000
22/22 - 1s - loss: 1.8615 - val_loss: 2.3010
Epoch 726/2000
22/22 - 1s - loss: 1.8617 - val_loss: 2.2988
Epoch 727/2000
22/22 - 1s - loss: 1.8594 - val_loss: 2.3005
Epoch 728/2000
22/22 - 1s - loss: 1.8580 - val_loss: 2.2994
Epoch 729/2000
22/22 - 1s - loss: 1.8560 - val_loss: 2.2972
Epoch 730/2000
22/22 - 1s - loss: 1.8555 - val_loss: 2.2937
Epoch 00730: val_loss improved from 2.30819 to 2.29373, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 731/2000
22/22 - 1s - loss: 1.8508 - val_loss: 2.2932
Epoch 732/2000
22/22 - 1s - loss: 1.8490 - val_loss: 2.2895
Epoch 733/2000
22/22 - 1s - loss: 1.8506 - val_loss: 2.2887
Epoch 734/2000
22/22 - 1s - loss: 1.8487 - val_loss: 2.2898
Epoch 735/2000
22/22 - 1s - loss: 1.8467 - val_loss: 2.2870
Epoch 736/2000
22/22 - 1s - loss: 1.8473 - val_loss: 2.2849
Epoch 737/2000
22/22 - 1s - loss: 1.8465 - val_loss: 2.2867
Epoch 738/2000
22/22 - 1s - loss: 1.8447 - val_loss: 2.2845
Epoch 739/2000
22/22 - 1s - loss: 1.8407 - val_loss: 2.2814
Epoch 740/2000
22/22 - 1s - loss: 1.8410 - val_loss: 2.2819
Epoch 00740: val_loss improved from 2.29373 to 2.28187, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 741/2000
22/22 - 1s - loss: 1.8385 - val_loss: 2.2790
Epoch 742/2000
22/22 - 1s - loss: 1.8379 - val_loss: 2.2790
Epoch 743/2000
22/22 - 1s - loss: 1.8352 - val_loss: 2.2774
Epoch 744/2000
22/22 - 1s - loss: 1.8342 - val_loss: 2.2770
Epoch 745/2000
22/22 - 1s - loss: 1.8335 - val_loss: 2.2726
Epoch 746/2000
22/22 - 1s - loss: 1.8320 - val_loss: 2.2733
Epoch 747/2000
22/22 - 1s - loss: 1.8296 - val_loss: 2.2691
Epoch 748/2000
22/22 - 1s - loss: 1.8303 - val_loss: 2.2679
Epoch 749/2000
22/22 - 1s - loss: 1.8286 - val_loss: 2.2675
Epoch 750/2000
22/22 - 1s - loss: 1.8245 - val_loss: 2.2670
Epoch 00750: val_loss improved from 2.28187 to 2.26698, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 751/2000
22/22 - 1s - loss: 1.8254 - val_loss: 2.2651
Epoch 752/2000
22/22 - 1s - loss: 1.8223 - val_loss: 2.2632
Epoch 753/2000
22/22 - 1s - loss: 1.8221 - val_loss: 2.2615
Epoch 754/2000
22/22 - 1s - loss: 1.8198 - val_loss: 2.2614
Epoch 755/2000
22/22 - 1s - loss: 1.8168 - val_loss: 2.2609
Epoch 756/2000
22/22 - 1s - loss: 1.8170 - val_loss: 2.2606
Epoch 757/2000
22/22 - 1s - loss: 1.8179 - val_loss: 2.2583
Epoch 758/2000
22/22 - 1s - loss: 1.8145 - val_loss: 2.2553
Epoch 759/2000
22/22 - 1s - loss: 1.8133 - val_loss: 2.2560
Epoch 760/2000
22/22 - 1s - loss: 1.8092 - val_loss: 2.2513
Epoch 00760: val_loss improved from 2.26698 to 2.25126, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 761/2000
22/22 - 1s - loss: 1.8102 - val_loss: 2.2507
Epoch 762/2000
22/22 - 1s - loss: 1.8098 - val_loss: 2.2503
Epoch 763/2000
22/22 - 1s - loss: 1.8080 - val_loss: 2.2488
Epoch 764/2000
22/22 - 1s - loss: 1.8078 - val_loss: 2.2463
Epoch 765/2000
22/22 - 1s - loss: 1.8054 - val_loss: 2.2458
Epoch 766/2000
22/22 - 1s - loss: 1.8048 - val_loss: 2.2466
Epoch 767/2000
22/22 - 1s - loss: 1.8026 - val_loss: 2.2445
Epoch 768/2000
22/22 - 1s - loss: 1.8019 - val_loss: 2.2440
Epoch 769/2000
22/22 - 1s - loss: 1.7998 - val_loss: 2.2403
Epoch 770/2000
22/22 - 1s - loss: 1.7989 - val_loss: 2.2411
Epoch 00770: val_loss improved from 2.25126 to 2.24106, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 771/2000
22/22 - 1s - loss: 1.7963 - val_loss: 2.2386
Epoch 772/2000
22/22 - 1s - loss: 1.7957 - val_loss: 2.2375
Epoch 773/2000
22/22 - 1s - loss: 1.7925 - val_loss: 2.2353
Epoch 774/2000
22/22 - 1s - loss: 1.7918 - val_loss: 2.2329
Epoch 775/2000
22/22 - 1s - loss: 1.7909 - val_loss: 2.2330
Epoch 776/2000
22/22 - 1s - loss: 1.7910 - val_loss: 2.2318
Epoch 777/2000
22/22 - 1s - loss: 1.7891 - val_loss: 2.2303
Epoch 778/2000
22/22 - 1s - loss: 1.7896 - val_loss: 2.2292
Epoch 779/2000
22/22 - 1s - loss: 1.7856 - val_loss: 2.2274
Epoch 780/2000
22/22 - 1s - loss: 1.7842 - val_loss: 2.2262
Epoch 00780: val_loss improved from 2.24106 to 2.22619, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 781/2000
22/22 - 1s - loss: 1.7813 - val_loss: 2.2251
Epoch 782/2000
22/22 - 1s - loss: 1.7826 - val_loss: 2.2240
Epoch 783/2000
22/22 - 1s - loss: 1.7790 - val_loss: 2.2218
Epoch 784/2000
22/22 - 1s - loss: 1.7783 - val_loss: 2.2222
Epoch 785/2000
22/22 - 1s - loss: 1.7764 - val_loss: 2.2196
Epoch 786/2000
22/22 - 1s - loss: 1.7769 - val_loss: 2.2198
Epoch 787/2000
22/22 - 1s - loss: 1.7764 - val_loss: 2.2170
Epoch 788/2000
22/22 - 1s - loss: 1.7750 - val_loss: 2.2167
Epoch 789/2000
22/22 - 1s - loss: 1.7734 - val_loss: 2.2155
Epoch 790/2000
22/22 - 1s - loss: 1.7684 - val_loss: 2.2135
Epoch 00790: val_loss improved from 2.22619 to 2.21350, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 791/2000
22/22 - 1s - loss: 1.7687 - val_loss: 2.2113
Epoch 792/2000
22/22 - 1s - loss: 1.7677 - val_loss: 2.2115
Epoch 793/2000
22/22 - 1s - loss: 1.7669 - val_loss: 2.2090
Epoch 794/2000
22/22 - 1s - loss: 1.7668 - val_loss: 2.2071
Epoch 795/2000
22/22 - 1s - loss: 1.7668 - val_loss: 2.2053
Epoch 796/2000
22/22 - 1s - loss: 1.7629 - val_loss: 2.2060
Epoch 797/2000
22/22 - 1s - loss: 1.7612 - val_loss: 2.2048
Epoch 798/2000
22/22 - 1s - loss: 1.7611 - val_loss: 2.2008
Epoch 799/2000
22/22 - 1s - loss: 1.7588 - val_loss: 2.2022
Epoch 800/2000
22/22 - 1s - loss: 1.7585 - val_loss: 2.1998
Epoch 00800: val_loss improved from 2.21350 to 2.19977, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 801/2000
22/22 - 1s - loss: 1.7564 - val_loss: 2.1990
Epoch 802/2000
22/22 - 1s - loss: 1.7537 - val_loss: 2.1980
Epoch 803/2000
22/22 - 1s - loss: 1.7540 - val_loss: 2.1969
Epoch 804/2000
22/22 - 1s - loss: 1.7524 - val_loss: 2.1958
Epoch 805/2000
22/22 - 1s - loss: 1.7504 - val_loss: 2.1945
Epoch 806/2000
22/22 - 1s - loss: 1.7500 - val_loss: 2.1930
Epoch 807/2000
22/22 - 1s - loss: 1.7479 - val_loss: 2.1908
Epoch 808/2000
22/22 - 1s - loss: 1.7465 - val_loss: 2.1901
Epoch 809/2000
22/22 - 1s - loss: 1.7450 - val_loss: 2.1911
Epoch 810/2000
22/22 - 1s - loss: 1.7441 - val_loss: 2.1881
Epoch 00810: val_loss improved from 2.19977 to 2.18808, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 811/2000
22/22 - 1s - loss: 1.7437 - val_loss: 2.1868
Epoch 812/2000
22/22 - 1s - loss: 1.7410 - val_loss: 2.1856
Epoch 813/2000
22/22 - 1s - loss: 1.7395 - val_loss: 2.1838
Epoch 814/2000
22/22 - 1s - loss: 1.7397 - val_loss: 2.1828
Epoch 815/2000
22/22 - 1s - loss: 1.7370 - val_loss: 2.1849
Epoch 816/2000
22/22 - 1s - loss: 1.7373 - val_loss: 2.1816
Epoch 817/2000
22/22 - 1s - loss: 1.7350 - val_loss: 2.1802
Epoch 818/2000
22/22 - 1s - loss: 1.7346 - val_loss: 2.1782
Epoch 819/2000
22/22 - 1s - loss: 1.7316 - val_loss: 2.1791
Epoch 820/2000
22/22 - 1s - loss: 1.7300 - val_loss: 2.1766
Epoch 00820: val_loss improved from 2.18808 to 2.17660, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 821/2000
22/22 - 1s - loss: 1.7293 - val_loss: 2.1744
Epoch 822/2000
22/22 - 1s - loss: 1.7285 - val_loss: 2.1712
Epoch 823/2000
22/22 - 1s - loss: 1.7277 - val_loss: 2.1706
Epoch 824/2000
22/22 - 1s - loss: 1.7254 - val_loss: 2.1705
Epoch 825/2000
22/22 - 1s - loss: 1.7240 - val_loss: 2.1686
Epoch 826/2000
22/22 - 1s - loss: 1.7235 - val_loss: 2.1685
Epoch 827/2000
22/22 - 1s - loss: 1.7225 - val_loss: 2.1657
Epoch 828/2000
22/22 - 1s - loss: 1.7194 - val_loss: 2.1663
Epoch 829/2000
22/22 - 1s - loss: 1.7192 - val_loss: 2.1647
Epoch 830/2000
22/22 - 1s - loss: 1.7177 - val_loss: 2.1618
Epoch 00830: val_loss improved from 2.17660 to 2.16182, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 831/2000
22/22 - 1s - loss: 1.7154 - val_loss: 2.1605
Epoch 832/2000
22/22 - 1s - loss: 1.7158 - val_loss: 2.1614
Epoch 833/2000
22/22 - 1s - loss: 1.7124 - val_loss: 2.1585
Epoch 834/2000
22/22 - 1s - loss: 1.7145 - val_loss: 2.1567
Epoch 835/2000
22/22 - 1s - loss: 1.7135 - val_loss: 2.1546
Epoch 836/2000
22/22 - 1s - loss: 1.7119 - val_loss: 2.1536
Epoch 837/2000
22/22 - 1s - loss: 1.7079 - val_loss: 2.1550
Epoch 838/2000
22/22 - 1s - loss: 1.7081 - val_loss: 2.1524
Epoch 839/2000
22/22 - 1s - loss: 1.7085 - val_loss: 2.1516
Epoch 840/2000
22/22 - 1s - loss: 1.7049 - val_loss: 2.1485
Epoch 00840: val_loss improved from 2.16182 to 2.14847, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 841/2000
22/22 - 1s - loss: 1.7038 - val_loss: 2.1482
Epoch 842/2000
22/22 - 1s - loss: 1.7020 - val_loss: 2.1461
Epoch 843/2000
22/22 - 1s - loss: 1.7021 - val_loss: 2.1452
Epoch 844/2000
22/22 - 1s - loss: 1.7018 - val_loss: 2.1431
Epoch 845/2000
22/22 - 1s - loss: 1.6987 - val_loss: 2.1445
Epoch 846/2000
22/22 - 1s - loss: 1.6969 - val_loss: 2.1420
Epoch 847/2000
22/22 - 1s - loss: 1.6993 - val_loss: 2.1395
Epoch 848/2000
22/22 - 1s - loss: 1.6963 - val_loss: 2.1380
Epoch 849/2000
22/22 - 1s - loss: 1.6950 - val_loss: 2.1377
Epoch 850/2000
22/22 - 1s - loss: 1.6936 - val_loss: 2.1370
Epoch 00850: val_loss improved from 2.14847 to 2.13701, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 851/2000
22/22 - 1s - loss: 1.6935 - val_loss: 2.1366
Epoch 852/2000
22/22 - 1s - loss: 1.6924 - val_loss: 2.1353
Epoch 853/2000
22/22 - 1s - loss: 1.6901 - val_loss: 2.1353
Epoch 854/2000
22/22 - 1s - loss: 1.6885 - val_loss: 2.1339
Epoch 855/2000
22/22 - 1s - loss: 1.6861 - val_loss: 2.1322
Epoch 856/2000
22/22 - 1s - loss: 1.6859 - val_loss: 2.1292
Epoch 857/2000
22/22 - 1s - loss: 1.6858 - val_loss: 2.1286
Epoch 858/2000
22/22 - 1s - loss: 1.6836 - val_loss: 2.1263
Epoch 859/2000
22/22 - 1s - loss: 1.6817 - val_loss: 2.1253
Epoch 860/2000
22/22 - 1s - loss: 1.6834 - val_loss: 2.1238
Epoch 00860: val_loss improved from 2.13701 to 2.12382, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 861/2000
22/22 - 1s - loss: 1.6795 - val_loss: 2.1242
Epoch 862/2000
22/22 - 1s - loss: 1.6781 - val_loss: 2.1216
Epoch 863/2000
22/22 - 1s - loss: 1.6781 - val_loss: 2.1212
Epoch 864/2000
22/22 - 1s - loss: 1.6757 - val_loss: 2.1196
Epoch 865/2000
22/22 - 1s - loss: 1.6747 - val_loss: 2.1195
Epoch 866/2000
22/22 - 1s - loss: 1.6737 - val_loss: 2.1203
Epoch 867/2000
22/22 - 1s - loss: 1.6720 - val_loss: 2.1163
Epoch 868/2000
22/22 - 1s - loss: 1.6694 - val_loss: 2.1158
Epoch 869/2000
22/22 - 1s - loss: 1.6695 - val_loss: 2.1145
Epoch 870/2000
22/22 - 1s - loss: 1.6679 - val_loss: 2.1121
Epoch 00870: val_loss improved from 2.12382 to 2.11211, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 871/2000
22/22 - 1s - loss: 1.6681 - val_loss: 2.1106
Epoch 872/2000
22/22 - 1s - loss: 1.6674 - val_loss: 2.1096
Epoch 873/2000
22/22 - 1s - loss: 1.6651 - val_loss: 2.1090
Epoch 874/2000
22/22 - 1s - loss: 1.6632 - val_loss: 2.1087
Epoch 875/2000
22/22 - 1s - loss: 1.6628 - val_loss: 2.1081
Epoch 876/2000
22/22 - 1s - loss: 1.6624 - val_loss: 2.1052
Epoch 877/2000
22/22 - 1s - loss: 1.6594 - val_loss: 2.1061
Epoch 878/2000
22/22 - 1s - loss: 1.6581 - val_loss: 2.1051
Epoch 879/2000
22/22 - 1s - loss: 1.6579 - val_loss: 2.1009
Epoch 880/2000
22/22 - 1s - loss: 1.6575 - val_loss: 2.0992
Epoch 00880: val_loss improved from 2.11211 to 2.09921, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 881/2000
22/22 - 1s - loss: 1.6549 - val_loss: 2.0984
Epoch 882/2000
22/22 - 1s - loss: 1.6529 - val_loss: 2.0979
Epoch 883/2000
22/22 - 1s - loss: 1.6543 - val_loss: 2.0963
Epoch 884/2000
22/22 - 1s - loss: 1.6508 - val_loss: 2.0957
Epoch 885/2000
22/22 - 1s - loss: 1.6498 - val_loss: 2.0955
Epoch 886/2000
22/22 - 1s - loss: 1.6498 - val_loss: 2.0937
Epoch 887/2000
22/22 - 1s - loss: 1.6470 - val_loss: 2.0915
Epoch 888/2000
22/22 - 1s - loss: 1.6471 - val_loss: 2.0929
Epoch 889/2000
22/22 - 1s - loss: 1.6451 - val_loss: 2.0912
Epoch 890/2000
22/22 - 1s - loss: 1.6447 - val_loss: 2.0886
Epoch 00890: val_loss improved from 2.09921 to 2.08856, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 891/2000
22/22 - 1s - loss: 1.6452 - val_loss: 2.0875
Epoch 892/2000
22/22 - 1s - loss: 1.6405 - val_loss: 2.0865
Epoch 893/2000
22/22 - 1s - loss: 1.6409 - val_loss: 2.0841
Epoch 894/2000
22/22 - 1s - loss: 1.6383 - val_loss: 2.0831
Epoch 895/2000
22/22 - 1s - loss: 1.6373 - val_loss: 2.0843
Epoch 896/2000
22/22 - 1s - loss: 1.6368 - val_loss: 2.0810
Epoch 897/2000
22/22 - 1s - loss: 1.6368 - val_loss: 2.0795
Epoch 898/2000
22/22 - 1s - loss: 1.6324 - val_loss: 2.0771
Epoch 899/2000
22/22 - 1s - loss: 1.6345 - val_loss: 2.0768
Epoch 900/2000
22/22 - 1s - loss: 1.6320 - val_loss: 2.0767
Epoch 00900: val_loss improved from 2.08856 to 2.07670, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 901/2000
22/22 - 1s - loss: 1.6299 - val_loss: 2.0765
Epoch 902/2000
22/22 - 1s - loss: 1.6314 - val_loss: 2.0750
Epoch 903/2000
22/22 - 1s - loss: 1.6306 - val_loss: 2.0750
Epoch 904/2000
22/22 - 1s - loss: 1.6279 - val_loss: 2.0740
Epoch 905/2000
22/22 - 1s - loss: 1.6286 - val_loss: 2.0711
Epoch 906/2000
22/22 - 1s - loss: 1.6231 - val_loss: 2.0699
Epoch 907/2000
22/22 - 1s - loss: 1.6256 - val_loss: 2.0677
Epoch 908/2000
22/22 - 1s - loss: 1.6238 - val_loss: 2.0675
Epoch 909/2000
22/22 - 1s - loss: 1.6229 - val_loss: 2.0661
Epoch 910/2000
22/22 - 1s - loss: 1.6219 - val_loss: 2.0660
Epoch 00910: val_loss improved from 2.07670 to 2.06597, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 911/2000
22/22 - 1s - loss: 1.6185 - val_loss: 2.0638
Epoch 912/2000
22/22 - 1s - loss: 1.6191 - val_loss: 2.0628
Epoch 913/2000
22/22 - 1s - loss: 1.6170 - val_loss: 2.0627
Epoch 914/2000
22/22 - 1s - loss: 1.6180 - val_loss: 2.0607
Epoch 915/2000
22/22 - 1s - loss: 1.6155 - val_loss: 2.0600
Epoch 916/2000
22/22 - 1s - loss: 1.6126 - val_loss: 2.0598
Epoch 917/2000
22/22 - 1s - loss: 1.6129 - val_loss: 2.0599
Epoch 918/2000
22/22 - 1s - loss: 1.6129 - val_loss: 2.0573
Epoch 919/2000
22/22 - 1s - loss: 1.6087 - val_loss: 2.0572
Epoch 920/2000
22/22 - 1s - loss: 1.6090 - val_loss: 2.0558
Epoch 00920: val_loss improved from 2.06597 to 2.05577, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 921/2000
22/22 - 1s - loss: 1.6077 - val_loss: 2.0525
Epoch 922/2000
22/22 - 1s - loss: 1.6096 - val_loss: 2.0521
Epoch 923/2000
22/22 - 1s - loss: 1.6064 - val_loss: 2.0495
Epoch 924/2000
22/22 - 1s - loss: 1.6056 - val_loss: 2.0496
Epoch 925/2000
22/22 - 1s - loss: 1.6024 - val_loss: 2.0495
Epoch 926/2000
22/22 - 1s - loss: 1.6025 - val_loss: 2.0487
Epoch 927/2000
22/22 - 1s - loss: 1.6000 - val_loss: 2.0459
Epoch 928/2000
22/22 - 1s - loss: 1.5997 - val_loss: 2.0465
Epoch 929/2000
22/22 - 1s - loss: 1.6010 - val_loss: 2.0447
Epoch 930/2000
22/22 - 1s - loss: 1.5970 - val_loss: 2.0436
Epoch 00930: val_loss improved from 2.05577 to 2.04364, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 931/2000
22/22 - 1s - loss: 1.5985 - val_loss: 2.0406
Epoch 932/2000
22/22 - 1s - loss: 1.5956 - val_loss: 2.0380
Epoch 933/2000
22/22 - 1s - loss: 1.5922 - val_loss: 2.0385
Epoch 934/2000
22/22 - 1s - loss: 1.5940 - val_loss: 2.0372
Epoch 935/2000
22/22 - 1s - loss: 1.5926 - val_loss: 2.0368
Epoch 936/2000
22/22 - 1s - loss: 1.5910 - val_loss: 2.0368
Epoch 937/2000
22/22 - 1s - loss: 1.5885 - val_loss: 2.0355
Epoch 938/2000
22/22 - 1s - loss: 1.5896 - val_loss: 2.0352
Epoch 939/2000
22/22 - 1s - loss: 1.5874 - val_loss: 2.0336
Epoch 940/2000
22/22 - 1s - loss: 1.5872 - val_loss: 2.0342
Epoch 00940: val_loss improved from 2.04364 to 2.03416, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 941/2000
22/22 - 1s - loss: 1.5872 - val_loss: 2.0308
Epoch 942/2000
22/22 - 1s - loss: 1.5845 - val_loss: 2.0308
Epoch 943/2000
22/22 - 1s - loss: 1.5834 - val_loss: 2.0299
Epoch 944/2000
22/22 - 1s - loss: 1.5828 - val_loss: 2.0277
Epoch 945/2000
22/22 - 1s - loss: 1.5817 - val_loss: 2.0286
Epoch 946/2000
22/22 - 1s - loss: 1.5796 - val_loss: 2.0266
Epoch 947/2000
22/22 - 1s - loss: 1.5791 - val_loss: 2.0261
Epoch 948/2000
22/22 - 1s - loss: 1.5805 - val_loss: 2.0245
Epoch 949/2000
22/22 - 1s - loss: 1.5765 - val_loss: 2.0219
Epoch 950/2000
22/22 - 1s - loss: 1.5779 - val_loss: 2.0209
Epoch 00950: val_loss improved from 2.03416 to 2.02090, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 951/2000
22/22 - 1s - loss: 1.5739 - val_loss: 2.0210
Epoch 952/2000
22/22 - 1s - loss: 1.5733 - val_loss: 2.0200
Epoch 953/2000
22/22 - 1s - loss: 1.5731 - val_loss: 2.0201
Epoch 954/2000
22/22 - 1s - loss: 1.5723 - val_loss: 2.0192
Epoch 955/2000
22/22 - 1s - loss: 1.5696 - val_loss: 2.0158
Epoch 956/2000
22/22 - 1s - loss: 1.5672 - val_loss: 2.0155
Epoch 957/2000
22/22 - 1s - loss: 1.5663 - val_loss: 2.0175
Epoch 958/2000
22/22 - 1s - loss: 1.5678 - val_loss: 2.0124
Epoch 959/2000
22/22 - 1s - loss: 1.5647 - val_loss: 2.0112
Epoch 960/2000
22/22 - 1s - loss: 1.5632 - val_loss: 2.0118
Epoch 00960: val_loss improved from 2.02090 to 2.01180, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 961/2000
22/22 - 1s - loss: 1.5643 - val_loss: 2.0107
Epoch 962/2000
22/22 - 1s - loss: 1.5643 - val_loss: 2.0100
Epoch 963/2000
22/22 - 1s - loss: 1.5608 - val_loss: 2.0092
Epoch 964/2000
22/22 - 1s - loss: 1.5614 - val_loss: 2.0064
Epoch 965/2000
22/22 - 1s - loss: 1.5572 - val_loss: 2.0048
Epoch 966/2000
22/22 - 1s - loss: 1.5579 - val_loss: 2.0056
Epoch 967/2000
22/22 - 1s - loss: 1.5565 - val_loss: 2.0036
Epoch 968/2000
22/22 - 1s - loss: 1.5566 - val_loss: 2.0034
Epoch 969/2000
22/22 - 1s - loss: 1.5533 - val_loss: 2.0005
Epoch 970/2000
22/22 - 1s - loss: 1.5534 - val_loss: 2.0008
Epoch 00970: val_loss improved from 2.01180 to 2.00080, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 971/2000
22/22 - 1s - loss: 1.5538 - val_loss: 1.9982
Epoch 972/2000
22/22 - 1s - loss: 1.5515 - val_loss: 1.9984
Epoch 973/2000
22/22 - 1s - loss: 1.5512 - val_loss: 1.9974
Epoch 974/2000
22/22 - 1s - loss: 1.5495 - val_loss: 1.9952
Epoch 975/2000
22/22 - 1s - loss: 1.5487 - val_loss: 1.9946
Epoch 976/2000
22/22 - 1s - loss: 1.5468 - val_loss: 1.9937
Epoch 977/2000
22/22 - 1s - loss: 1.5465 - val_loss: 1.9927
Epoch 978/2000
22/22 - 1s - loss: 1.5434 - val_loss: 1.9915
Epoch 979/2000
22/22 - 1s - loss: 1.5429 - val_loss: 1.9917
Epoch 980/2000
22/22 - 1s - loss: 1.5431 - val_loss: 1.9883
Epoch 00980: val_loss improved from 2.00080 to 1.98830, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 981/2000
22/22 - 1s - loss: 1.5437 - val_loss: 1.9886
Epoch 982/2000
22/22 - 1s - loss: 1.5405 - val_loss: 1.9879
Epoch 983/2000
22/22 - 1s - loss: 1.5400 - val_loss: 1.9873
Epoch 984/2000
22/22 - 1s - loss: 1.5377 - val_loss: 1.9858
Epoch 985/2000
22/22 - 1s - loss: 1.5390 - val_loss: 1.9849
Epoch 986/2000
22/22 - 1s - loss: 1.5374 - val_loss: 1.9864
Epoch 987/2000
22/22 - 1s - loss: 1.5352 - val_loss: 1.9826
Epoch 988/2000
22/22 - 1s - loss: 1.5317 - val_loss: 1.9814
Epoch 989/2000
22/22 - 1s - loss: 1.5318 - val_loss: 1.9800
Epoch 990/2000
22/22 - 1s - loss: 1.5328 - val_loss: 1.9786
Epoch 00990: val_loss improved from 1.98830 to 1.97864, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 991/2000
22/22 - 1s - loss: 1.5305 - val_loss: 1.9769
Epoch 992/2000
22/22 - 1s - loss: 1.5319 - val_loss: 1.9788
Epoch 993/2000
22/22 - 1s - loss: 1.5276 - val_loss: 1.9778
Epoch 994/2000
22/22 - 1s - loss: 1.5296 - val_loss: 1.9749
Epoch 995/2000
22/22 - 1s - loss: 1.5296 - val_loss: 1.9749
Epoch 996/2000
22/22 - 1s - loss: 1.5264 - val_loss: 1.9719
Epoch 997/2000
22/22 - 1s - loss: 1.5248 - val_loss: 1.9709
Epoch 998/2000
22/22 - 1s - loss: 1.5242 - val_loss: 1.9710
Epoch 999/2000
22/22 - 1s - loss: 1.5217 - val_loss: 1.9699
Epoch 1000/2000
22/22 - 1s - loss: 1.5222 - val_loss: 1.9685
Epoch 01000: val_loss improved from 1.97864 to 1.96853, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1001/2000
22/22 - 1s - loss: 1.5214 - val_loss: 1.9694
Epoch 1002/2000
22/22 - 1s - loss: 1.5189 - val_loss: 1.9649
Epoch 1003/2000
22/22 - 1s - loss: 1.5175 - val_loss: 1.9677
Epoch 1004/2000
22/22 - 1s - loss: 1.5151 - val_loss: 1.9652
Epoch 1005/2000
22/22 - 1s - loss: 1.5176 - val_loss: 1.9649
Epoch 1006/2000
22/22 - 1s - loss: 1.5167 - val_loss: 1.9637
Epoch 1007/2000
22/22 - 1s - loss: 1.5150 - val_loss: 1.9591
Epoch 1008/2000
22/22 - 1s - loss: 1.5130 - val_loss: 1.9598
Epoch 1009/2000
22/22 - 1s - loss: 1.5127 - val_loss: 1.9587
Epoch 1010/2000
22/22 - 1s - loss: 1.5107 - val_loss: 1.9576
Epoch 01010: val_loss improved from 1.96853 to 1.95760, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1011/2000
22/22 - 1s - loss: 1.5094 - val_loss: 1.9571
Epoch 1012/2000
22/22 - 1s - loss: 1.5089 - val_loss: 1.9557
Epoch 1013/2000
22/22 - 1s - loss: 1.5070 - val_loss: 1.9542
Epoch 1014/2000
22/22 - 1s - loss: 1.5073 - val_loss: 1.9539
Epoch 1015/2000
22/22 - 1s - loss: 1.5054 - val_loss: 1.9538
Epoch 1016/2000
22/22 - 1s - loss: 1.5049 - val_loss: 1.9515
Epoch 1017/2000
22/22 - 1s - loss: 1.5039 - val_loss: 1.9496
Epoch 1018/2000
22/22 - 1s - loss: 1.5030 - val_loss: 1.9506
Epoch 1019/2000
22/22 - 1s - loss: 1.5002 - val_loss: 1.9494
Epoch 1020/2000
22/22 - 1s - loss: 1.5010 - val_loss: 1.9479
Epoch 01020: val_loss improved from 1.95760 to 1.94786, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1021/2000
22/22 - 1s - loss: 1.5004 - val_loss: 1.9475
Epoch 1022/2000
22/22 - 1s - loss: 1.4987 - val_loss: 1.9467
Epoch 1023/2000
22/22 - 1s - loss: 1.4985 - val_loss: 1.9449
Epoch 1024/2000
22/22 - 1s - loss: 1.4991 - val_loss: 1.9443
Epoch 1025/2000
22/22 - 1s - loss: 1.4959 - val_loss: 1.9418
Epoch 1026/2000
22/22 - 1s - loss: 1.4962 - val_loss: 1.9425
Epoch 1027/2000
22/22 - 1s - loss: 1.4932 - val_loss: 1.9424
Epoch 1028/2000
22/22 - 1s - loss: 1.4926 - val_loss: 1.9411
Epoch 1029/2000
22/22 - 1s - loss: 1.4917 - val_loss: 1.9412
Epoch 1030/2000
22/22 - 1s - loss: 1.4925 - val_loss: 1.9389
Epoch 01030: val_loss improved from 1.94786 to 1.93893, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1031/2000
22/22 - 1s - loss: 1.4883 - val_loss: 1.9362
Epoch 1032/2000
22/22 - 1s - loss: 1.4891 - val_loss: 1.9367
Epoch 1033/2000
22/22 - 1s - loss: 1.4883 - val_loss: 1.9337
Epoch 1034/2000
22/22 - 1s - loss: 1.4883 - val_loss: 1.9339
Epoch 1035/2000
22/22 - 1s - loss: 1.4864 - val_loss: 1.9341
Epoch 1036/2000
22/22 - 1s - loss: 1.4865 - val_loss: 1.9329
Epoch 1037/2000
22/22 - 1s - loss: 1.4852 - val_loss: 1.9336
Epoch 1038/2000
22/22 - 1s - loss: 1.4830 - val_loss: 1.9313
Epoch 1039/2000
22/22 - 1s - loss: 1.4808 - val_loss: 1.9306
Epoch 1040/2000
22/22 - 1s - loss: 1.4799 - val_loss: 1.9280
Epoch 01040: val_loss improved from 1.93893 to 1.92799, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1041/2000
22/22 - 1s - loss: 1.4804 - val_loss: 1.9275
Epoch 1042/2000
22/22 - 1s - loss: 1.4773 - val_loss: 1.9274
Epoch 1043/2000
22/22 - 1s - loss: 1.4782 - val_loss: 1.9271
Epoch 1044/2000
22/22 - 1s - loss: 1.4759 - val_loss: 1.9255
Epoch 1045/2000
22/22 - 1s - loss: 1.4736 - val_loss: 1.9242
Epoch 1046/2000
22/22 - 1s - loss: 1.4764 - val_loss: 1.9231
Epoch 1047/2000
22/22 - 1s - loss: 1.4748 - val_loss: 1.9253
Epoch 1048/2000
22/22 - 1s - loss: 1.4722 - val_loss: 1.9220
Epoch 1049/2000
22/22 - 1s - loss: 1.4718 - val_loss: 1.9203
Epoch 1050/2000
22/22 - 1s - loss: 1.4698 - val_loss: 1.9201
Epoch 01050: val_loss improved from 1.92799 to 1.92007, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1051/2000
22/22 - 1s - loss: 1.4691 - val_loss: 1.9196
Epoch 1052/2000
22/22 - 1s - loss: 1.4696 - val_loss: 1.9181
Epoch 1053/2000
22/22 - 1s - loss: 1.4668 - val_loss: 1.9162
Epoch 1054/2000
22/22 - 1s - loss: 1.4659 - val_loss: 1.9178
Epoch 1055/2000
22/22 - 1s - loss: 1.4668 - val_loss: 1.9146
Epoch 1056/2000
22/22 - 1s - loss: 1.4648 - val_loss: 1.9150
Epoch 1057/2000
22/22 - 1s - loss: 1.4658 - val_loss: 1.9135
Epoch 1058/2000
22/22 - 1s - loss: 1.4646 - val_loss: 1.9134
Epoch 1059/2000
22/22 - 1s - loss: 1.4639 - val_loss: 1.9122
Epoch 1060/2000
22/22 - 1s - loss: 1.4612 - val_loss: 1.9111
Epoch 01060: val_loss improved from 1.92007 to 1.91115, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1061/2000
22/22 - 1s - loss: 1.4599 - val_loss: 1.9088
Epoch 1062/2000
22/22 - 1s - loss: 1.4576 - val_loss: 1.9083
Epoch 1063/2000
22/22 - 1s - loss: 1.4573 - val_loss: 1.9068
Epoch 1064/2000
22/22 - 1s - loss: 1.4565 - val_loss: 1.9063
Epoch 1065/2000
22/22 - 1s - loss: 1.4546 - val_loss: 1.9075
Epoch 1066/2000
22/22 - 1s - loss: 1.4537 - val_loss: 1.9078
Epoch 1067/2000
22/22 - 1s - loss: 1.4534 - val_loss: 1.9062
Epoch 1068/2000
22/22 - 1s - loss: 1.4538 - val_loss: 1.9039
Epoch 1069/2000
22/22 - 1s - loss: 1.4521 - val_loss: 1.9042
Epoch 1070/2000
22/22 - 1s - loss: 1.4516 - val_loss: 1.9029
Epoch 01070: val_loss improved from 1.91115 to 1.90294, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1071/2000
22/22 - 1s - loss: 1.4508 - val_loss: 1.8993
Epoch 1072/2000
22/22 - 1s - loss: 1.4495 - val_loss: 1.8974
Epoch 1073/2000
22/22 - 1s - loss: 1.4489 - val_loss: 1.8987
Epoch 1074/2000
22/22 - 1s - loss: 1.4475 - val_loss: 1.8963
Epoch 1075/2000
22/22 - 1s - loss: 1.4468 - val_loss: 1.8972
Epoch 1076/2000
22/22 - 1s - loss: 1.4487 - val_loss: 1.8957
Epoch 1077/2000
22/22 - 1s - loss: 1.4447 - val_loss: 1.8948
Epoch 1078/2000
22/22 - 1s - loss: 1.4438 - val_loss: 1.8943
Epoch 1079/2000
22/22 - 1s - loss: 1.4425 - val_loss: 1.8927
Epoch 1080/2000
22/22 - 1s - loss: 1.4431 - val_loss: 1.8921
Epoch 01080: val_loss improved from 1.90294 to 1.89207, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1081/2000
22/22 - 1s - loss: 1.4420 - val_loss: 1.8895
Epoch 1082/2000
22/22 - 1s - loss: 1.4405 - val_loss: 1.8885
Epoch 1083/2000
22/22 - 1s - loss: 1.4408 - val_loss: 1.8917
Epoch 1084/2000
22/22 - 1s - loss: 1.4387 - val_loss: 1.8904
Epoch 1085/2000
22/22 - 1s - loss: 1.4388 - val_loss: 1.8878
Epoch 1086/2000
22/22 - 1s - loss: 1.4352 - val_loss: 1.8859
Epoch 1087/2000
22/22 - 1s - loss: 1.4335 - val_loss: 1.8843
Epoch 1088/2000
22/22 - 1s - loss: 1.4336 - val_loss: 1.8832
Epoch 1089/2000
22/22 - 1s - loss: 1.4326 - val_loss: 1.8830
Epoch 1090/2000
22/22 - 1s - loss: 1.4318 - val_loss: 1.8816
Epoch 01090: val_loss improved from 1.89207 to 1.88160, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1091/2000
22/22 - 1s - loss: 1.4305 - val_loss: 1.8810
Epoch 1092/2000
22/22 - 1s - loss: 1.4306 - val_loss: 1.8800
Epoch 1093/2000
22/22 - 1s - loss: 1.4294 - val_loss: 1.8803
Epoch 1094/2000
22/22 - 1s - loss: 1.4295 - val_loss: 1.8793
Epoch 1095/2000
22/22 - 1s - loss: 1.4290 - val_loss: 1.8787
Epoch 1096/2000
22/22 - 1s - loss: 1.4268 - val_loss: 1.8786
Epoch 1097/2000
22/22 - 1s - loss: 1.4237 - val_loss: 1.8749
Epoch 1098/2000
22/22 - 1s - loss: 1.4238 - val_loss: 1.8744
Epoch 1099/2000
22/22 - 1s - loss: 1.4223 - val_loss: 1.8737
Epoch 1100/2000
22/22 - 1s - loss: 1.4218 - val_loss: 1.8725
Epoch 01100: val_loss improved from 1.88160 to 1.87251, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1101/2000
22/22 - 1s - loss: 1.4215 - val_loss: 1.8712
Epoch 1102/2000
22/22 - 1s - loss: 1.4227 - val_loss: 1.8696
Epoch 1103/2000
22/22 - 1s - loss: 1.4210 - val_loss: 1.8700
Epoch 1104/2000
22/22 - 1s - loss: 1.4189 - val_loss: 1.8701
Epoch 1105/2000
22/22 - 1s - loss: 1.4178 - val_loss: 1.8707
Epoch 1106/2000
22/22 - 1s - loss: 1.4187 - val_loss: 1.8663
Epoch 1107/2000
22/22 - 1s - loss: 1.4163 - val_loss: 1.8681
Epoch 1108/2000
22/22 - 1s - loss: 1.4155 - val_loss: 1.8654
Epoch 1109/2000
22/22 - 1s - loss: 1.4146 - val_loss: 1.8636
Epoch 1110/2000
22/22 - 1s - loss: 1.4140 - val_loss: 1.8646
Epoch 01110: val_loss improved from 1.87251 to 1.86463, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1111/2000
22/22 - 1s - loss: 1.4130 - val_loss: 1.8629
Epoch 1112/2000
22/22 - 1s - loss: 1.4132 - val_loss: 1.8618
Epoch 1113/2000
22/22 - 1s - loss: 1.4091 - val_loss: 1.8599
Epoch 1114/2000
22/22 - 1s - loss: 1.4109 - val_loss: 1.8583
Epoch 1115/2000
22/22 - 1s - loss: 1.4107 - val_loss: 1.8572
Epoch 1116/2000
22/22 - 1s - loss: 1.4065 - val_loss: 1.8573
Epoch 1117/2000
22/22 - 1s - loss: 1.4075 - val_loss: 1.8554
Epoch 1118/2000
22/22 - 1s - loss: 1.4074 - val_loss: 1.8570
Epoch 1119/2000
22/22 - 1s - loss: 1.4048 - val_loss: 1.8557
Epoch 1120/2000
22/22 - 1s - loss: 1.4028 - val_loss: 1.8544
Epoch 01120: val_loss improved from 1.86463 to 1.85443, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1121/2000
22/22 - 1s - loss: 1.4034 - val_loss: 1.8531
Epoch 1122/2000
22/22 - 1s - loss: 1.4039 - val_loss: 1.8510
Epoch 1123/2000
22/22 - 1s - loss: 1.4006 - val_loss: 1.8521
Epoch 1124/2000
22/22 - 1s - loss: 1.4020 - val_loss: 1.8499
Epoch 1125/2000
22/22 - 1s - loss: 1.3997 - val_loss: 1.8493
Epoch 1126/2000
22/22 - 1s - loss: 1.3993 - val_loss: 1.8501
Epoch 1127/2000
22/22 - 1s - loss: 1.3970 - val_loss: 1.8496
Epoch 1128/2000
22/22 - 1s - loss: 1.3985 - val_loss: 1.8475
Epoch 1129/2000
22/22 - 1s - loss: 1.3973 - val_loss: 1.8455
Epoch 1130/2000
22/22 - 1s - loss: 1.3943 - val_loss: 1.8446
Epoch 01130: val_loss improved from 1.85443 to 1.84464, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1131/2000
22/22 - 1s - loss: 1.3955 - val_loss: 1.8440
Epoch 1132/2000
22/22 - 1s - loss: 1.3934 - val_loss: 1.8437
Epoch 1133/2000
22/22 - 1s - loss: 1.3904 - val_loss: 1.8411
Epoch 1134/2000
22/22 - 1s - loss: 1.3905 - val_loss: 1.8389
Epoch 1135/2000
22/22 - 1s - loss: 1.3900 - val_loss: 1.8410
Epoch 1136/2000
22/22 - 1s - loss: 1.3916 - val_loss: 1.8405
Epoch 1137/2000
22/22 - 1s - loss: 1.3889 - val_loss: 1.8387
Epoch 1138/2000
22/22 - 1s - loss: 1.3886 - val_loss: 1.8377
Epoch 1139/2000
22/22 - 1s - loss: 1.3850 - val_loss: 1.8353
Epoch 1140/2000
22/22 - 1s - loss: 1.3869 - val_loss: 1.8356
Epoch 01140: val_loss improved from 1.84464 to 1.83561, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1141/2000
22/22 - 1s - loss: 1.3844 - val_loss: 1.8361
Epoch 1142/2000
22/22 - 1s - loss: 1.3860 - val_loss: 1.8360
Epoch 1143/2000
22/22 - 1s - loss: 1.3828 - val_loss: 1.8342
Epoch 1144/2000
22/22 - 1s - loss: 1.3845 - val_loss: 1.8333
Epoch 1145/2000
22/22 - 1s - loss: 1.3818 - val_loss: 1.8325
Epoch 1146/2000
22/22 - 1s - loss: 1.3813 - val_loss: 1.8308
Epoch 1147/2000
22/22 - 1s - loss: 1.3802 - val_loss: 1.8291
Epoch 1148/2000
22/22 - 1s - loss: 1.3779 - val_loss: 1.8298
Epoch 1149/2000
22/22 - 1s - loss: 1.3791 - val_loss: 1.8310
Epoch 1150/2000
22/22 - 1s - loss: 1.3771 - val_loss: 1.8280
Epoch 01150: val_loss improved from 1.83561 to 1.82798, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1151/2000
22/22 - 1s - loss: 1.3793 - val_loss: 1.8281
Epoch 1152/2000
22/22 - 1s - loss: 1.3755 - val_loss: 1.8240
Epoch 1153/2000
22/22 - 1s - loss: 1.3724 - val_loss: 1.8224
Epoch 1154/2000
22/22 - 1s - loss: 1.3721 - val_loss: 1.8235
Epoch 1155/2000
22/22 - 1s - loss: 1.3720 - val_loss: 1.8213
Epoch 1156/2000
22/22 - 1s - loss: 1.3709 - val_loss: 1.8228
Epoch 1157/2000
22/22 - 1s - loss: 1.3736 - val_loss: 1.8204
Epoch 1158/2000
22/22 - 1s - loss: 1.3709 - val_loss: 1.8207
Epoch 1159/2000
22/22 - 1s - loss: 1.3706 - val_loss: 1.8204
Epoch 1160/2000
22/22 - 1s - loss: 1.3688 - val_loss: 1.8190
Epoch 01160: val_loss improved from 1.82798 to 1.81903, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1161/2000
22/22 - 1s - loss: 1.3666 - val_loss: 1.8193
Epoch 1162/2000
22/22 - 1s - loss: 1.3650 - val_loss: 1.8183
Epoch 1163/2000
22/22 - 1s - loss: 1.3671 - val_loss: 1.8167
Epoch 1164/2000
22/22 - 1s - loss: 1.3656 - val_loss: 1.8164
Epoch 1165/2000
22/22 - 1s - loss: 1.3637 - val_loss: 1.8130
Epoch 1166/2000
22/22 - 1s - loss: 1.3643 - val_loss: 1.8146
Epoch 1167/2000
22/22 - 1s - loss: 1.3633 - val_loss: 1.8147
Epoch 1168/2000
22/22 - 1s - loss: 1.3609 - val_loss: 1.8146
Epoch 1169/2000
22/22 - 1s - loss: 1.3617 - val_loss: 1.8133
Epoch 1170/2000
22/22 - 1s - loss: 1.3582 - val_loss: 1.8109
Epoch 01170: val_loss improved from 1.81903 to 1.81093, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1171/2000
22/22 - 1s - loss: 1.3614 - val_loss: 1.8112
Epoch 1172/2000
22/22 - 1s - loss: 1.3585 - val_loss: 1.8103
Epoch 1173/2000
22/22 - 1s - loss: 1.3568 - val_loss: 1.8092
Epoch 1174/2000
22/22 - 1s - loss: 1.3570 - val_loss: 1.8089
Epoch 1175/2000
22/22 - 1s - loss: 1.3560 - val_loss: 1.8073
Epoch 1176/2000
22/22 - 1s - loss: 1.3571 - val_loss: 1.8060
Epoch 1177/2000
22/22 - 1s - loss: 1.3533 - val_loss: 1.8055
Epoch 1178/2000
22/22 - 1s - loss: 1.3548 - val_loss: 1.8036
Epoch 1179/2000
22/22 - 1s - loss: 1.3533 - val_loss: 1.8040
Epoch 1180/2000
22/22 - 1s - loss: 1.3528 - val_loss: 1.8044
Epoch 01180: val_loss improved from 1.81093 to 1.80442, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1181/2000
22/22 - 1s - loss: 1.3494 - val_loss: 1.8027
Epoch 1182/2000
22/22 - 1s - loss: 1.3492 - val_loss: 1.8001
Epoch 1183/2000
22/22 - 1s - loss: 1.3483 - val_loss: 1.8001
Epoch 1184/2000
22/22 - 1s - loss: 1.3474 - val_loss: 1.7994
Epoch 1185/2000
22/22 - 1s - loss: 1.3462 - val_loss: 1.7983
Epoch 1186/2000
22/22 - 1s - loss: 1.3470 - val_loss: 1.7982
Epoch 1187/2000
22/22 - 1s - loss: 1.3448 - val_loss: 1.7964
Epoch 1188/2000
22/22 - 1s - loss: 1.3434 - val_loss: 1.7962
Epoch 1189/2000
22/22 - 1s - loss: 1.3420 - val_loss: 1.7925
Epoch 1190/2000
22/22 - 1s - loss: 1.3441 - val_loss: 1.7941
Epoch 01190: val_loss improved from 1.80442 to 1.79409, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1191/2000
22/22 - 1s - loss: 1.3423 - val_loss: 1.7914
Epoch 1192/2000
22/22 - 1s - loss: 1.3420 - val_loss: 1.7922
Epoch 1193/2000
22/22 - 1s - loss: 1.3400 - val_loss: 1.7903
Epoch 1194/2000
22/22 - 1s - loss: 1.3389 - val_loss: 1.7910
Epoch 1195/2000
22/22 - 1s - loss: 1.3389 - val_loss: 1.7893
Epoch 1196/2000
22/22 - 1s - loss: 1.3394 - val_loss: 1.7903
Epoch 1197/2000
22/22 - 1s - loss: 1.3388 - val_loss: 1.7894
Epoch 1198/2000
22/22 - 1s - loss: 1.3358 - val_loss: 1.7890
Epoch 1199/2000
22/22 - 1s - loss: 1.3377 - val_loss: 1.7864
Epoch 1200/2000
22/22 - 1s - loss: 1.3371 - val_loss: 1.7865
Epoch 01200: val_loss improved from 1.79409 to 1.78652, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1201/2000
22/22 - 1s - loss: 1.3334 - val_loss: 1.7852
Epoch 1202/2000
22/22 - 1s - loss: 1.3332 - val_loss: 1.7849
Epoch 1203/2000
22/22 - 1s - loss: 1.3313 - val_loss: 1.7828
Epoch 1204/2000
22/22 - 1s - loss: 1.3307 - val_loss: 1.7825
Epoch 1205/2000
22/22 - 1s - loss: 1.3302 - val_loss: 1.7818
Epoch 1206/2000
22/22 - 1s - loss: 1.3272 - val_loss: 1.7822
Epoch 1207/2000
22/22 - 1s - loss: 1.3281 - val_loss: 1.7806
Epoch 1208/2000
22/22 - 1s - loss: 1.3280 - val_loss: 1.7790
Epoch 1209/2000
22/22 - 1s - loss: 1.3270 - val_loss: 1.7802
Epoch 1210/2000
22/22 - 1s - loss: 1.3256 - val_loss: 1.7793
Epoch 01210: val_loss improved from 1.78652 to 1.77933, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1211/2000
22/22 - 1s - loss: 1.3259 - val_loss: 1.7784
Epoch 1212/2000
22/22 - 1s - loss: 1.3257 - val_loss: 1.7770
Epoch 1213/2000
22/22 - 1s - loss: 1.3241 - val_loss: 1.7741
Epoch 1214/2000
22/22 - 1s - loss: 1.3227 - val_loss: 1.7739
Epoch 1215/2000
22/22 - 1s - loss: 1.3218 - val_loss: 1.7707
Epoch 1216/2000
22/22 - 1s - loss: 1.3203 - val_loss: 1.7739
Epoch 1217/2000
22/22 - 1s - loss: 1.3214 - val_loss: 1.7738
Epoch 1218/2000
22/22 - 1s - loss: 1.3199 - val_loss: 1.7719
Epoch 1219/2000
22/22 - 1s - loss: 1.3188 - val_loss: 1.7697
Epoch 1220/2000
22/22 - 1s - loss: 1.3198 - val_loss: 1.7685
Epoch 01220: val_loss improved from 1.77933 to 1.76848, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1221/2000
22/22 - 1s - loss: 1.3180 - val_loss: 1.7685
Epoch 1222/2000
22/22 - 1s - loss: 1.3166 - val_loss: 1.7687
Epoch 1223/2000
22/22 - 1s - loss: 1.3160 - val_loss: 1.7673
Epoch 1224/2000
22/22 - 1s - loss: 1.3127 - val_loss: 1.7653
Epoch 1225/2000
22/22 - 1s - loss: 1.3132 - val_loss: 1.7631
Epoch 1226/2000
22/22 - 1s - loss: 1.3138 - val_loss: 1.7638
Epoch 1227/2000
22/22 - 1s - loss: 1.3119 - val_loss: 1.7624
Epoch 1228/2000
22/22 - 1s - loss: 1.3094 - val_loss: 1.7623
Epoch 1229/2000
22/22 - 1s - loss: 1.3103 - val_loss: 1.7623
Epoch 1230/2000
22/22 - 1s - loss: 1.3115 - val_loss: 1.7619
Epoch 01230: val_loss improved from 1.76848 to 1.76189, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1231/2000
22/22 - 1s - loss: 1.3096 - val_loss: 1.7603
Epoch 1232/2000
22/22 - 1s - loss: 1.3085 - val_loss: 1.7623
Epoch 1233/2000
22/22 - 1s - loss: 1.3077 - val_loss: 1.7574
Epoch 1234/2000
22/22 - 1s - loss: 1.3078 - val_loss: 1.7565
Epoch 1235/2000
22/22 - 1s - loss: 1.3042 - val_loss: 1.7551
Epoch 1236/2000
22/22 - 1s - loss: 1.3044 - val_loss: 1.7557
Epoch 1237/2000
22/22 - 1s - loss: 1.3069 - val_loss: 1.7534
Epoch 1238/2000
22/22 - 1s - loss: 1.3051 - val_loss: 1.7541
Epoch 1239/2000
22/22 - 1s - loss: 1.3033 - val_loss: 1.7551
Epoch 1240/2000
22/22 - 1s - loss: 1.3013 - val_loss: 1.7526
Epoch 01240: val_loss improved from 1.76189 to 1.75259, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1241/2000
22/22 - 1s - loss: 1.3011 - val_loss: 1.7532
Epoch 1242/2000
22/22 - 1s - loss: 1.2999 - val_loss: 1.7507
Epoch 1243/2000
22/22 - 1s - loss: 1.2994 - val_loss: 1.7498
Epoch 1244/2000
22/22 - 1s - loss: 1.2993 - val_loss: 1.7501
Epoch 1245/2000
22/22 - 1s - loss: 1.2970 - val_loss: 1.7496
Epoch 1246/2000
22/22 - 1s - loss: 1.2981 - val_loss: 1.7468
Epoch 1247/2000
22/22 - 1s - loss: 1.2976 - val_loss: 1.7478
Epoch 1248/2000
22/22 - 1s - loss: 1.2950 - val_loss: 1.7486
Epoch 1249/2000
22/22 - 1s - loss: 1.2961 - val_loss: 1.7446
Epoch 1250/2000
22/22 - 1s - loss: 1.2934 - val_loss: 1.7452
Epoch 01250: val_loss improved from 1.75259 to 1.74519, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1251/2000
22/22 - 1s - loss: 1.2913 - val_loss: 1.7434
Epoch 1252/2000
22/22 - 1s - loss: 1.2902 - val_loss: 1.7436
Epoch 1253/2000
22/22 - 1s - loss: 1.2907 - val_loss: 1.7428
Epoch 1254/2000
22/22 - 1s - loss: 1.2892 - val_loss: 1.7438
Epoch 1255/2000
22/22 - 1s - loss: 1.2891 - val_loss: 1.7402
Epoch 1256/2000
22/22 - 1s - loss: 1.2865 - val_loss: 1.7422
Epoch 1257/2000
22/22 - 1s - loss: 1.2888 - val_loss: 1.7424
Epoch 1258/2000
22/22 - 1s - loss: 1.2886 - val_loss: 1.7394
Epoch 1259/2000
22/22 - 1s - loss: 1.2859 - val_loss: 1.7396
Epoch 1260/2000
22/22 - 1s - loss: 1.2858 - val_loss: 1.7362
Epoch 01260: val_loss improved from 1.74519 to 1.73624, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1261/2000
22/22 - 1s - loss: 1.2851 - val_loss: 1.7380
Epoch 1262/2000
22/22 - 1s - loss: 1.2842 - val_loss: 1.7354
Epoch 1263/2000
22/22 - 1s - loss: 1.2841 - val_loss: 1.7368
Epoch 1264/2000
22/22 - 1s - loss: 1.2817 - val_loss: 1.7332
Epoch 1265/2000
22/22 - 1s - loss: 1.2812 - val_loss: 1.7339
Epoch 1266/2000
22/22 - 1s - loss: 1.2825 - val_loss: 1.7324
Epoch 1267/2000
22/22 - 1s - loss: 1.2786 - val_loss: 1.7331
Epoch 1268/2000
22/22 - 1s - loss: 1.2780 - val_loss: 1.7335
Epoch 1269/2000
22/22 - 1s - loss: 1.2766 - val_loss: 1.7293
Epoch 1270/2000
22/22 - 1s - loss: 1.2771 - val_loss: 1.7305
Epoch 01270: val_loss improved from 1.73624 to 1.73055, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1271/2000
22/22 - 1s - loss: 1.2766 - val_loss: 1.7295
Epoch 1272/2000
22/22 - 1s - loss: 1.2752 - val_loss: 1.7273
Epoch 1273/2000
22/22 - 1s - loss: 1.2768 - val_loss: 1.7276
Epoch 1274/2000
22/22 - 1s - loss: 1.2771 - val_loss: 1.7258
Epoch 1275/2000
22/22 - 1s - loss: 1.2730 - val_loss: 1.7242
Epoch 1276/2000
22/22 - 1s - loss: 1.2741 - val_loss: 1.7262
Epoch 1277/2000
22/22 - 1s - loss: 1.2730 - val_loss: 1.7231
Epoch 1278/2000
22/22 - 1s - loss: 1.2734 - val_loss: 1.7239
Epoch 1279/2000
22/22 - 1s - loss: 1.2719 - val_loss: 1.7214
Epoch 1280/2000
22/22 - 1s - loss: 1.2685 - val_loss: 1.7222
Epoch 01280: val_loss improved from 1.73055 to 1.72217, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1281/2000
22/22 - 1s - loss: 1.2694 - val_loss: 1.7201
Epoch 1282/2000
22/22 - 1s - loss: 1.2691 - val_loss: 1.7206
Epoch 1283/2000
22/22 - 1s - loss: 1.2676 - val_loss: 1.7189
Epoch 1284/2000
22/22 - 1s - loss: 1.2683 - val_loss: 1.7192
Epoch 1285/2000
22/22 - 1s - loss: 1.2670 - val_loss: 1.7171
Epoch 1286/2000
22/22 - 1s - loss: 1.2613 - val_loss: 1.7194
Epoch 1287/2000
22/22 - 1s - loss: 1.2658 - val_loss: 1.7160
Epoch 1288/2000
22/22 - 1s - loss: 1.2629 - val_loss: 1.7134
Epoch 1289/2000
22/22 - 1s - loss: 1.2647 - val_loss: 1.7137
Epoch 1290/2000
22/22 - 1s - loss: 1.2626 - val_loss: 1.7142
Epoch 01290: val_loss improved from 1.72217 to 1.71419, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1291/2000
22/22 - 1s - loss: 1.2615 - val_loss: 1.7137
Epoch 1292/2000
22/22 - 1s - loss: 1.2616 - val_loss: 1.7119
Epoch 1293/2000
22/22 - 1s - loss: 1.2614 - val_loss: 1.7123
Epoch 1294/2000
22/22 - 1s - loss: 1.2579 - val_loss: 1.7090
Epoch 1295/2000
22/22 - 1s - loss: 1.2578 - val_loss: 1.7099
Epoch 1296/2000
22/22 - 1s - loss: 1.2587 - val_loss: 1.7078
Epoch 1297/2000
22/22 - 1s - loss: 1.2577 - val_loss: 1.7074
Epoch 1298/2000
22/22 - 1s - loss: 1.2568 - val_loss: 1.7068
Epoch 1299/2000
22/22 - 1s - loss: 1.2548 - val_loss: 1.7081
Epoch 1300/2000
22/22 - 1s - loss: 1.2539 - val_loss: 1.7055
Epoch 01300: val_loss improved from 1.71419 to 1.70548, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1301/2000
22/22 - 1s - loss: 1.2545 - val_loss: 1.7073
Epoch 1302/2000
22/22 - 1s - loss: 1.2536 - val_loss: 1.7061
Epoch 1303/2000
22/22 - 1s - loss: 1.2533 - val_loss: 1.7021
Epoch 1304/2000
22/22 - 1s - loss: 1.2504 - val_loss: 1.7043
Epoch 1305/2000
22/22 - 1s - loss: 1.2511 - val_loss: 1.7029
Epoch 1306/2000
22/22 - 1s - loss: 1.2521 - val_loss: 1.7015
Epoch 1307/2000
22/22 - 1s - loss: 1.2482 - val_loss: 1.7015
Epoch 1308/2000
22/22 - 1s - loss: 1.2485 - val_loss: 1.7014
Epoch 1309/2000
22/22 - 1s - loss: 1.2497 - val_loss: 1.7010
Epoch 1310/2000
22/22 - 1s - loss: 1.2470 - val_loss: 1.7001
Epoch 01310: val_loss improved from 1.70548 to 1.70014, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1311/2000
22/22 - 1s - loss: 1.2477 - val_loss: 1.6974
Epoch 1312/2000
22/22 - 1s - loss: 1.2450 - val_loss: 1.6984
Epoch 1313/2000
22/22 - 1s - loss: 1.2470 - val_loss: 1.6968
Epoch 1314/2000
22/22 - 1s - loss: 1.2407 - val_loss: 1.6961
Epoch 1315/2000
22/22 - 1s - loss: 1.2439 - val_loss: 1.6952
Epoch 1316/2000
22/22 - 1s - loss: 1.2442 - val_loss: 1.6935
Epoch 1317/2000
22/22 - 1s - loss: 1.2423 - val_loss: 1.6923
Epoch 1318/2000
22/22 - 1s - loss: 1.2415 - val_loss: 1.6917
Epoch 1319/2000
22/22 - 1s - loss: 1.2400 - val_loss: 1.6917
Epoch 1320/2000
22/22 - 1s - loss: 1.2395 - val_loss: 1.6930
Epoch 01320: val_loss improved from 1.70014 to 1.69298, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1321/2000
22/22 - 1s - loss: 1.2387 - val_loss: 1.6909
Epoch 1322/2000
22/22 - 1s - loss: 1.2382 - val_loss: 1.6893
Epoch 1323/2000
22/22 - 1s - loss: 1.2378 - val_loss: 1.6904
Epoch 1324/2000
22/22 - 1s - loss: 1.2366 - val_loss: 1.6884
Epoch 1325/2000
22/22 - 1s - loss: 1.2361 - val_loss: 1.6864
Epoch 1326/2000
22/22 - 1s - loss: 1.2356 - val_loss: 1.6872
Epoch 1327/2000
22/22 - 1s - loss: 1.2332 - val_loss: 1.6871
Epoch 1328/2000
22/22 - 1s - loss: 1.2326 - val_loss: 1.6867
Epoch 1329/2000
22/22 - 1s - loss: 1.2334 - val_loss: 1.6869
Epoch 1330/2000
22/22 - 1s - loss: 1.2317 - val_loss: 1.6837
Epoch 01330: val_loss improved from 1.69298 to 1.68375, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1331/2000
22/22 - 1s - loss: 1.2321 - val_loss: 1.6854
Epoch 1332/2000
22/22 - 1s - loss: 1.2327 - val_loss: 1.6818
Epoch 1333/2000
22/22 - 1s - loss: 1.2296 - val_loss: 1.6829
Epoch 1334/2000
22/22 - 1s - loss: 1.2265 - val_loss: 1.6818
Epoch 1335/2000
22/22 - 1s - loss: 1.2286 - val_loss: 1.6830
Epoch 1336/2000
22/22 - 1s - loss: 1.2288 - val_loss: 1.6780
Epoch 1337/2000
22/22 - 1s - loss: 1.2284 - val_loss: 1.6815
Epoch 1338/2000
22/22 - 1s - loss: 1.2277 - val_loss: 1.6805
Epoch 1339/2000
22/22 - 1s - loss: 1.2271 - val_loss: 1.6791
Epoch 1340/2000
22/22 - 1s - loss: 1.2250 - val_loss: 1.6793
Epoch 01340: val_loss improved from 1.68375 to 1.67933, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1341/2000
22/22 - 1s - loss: 1.2252 - val_loss: 1.6759
Epoch 1342/2000
22/22 - 1s - loss: 1.2244 - val_loss: 1.6765
Epoch 1343/2000
22/22 - 1s - loss: 1.2233 - val_loss: 1.6755
Epoch 1344/2000
22/22 - 1s - loss: 1.2212 - val_loss: 1.6763
Epoch 1345/2000
22/22 - 1s - loss: 1.2192 - val_loss: 1.6738
Epoch 1346/2000
22/22 - 1s - loss: 1.2202 - val_loss: 1.6756
Epoch 1347/2000
22/22 - 1s - loss: 1.2199 - val_loss: 1.6731
Epoch 1348/2000
22/22 - 1s - loss: 1.2202 - val_loss: 1.6716
Epoch 1349/2000
22/22 - 1s - loss: 1.2157 - val_loss: 1.6716
Epoch 1350/2000
22/22 - 1s - loss: 1.2163 - val_loss: 1.6706
Epoch 01350: val_loss improved from 1.67933 to 1.67059, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1351/2000
22/22 - 1s - loss: 1.2162 - val_loss: 1.6735
Epoch 1352/2000
22/22 - 1s - loss: 1.2152 - val_loss: 1.6687
Epoch 1353/2000
22/22 - 1s - loss: 1.2137 - val_loss: 1.6682
Epoch 1354/2000
22/22 - 1s - loss: 1.2172 - val_loss: 1.6670
Epoch 1355/2000
22/22 - 1s - loss: 1.2145 - val_loss: 1.6665
Epoch 1356/2000
22/22 - 1s - loss: 1.2142 - val_loss: 1.6659
Epoch 1357/2000
22/22 - 1s - loss: 1.2114 - val_loss: 1.6639
Epoch 1358/2000
22/22 - 1s - loss: 1.2104 - val_loss: 1.6628
Epoch 1359/2000
22/22 - 1s - loss: 1.2098 - val_loss: 1.6617
Epoch 1360/2000
22/22 - 1s - loss: 1.2102 - val_loss: 1.6627
Epoch 01360: val_loss improved from 1.67059 to 1.66274, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1361/2000
22/22 - 1s - loss: 1.2114 - val_loss: 1.6622
Epoch 1362/2000
22/22 - 1s - loss: 1.2072 - val_loss: 1.6606
Epoch 1363/2000
22/22 - 1s - loss: 1.2084 - val_loss: 1.6621
Epoch 1364/2000
22/22 - 1s - loss: 1.2105 - val_loss: 1.6592
Epoch 1365/2000
22/22 - 1s - loss: 1.2070 - val_loss: 1.6592
Epoch 1366/2000
22/22 - 1s - loss: 1.2068 - val_loss: 1.6592
Epoch 1367/2000
22/22 - 1s - loss: 1.2048 - val_loss: 1.6581
Epoch 1368/2000
22/22 - 1s - loss: 1.2029 - val_loss: 1.6566
Epoch 1369/2000
22/22 - 1s - loss: 1.2041 - val_loss: 1.6559
Epoch 1370/2000
22/22 - 1s - loss: 1.2042 - val_loss: 1.6538
Epoch 01370: val_loss improved from 1.66274 to 1.65383, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1371/2000
22/22 - 1s - loss: 1.2029 - val_loss: 1.6546
Epoch 1372/2000
22/22 - 1s - loss: 1.2021 - val_loss: 1.6529
Epoch 1373/2000
22/22 - 1s - loss: 1.2023 - val_loss: 1.6553
Epoch 1374/2000
22/22 - 1s - loss: 1.2009 - val_loss: 1.6533
Epoch 1375/2000
22/22 - 1s - loss: 1.2007 - val_loss: 1.6519
Epoch 1376/2000
22/22 - 1s - loss: 1.1992 - val_loss: 1.6521
Epoch 1377/2000
22/22 - 1s - loss: 1.1991 - val_loss: 1.6520
Epoch 1378/2000
22/22 - 1s - loss: 1.1977 - val_loss: 1.6514
Epoch 1379/2000
22/22 - 1s - loss: 1.1985 - val_loss: 1.6516
Epoch 1380/2000
22/22 - 1s - loss: 1.1956 - val_loss: 1.6492
Epoch 01380: val_loss improved from 1.65383 to 1.64918, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1381/2000
22/22 - 1s - loss: 1.1948 - val_loss: 1.6488
Epoch 1382/2000
22/22 - 1s - loss: 1.1951 - val_loss: 1.6472
Epoch 1383/2000
22/22 - 1s - loss: 1.1928 - val_loss: 1.6462
Epoch 1384/2000
22/22 - 1s - loss: 1.1924 - val_loss: 1.6463
Epoch 1385/2000
22/22 - 1s - loss: 1.1936 - val_loss: 1.6454
Epoch 1386/2000
22/22 - 1s - loss: 1.1933 - val_loss: 1.6435
Epoch 1387/2000
22/22 - 1s - loss: 1.1914 - val_loss: 1.6442
Epoch 1388/2000
22/22 - 1s - loss: 1.1907 - val_loss: 1.6440
Epoch 1389/2000
22/22 - 1s - loss: 1.1882 - val_loss: 1.6433
Epoch 1390/2000
22/22 - 1s - loss: 1.1893 - val_loss: 1.6433
Epoch 01390: val_loss improved from 1.64918 to 1.64330, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1391/2000
22/22 - 1s - loss: 1.1870 - val_loss: 1.6427
Epoch 1392/2000
22/22 - 1s - loss: 1.1896 - val_loss: 1.6422
Epoch 1393/2000
22/22 - 1s - loss: 1.1874 - val_loss: 1.6420
Epoch 1394/2000
22/22 - 1s - loss: 1.1871 - val_loss: 1.6397
Epoch 1395/2000
22/22 - 1s - loss: 1.1865 - val_loss: 1.6392
Epoch 1396/2000
22/22 - 1s - loss: 1.1861 - val_loss: 1.6388
Epoch 1397/2000
22/22 - 1s - loss: 1.1835 - val_loss: 1.6380
Epoch 1398/2000
22/22 - 1s - loss: 1.1838 - val_loss: 1.6378
Epoch 1399/2000
22/22 - 1s - loss: 1.1833 - val_loss: 1.6370
Epoch 1400/2000
22/22 - 1s - loss: 1.1813 - val_loss: 1.6364
Epoch 01400: val_loss improved from 1.64330 to 1.63645, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1401/2000
22/22 - 1s - loss: 1.1819 - val_loss: 1.6339
Epoch 1402/2000
22/22 - 1s - loss: 1.1813 - val_loss: 1.6341
Epoch 1403/2000
22/22 - 1s - loss: 1.1817 - val_loss: 1.6343
Epoch 1404/2000
22/22 - 1s - loss: 1.1785 - val_loss: 1.6312
Epoch 1405/2000
22/22 - 1s - loss: 1.1768 - val_loss: 1.6319
Epoch 1406/2000
22/22 - 1s - loss: 1.1811 - val_loss: 1.6289
Epoch 1407/2000
22/22 - 1s - loss: 1.1788 - val_loss: 1.6312
Epoch 1408/2000
22/22 - 1s - loss: 1.1766 - val_loss: 1.6303
Epoch 1409/2000
22/22 - 1s - loss: 1.1770 - val_loss: 1.6299
Epoch 1410/2000
22/22 - 1s - loss: 1.1754 - val_loss: 1.6296
Epoch 01410: val_loss improved from 1.63645 to 1.62959, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1411/2000
22/22 - 1s - loss: 1.1728 - val_loss: 1.6280
Epoch 1412/2000
22/22 - 1s - loss: 1.1752 - val_loss: 1.6285
Epoch 1413/2000
22/22 - 1s - loss: 1.1740 - val_loss: 1.6259
Epoch 1414/2000
22/22 - 1s - loss: 1.1731 - val_loss: 1.6284
Epoch 1415/2000
22/22 - 1s - loss: 1.1714 - val_loss: 1.6265
Epoch 1416/2000
22/22 - 1s - loss: 1.1713 - val_loss: 1.6238
Epoch 1417/2000
22/22 - 1s - loss: 1.1711 - val_loss: 1.6255
Epoch 1418/2000
22/22 - 1s - loss: 1.1702 - val_loss: 1.6236
Epoch 1419/2000
22/22 - 1s - loss: 1.1667 - val_loss: 1.6232
Epoch 1420/2000
22/22 - 1s - loss: 1.1680 - val_loss: 1.6219
Epoch 01420: val_loss improved from 1.62959 to 1.62193, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1421/2000
22/22 - 1s - loss: 1.1681 - val_loss: 1.6209
Epoch 1422/2000
22/22 - 1s - loss: 1.1680 - val_loss: 1.6191
Epoch 1423/2000
22/22 - 1s - loss: 1.1676 - val_loss: 1.6227
Epoch 1424/2000
22/22 - 1s - loss: 1.1651 - val_loss: 1.6191
Epoch 1425/2000
22/22 - 1s - loss: 1.1642 - val_loss: 1.6219
Epoch 1426/2000
22/22 - 1s - loss: 1.1632 - val_loss: 1.6180
Epoch 1427/2000
22/22 - 1s - loss: 1.1645 - val_loss: 1.6191
Epoch 1428/2000
22/22 - 1s - loss: 1.1645 - val_loss: 1.6164
Epoch 1429/2000
22/22 - 1s - loss: 1.1619 - val_loss: 1.6176
Epoch 1430/2000
22/22 - 1s - loss: 1.1620 - val_loss: 1.6170
Epoch 01430: val_loss improved from 1.62193 to 1.61703, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1431/2000
22/22 - 1s - loss: 1.1618 - val_loss: 1.6152
Epoch 1432/2000
22/22 - 1s - loss: 1.1603 - val_loss: 1.6145
Epoch 1433/2000
22/22 - 1s - loss: 1.1585 - val_loss: 1.6156
Epoch 1434/2000
22/22 - 1s - loss: 1.1596 - val_loss: 1.6114
Epoch 1435/2000
22/22 - 1s - loss: 1.1571 - val_loss: 1.6131
Epoch 1436/2000
22/22 - 1s - loss: 1.1579 - val_loss: 1.6111
Epoch 1437/2000
22/22 - 1s - loss: 1.1574 - val_loss: 1.6132
Epoch 1438/2000
22/22 - 1s - loss: 1.1586 - val_loss: 1.6097
Epoch 1439/2000
22/22 - 1s - loss: 1.1555 - val_loss: 1.6091
Epoch 1440/2000
22/22 - 1s - loss: 1.1548 - val_loss: 1.6098
Epoch 01440: val_loss improved from 1.61703 to 1.60980, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1441/2000
22/22 - 1s - loss: 1.1555 - val_loss: 1.6091
Epoch 1442/2000
22/22 - 1s - loss: 1.1546 - val_loss: 1.6093
Epoch 1443/2000
22/22 - 1s - loss: 1.1516 - val_loss: 1.6077
Epoch 1444/2000
22/22 - 1s - loss: 1.1521 - val_loss: 1.6075
Epoch 1445/2000
22/22 - 1s - loss: 1.1538 - val_loss: 1.6054
Epoch 1446/2000
22/22 - 1s - loss: 1.1532 - val_loss: 1.6045
Epoch 1447/2000
22/22 - 1s - loss: 1.1510 - val_loss: 1.6055
Epoch 1448/2000
22/22 - 1s - loss: 1.1507 - val_loss: 1.6038
Epoch 1449/2000
22/22 - 1s - loss: 1.1503 - val_loss: 1.6029
Epoch 1450/2000
22/22 - 1s - loss: 1.1477 - val_loss: 1.6017
Epoch 01450: val_loss improved from 1.60980 to 1.60166, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1451/2000
22/22 - 1s - loss: 1.1486 - val_loss: 1.6007
Epoch 1452/2000
22/22 - 1s - loss: 1.1491 - val_loss: 1.6010
Epoch 1453/2000
22/22 - 1s - loss: 1.1469 - val_loss: 1.6001
Epoch 1454/2000
22/22 - 1s - loss: 1.1461 - val_loss: 1.5990
Epoch 1455/2000
22/22 - 1s - loss: 1.1462 - val_loss: 1.5969
Epoch 1456/2000
22/22 - 1s - loss: 1.1446 - val_loss: 1.5963
Epoch 1457/2000
22/22 - 1s - loss: 1.1446 - val_loss: 1.5988
Epoch 1458/2000
22/22 - 1s - loss: 1.1431 - val_loss: 1.5975
Epoch 1459/2000
22/22 - 1s - loss: 1.1432 - val_loss: 1.5948
Epoch 1460/2000
22/22 - 1s - loss: 1.1430 - val_loss: 1.5984
Epoch 01460: val_loss improved from 1.60166 to 1.59840, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1461/2000
22/22 - 1s - loss: 1.1423 - val_loss: 1.5956
Epoch 1462/2000
22/22 - 1s - loss: 1.1407 - val_loss: 1.5947
Epoch 1463/2000
22/22 - 1s - loss: 1.1407 - val_loss: 1.5931
Epoch 1464/2000
22/22 - 1s - loss: 1.1399 - val_loss: 1.5918
Epoch 1465/2000
22/22 - 1s - loss: 1.1391 - val_loss: 1.5929
Epoch 1466/2000
22/22 - 1s - loss: 1.1373 - val_loss: 1.5914
Epoch 1467/2000
22/22 - 1s - loss: 1.1369 - val_loss: 1.5910
Epoch 1468/2000
22/22 - 1s - loss: 1.1379 - val_loss: 1.5934
Epoch 1469/2000
22/22 - 1s - loss: 1.1358 - val_loss: 1.5906
Epoch 1470/2000
22/22 - 1s - loss: 1.1374 - val_loss: 1.5899
Epoch 01470: val_loss improved from 1.59840 to 1.58990, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1471/2000
22/22 - 1s - loss: 1.1356 - val_loss: 1.5877
Epoch 1472/2000
22/22 - 1s - loss: 1.1355 - val_loss: 1.5853
Epoch 1473/2000
22/22 - 1s - loss: 1.1364 - val_loss: 1.5883
Epoch 1474/2000
22/22 - 1s - loss: 1.1331 - val_loss: 1.5864
Epoch 1475/2000
22/22 - 1s - loss: 1.1327 - val_loss: 1.5856
Epoch 1476/2000
22/22 - 1s - loss: 1.1335 - val_loss: 1.5857
Epoch 1477/2000
22/22 - 1s - loss: 1.1306 - val_loss: 1.5870
Epoch 1478/2000
22/22 - 1s - loss: 1.1324 - val_loss: 1.5850
Epoch 1479/2000
22/22 - 1s - loss: 1.1319 - val_loss: 1.5830
Epoch 1480/2000
22/22 - 1s - loss: 1.1292 - val_loss: 1.5816
Epoch 01480: val_loss improved from 1.58990 to 1.58159, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1481/2000
22/22 - 1s - loss: 1.1290 - val_loss: 1.5838
Epoch 1482/2000
22/22 - 1s - loss: 1.1301 - val_loss: 1.5806
Epoch 1483/2000
22/22 - 1s - loss: 1.1278 - val_loss: 1.5818
Epoch 1484/2000
22/22 - 1s - loss: 1.1268 - val_loss: 1.5797
Epoch 1485/2000
22/22 - 1s - loss: 1.1266 - val_loss: 1.5788
Epoch 1486/2000
22/22 - 1s - loss: 1.1268 - val_loss: 1.5768
Epoch 1487/2000
22/22 - 1s - loss: 1.1238 - val_loss: 1.5797
Epoch 1488/2000
22/22 - 1s - loss: 1.1251 - val_loss: 1.5805
Epoch 1489/2000
22/22 - 1s - loss: 1.1241 - val_loss: 1.5781
Epoch 1490/2000
22/22 - 1s - loss: 1.1214 - val_loss: 1.5752
Epoch 01490: val_loss improved from 1.58159 to 1.57525, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1491/2000
22/22 - 1s - loss: 1.1231 - val_loss: 1.5746
Epoch 1492/2000
22/22 - 1s - loss: 1.1200 - val_loss: 1.5760
Epoch 1493/2000
22/22 - 1s - loss: 1.1213 - val_loss: 1.5757
Epoch 1494/2000
22/22 - 1s - loss: 1.1216 - val_loss: 1.5742
Epoch 1495/2000
22/22 - 1s - loss: 1.1205 - val_loss: 1.5751
Epoch 1496/2000
22/22 - 1s - loss: 1.1201 - val_loss: 1.5737
Epoch 1497/2000
22/22 - 1s - loss: 1.1190 - val_loss: 1.5733
Epoch 1498/2000
22/22 - 1s - loss: 1.1186 - val_loss: 1.5713
Epoch 1499/2000
22/22 - 1s - loss: 1.1180 - val_loss: 1.5725
Epoch 1500/2000
22/22 - 1s - loss: 1.1164 - val_loss: 1.5709
Epoch 01500: val_loss improved from 1.57525 to 1.57095, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1501/2000
22/22 - 1s - loss: 1.1181 - val_loss: 1.5716
Epoch 1502/2000
22/22 - 1s - loss: 1.1146 - val_loss: 1.5686
Epoch 1503/2000
22/22 - 1s - loss: 1.1165 - val_loss: 1.5672
Epoch 1504/2000
22/22 - 1s - loss: 1.1153 - val_loss: 1.5679
Epoch 1505/2000
22/22 - 1s - loss: 1.1132 - val_loss: 1.5675
Epoch 1506/2000
22/22 - 1s - loss: 1.1129 - val_loss: 1.5674
Epoch 1507/2000
22/22 - 1s - loss: 1.1131 - val_loss: 1.5661
Epoch 1508/2000
22/22 - 1s - loss: 1.1114 - val_loss: 1.5653
Epoch 1509/2000
22/22 - 1s - loss: 1.1096 - val_loss: 1.5669
Epoch 1510/2000
22/22 - 1s - loss: 1.1106 - val_loss: 1.5651
Epoch 01510: val_loss improved from 1.57095 to 1.56508, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1511/2000
22/22 - 1s - loss: 1.1109 - val_loss: 1.5634
Epoch 1512/2000
22/22 - 1s - loss: 1.1081 - val_loss: 1.5624
Epoch 1513/2000
22/22 - 1s - loss: 1.1086 - val_loss: 1.5627
Epoch 1514/2000
22/22 - 1s - loss: 1.1087 - val_loss: 1.5631
Epoch 1515/2000
22/22 - 1s - loss: 1.1093 - val_loss: 1.5590
Epoch 1516/2000
22/22 - 1s - loss: 1.1084 - val_loss: 1.5623
Epoch 1517/2000
22/22 - 1s - loss: 1.1069 - val_loss: 1.5605
Epoch 1518/2000
22/22 - 1s - loss: 1.1067 - val_loss: 1.5606
Epoch 1519/2000
22/22 - 1s - loss: 1.1052 - val_loss: 1.5576
Epoch 1520/2000
22/22 - 1s - loss: 1.1062 - val_loss: 1.5574
Epoch 01520: val_loss improved from 1.56508 to 1.55736, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1521/2000
22/22 - 1s - loss: 1.1027 - val_loss: 1.5576
Epoch 1522/2000
22/22 - 1s - loss: 1.1020 - val_loss: 1.5589
Epoch 1523/2000
22/22 - 1s - loss: 1.1043 - val_loss: 1.5568
Epoch 1524/2000
22/22 - 1s - loss: 1.1020 - val_loss: 1.5555
Epoch 1525/2000
22/22 - 1s - loss: 1.1035 - val_loss: 1.5546
Epoch 1526/2000
22/22 - 1s - loss: 1.1024 - val_loss: 1.5545
Epoch 1527/2000
22/22 - 1s - loss: 1.1018 - val_loss: 1.5567
Epoch 1528/2000
22/22 - 1s - loss: 1.1002 - val_loss: 1.5540
Epoch 1529/2000
22/22 - 1s - loss: 1.1012 - val_loss: 1.5524
Epoch 1530/2000
22/22 - 1s - loss: 1.0980 - val_loss: 1.5525
Epoch 01530: val_loss improved from 1.55736 to 1.55249, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1531/2000
22/22 - 1s - loss: 1.0983 - val_loss: 1.5511
Epoch 1532/2000
22/22 - 1s - loss: 1.0969 - val_loss: 1.5504
Epoch 1533/2000
22/22 - 1s - loss: 1.0968 - val_loss: 1.5521
Epoch 1534/2000
22/22 - 1s - loss: 1.0951 - val_loss: 1.5480
Epoch 1535/2000
22/22 - 1s - loss: 1.0966 - val_loss: 1.5495
Epoch 1536/2000
22/22 - 1s - loss: 1.0949 - val_loss: 1.5454
Epoch 1537/2000
22/22 - 1s - loss: 1.0937 - val_loss: 1.5487
Epoch 1538/2000
22/22 - 1s - loss: 1.0942 - val_loss: 1.5492
Epoch 1539/2000
22/22 - 1s - loss: 1.0914 - val_loss: 1.5465
Epoch 1540/2000
22/22 - 1s - loss: 1.0929 - val_loss: 1.5465
Epoch 01540: val_loss improved from 1.55249 to 1.54654, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1541/2000
22/22 - 1s - loss: 1.0922 - val_loss: 1.5496
Epoch 1542/2000
22/22 - 1s - loss: 1.0922 - val_loss: 1.5467
Epoch 1543/2000
22/22 - 1s - loss: 1.0906 - val_loss: 1.5445
Epoch 1544/2000
22/22 - 1s - loss: 1.0904 - val_loss: 1.5439
Epoch 1545/2000
22/22 - 1s - loss: 1.0881 - val_loss: 1.5422
Epoch 1546/2000
22/22 - 1s - loss: 1.0882 - val_loss: 1.5429
Epoch 1547/2000
22/22 - 1s - loss: 1.0888 - val_loss: 1.5425
Epoch 1548/2000
22/22 - 1s - loss: 1.0893 - val_loss: 1.5421
Epoch 1549/2000
22/22 - 1s - loss: 1.0878 - val_loss: 1.5402
Epoch 1550/2000
22/22 - 1s - loss: 1.0871 - val_loss: 1.5405
Epoch 01550: val_loss improved from 1.54654 to 1.54054, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1551/2000
22/22 - 1s - loss: 1.0868 - val_loss: 1.5388
Epoch 1552/2000
22/22 - 1s - loss: 1.0851 - val_loss: 1.5401
Epoch 1553/2000
22/22 - 1s - loss: 1.0846 - val_loss: 1.5390
Epoch 1554/2000
22/22 - 1s - loss: 1.0851 - val_loss: 1.5401
Epoch 1555/2000
22/22 - 1s - loss: 1.0840 - val_loss: 1.5379
Epoch 1556/2000
22/22 - 1s - loss: 1.0840 - val_loss: 1.5370
Epoch 1557/2000
22/22 - 1s - loss: 1.0815 - val_loss: 1.5359
Epoch 1558/2000
22/22 - 1s - loss: 1.0821 - val_loss: 1.5353
Epoch 1559/2000
22/22 - 1s - loss: 1.0817 - val_loss: 1.5355
Epoch 1560/2000
22/22 - 1s - loss: 1.0818 - val_loss: 1.5337
Epoch 01560: val_loss improved from 1.54054 to 1.53372, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1561/2000
22/22 - 1s - loss: 1.0791 - val_loss: 1.5349
Epoch 1562/2000
22/22 - 1s - loss: 1.0818 - val_loss: 1.5309
Epoch 1563/2000
22/22 - 1s - loss: 1.0788 - val_loss: 1.5334
Epoch 1564/2000
22/22 - 1s - loss: 1.0783 - val_loss: 1.5306
Epoch 1565/2000
22/22 - 1s - loss: 1.0768 - val_loss: 1.5311
Epoch 1566/2000
22/22 - 1s - loss: 1.0769 - val_loss: 1.5307
Epoch 1567/2000
22/22 - 1s - loss: 1.0771 - val_loss: 1.5302
Epoch 1568/2000
22/22 - 1s - loss: 1.0746 - val_loss: 1.5295
Epoch 1569/2000
22/22 - 1s - loss: 1.0754 - val_loss: 1.5284
Epoch 1570/2000
22/22 - 1s - loss: 1.0759 - val_loss: 1.5283
Epoch 01570: val_loss improved from 1.53372 to 1.52830, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1571/2000
22/22 - 1s - loss: 1.0746 - val_loss: 1.5289
Epoch 1572/2000
22/22 - 1s - loss: 1.0733 - val_loss: 1.5285
Epoch 1573/2000
22/22 - 1s - loss: 1.0723 - val_loss: 1.5281
Epoch 1574/2000
22/22 - 1s - loss: 1.0727 - val_loss: 1.5277
Epoch 1575/2000
22/22 - 1s - loss: 1.0708 - val_loss: 1.5261
Epoch 1576/2000
22/22 - 1s - loss: 1.0724 - val_loss: 1.5257
Epoch 1577/2000
22/22 - 1s - loss: 1.0720 - val_loss: 1.5260
Epoch 1578/2000
22/22 - 1s - loss: 1.0712 - val_loss: 1.5243
Epoch 1579/2000
22/22 - 1s - loss: 1.0686 - val_loss: 1.5245
Epoch 1580/2000
22/22 - 1s - loss: 1.0681 - val_loss: 1.5239
Epoch 01580: val_loss improved from 1.52830 to 1.52392, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1581/2000
22/22 - 1s - loss: 1.0688 - val_loss: 1.5239
Epoch 1582/2000
22/22 - 1s - loss: 1.0675 - val_loss: 1.5223
Epoch 1583/2000
22/22 - 1s - loss: 1.0679 - val_loss: 1.5222
Epoch 1584/2000
22/22 - 1s - loss: 1.0671 - val_loss: 1.5217
Epoch 1585/2000
22/22 - 1s - loss: 1.0657 - val_loss: 1.5209
Epoch 1586/2000
22/22 - 1s - loss: 1.0655 - val_loss: 1.5199
Epoch 1587/2000
22/22 - 1s - loss: 1.0653 - val_loss: 1.5179
Epoch 1588/2000
22/22 - 1s - loss: 1.0663 - val_loss: 1.5190
Epoch 1589/2000
22/22 - 1s - loss: 1.0634 - val_loss: 1.5177
Epoch 1590/2000
22/22 - 1s - loss: 1.0620 - val_loss: 1.5165
Epoch 01590: val_loss improved from 1.52392 to 1.51654, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1591/2000
22/22 - 1s - loss: 1.0613 - val_loss: 1.5166
Epoch 1592/2000
22/22 - 1s - loss: 1.0625 - val_loss: 1.5162
Epoch 1593/2000
22/22 - 1s - loss: 1.0627 - val_loss: 1.5128
Epoch 1594/2000
22/22 - 1s - loss: 1.0602 - val_loss: 1.5144
Epoch 1595/2000
22/22 - 1s - loss: 1.0609 - val_loss: 1.5128
Epoch 1596/2000
22/22 - 1s - loss: 1.0606 - val_loss: 1.5147
Epoch 1597/2000
22/22 - 1s - loss: 1.0599 - val_loss: 1.5123
Epoch 1598/2000
22/22 - 1s - loss: 1.0584 - val_loss: 1.5121
Epoch 1599/2000
22/22 - 1s - loss: 1.0577 - val_loss: 1.5118
Epoch 1600/2000
22/22 - 1s - loss: 1.0584 - val_loss: 1.5092
Epoch 01600: val_loss improved from 1.51654 to 1.50922, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1601/2000
22/22 - 1s - loss: 1.0600 - val_loss: 1.5111
Epoch 1602/2000
22/22 - 1s - loss: 1.0563 - val_loss: 1.5099
Epoch 1603/2000
22/22 - 1s - loss: 1.0578 - val_loss: 1.5107
Epoch 1604/2000
22/22 - 1s - loss: 1.0544 - val_loss: 1.5103
Epoch 1605/2000
22/22 - 1s - loss: 1.0554 - val_loss: 1.5092
Epoch 1606/2000
22/22 - 1s - loss: 1.0547 - val_loss: 1.5090
Epoch 1607/2000
22/22 - 1s - loss: 1.0536 - val_loss: 1.5082
Epoch 1608/2000
22/22 - 1s - loss: 1.0534 - val_loss: 1.5089
Epoch 1609/2000
22/22 - 1s - loss: 1.0541 - val_loss: 1.5053
Epoch 1610/2000
22/22 - 1s - loss: 1.0517 - val_loss: 1.5076
Epoch 01610: val_loss improved from 1.50922 to 1.50756, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1611/2000
22/22 - 1s - loss: 1.0524 - val_loss: 1.5074
Epoch 1612/2000
22/22 - 1s - loss: 1.0512 - val_loss: 1.5057
Epoch 1613/2000
22/22 - 1s - loss: 1.0499 - val_loss: 1.5060
Epoch 1614/2000
22/22 - 1s - loss: 1.0517 - val_loss: 1.5042
Epoch 1615/2000
22/22 - 1s - loss: 1.0508 - val_loss: 1.5042
Epoch 1616/2000
22/22 - 1s - loss: 1.0483 - val_loss: 1.5012
Epoch 1617/2000
22/22 - 1s - loss: 1.0482 - val_loss: 1.5002
Epoch 1618/2000
22/22 - 1s - loss: 1.0478 - val_loss: 1.5008
Epoch 1619/2000
22/22 - 1s - loss: 1.0468 - val_loss: 1.5010
Epoch 1620/2000
22/22 - 1s - loss: 1.0466 - val_loss: 1.5019
Epoch 01620: val_loss improved from 1.50756 to 1.50191, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1621/2000
22/22 - 1s - loss: 1.0458 - val_loss: 1.5002
Epoch 1622/2000
22/22 - 1s - loss: 1.0454 - val_loss: 1.4998
Epoch 1623/2000
22/22 - 1s - loss: 1.0434 - val_loss: 1.4990
Epoch 1624/2000
22/22 - 1s - loss: 1.0433 - val_loss: 1.4977
Epoch 1625/2000
22/22 - 1s - loss: 1.0440 - val_loss: 1.5004
Epoch 1626/2000
22/22 - 1s - loss: 1.0447 - val_loss: 1.4966
Epoch 1627/2000
22/22 - 1s - loss: 1.0427 - val_loss: 1.4966
Epoch 1628/2000
22/22 - 1s - loss: 1.0425 - val_loss: 1.4971
Epoch 1629/2000
22/22 - 1s - loss: 1.0436 - val_loss: 1.4974
Epoch 1630/2000
22/22 - 1s - loss: 1.0417 - val_loss: 1.4954
Epoch 01630: val_loss improved from 1.50191 to 1.49536, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1631/2000
22/22 - 1s - loss: 1.0384 - val_loss: 1.4956
Epoch 1632/2000
22/22 - 1s - loss: 1.0402 - val_loss: 1.4950
Epoch 1633/2000
22/22 - 1s - loss: 1.0394 - val_loss: 1.4943
Epoch 1634/2000
22/22 - 1s - loss: 1.0386 - val_loss: 1.4955
Epoch 1635/2000
22/22 - 1s - loss: 1.0400 - val_loss: 1.4929
Epoch 1636/2000
22/22 - 1s - loss: 1.0370 - val_loss: 1.4907
Epoch 1637/2000
22/22 - 1s - loss: 1.0359 - val_loss: 1.4921
Epoch 1638/2000
22/22 - 1s - loss: 1.0361 - val_loss: 1.4904
Epoch 1639/2000
22/22 - 1s - loss: 1.0353 - val_loss: 1.4903
Epoch 1640/2000
22/22 - 1s - loss: 1.0375 - val_loss: 1.4893
Epoch 01640: val_loss improved from 1.49536 to 1.48933, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1641/2000
22/22 - 1s - loss: 1.0355 - val_loss: 1.4913
Epoch 1642/2000
22/22 - 1s - loss: 1.0356 - val_loss: 1.4886
Epoch 1643/2000
22/22 - 1s - loss: 1.0336 - val_loss: 1.4883
Epoch 1644/2000
22/22 - 1s - loss: 1.0331 - val_loss: 1.4883
Epoch 1645/2000
22/22 - 1s - loss: 1.0323 - val_loss: 1.4881
Epoch 1646/2000
22/22 - 1s - loss: 1.0331 - val_loss: 1.4868
Epoch 1647/2000
22/22 - 1s - loss: 1.0317 - val_loss: 1.4899
Epoch 1648/2000
22/22 - 1s - loss: 1.0316 - val_loss: 1.4851
Epoch 1649/2000
22/22 - 1s - loss: 1.0293 - val_loss: 1.4862
Epoch 1650/2000
22/22 - 1s - loss: 1.0305 - val_loss: 1.4853
Epoch 01650: val_loss improved from 1.48933 to 1.48527, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1651/2000
22/22 - 1s - loss: 1.0280 - val_loss: 1.4831
Epoch 1652/2000
22/22 - 1s - loss: 1.0316 - val_loss: 1.4852
Epoch 1653/2000
22/22 - 1s - loss: 1.0273 - val_loss: 1.4843
Epoch 1654/2000
22/22 - 1s - loss: 1.0288 - val_loss: 1.4830
Epoch 1655/2000
22/22 - 1s - loss: 1.0270 - val_loss: 1.4828
Epoch 1656/2000
22/22 - 1s - loss: 1.0271 - val_loss: 1.4823
Epoch 1657/2000
22/22 - 1s - loss: 1.0275 - val_loss: 1.4830
Epoch 1658/2000
22/22 - 1s - loss: 1.0255 - val_loss: 1.4804
Epoch 1659/2000
22/22 - 1s - loss: 1.0254 - val_loss: 1.4804
Epoch 1660/2000
22/22 - 1s - loss: 1.0235 - val_loss: 1.4777
Epoch 01660: val_loss improved from 1.48527 to 1.47769, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1661/2000
22/22 - 1s - loss: 1.0231 - val_loss: 1.4790
Epoch 1662/2000
22/22 - 1s - loss: 1.0245 - val_loss: 1.4756
Epoch 1663/2000
22/22 - 1s - loss: 1.0239 - val_loss: 1.4746
Epoch 1664/2000
22/22 - 1s - loss: 1.0228 - val_loss: 1.4767
Epoch 1665/2000
22/22 - 1s - loss: 1.0221 - val_loss: 1.4736
Epoch 1666/2000
22/22 - 1s - loss: 1.0224 - val_loss: 1.4764
Epoch 1667/2000
22/22 - 1s - loss: 1.0215 - val_loss: 1.4764
Epoch 1668/2000
22/22 - 1s - loss: 1.0199 - val_loss: 1.4741
Epoch 1669/2000
22/22 - 1s - loss: 1.0206 - val_loss: 1.4739
Epoch 1670/2000
22/22 - 1s - loss: 1.0198 - val_loss: 1.4726
Epoch 01670: val_loss improved from 1.47769 to 1.47260, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1671/2000
22/22 - 1s - loss: 1.0181 - val_loss: 1.4720
Epoch 1672/2000
22/22 - 1s - loss: 1.0191 - val_loss: 1.4712
Epoch 1673/2000
22/22 - 1s - loss: 1.0185 - val_loss: 1.4715
Epoch 1674/2000
22/22 - 1s - loss: 1.0175 - val_loss: 1.4711
Epoch 1675/2000
22/22 - 1s - loss: 1.0176 - val_loss: 1.4699
Epoch 1676/2000
22/22 - 1s - loss: 1.0154 - val_loss: 1.4719
Epoch 1677/2000
22/22 - 1s - loss: 1.0152 - val_loss: 1.4718
Epoch 1678/2000
22/22 - 1s - loss: 1.0164 - val_loss: 1.4699
Epoch 1679/2000
22/22 - 1s - loss: 1.0148 - val_loss: 1.4687
Epoch 1680/2000
22/22 - 1s - loss: 1.0157 - val_loss: 1.4698
Epoch 01680: val_loss improved from 1.47260 to 1.46984, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1681/2000
22/22 - 1s - loss: 1.0152 - val_loss: 1.4685
Epoch 1682/2000
22/22 - 1s - loss: 1.0124 - val_loss: 1.4666
Epoch 1683/2000
22/22 - 1s - loss: 1.0124 - val_loss: 1.4653
Epoch 1684/2000
22/22 - 1s - loss: 1.0122 - val_loss: 1.4657
Epoch 1685/2000
22/22 - 1s - loss: 1.0120 - val_loss: 1.4667
Epoch 1686/2000
22/22 - 1s - loss: 1.0110 - val_loss: 1.4656
Epoch 1687/2000
22/22 - 1s - loss: 1.0099 - val_loss: 1.4640
Epoch 1688/2000
22/22 - 1s - loss: 1.0095 - val_loss: 1.4637
Epoch 1689/2000
22/22 - 1s - loss: 1.0102 - val_loss: 1.4631
Epoch 1690/2000
22/22 - 1s - loss: 1.0091 - val_loss: 1.4633
Epoch 01690: val_loss improved from 1.46984 to 1.46332, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1691/2000
22/22 - 1s - loss: 1.0085 - val_loss: 1.4625
Epoch 1692/2000
22/22 - 1s - loss: 1.0093 - val_loss: 1.4615
Epoch 1693/2000
22/22 - 1s - loss: 1.0079 - val_loss: 1.4618
Epoch 1694/2000
22/22 - 1s - loss: 1.0073 - val_loss: 1.4630
Epoch 1695/2000
22/22 - 1s - loss: 1.0077 - val_loss: 1.4605
Epoch 1696/2000
22/22 - 1s - loss: 1.0065 - val_loss: 1.4596
Epoch 1697/2000
22/22 - 1s - loss: 1.0078 - val_loss: 1.4614
Epoch 1698/2000
22/22 - 1s - loss: 1.0058 - val_loss: 1.4600
Epoch 1699/2000
22/22 - 1s - loss: 1.0047 - val_loss: 1.4611
Epoch 1700/2000
22/22 - 1s - loss: 1.0047 - val_loss: 1.4586
Epoch 01700: val_loss improved from 1.46332 to 1.45858, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1701/2000
22/22 - 1s - loss: 1.0044 - val_loss: 1.4589
Epoch 1702/2000
22/22 - 1s - loss: 1.0011 - val_loss: 1.4582
Epoch 1703/2000
22/22 - 1s - loss: 1.0035 - val_loss: 1.4594
Epoch 1704/2000
22/22 - 1s - loss: 1.0024 - val_loss: 1.4568
Epoch 1705/2000
22/22 - 1s - loss: 1.0014 - val_loss: 1.4573
Epoch 1706/2000
22/22 - 1s - loss: 1.0016 - val_loss: 1.4561
Epoch 1707/2000
22/22 - 1s - loss: 0.9998 - val_loss: 1.4548
Epoch 1708/2000
22/22 - 1s - loss: 0.9997 - val_loss: 1.4540
Epoch 1709/2000
22/22 - 1s - loss: 0.9991 - val_loss: 1.4549
Epoch 1710/2000
22/22 - 1s - loss: 1.0004 - val_loss: 1.4544
Epoch 01710: val_loss improved from 1.45858 to 1.45444, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1711/2000
22/22 - 1s - loss: 0.9991 - val_loss: 1.4535
Epoch 1712/2000
22/22 - 1s - loss: 0.9981 - val_loss: 1.4545
Epoch 1713/2000
22/22 - 1s - loss: 0.9969 - val_loss: 1.4536
Epoch 1714/2000
22/22 - 1s - loss: 0.9975 - val_loss: 1.4545
Epoch 1715/2000
22/22 - 1s - loss: 0.9977 - val_loss: 1.4539
Epoch 1716/2000
22/22 - 1s - loss: 0.9953 - val_loss: 1.4519
Epoch 1717/2000
22/22 - 1s - loss: 0.9965 - val_loss: 1.4497
Epoch 1718/2000
22/22 - 1s - loss: 0.9945 - val_loss: 1.4501
Epoch 1719/2000
22/22 - 1s - loss: 0.9948 - val_loss: 1.4487
Epoch 1720/2000
22/22 - 1s - loss: 0.9944 - val_loss: 1.4486
Epoch 01720: val_loss improved from 1.45444 to 1.44864, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1721/2000
22/22 - 1s - loss: 0.9940 - val_loss: 1.4473
Epoch 1722/2000
22/22 - 1s - loss: 0.9944 - val_loss: 1.4476
Epoch 1723/2000
22/22 - 1s - loss: 0.9930 - val_loss: 1.4474
Epoch 1724/2000
22/22 - 1s - loss: 0.9917 - val_loss: 1.4464
Epoch 1725/2000
22/22 - 1s - loss: 0.9928 - val_loss: 1.4449
Epoch 1726/2000
22/22 - 1s - loss: 0.9903 - val_loss: 1.4441
Epoch 1727/2000
22/22 - 1s - loss: 0.9912 - val_loss: 1.4460
Epoch 1728/2000
22/22 - 1s - loss: 0.9890 - val_loss: 1.4451
Epoch 1729/2000
22/22 - 1s - loss: 0.9868 - val_loss: 1.4444
Epoch 1730/2000
22/22 - 1s - loss: 0.9888 - val_loss: 1.4458
Epoch 01730: val_loss improved from 1.44864 to 1.44579, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1731/2000
22/22 - 1s - loss: 0.9878 - val_loss: 1.4424
Epoch 1732/2000
22/22 - 1s - loss: 0.9901 - val_loss: 1.4450
Epoch 1733/2000
22/22 - 1s - loss: 0.9883 - val_loss: 1.4427
Epoch 1734/2000
22/22 - 1s - loss: 0.9864 - val_loss: 1.4427
Epoch 1735/2000
22/22 - 1s - loss: 0.9860 - val_loss: 1.4417
Epoch 1736/2000
22/22 - 1s - loss: 0.9868 - val_loss: 1.4406
Epoch 1737/2000
22/22 - 1s - loss: 0.9858 - val_loss: 1.4424
Epoch 1738/2000
22/22 - 1s - loss: 0.9833 - val_loss: 1.4405
Epoch 1739/2000
22/22 - 1s - loss: 0.9838 - val_loss: 1.4399
Epoch 1740/2000
22/22 - 1s - loss: 0.9823 - val_loss: 1.4417
Epoch 01740: val_loss improved from 1.44579 to 1.44171, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1741/2000
22/22 - 1s - loss: 0.9844 - val_loss: 1.4405
Epoch 1742/2000
22/22 - 1s - loss: 0.9829 - val_loss: 1.4375
Epoch 1743/2000
22/22 - 1s - loss: 0.9848 - val_loss: 1.4383
Epoch 1744/2000
22/22 - 1s - loss: 0.9826 - val_loss: 1.4386
Epoch 1745/2000
22/22 - 1s - loss: 0.9826 - val_loss: 1.4383
Epoch 1746/2000
22/22 - 1s - loss: 0.9819 - val_loss: 1.4365
Epoch 1747/2000
22/22 - 1s - loss: 0.9805 - val_loss: 1.4352
Epoch 1748/2000
22/22 - 1s - loss: 0.9798 - val_loss: 1.4361
Epoch 1749/2000
22/22 - 1s - loss: 0.9793 - val_loss: 1.4358
Epoch 1750/2000
22/22 - 1s - loss: 0.9789 - val_loss: 1.4350
Epoch 01750: val_loss improved from 1.44171 to 1.43503, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1751/2000
22/22 - 1s - loss: 0.9789 - val_loss: 1.4354
Epoch 1752/2000
22/22 - 1s - loss: 0.9773 - val_loss: 1.4336
Epoch 1753/2000
22/22 - 1s - loss: 0.9773 - val_loss: 1.4345
Epoch 1754/2000
22/22 - 1s - loss: 0.9776 - val_loss: 1.4319
Epoch 1755/2000
22/22 - 1s - loss: 0.9763 - val_loss: 1.4330
Epoch 1756/2000
22/22 - 1s - loss: 0.9756 - val_loss: 1.4316
Epoch 1757/2000
22/22 - 1s - loss: 0.9754 - val_loss: 1.4334
Epoch 1758/2000
22/22 - 1s - loss: 0.9750 - val_loss: 1.4322
Epoch 1759/2000
22/22 - 1s - loss: 0.9740 - val_loss: 1.4306
Epoch 1760/2000
22/22 - 1s - loss: 0.9735 - val_loss: 1.4306
Epoch 01760: val_loss improved from 1.43503 to 1.43063, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1761/2000
22/22 - 1s - loss: 0.9733 - val_loss: 1.4297
Epoch 1762/2000
22/22 - 1s - loss: 0.9727 - val_loss: 1.4294
Epoch 1763/2000
22/22 - 1s - loss: 0.9728 - val_loss: 1.4292
Epoch 1764/2000
22/22 - 1s - loss: 0.9719 - val_loss: 1.4274
Epoch 1765/2000
22/22 - 1s - loss: 0.9719 - val_loss: 1.4274
Epoch 1766/2000
22/22 - 1s - loss: 0.9725 - val_loss: 1.4286
Epoch 1767/2000
22/22 - 1s - loss: 0.9710 - val_loss: 1.4272
Epoch 1768/2000
22/22 - 1s - loss: 0.9717 - val_loss: 1.4250
Epoch 1769/2000
22/22 - 1s - loss: 0.9700 - val_loss: 1.4256
Epoch 1770/2000
22/22 - 1s - loss: 0.9692 - val_loss: 1.4250
Epoch 01770: val_loss improved from 1.43063 to 1.42497, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1771/2000
22/22 - 1s - loss: 0.9683 - val_loss: 1.4256
Epoch 1772/2000
22/22 - 1s - loss: 0.9679 - val_loss: 1.4246
Epoch 1773/2000
22/22 - 1s - loss: 0.9685 - val_loss: 1.4228
Epoch 1774/2000
22/22 - 1s - loss: 0.9680 - val_loss: 1.4225
Epoch 1775/2000
22/22 - 1s - loss: 0.9672 - val_loss: 1.4242
Epoch 1776/2000
22/22 - 1s - loss: 0.9664 - val_loss: 1.4222
Epoch 1777/2000
22/22 - 1s - loss: 0.9679 - val_loss: 1.4238
Epoch 1778/2000
22/22 - 1s - loss: 0.9666 - val_loss: 1.4197
Epoch 1779/2000
22/22 - 1s - loss: 0.9654 - val_loss: 1.4190
Epoch 1780/2000
22/22 - 1s - loss: 0.9649 - val_loss: 1.4209
Epoch 01780: val_loss improved from 1.42497 to 1.42094, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1781/2000
22/22 - 1s - loss: 0.9625 - val_loss: 1.4205
Epoch 1782/2000
22/22 - 1s - loss: 0.9635 - val_loss: 1.4198
Epoch 1783/2000
22/22 - 1s - loss: 0.9644 - val_loss: 1.4197
Epoch 1784/2000
22/22 - 1s - loss: 0.9629 - val_loss: 1.4193
Epoch 1785/2000
22/22 - 1s - loss: 0.9606 - val_loss: 1.4188
Epoch 1786/2000
22/22 - 1s - loss: 0.9617 - val_loss: 1.4178
Epoch 1787/2000
22/22 - 1s - loss: 0.9628 - val_loss: 1.4183
Epoch 1788/2000
22/22 - 1s - loss: 0.9614 - val_loss: 1.4178
Epoch 1789/2000
22/22 - 1s - loss: 0.9620 - val_loss: 1.4175
Epoch 1790/2000
22/22 - 1s - loss: 0.9615 - val_loss: 1.4184
Epoch 01790: val_loss improved from 1.42094 to 1.41839, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1791/2000
22/22 - 1s - loss: 0.9596 - val_loss: 1.4176
Epoch 1792/2000
22/22 - 1s - loss: 0.9608 - val_loss: 1.4156
Epoch 1793/2000
22/22 - 1s - loss: 0.9583 - val_loss: 1.4157
Epoch 1794/2000
22/22 - 1s - loss: 0.9576 - val_loss: 1.4127
Epoch 1795/2000
22/22 - 1s - loss: 0.9569 - val_loss: 1.4125
Epoch 1796/2000
22/22 - 1s - loss: 0.9560 - val_loss: 1.4127
Epoch 1797/2000
22/22 - 1s - loss: 0.9587 - val_loss: 1.4109
Epoch 1798/2000
22/22 - 1s - loss: 0.9563 - val_loss: 1.4112
Epoch 1799/2000
22/22 - 1s - loss: 0.9561 - val_loss: 1.4118
Epoch 1800/2000
22/22 - 1s - loss: 0.9556 - val_loss: 1.4101
Epoch 01800: val_loss improved from 1.41839 to 1.41014, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1801/2000
22/22 - 1s - loss: 0.9546 - val_loss: 1.4103
Epoch 1802/2000
22/22 - 1s - loss: 0.9553 - val_loss: 1.4095
Epoch 1803/2000
22/22 - 1s - loss: 0.9543 - val_loss: 1.4111
Epoch 1804/2000
22/22 - 1s - loss: 0.9575 - val_loss: 1.4099
Epoch 1805/2000
22/22 - 1s - loss: 0.9545 - val_loss: 1.4104
Epoch 1806/2000
22/22 - 1s - loss: 0.9537 - val_loss: 1.4089
Epoch 1807/2000
22/22 - 1s - loss: 0.9521 - val_loss: 1.4084
Epoch 1808/2000
22/22 - 1s - loss: 0.9511 - val_loss: 1.4075
Epoch 1809/2000
22/22 - 1s - loss: 0.9506 - val_loss: 1.4055
Epoch 1810/2000
22/22 - 1s - loss: 0.9498 - val_loss: 1.4077
Epoch 01810: val_loss improved from 1.41014 to 1.40773, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1811/2000
22/22 - 1s - loss: 0.9499 - val_loss: 1.4060
Epoch 1812/2000
22/22 - 1s - loss: 0.9503 - val_loss: 1.4061
Epoch 1813/2000
22/22 - 1s - loss: 0.9503 - val_loss: 1.4040
Epoch 1814/2000
22/22 - 1s - loss: 0.9471 - val_loss: 1.4065
Epoch 1815/2000
22/22 - 1s - loss: 0.9495 - val_loss: 1.4038
Epoch 1816/2000
22/22 - 1s - loss: 0.9471 - val_loss: 1.4033
Epoch 1817/2000
22/22 - 1s - loss: 0.9482 - val_loss: 1.4038
Epoch 1818/2000
22/22 - 1s - loss: 0.9479 - val_loss: 1.4030
Epoch 1819/2000
22/22 - 1s - loss: 0.9456 - val_loss: 1.4031
Epoch 1820/2000
22/22 - 1s - loss: 0.9461 - val_loss: 1.4009
Epoch 01820: val_loss improved from 1.40773 to 1.40093, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1821/2000
22/22 - 1s - loss: 0.9452 - val_loss: 1.4031
Epoch 1822/2000
22/22 - 1s - loss: 0.9450 - val_loss: 1.4008
Epoch 1823/2000
22/22 - 1s - loss: 0.9454 - val_loss: 1.4022
Epoch 1824/2000
22/22 - 1s - loss: 0.9455 - val_loss: 1.4008
Epoch 1825/2000
22/22 - 1s - loss: 0.9446 - val_loss: 1.3987
Epoch 1826/2000
22/22 - 1s - loss: 0.9444 - val_loss: 1.3989
Epoch 1827/2000
22/22 - 1s - loss: 0.9422 - val_loss: 1.4007
Epoch 1828/2000
22/22 - 1s - loss: 0.9430 - val_loss: 1.3981
Epoch 1829/2000
22/22 - 1s - loss: 0.9433 - val_loss: 1.3968
Epoch 1830/2000
22/22 - 1s - loss: 0.9413 - val_loss: 1.3955
Epoch 01830: val_loss improved from 1.40093 to 1.39552, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1831/2000
22/22 - 1s - loss: 0.9415 - val_loss: 1.3979
Epoch 1832/2000
22/22 - 1s - loss: 0.9409 - val_loss: 1.3970
Epoch 1833/2000
22/22 - 1s - loss: 0.9408 - val_loss: 1.3961
Epoch 1834/2000
22/22 - 1s - loss: 0.9389 - val_loss: 1.3966
Epoch 1835/2000
22/22 - 1s - loss: 0.9390 - val_loss: 1.3954
Epoch 1836/2000
22/22 - 1s - loss: 0.9396 - val_loss: 1.3953
Epoch 1837/2000
22/22 - 1s - loss: 0.9384 - val_loss: 1.3945
Epoch 1838/2000
22/22 - 1s - loss: 0.9393 - val_loss: 1.3959
Epoch 1839/2000
22/22 - 1s - loss: 0.9373 - val_loss: 1.3957
Epoch 1840/2000
22/22 - 1s - loss: 0.9358 - val_loss: 1.3932
Epoch 01840: val_loss improved from 1.39552 to 1.39324, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1841/2000
22/22 - 1s - loss: 0.9366 - val_loss: 1.3942
Epoch 1842/2000
22/22 - 1s - loss: 0.9356 - val_loss: 1.3922
Epoch 1843/2000
22/22 - 1s - loss: 0.9368 - val_loss: 1.3920
Epoch 1844/2000
22/22 - 1s - loss: 0.9349 - val_loss: 1.3914
Epoch 1845/2000
22/22 - 1s - loss: 0.9347 - val_loss: 1.3915
Epoch 1846/2000
22/22 - 1s - loss: 0.9338 - val_loss: 1.3898
Epoch 1847/2000
22/22 - 1s - loss: 0.9348 - val_loss: 1.3911
Epoch 1848/2000
22/22 - 1s - loss: 0.9341 - val_loss: 1.3898
Epoch 1849/2000
22/22 - 1s - loss: 0.9342 - val_loss: 1.3910
Epoch 1850/2000
22/22 - 1s - loss: 0.9334 - val_loss: 1.3902
Epoch 01850: val_loss improved from 1.39324 to 1.39016, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1851/2000
22/22 - 1s - loss: 0.9319 - val_loss: 1.3903
Epoch 1852/2000
22/22 - 1s - loss: 0.9326 - val_loss: 1.3892
Epoch 1853/2000
22/22 - 1s - loss: 0.9325 - val_loss: 1.3885
Epoch 1854/2000
22/22 - 1s - loss: 0.9302 - val_loss: 1.3901
Epoch 1855/2000
22/22 - 1s - loss: 0.9306 - val_loss: 1.3876
Epoch 1856/2000
22/22 - 1s - loss: 0.9289 - val_loss: 1.3891
Epoch 1857/2000
22/22 - 1s - loss: 0.9295 - val_loss: 1.3880
Epoch 1858/2000
22/22 - 1s - loss: 0.9299 - val_loss: 1.3861
Epoch 1859/2000
22/22 - 1s - loss: 0.9287 - val_loss: 1.3867
Epoch 1860/2000
22/22 - 1s - loss: 0.9288 - val_loss: 1.3860
Epoch 01860: val_loss improved from 1.39016 to 1.38602, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1861/2000
22/22 - 1s - loss: 0.9283 - val_loss: 1.3853
Epoch 1862/2000
22/22 - 1s - loss: 0.9286 - val_loss: 1.3874
Epoch 1863/2000
22/22 - 1s - loss: 0.9298 - val_loss: 1.3821
Epoch 1864/2000
22/22 - 1s - loss: 0.9256 - val_loss: 1.3850
Epoch 1865/2000
22/22 - 1s - loss: 0.9260 - val_loss: 1.3830
Epoch 1866/2000
22/22 - 1s - loss: 0.9260 - val_loss: 1.3834
Epoch 1867/2000
22/22 - 1s - loss: 0.9229 - val_loss: 1.3842
Epoch 1868/2000
22/22 - 1s - loss: 0.9257 - val_loss: 1.3823
Epoch 1869/2000
22/22 - 1s - loss: 0.9235 - val_loss: 1.3828
Epoch 1870/2000
22/22 - 1s - loss: 0.9228 - val_loss: 1.3819
Epoch 01870: val_loss improved from 1.38602 to 1.38193, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1871/2000
22/22 - 1s - loss: 0.9228 - val_loss: 1.3812
Epoch 1872/2000
22/22 - 1s - loss: 0.9229 - val_loss: 1.3803
Epoch 1873/2000
22/22 - 1s - loss: 0.9235 - val_loss: 1.3784
Epoch 1874/2000
22/22 - 1s - loss: 0.9218 - val_loss: 1.3789
Epoch 1875/2000
22/22 - 1s - loss: 0.9222 - val_loss: 1.3787
Epoch 1876/2000
22/22 - 1s - loss: 0.9221 - val_loss: 1.3805
Epoch 1877/2000
22/22 - 1s - loss: 0.9210 - val_loss: 1.3758
Epoch 1878/2000
22/22 - 1s - loss: 0.9213 - val_loss: 1.3778
Epoch 1879/2000
22/22 - 1s - loss: 0.9189 - val_loss: 1.3761
Epoch 1880/2000
22/22 - 1s - loss: 0.9186 - val_loss: 1.3778
Epoch 01880: val_loss improved from 1.38193 to 1.37781, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1881/2000
22/22 - 1s - loss: 0.9198 - val_loss: 1.3759
Epoch 1882/2000
22/22 - 1s - loss: 0.9208 - val_loss: 1.3751
Epoch 1883/2000
22/22 - 1s - loss: 0.9179 - val_loss: 1.3736
Epoch 1884/2000
22/22 - 1s - loss: 0.9180 - val_loss: 1.3752
Epoch 1885/2000
22/22 - 1s - loss: 0.9177 - val_loss: 1.3729
Epoch 1886/2000
22/22 - 1s - loss: 0.9172 - val_loss: 1.3742
Epoch 1887/2000
22/22 - 1s - loss: 0.9166 - val_loss: 1.3715
Epoch 1888/2000
22/22 - 1s - loss: 0.9163 - val_loss: 1.3734
Epoch 1889/2000
22/22 - 1s - loss: 0.9158 - val_loss: 1.3747
Epoch 1890/2000
22/22 - 1s - loss: 0.9165 - val_loss: 1.3730
Epoch 01890: val_loss improved from 1.37781 to 1.37300, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1891/2000
22/22 - 1s - loss: 0.9169 - val_loss: 1.3712
Epoch 1892/2000
22/22 - 1s - loss: 0.9158 - val_loss: 1.3726
Epoch 1893/2000
22/22 - 1s - loss: 0.9146 - val_loss: 1.3712
Epoch 1894/2000
22/22 - 1s - loss: 0.9132 - val_loss: 1.3700
Epoch 1895/2000
22/22 - 1s - loss: 0.9137 - val_loss: 1.3717
Epoch 1896/2000
22/22 - 1s - loss: 0.9130 - val_loss: 1.3694
Epoch 1897/2000
22/22 - 1s - loss: 0.9135 - val_loss: 1.3682
Epoch 1898/2000
22/22 - 1s - loss: 0.9111 - val_loss: 1.3679
Epoch 1899/2000
22/22 - 1s - loss: 0.9098 - val_loss: 1.3670
Epoch 1900/2000
22/22 - 1s - loss: 0.9105 - val_loss: 1.3675
Epoch 01900: val_loss improved from 1.37300 to 1.36754, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1901/2000
22/22 - 1s - loss: 0.9099 - val_loss: 1.3664
Epoch 1902/2000
22/22 - 1s - loss: 0.9084 - val_loss: 1.3676
Epoch 1903/2000
22/22 - 1s - loss: 0.9107 - val_loss: 1.3679
Epoch 1904/2000
22/22 - 1s - loss: 0.9097 - val_loss: 1.3654
Epoch 1905/2000
22/22 - 1s - loss: 0.9087 - val_loss: 1.3670
Epoch 1906/2000
22/22 - 1s - loss: 0.9081 - val_loss: 1.3660
Epoch 1907/2000
22/22 - 1s - loss: 0.9066 - val_loss: 1.3652
Epoch 1908/2000
22/22 - 1s - loss: 0.9066 - val_loss: 1.3659
Epoch 1909/2000
22/22 - 1s - loss: 0.9071 - val_loss: 1.3651
Epoch 1910/2000
22/22 - 1s - loss: 0.9064 - val_loss: 1.3650
Epoch 01910: val_loss improved from 1.36754 to 1.36503, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1911/2000
22/22 - 1s - loss: 0.9071 - val_loss: 1.3659
Epoch 1912/2000
22/22 - 1s - loss: 0.9054 - val_loss: 1.3613
Epoch 1913/2000
22/22 - 1s - loss: 0.9076 - val_loss: 1.3634
Epoch 1914/2000
22/22 - 1s - loss: 0.9047 - val_loss: 1.3611
Epoch 1915/2000
22/22 - 1s - loss: 0.9061 - val_loss: 1.3596
Epoch 1916/2000
22/22 - 1s - loss: 0.9041 - val_loss: 1.3616
Epoch 1917/2000
22/22 - 1s - loss: 0.9034 - val_loss: 1.3587
Epoch 1918/2000
22/22 - 1s - loss: 0.9023 - val_loss: 1.3593
Epoch 1919/2000
22/22 - 1s - loss: 0.9053 - val_loss: 1.3614
Epoch 1920/2000
22/22 - 1s - loss: 0.9018 - val_loss: 1.3596
Epoch 01920: val_loss improved from 1.36503 to 1.35957, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1921/2000
22/22 - 1s - loss: 0.9011 - val_loss: 1.3592
Epoch 1922/2000
22/22 - 1s - loss: 0.9023 - val_loss: 1.3582
Epoch 1923/2000
22/22 - 1s - loss: 0.9000 - val_loss: 1.3573
Epoch 1924/2000
22/22 - 1s - loss: 0.9030 - val_loss: 1.3569
Epoch 1925/2000
22/22 - 1s - loss: 0.9014 - val_loss: 1.3559
Epoch 1926/2000
22/22 - 1s - loss: 0.9005 - val_loss: 1.3578
Epoch 1927/2000
22/22 - 1s - loss: 0.9001 - val_loss: 1.3564
Epoch 1928/2000
22/22 - 1s - loss: 0.8993 - val_loss: 1.3556
Epoch 1929/2000
22/22 - 1s - loss: 0.9003 - val_loss: 1.3555
Epoch 1930/2000
22/22 - 1s - loss: 0.8974 - val_loss: 1.3544
Epoch 01930: val_loss improved from 1.35957 to 1.35442, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1931/2000
22/22 - 1s - loss: 0.8974 - val_loss: 1.3548
Epoch 1932/2000
22/22 - 1s - loss: 0.8980 - val_loss: 1.3530
Epoch 1933/2000
22/22 - 1s - loss: 0.8980 - val_loss: 1.3554
Epoch 1934/2000
22/22 - 1s - loss: 0.8966 - val_loss: 1.3549
Epoch 1935/2000
22/22 - 1s - loss: 0.8965 - val_loss: 1.3535
Epoch 1936/2000
22/22 - 1s - loss: 0.8947 - val_loss: 1.3526
Epoch 1937/2000
22/22 - 1s - loss: 0.8951 - val_loss: 1.3522
Epoch 1938/2000
22/22 - 1s - loss: 0.8953 - val_loss: 1.3534
Epoch 1939/2000
22/22 - 1s - loss: 0.8950 - val_loss: 1.3506
Epoch 1940/2000
22/22 - 1s - loss: 0.8946 - val_loss: 1.3518
Epoch 01940: val_loss improved from 1.35442 to 1.35177, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1941/2000
22/22 - 1s - loss: 0.8954 - val_loss: 1.3486
Epoch 1942/2000
22/22 - 1s - loss: 0.8931 - val_loss: 1.3498
Epoch 1943/2000
22/22 - 1s - loss: 0.8935 - val_loss: 1.3486
Epoch 1944/2000
22/22 - 1s - loss: 0.8913 - val_loss: 1.3505
Epoch 1945/2000
22/22 - 1s - loss: 0.8931 - val_loss: 1.3487
Epoch 1946/2000
22/22 - 1s - loss: 0.8925 - val_loss: 1.3498
Epoch 1947/2000
22/22 - 1s - loss: 0.8917 - val_loss: 1.3499
Epoch 1948/2000
22/22 - 1s - loss: 0.8902 - val_loss: 1.3467
Epoch 1949/2000
22/22 - 1s - loss: 0.8916 - val_loss: 1.3463
Epoch 1950/2000
22/22 - 1s - loss: 0.8904 - val_loss: 1.3480
Epoch 01950: val_loss improved from 1.35177 to 1.34798, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1951/2000
22/22 - 1s - loss: 0.8904 - val_loss: 1.3473
Epoch 1952/2000
22/22 - 1s - loss: 0.8888 - val_loss: 1.3496
Epoch 1953/2000
22/22 - 1s - loss: 0.8886 - val_loss: 1.3456
Epoch 1954/2000
22/22 - 1s - loss: 0.8883 - val_loss: 1.3464
Epoch 1955/2000
22/22 - 1s - loss: 0.8875 - val_loss: 1.3465
Epoch 1956/2000
22/22 - 1s - loss: 0.8879 - val_loss: 1.3436
Epoch 1957/2000
22/22 - 1s - loss: 0.8868 - val_loss: 1.3458
Epoch 1958/2000
22/22 - 1s - loss: 0.8867 - val_loss: 1.3452
Epoch 1959/2000
22/22 - 1s - loss: 0.8886 - val_loss: 1.3439
Epoch 1960/2000
22/22 - 1s - loss: 0.8851 - val_loss: 1.3438
Epoch 01960: val_loss improved from 1.34798 to 1.34379, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1961/2000
22/22 - 1s - loss: 0.8854 - val_loss: 1.3443
Epoch 1962/2000
22/22 - 1s - loss: 0.8853 - val_loss: 1.3417
Epoch 1963/2000
22/22 - 1s - loss: 0.8849 - val_loss: 1.3414
Epoch 1964/2000
22/22 - 1s - loss: 0.8847 - val_loss: 1.3440
Epoch 1965/2000
22/22 - 1s - loss: 0.8831 - val_loss: 1.3429
Epoch 1966/2000
22/22 - 1s - loss: 0.8843 - val_loss: 1.3421
Epoch 1967/2000
22/22 - 1s - loss: 0.8833 - val_loss: 1.3418
Epoch 1968/2000
22/22 - 1s - loss: 0.8837 - val_loss: 1.3408
Epoch 1969/2000
22/22 - 1s - loss: 0.8832 - val_loss: 1.3419
Epoch 1970/2000
22/22 - 1s - loss: 0.8822 - val_loss: 1.3413
Epoch 01970: val_loss improved from 1.34379 to 1.34125, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1971/2000
22/22 - 1s - loss: 0.8808 - val_loss: 1.3410
Epoch 1972/2000
22/22 - 1s - loss: 0.8821 - val_loss: 1.3391
Epoch 1973/2000
22/22 - 1s - loss: 0.8796 - val_loss: 1.3410
Epoch 1974/2000
22/22 - 1s - loss: 0.8813 - val_loss: 1.3385
Epoch 1975/2000
22/22 - 1s - loss: 0.8808 - val_loss: 1.3372
Epoch 1976/2000
22/22 - 1s - loss: 0.8804 - val_loss: 1.3364
Epoch 1977/2000
22/22 - 1s - loss: 0.8807 - val_loss: 1.3379
Epoch 1978/2000
22/22 - 1s - loss: 0.8801 - val_loss: 1.3363
Epoch 1979/2000
22/22 - 1s - loss: 0.8784 - val_loss: 1.3369
Epoch 1980/2000
22/22 - 1s - loss: 0.8789 - val_loss: 1.3369
Epoch 01980: val_loss improved from 1.34125 to 1.33692, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1981/2000
22/22 - 1s - loss: 0.8787 - val_loss: 1.3347
Epoch 1982/2000
22/22 - 1s - loss: 0.8783 - val_loss: 1.3353
Epoch 1983/2000
22/22 - 1s - loss: 0.8757 - val_loss: 1.3355
Epoch 1984/2000
22/22 - 1s - loss: 0.8773 - val_loss: 1.3348
Epoch 1985/2000
22/22 - 1s - loss: 0.8767 - val_loss: 1.3333
Epoch 1986/2000
22/22 - 1s - loss: 0.8753 - val_loss: 1.3325
Epoch 1987/2000
22/22 - 1s - loss: 0.8747 - val_loss: 1.3330
Epoch 1988/2000
22/22 - 1s - loss: 0.8771 - val_loss: 1.3329
Epoch 1989/2000
22/22 - 1s - loss: 0.8749 - val_loss: 1.3301
Epoch 1990/2000
22/22 - 1s - loss: 0.8735 - val_loss: 1.3334
Epoch 01990: val_loss improved from 1.33692 to 1.33343, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1991/2000
22/22 - 1s - loss: 0.8751 - val_loss: 1.3327
Epoch 1992/2000
22/22 - 1s - loss: 0.8744 - val_loss: 1.3309
Epoch 1993/2000
22/22 - 1s - loss: 0.8737 - val_loss: 1.3301
Epoch 1994/2000
22/22 - 1s - loss: 0.8736 - val_loss: 1.3301
Epoch 1995/2000
22/22 - 1s - loss: 0.8725 - val_loss: 1.3302
Epoch 1996/2000
22/22 - 1s - loss: 0.8723 - val_loss: 1.3319
Epoch 1997/2000
22/22 - 1s - loss: 0.8691 - val_loss: 1.3279
Epoch 1998/2000
22/22 - 1s - loss: 0.8723 - val_loss: 1.3288
Epoch 1999/2000
22/22 - 1s - loss: 0.8706 - val_loss: 1.3263
Epoch 2000/2000
22/22 - 1s - loss: 0.8712 - val_loss: 1.3274
Epoch 02000: val_loss improved from 1.33343 to 1.32738, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
INFO     Computation time for training the single-label model for AR: 38.42 min
INFO     Network type: REG
INFO     Model: "sequential"
INFO     _________________________________________________________________
INFO     Layer (type)                 Output Shape              Param #
INFO     =================================================================
INFO     dense (Dense)                (None, 1024)              2098176
INFO     _________________________________________________________________
INFO     dropout (Dropout)            (None, 1024)              0
INFO     _________________________________________________________________
INFO     dense_1 (Dense)              (None, 512)               524800
INFO     _________________________________________________________________
INFO     dropout_1 (Dropout)          (None, 512)               0
INFO     _________________________________________________________________
INFO     dense_2 (Dense)              (None, 256)               131328
INFO     _________________________________________________________________
INFO     dropout_2 (Dropout)          (None, 256)               0
INFO     _________________________________________________________________
INFO     dense_3 (Dense)              (None, 128)               32896
INFO     _________________________________________________________________
INFO     dropout_3 (Dropout)          (None, 128)               0
INFO     _________________________________________________________________
INFO     dense_4 (Dense)              (None, 1)                 129
INFO     =================================================================
INFO     Total params: 2,787,329
INFO     Trainable params: 2,787,329
INFO     Non-trainable params: 0
INFO     _________________________________________________________________
INFO     Evaluating trained model 'AR single-labeled Fold-2' on test data
Epoch 1/2000
INFO     Training of fold number: 3
INFO     Training sample distribution: train data: {-1.2016366720199585: 7, -1.20163094997406: 4, -1.2016353607177734: 4, -1.2016363143920898: 3, -1.201635479927063: 3, -1.2016377449035645: 3, -1.201637625694275: 3, -1.201636552810669: 3, -1.2016041278839111: 2, -1.2016191482543945: 2, -1.201636791229248: 2, -1.2016342878341675: 2, -1.2016339302062988: 2, -1.2016304731369019: 2, -1.2016302347183228: 2, -1.2016369104385376: 2, -1.2016288042068481: 2, -1.2016327381134033: 2, -1.2016384601593018: 2, -1.2016324996948242: 2, -1.2016347646713257: 2, -1.201635718345642: 2, 0.8440163731575012: 1, -0.4307803809642792: 1, 0.26908448338508606: 1, 0.06883639097213745: 1, 1.605971336364746: 1, 0.1385408341884613: 1, 0.36003923416137695: 1, 1.5088152885437012: 1, 0.2661615014076233: 1, 0.22961212694644928: 1, -0.5706648826599121: 1, 0.663663923740387: 1, -0.30124133825302124: 1, -0.3344474732875824: 1, -0.31671836972236633: 1, 0.29451489448547363: 1, 0.36155056953430176: 1, -0.026365874335169792: 1, -0.4993174076080322: 1, 0.7279888391494751: 1, 0.7412875294685364: 1, 1.5035943984985352: 1, 0.2609155476093292: 1, -0.10739652067422867: 1, 1.4158756732940674: 1, 1.571059226989746: 1, 1.2753889560699463: 1, 1.4251669645309448: 1, 0.32629087567329407: 1, -1.0201867818832397: 1, 0.8664619326591492: 1, 1.3083291053771973: 1, 1.415509581565857: 1, -0.425843745470047: 1, 0.6442342400550842: 1, 0.6829988956451416: 1, 1.5720155239105225: 1, 0.5732383131980896: 1, 0.31096193194389343: 1, -1.1963087320327759: 1, -1.2016195058822632: 1, -0.16027171909809113: 1, 0.2713952362537384: 1, 1.6089627742767334: 1, 1.4458893537521362: 1, 0.06667295098304749: 1, 0.3489625155925751: 1, -0.3774075210094452: 1, -0.31709083914756775: 1, 0.08119866997003555: 1, -0.35442060232162476: 1, 0.22109845280647278: 1, -0.6966411471366882: 1, 1.4237861633300781: 1, 0.831580638885498: 1, 1.3986883163452148: 1, 0.21748410165309906: 1, 0.8396581411361694: 1, -0.37317758798599243: 1, 0.2500914931297302: 1, -0.3040960133075714: 1, -0.5348793864250183: 1, 0.5601941347122192: 1, 1.6106586456298828: 1, 1.168498158454895: 1, -1.0873279571533203: 1, -0.2516374886035919: 1, 0.10525540262460709: 1, 0.3664189279079437: 1, -0.4459679424762726: 1, 0.5256680846214294: 1, 1.4836735725402832: 1, 0.6158161163330078: 1, 1.9054079055786133: 1, -0.6546655297279358: 1, 0.21625953912734985: 1, 0.13578376173973083: 1, -0.02133699133992195: 1, 1.5805106163024902: 1, 0.637361466884613: 1, 1.3996703624725342: 1, 1.0551327466964722: 1, -0.21738800406455994: 1, 0.20390741527080536: 1, 0.912930965423584: 1, 0.7471779584884644: 1, -1.115770697593689: 1, 1.090896725654602: 1, 1.1508636474609375: 1, 1.0995265245437622: 1, 0.34950539469718933: 1, 0.9149655103683472: 1, -0.040320102125406265: 1, -1.1746125221252441: 1, -0.4253336787223816: 1, -0.059458885341882706: 1, 1.5370275974273682: 1, 0.7950616478919983: 1, 0.7611046433448792: 1, -1.113705039024353: 1, 0.7825468182563782: 1, 1.4694101810455322: 1, 0.4633817672729492: 1, 1.427628755569458: 1, 0.35480692982673645: 1, 1.4536134004592896: 1, 0.699573814868927: 1, 0.3411409258842468: 1, 0.394859254360199: 1, -1.201627254486084: 1, 0.23328566551208496: 1, -0.030014334246516228: 1, -0.4297850430011749: 1, 1.6003168821334839: 1, -0.20539666712284088: 1, 0.3443826735019684: 1, 0.7171676754951477: 1, 0.695344090461731: 1, 0.8569538593292236: 1, 0.8028292059898376: 1, 0.6465133428573608: 1, 0.7402693033218384: 1, -1.0873202085494995: 1, 0.540539026260376: 1, 0.10738043487071991: 1, 1.4755654335021973: 1, 1.166581153869629: 1, 1.2997812032699585: 1, 1.2994223833084106: 1, 1.4923255443572998: 1, 0.5403873920440674: 1, -0.41311657428741455: 1, -0.17338967323303223: 1, 0.9650787115097046: 1, -0.9669303297996521: 1, 1.1683435440063477: 1, -0.2393776774406433: 1, -0.20045150816440582: 1, 0.7614589929580688: 1, 0.31334978342056274: 1, -0.24653010070323944: 1, 1.088638186454773: 1, 1.5124294757843018: 1, -0.22351212799549103: 1, 0.9285130500793457: 1, 0.5170465111732483: 1, -1.1701372861862183: 1, 0.8274267315864563: 1, 1.457643747329712: 1, 0.3245508372783661: 1, -0.48075738549232483: 1, -0.13643299043178558: 1, 0.49536043405532837: 1, 0.23716896772384644: 1, -0.10035426914691925: 1, -1.189130187034607: 1, 1.4424492120742798: 1, 0.10213274508714676: 1, 0.25783771276474: 1, -1.201627492904663: 1, -1.1476235389709473: 1, -1.2007057666778564: 1, -1.1639471054077148: 1, -1.2007761001586914: 1, -1.045175313949585: 1, -1.197718858718872: 1, -0.7745821475982666: 1, -0.9927355051040649: 1, -1.1964974403381348: 1, -1.1945018768310547: 1, -1.197901725769043: 1, -1.1866750717163086: 1, -1.1664562225341797: 1, -1.1972938776016235: 1, 0.005114047322422266: 1, -1.1962217092514038: 1, -1.1939657926559448: 1, -1.1927686929702759: 1, 0.5436715483665466: 1, -1.2000701427459717: 1, -1.1980621814727783: 1, -1.1323087215423584: 1, -0.2831217646598816: 1, -1.201596975326538: 1, -1.2016253471374512: 1, -1.201615810394287: 1, -1.186226725578308: 1, -0.27123570442199707: 1, -1.1996524333953857: 1, 0.39954620599746704: 1, -1.1674489974975586: 1, -1.201310396194458: 1, 1.14208984375: 1, -0.9657180905342102: 1, -1.1987498998641968: 1, 0.4001854956150055: 1, -0.512914776802063: 1, 0.034827083349227905: 1, 1.520749807357788: 1, -1.2016290426254272: 1, -1.198028802871704: 1, 0.28807583451271057: 1, -0.27864202857017517: 1, -0.32787415385246277: 1, 0.2115481197834015: 1, -1.177890419960022: 1, -0.8235211968421936: 1, 0.2787120044231415: 1, -0.683555543422699: 1, -0.30772721767425537: 1, 0.5139665007591248: 1, -0.3334684669971466: 1, 0.7675377726554871: 1, 1.6281145811080933: 1, -0.883184015750885: 1, 0.0290671493858099: 1, 1.0926192998886108: 1, 0.2407364845275879: 1, 0.21325090527534485: 1, 0.6560998558998108: 1, -0.5042855143547058: 1, -0.5222632884979248: 1, -1.1962871551513672: 1, 0.5479292273521423: 1, -1.201625108718872: 1, -0.7543148398399353: 1, -0.7196366786956787: 1, -0.2302703857421875: 1, -0.5718616843223572: 1, 1.7539664506912231: 1, -0.005905755329877138: 1, -1.1402051448822021: 1, -0.07565759867429733: 1, -0.26343750953674316: 1, 1.4295574426651: 1, -0.00951747503131628: 1, -0.09802207350730896: 1, 0.9686956405639648: 1, -0.5639210939407349: 1, -0.39423879981040955: 1, -0.29486238956451416: 1, -1.2015609741210938: 1, -0.06465810537338257: 1, -1.1205617189407349: 1, -0.5027830004692078: 1, 1.3625078201293945: 1, 0.12603989243507385: 1, 0.32263097167015076: 1, 1.1153340339660645: 1, 0.44327667355537415: 1, 1.4907145500183105: 1, 0.007752139586955309: 1, -1.2012096643447876: 1, -1.2015366554260254: 1, 0.37413451075553894: 1, 0.16982176899909973: 1, -0.8657602667808533: 1, -0.6204319000244141: 1, -1.201349139213562: 1, -0.46576231718063354: 1, 0.339458167552948: 1, -1.137963891029358: 1, -0.399366557598114: 1, 0.007953275926411152: 1, -0.4154701232910156: 1, -0.5620038509368896: 1, 0.4933412969112396: 1, 0.3488617241382599: 1, -0.6442912220954895: 1, 1.1998642683029175: 1, -0.9223102331161499: 1, 1.5978947877883911: 1, -0.3203604817390442: 1, 1.1970174312591553: 1, -0.09881063550710678: 1, -0.29657527804374695: 1, 1.5410983562469482: 1, 0.21768306195735931: 1, 1.3818247318267822: 1, 0.21878471970558167: 1, 1.3501269817352295: 1, 0.34516385197639465: 1, 0.11128426343202591: 1, -0.9888867139816284: 1, -1.2016162872314453: 1, -0.3419588804244995: 1, 0.6741006970405579: 1, -1.2016221284866333: 1, 0.812082827091217: 1, -1.201629400253296: 1, -0.21383443474769592: 1, -1.2006030082702637: 1, -1.201606273651123: 1, 0.6647039651870728: 1, 0.46835482120513916: 1, -1.201348900794983: 1, -1.1382324695587158: 1, -0.8027163147926331: 1, -1.201614260673523: 1, -1.2002341747283936: 1, -1.2014697790145874: 1, -1.2014739513397217: 1, -0.7784246206283569: 1, -1.0070439577102661: 1, -1.2015589475631714: 1, -1.2015974521636963: 1, -1.1959139108657837: 1, 0.603252649307251: 1, 0.9611315131187439: 1, -1.2016046047210693: 1, -0.506174623966217: 1, -1.180529236793518: 1, -1.2015936374664307: 1, 0.8243502378463745: 1, -1.2014310359954834: 1, -0.4781683385372162: 1, -1.201509714126587: 1, -1.2016127109527588: 1, -1.201597809791565: 1, 0.10008653253316879: 1, 0.1308048814535141: 1, -0.1883729249238968: 1, -1.055851936340332: 1, -1.2004907131195068: 1, 1.3784377574920654: 1, -0.8510425090789795: 1, -0.930620014667511: 1, 1.8447388410568237: 1, 0.10187211632728577: 1, -0.9047161340713501: 1, -1.0422825813293457: 1, -0.6316778063774109: 1, 1.4887964725494385: 1, -0.8920286297798157: 1, 1.788353681564331: 1, 0.43905025720596313: 1, -1.1907743215560913: 1, -1.1573199033737183: 1, -1.1941906213760376: 1, -0.26542410254478455: 1, -1.0395952463150024: 1, -0.3497014045715332: 1, -0.9432769417762756: 1, -1.0090559720993042: 1, 1.3791615962982178: 1, 1.577986240386963: 1, 0.7908697128295898: 1, -0.5884833335876465: 1, -0.18136551976203918: 1, -0.5754680633544922: 1, 1.3862724304199219: 1, 1.6304298639297485: 1, -1.0519613027572632: 1, -1.18907630443573: 1, -0.7096079587936401: 1, -1.197619080543518: 1, -0.2589719295501709: 1, -1.2003082036972046: 1, -0.4400210678577423: 1, -1.1370965242385864: 1, -0.8680421113967896: 1, -1.193503737449646: 1, -0.45158419013023376: 1, -1.1848548650741577: 1, -1.188598394393921: 1, 1.192413568496704: 1, -1.1560314893722534: 1, 1.3899286985397339: 1, -1.098894715309143: 1, -1.1041064262390137: 1, 0.9790754318237305: 1, -1.1999518871307373: 1, 0.06941241025924683: 1, 1.5657020807266235: 1, -1.1800106763839722: 1, -1.1358855962753296: 1, -1.18631112575531: 1, -0.3605387806892395: 1, 0.292474627494812: 1, 0.20924636721611023: 1, -0.24214474856853485: 1, -0.5120261311531067: 1, 0.6492028832435608: 1, 0.5685755014419556: 1, 0.46832725405693054: 1, 0.20812031626701355: 1, -1.0893759727478027: 1, 0.8691079020500183: 1, -1.1004241704940796: 1, -1.0407896041870117: 1, -0.12463472783565521: 1, 1.7476170063018799: 1, 0.15106460452079773: 1, -0.3594827950000763: 1, -1.1312958002090454: 1, -1.2013144493103027: 1, 1.5167564153671265: 1, -0.8993450403213501: 1, -1.1072322130203247: 1, -1.1329883337020874: 1, 1.8358889818191528: 1, -0.8241826295852661: 1, -1.0799649953842163: 1, -1.1919200420379639: 1, -0.07864277809858322: 1, -0.8634552955627441: 1, -0.9751180410385132: 1, -1.1442792415618896: 1, 0.2268732339143753: 1, -0.5990430116653442: 1, -1.0393953323364258: 1, -1.1646332740783691: 1, 1.7107861042022705: 1, 0.5575955510139465: 1, -0.67027348279953: 1, 1.8432141542434692: 1, -0.8671839237213135: 1, -0.8936908841133118: 1, 1.4807751178741455: 1, -1.0776159763336182: 1, -0.08619289845228195: 1, -0.9803085327148438: 1, 0.4876076281070709: 1, -1.024053692817688: 1, 1.0163378715515137: 1, -0.9919153451919556: 1, -0.233070969581604: 1, -1.1343045234680176: 1, -0.8108161091804504: 1, 0.5211433172225952: 1, -0.9951556921005249: 1, -1.0858170986175537: 1, -1.19014310836792: 1, -0.7166287899017334: 1, -0.974824070930481: 1, -0.04211708903312683: 1, -0.009973025880753994: 1, 0.3276282548904419: 1, -1.1851680278778076: 1, 1.2598285675048828: 1, -0.6757361888885498: 1, 1.4938876628875732: 1, -0.20395949482917786: 1, -0.04237562045454979: 1, 0.9760450720787048: 1, -0.13872799277305603: 1, -1.1716605424880981: 1, -1.1579349040985107: 1, 0.46070119738578796: 1, 1.4680920839309692: 1, -0.9134752750396729: 1, -0.49170398712158203: 1, -0.8270519375801086: 1, -0.90742427110672: 1, -0.7701697945594788: 1, -1.1712636947631836: 1, 1.0116826295852661: 1, -0.09378552436828613: 1, -0.8962797522544861: 1, 0.30057549476623535: 1, 0.9532740116119385: 1, 0.8790757060050964: 1, -0.9627416729927063: 1, -0.608140766620636: 1, -1.0076677799224854: 1, -1.2001420259475708: 1, -0.5982488989830017: 1, -1.1845873594284058: 1, -1.2009141445159912: 1, -1.0507465600967407: 1, -1.1325860023498535: 1, 1.8212419748306274: 1, -0.9299888610839844: 1, -1.0714704990386963: 1, -0.4354245066642761: 1, 0.025435535237193108: 1, -1.1946264505386353: 1, 0.3200169503688812: 1, -0.8923998475074768: 1, -0.3573164939880371: 1, 1.3890480995178223: 1, -1.1605626344680786: 1, -0.8580590486526489: 1, -0.7244925498962402: 1, -1.1898142099380493: 1, 0.6293842196464539: 1, -0.8917420506477356: 1, -1.1212905645370483: 1, 1.4690353870391846: 1, -1.1791499853134155: 1, 1.536348581314087: 1, 1.8480533361434937: 1, 1.0025136470794678: 1, -1.094030499458313: 1, 0.5474697947502136: 1, -0.7628646492958069: 1, 0.5786248445510864: 1, -0.26597675681114197: 1, -1.0213873386383057: 1, 0.8268551230430603: 1, -0.26631277799606323: 1, 1.4656307697296143: 1, 0.16298139095306396: 1, 0.02830575592815876: 1, 0.6718612909317017: 1, 1.3615977764129639: 1, -0.9881011247634888: 1, -1.2016340494155884: 1, -0.7180438041687012: 1, 1.0303751230239868: 1, 1.0402084589004517: 1, -0.7170498371124268: 1, 0.6606081128120422: 1, 0.8417104482650757: 1, 0.8741052150726318: 1, 0.8892757296562195: 1, 1.0541952848434448: 1, 1.2882025241851807: 1, 0.2640135586261749: 1, 0.2808535397052765: 1, 0.003300704760476947: 1, 0.16384904086589813: 1, 1.7614071369171143: 1, 0.3255096673965454: 1, 0.9005318284034729: 1, -0.18478137254714966: 1, 0.3698660731315613: 1, 0.2634084224700928: 1, -0.021469445899128914: 1, 1.053705096244812: 1, 1.001185417175293: 1, 0.6039040088653564: 1, 0.3286954462528229: 1, 0.46373531222343445: 1, -0.016411839053034782: 1, 0.10751932114362717: 1, -0.5022063255310059: 1, 0.31799715757369995: 1, 0.1881372481584549: 1, -0.01062939316034317: 1, 1.5870535373687744: 1, 1.281778335571289: 1, -0.08030800521373749: 1, -0.5288078188896179: 1, 1.224327802658081: 1, -0.9218448400497437: 1, 1.1008855104446411: 1, 1.4938230514526367: 1, 1.1383615732192993: 1, 0.8504006266593933: 1, 0.4723914861679077: 1, -0.06996402144432068: 1, 1.0355088710784912: 1, 0.36215391755104065: 1, -0.5825971961021423: 1, -0.16857680678367615: 1, 0.7533062696456909: 1, 1.5823100805282593: 1, 0.9470359086990356: 1, -0.5881722569465637: 1, 0.71575528383255: 1, 0.2403424084186554: 1, 0.465168297290802: 1, 1.9331234693527222: 1, 0.12876805663108826: 1, -1.107023000717163: 1, -0.16252407431602478: 1, 0.9657272696495056: 1, 1.2950940132141113: 1, 1.210314393043518: 1, 0.5456136465072632: 1, -0.16512484848499298: 1, 0.6780659556388855: 1, 1.495969295501709: 1, 1.3494455814361572: 1, 1.0170906782150269: 1, 0.6696186065673828: 1, -0.6162902116775513: 1, 1.3002121448516846: 1, 1.4811328649520874: 1, -0.15746326744556427: 1, -0.25767362117767334: 1, 0.12815426290035248: 1, -0.8486149311065674: 1, 0.32160520553588867: 1, -0.9050359129905701: 1, -1.199570655822754: 1, -0.5242934823036194: 1, -1.1873440742492676: 1, -0.8747767210006714: 1, -0.731924295425415: 1, -0.05734042823314667: 1, -0.45086827874183655: 1, -1.0084080696105957: 1, -0.5784834027290344: 1, 0.900846004486084: 1, -0.9131625890731812: 1, 1.7851945161819458: 1, 1.2659807205200195: 1, -0.8956496715545654: 1, -0.8754668831825256: 1, -0.2101057469844818: 1, -0.6876721978187561: 1, 1.6284458637237549: 1, 0.22341269254684448: 1, -0.748826265335083: 1, 0.784543514251709: 1, -0.7811231017112732: 1, -1.1798655986785889: 1, -0.4367877244949341: 1, -0.8407037854194641: 1, -1.1983946561813354: 1, -0.7376867532730103: 1, -1.1570571660995483: 1, -0.700495719909668: 1, -0.9755853414535522: 1, -0.5850008726119995: 1, -1.1196062564849854: 1, -1.17500901222229: 1, -1.1970343589782715: 1, -0.9793020486831665: 1, 1.324471354484558: 1, -1.2006902694702148: 1, -1.0523524284362793: 1, -0.5724524259567261: 1, -0.09434226900339127: 1, 1.1112544536590576: 1, -0.48030614852905273: 1, -0.4185396134853363: 1, 0.7527873516082764: 1, 0.03332117572426796: 1, 0.002877143444493413: 1, 1.4062000513076782: 1, -0.2782626748085022: 1, -0.047685928642749786: 1, -0.19720852375030518: 1, 0.3313300311565399: 1, 1.3262649774551392: 1, -0.11414719372987747: 1, 0.3607413172721863: 1, -0.18094922602176666: 1, -0.10225572437047958: 1, 1.366266131401062: 1, 0.9568199515342712: 1, -0.16261765360832214: 1, 0.1897212415933609: 1, 0.2964378595352173: 1, -0.14438967406749725: 1, 1.9067574739456177: 1, -0.23679472506046295: 1, 0.7951827049255371: 1, 0.07571198046207428: 1, -0.8877851366996765: 1, 1.6006011962890625: 1, -0.9158876538276672: 1, -1.1895103454589844: 1, -1.1913617849349976: 1, -1.0719594955444336: 1, 0.35307395458221436: 1, -1.1507015228271484: 1, -1.1607003211975098: 1, 1.9238320589065552: 1, 1.1217374801635742: 1, -1.1969212293624878: 1, -0.13197508454322815: 1, -1.07969331741333: 1, 0.1708119511604309: 1, 0.6321052312850952: 1, -0.47023993730545044: 1, 1.8953936100006104: 1, 0.11243647336959839: 1, 1.5100682973861694: 1, -1.2016189098358154: 1, -1.1264077425003052: 1, 1.0540943145751953: 1, -0.2686515748500824: 1, 1.8881990909576416: 1, -1.061113953590393: 1, 1.7208062410354614: 1, -1.2014974355697632: 1, 0.27857378125190735: 1, 1.2202951908111572: 1, -0.4105451703071594: 1, 1.280470848083496: 1, 0.28526046872138977: 1, -1.2013123035430908: 1, 1.6493014097213745: 1, -0.9635469913482666: 1, -0.7447215914726257: 1, -1.2006548643112183: 1, 0.9834553003311157: 1, -1.2016271352767944: 1, 1.76934015750885: 1, -0.025178229436278343: 1, 1.4182209968566895: 1, -0.2232077419757843: 1, 0.014584558084607124: 1, 1.891126036643982: 1, 0.8425688147544861: 1, -1.0590986013412476: 1, 1.9098440408706665: 1, 1.6939563751220703: 1, 0.2675780653953552: 1, 1.6441566944122314: 1, 1.4320223331451416: 1, -0.7435175180435181: 1, 1.7558945417404175: 1, -0.4567924439907074: 1, -0.26848453283309937: 1, 0.5131713151931763: 1, -0.35984915494918823: 1, 1.1998889446258545: 1, -1.102870225906372: 1, -0.5811882019042969: 1, 0.804813027381897: 1, 0.9758270978927612: 1, -0.253825306892395: 1, -1.2016383409500122: 1, -0.44384631514549255: 1, 1.7166484594345093: 1, 0.5419895052909851: 1, -1.0994056463241577: 1, 1.901644229888916: 1, 0.5713223814964294: 1, 1.3496158123016357: 1, 1.9020731449127197: 1, -1.2016111612319946: 1, -1.0791629552841187: 1, -0.5369133949279785: 1, -0.6532332301139832: 1, 0.9807740449905396: 1, 1.2709132432937622: 1, -0.2810556888580322: 1, -0.9806917309761047: 1, 1.4280599355697632: 1, -1.2016212940216064: 1, 0.454349547624588: 1, -0.5300003886222839: 1, 1.2889325618743896: 1, 0.35712626576423645: 1, -0.864342451095581: 1, 1.9024626016616821: 1, 1.6134487390518188: 1, 1.5396013259887695: 1, -1.1658029556274414: 1, -1.2016136646270752: 1, -1.0555497407913208: 1, -1.2002416849136353: 1, -0.8662946820259094: 1, -1.088794231414795: 1, -0.7490406036376953: 1, 0.10260368138551712: 1, 1.1086541414260864: 1, 0.7021965980529785: 1, 0.8739101886749268: 1, -0.4220041036605835: 1, 0.19334031641483307: 1, 1.8893579244613647: 1, -1.0097246170043945: 1, 0.39559683203697205: 1, -0.1754150390625: 1, -0.03317539766430855: 1, 1.7539278268814087: 1, 1.0263571739196777: 1, -0.4791189730167389: 1, 0.24442099034786224: 1, 0.9761800765991211: 1, 0.29674577713012695: 1, 0.7860816717147827: 1, 0.48093563318252563: 1, -0.2709258496761322: 1, -0.12864308059215546: 1, -0.180640310049057: 1, -0.6814844608306885: 1, 0.5755087733268738: 1, 1.3033519983291626: 1, 1.590468406677246: 1, 1.545008897781372: 1, -0.7589280009269714: 1, -0.03557446971535683: 1, 0.7833142876625061: 1, 0.81379234790802: 1, -0.7293344736099243: 1, 1.1236602067947388: 1, 0.26557838916778564: 1, 0.43645578622817993: 1, -0.2772659659385681: 1, 1.6143009662628174: 1, 0.2889866232872009: 1, 0.766596257686615: 1, -0.22210338711738586: 1, 0.9173278212547302: 1, 0.14515815675258636: 1, -0.03429622948169708: 1, 0.03225273638963699: 1, 0.4076603651046753: 1, 1.6906383037567139: 1, 1.5815212726593018: 1, -0.19816404581069946: 1, 0.1083393469452858: 1, -0.3399538993835449: 1, 0.723430871963501: 1, 0.9554869532585144: 1, 0.6886505484580994: 1, 0.5059344172477722: 1, 1.8263726234436035: 1, -0.04758370667695999: 1, 1.7850080728530884: 1, -0.2531147599220276: 1, 0.9327585697174072: 1, 0.8505056500434875: 1, 0.18436919152736664: 1, 0.9952530264854431: 1, 0.5438616275787354: 1, 0.591416597366333: 1, 0.27008119225502014: 1, 1.612230896949768: 1, 1.0885628461837769: 1, 1.7223035097122192: 1, 0.5330069065093994: 1, -0.22097758948802948: 1, 0.34220972657203674: 1, 0.6320856809616089: 1, 1.9327584505081177: 1, 0.6905925273895264: 1, 1.4221618175506592: 1, 1.4504951238632202: 1, -1.054260492324829: 1, -0.10395042598247528: 1, -0.0515512116253376: 1, 1.3716888427734375: 1, 0.949316143989563: 1, 1.865704894065857: 1, 1.7069745063781738: 1, 1.7395148277282715: 1, 1.0135881900787354: 1, 0.9889004230499268: 1, 1.1523829698562622: 1, 1.796111822128296: 1, 1.6013163328170776: 1, 1.7376213073730469: 1, -0.43114355206489563: 1, -0.7838892936706543: 1, -0.0861414223909378: 1, 1.6581584215164185: 1, 0.8071705102920532: 1, -1.2015763521194458: 1, 1.3225599527359009: 1, 0.38025134801864624: 1, 1.9389169216156006: 1, -0.94952791929245: 1, 1.2539737224578857: 1, -1.0571757555007935: 1, -0.9450681209564209: 1, 1.2595564126968384: 1, -0.13542917370796204: 1, 0.7080846428871155: 1, -1.2015223503112793: 1, 0.9156394600868225: 1, -1.201594352722168: 1, 1.2727433443069458: 1, 0.2591017186641693: 1, 1.0987391471862793: 1, 1.8749902248382568: 1, -1.201613187789917: 1, -1.002498984336853: 1, -0.20698606967926025: 1, 0.20394694805145264: 1, 0.780617356300354: 1, 1.8291547298431396: 1, 1.6759966611862183: 1, 1.0316096544265747: 1, 1.7389863729476929: 1, 1.896134853363037: 1, -1.040601134300232: 1, -0.8136187195777893: 1, 1.3174397945404053: 1, 1.16111421585083: 1, -1.085170865058899: 1, -0.7696746587753296: 1, -1.2016057968139648: 1, 0.4253986179828644: 1, 1.5920872688293457: 1, -0.34224218130111694: 1, 1.8261455297470093: 1, -0.3587249517440796: 1, -0.40272125601768494: 1, -1.130736231803894: 1, -0.24564200639724731: 1, -1.2007230520248413: 1, -0.20700129866600037: 1, -0.6896651983261108: 1, 1.3129916191101074: 1, 1.5496872663497925: 1, -1.2016278505325317: 1, -1.1936933994293213: 1, 0.4616207182407379: 1, -0.7448302507400513: 1, 1.8480027914047241: 1, -0.7542659640312195: 1, -1.1320221424102783: 1, -0.06900987029075623: 1, -1.0611162185668945: 1, 1.7425659894943237: 1, 0.4620521366596222: 1, 1.0934252738952637: 1, -1.14544677734375: 1, -0.6306192278862: 1, 0.9750880599021912: 1, 1.6034055948257446: 1, 0.1718989461660385: 1, -1.200726866722107: 1, -0.13886263966560364: 1, -0.24342182278633118: 1, -0.7379482984542847: 1, -0.5476839542388916: 1, -1.1302224397659302: 1, 1.3335411548614502: 1, 1.110692024230957: 1, -0.8793452382087708: 1, 1.7776927947998047: 1, 0.5805153250694275: 1, 1.8303353786468506: 1, 1.7461694478988647: 1, 1.5291143655776978: 1, 0.25442907214164734: 1, 1.6676998138427734: 1, -1.2014168500900269: 1, 0.9160022139549255: 1, 0.10331395268440247: 1, 0.9006003737449646: 1, 1.0343732833862305: 1, 1.4475048780441284: 1, 1.6221997737884521: 1, 1.9172451496124268: 1, 0.43482455611228943: 1, 1.8706549406051636: 1, 1.446770191192627: 1, 1.760259985923767: 1, -0.11584310233592987: 1, -0.9069592952728271: 1, 1.8416829109191895: 1, 1.7511584758758545: 1, 1.636014699935913: 1, 0.9216548800468445: 1, 0.08071509003639221: 1, 0.6508381366729736: 1, 0.3571586012840271: 1, 1.8800233602523804: 1, -0.7931265234947205: 1, -0.5870760679244995: 1, -0.4422439932823181: 1, 0.5422061085700989: 1, 1.5719680786132812: 1, -1.2016080617904663: 1, -0.7923315763473511: 1, 0.8521577715873718: 1, 1.114802360534668: 1, -0.7990305423736572: 1, -1.201361894607544: 1, -0.8557604551315308: 1, -0.08804576843976974: 1, 0.8335886001586914: 1, 0.04224063828587532: 1, 1.788615107536316: 1, 0.8788592219352722: 1, -0.655949056148529: 1, 1.0276689529418945: 1, 1.8225407600402832: 1, 1.7602899074554443: 1, 1.8266348838806152: 1, 1.058468222618103: 1, 1.7661612033843994: 1, -1.156775951385498: 1, -0.4194781482219696: 1, 0.1478143036365509: 1, 0.35647323727607727: 1, 1.5142070055007935: 1, -0.5466570258140564: 1, 1.0356202125549316: 1, 1.8354456424713135: 1, 1.8180814981460571: 1, 1.4667671918869019: 1, 1.8067305088043213: 1, -0.24759358167648315: 1, 0.019096076488494873: 1, -0.7692103385925293: 1, 1.5984208583831787: 1, -0.5458275079727173: 1, 0.63859623670578: 1, 1.7614209651947021: 1, -0.010684509761631489: 1, 1.4813575744628906: 1, -0.907404899597168: 1, 1.2843722105026245: 1, 1.7114263772964478: 1, 1.473099708557129: 1, 0.9352988600730896: 1, 1.597861409187317: 1, 0.9059203267097473: 1, 1.0176738500595093: 1, 0.9797016382217407: 1, 0.9968640804290771: 1, 1.5016316175460815: 1, 1.6772441864013672: 1, 1.7267848253250122: 1, 0.24924464523792267: 1, 0.0023630079813301563: 1, -0.30134135484695435: 1, 1.3759360313415527: 1, 1.2115764617919922: 1, 0.9276106357574463: 1, -0.47733497619628906: 1, -0.014680324122309685: 1, -0.24040797352790833: 1, -0.6007823944091797: 1, -0.7505127787590027: 1, 0.017721591517329216: 1, 0.3244344890117645: 1, 1.3913298845291138: 1, 1.1735796928405762: 1, -0.7031784653663635: 1, -0.37577909231185913: 1, -0.03591597080230713: 1, -0.14395083487033844: 1, 0.5354000329971313: 1, 0.171223446726799: 1, -0.34326422214508057: 1, -0.032225385308265686: 1, -0.8232591152191162: 1, -0.4729682505130768: 1, -0.9143604040145874: 1, 1.569981575012207: 1, 0.9121710062026978: 1, -0.34284013509750366: 1, -0.47152507305145264: 1, -1.093284010887146: 1, -0.16766004264354706: 1, -0.3081098794937134: 1, 0.006228437647223473: 1, -0.8751227855682373: 1, 1.542358636856079: 1, -0.46590861678123474: 1, -0.6440175771713257: 1, -0.7915438413619995: 1, -0.5345551371574402: 1, -0.4592617452144623: 1, -0.9877877235412598: 1, -0.3422335684299469: 1, 1.4956955909729004: 1, -0.33291712403297424: 1, -0.6786049008369446: 1, -0.8456059098243713: 1, -1.1333893537521362: 1, 0.22667939960956573: 1, 1.6555308103561401: 1, 0.6444940567016602: 1, -0.0690179392695427: 1, -0.06596078723669052: 1, 1.058951735496521: 1, 1.1691573858261108: 1, 1.4193427562713623: 1, 1.8792171478271484: 1, -0.42767956852912903: 1, -0.0695866122841835: 1, 1.93668532371521: 1, 0.3824501037597656: 1, -0.608808159828186: 1, -0.3237372636795044: 1, -0.8671026229858398: 1, -0.8443267941474915: 1, -0.9762966632843018: 1, -0.4369107186794281: 1, 1.3363116979599: 1, -0.48933231830596924: 1, 0.6785101294517517: 1, 0.29917436838150024: 1, -1.185569167137146: 1, -1.0817426443099976: 1, -0.31910526752471924: 1, -1.1891201734542847: 1, 1.3399251699447632: 1, -0.44544142484664917: 1, 0.15484686195850372: 1, -0.5857658982276917: 1, -1.1479105949401855: 1, -0.5629591941833496: 1, -0.24207068979740143: 1, 1.7606215476989746: 1, 0.6444322466850281: 1, -0.3169466555118561: 1, -0.9003047943115234: 1, 1.5233625173568726: 1, 0.16001862287521362: 1, -0.200442373752594: 1, 0.4303920269012451: 1, -0.13842415809631348: 1, -0.17455770075321198: 1, 1.4534815549850464: 1, -0.30685290694236755: 1, -0.1967451572418213: 1, 0.8612715601921082: 1, -0.5582360029220581: 1, 1.7616217136383057: 1, 1.483720302581787: 1, -0.4605187475681305: 1, 0.3248298168182373: 1, 1.4998488426208496: 1, -0.879592776298523: 1, -0.288830429315567: 1, -0.8833669424057007: 1, -0.007546336855739355: 1, 0.6150393486022949: 1, -0.7417653203010559: 1, 0.15043634176254272: 1, -0.2937408983707428: 1, -0.3424704372882843: 1, 1.3204209804534912: 1, -0.4007585644721985: 1, -0.116549052298069: 1, 0.48444247245788574: 1, 0.7365891337394714: 1, 1.477781891822815: 1, -0.03796708956360817: 1, -0.6538054943084717: 1, 1.5324621200561523: 1, -0.46823152899742126: 1, -0.07945768535137177: 1, 1.5701237916946411: 1, -1.1767117977142334: 1, 0.6789939403533936: 1, 1.0024393796920776: 1, -0.1308683305978775: 1, -0.5413272380828857: 1, -0.766767144203186: 1, -0.238590806722641: 1, -0.5338919758796692: 1, -0.49563685059547424: 1, -1.179208755493164: 1, 1.3037792444229126: 1, 0.17652635276317596: 1, -0.36514076590538025: 1, 0.31867286562919617: 1, 1.7650138139724731: 1, 0.025362450629472733: 1, -0.023513898253440857: 1, -0.6234576106071472: 1, 0.7159720659255981: 1, -0.0832226425409317: 1, -0.3753896951675415: 1, 1.5722923278808594: 1, -0.0211151335388422: 1, 0.19413244724273682: 1, -0.06991042196750641: 1, 0.2551988363265991: 1, -0.8521113991737366: 1, -0.5210259556770325: 1, -0.4835919141769409: 1, 1.2261123657226562: 1, -0.13618627190589905: 1, -0.5030357241630554: 1, 0.672914445400238: 1, 0.273947536945343: 1, -1.1766016483306885: 1, 1.376474142074585: 1, 1.5933094024658203: 1, 0.9130324721336365: 1, 0.7212937474250793: 1, 0.48826223611831665: 1, -0.5381679534912109: 1, 0.5300026535987854: 1, 0.5128249526023865: 1, 0.04851381108164787: 1, -0.5174428224563599: 1, -0.21868036687374115: 1, 0.6790488958358765: 1, 1.8596769571304321: 1, 1.2151012420654297: 1, 1.7544053792953491: 1, 1.0491001605987549: 1, -0.3135724663734436: 1, -1.1350090503692627: 1, -0.6055639982223511: 1, 0.12007596343755722: 1, -0.4845651388168335: 1, 0.5530001521110535: 1, 1.6552691459655762: 1, 1.0318180322647095: 1, -0.25207433104515076: 1, -0.5209143161773682: 1, 0.6704513430595398: 1, 0.2092854529619217: 1, 0.3744315505027771: 1, -0.24634599685668945: 1, 1.852098822593689: 1, 0.3915187418460846: 1, 0.49887171387672424: 1, 0.7721536755561829: 1, -0.1501469612121582: 1, -1.0406304597854614: 1, -0.5802597403526306: 1, 0.5874748826026917: 1, -1.1448076963424683: 1, -0.9971945881843567: 1, -0.04817575961351395: 1, 1.2531977891921997: 1, -0.46826034784317017: 1, 2.0270323753356934: 1, 0.04068145155906677: 1, 1.7796648740768433: 1, -0.5899453163146973: 1, 0.2126206010580063: 1, -1.1006834506988525: 1, 0.2422785758972168: 1, -0.21730461716651917: 1, 0.3640252351760864: 1, -0.9720814824104309: 1, -0.18857857584953308: 1, 1.5428107976913452: 1, 0.10666077584028244: 1, -1.2015986442565918: 1, -0.9726563692092896: 1, 1.2401331663131714: 1, 0.7334970831871033: 1, -0.2089931219816208: 1, -0.2558027505874634: 1, -0.4171464443206787: 1, -0.024080123752355576: 1, -0.6431723237037659: 1, -0.6276612877845764: 1, 0.7566526532173157: 1, -0.3100615441799164: 1, -0.22727444767951965: 1, 1.692647099494934: 1, 1.8799982070922852: 1, 1.2042120695114136: 1, -0.15924076735973358: 1, 1.7392624616622925: 1, 0.4917255938053131: 1, 0.11675674468278885: 1, -1.2016234397888184: 1, 0.1769435554742813: 1, 1.6697852611541748: 1, 0.46222802996635437: 1, 0.7304840087890625: 1, -0.3255411982536316: 1, -0.061833277344703674: 1, -0.26246213912963867: 1, 0.3141234815120697: 1, -0.22874142229557037: 1, -1.2016154527664185: 1, -1.2016232013702393: 1, 1.7097703218460083: 1, 0.6405824422836304: 1, 1.6683679819107056: 1, -0.8678878545761108: 1, 0.5500550270080566: 1, -0.5307965278625488: 1, -0.2874172031879425: 1, 1.4624748229980469: 1, 0.8171444535255432: 1, 1.552185297012329: 1, 0.3162490129470825: 1, 1.9273109436035156: 1, -0.7234905958175659: 1, -1.2009780406951904: 1, 1.5011951923370361: 1, 1.571593999862671: 1, -1.1689732074737549: 1, -0.44949105381965637: 1, -0.26545509696006775: 1, -0.9465891718864441: 1, -1.1831696033477783: 1, -0.6922621726989746: 1, -0.26783594489097595: 1, -0.8816695213317871: 1, -0.3383774757385254: 1, -0.29477736353874207: 1, -0.8523722290992737: 1, -0.7296833395957947: 1, 0.7230984568595886: 1, -1.0483030080795288: 1, -0.25122812390327454: 1, -0.7983001470565796: 1, -1.1498054265975952: 1, -0.3189155161380768: 1, -0.2183121144771576: 1, -0.906280517578125: 1, -0.22242674231529236: 1, 1.7963730096817017: 1, -0.9981749057769775: 1, -1.061142921447754: 1, 1.5116616487503052: 1, -0.24450848996639252: 1, -0.3431845009326935: 1, -0.563433825969696: 1, 1.896323323249817: 1, -0.3123464286327362: 1, -0.4653182625770569: 1, -1.1008306741714478: 1, -0.37186357378959656: 1, -0.27475300431251526: 1, -0.364163339138031: 1, 0.5143935680389404: 1, -1.110012173652649: 1, -1.1205850839614868: 1, -0.2223142683506012: 1, -0.5361114740371704: 1, -1.2013746500015259: 1, -1.1680830717086792: 1, -0.5809049010276794: 1, -1.1971925497055054: 1, -1.1336802244186401: 1, -1.155177116394043: 1, -1.1232612133026123: 1, -0.392406165599823: 1, -0.8350648880004883: 1, -1.103760838508606: 1, -1.188301682472229: 1, -0.8279891610145569: 1, -0.2970311641693115: 1, -0.7790790796279907: 1, -1.0625981092453003: 1, -0.6904935240745544: 1, 0.7078472375869751: 1, -1.1994960308074951: 1, -0.6403390765190125: 1, -1.1624175310134888: 1, -1.1406184434890747: 1, -0.9465845823287964: 1, -1.1945738792419434: 1, -1.1494790315628052: 1, -1.201560139656067: 1, -1.2015953063964844: 1, -1.1635701656341553: 1, -1.2015472650527954: 1, -1.1829675436019897: 1, 4.112330913543701: 1, -1.201536774635315: 1, -1.1448140144348145: 1, 4.5118513107299805: 1, 0.8056516647338867: 1, -1.201555848121643: 1, -1.1515345573425293: 1, -0.13113076984882355: 1, -0.30212122201919556: 1, 0.7332795858383179: 1, 0.008224710822105408: 1, -0.11124473065137863: 1, 0.1461368203163147: 1, 1.2778337001800537: 1, -1.1420694589614868: 1, -1.1992239952087402: 1, 0.02189079485833645: 1, -0.3552163541316986: 1, -0.9202075004577637: 1, -1.2008600234985352: 1, 0.7242860198020935: 1, 0.9517895579338074: 1, 1.2057219743728638: 1, -0.7557051777839661: 1, 0.8666924238204956: 1, -1.1981743574142456: 1, -0.16389766335487366: 1, 1.488932728767395: 1, 0.8550933599472046: 1, -0.05180063098669052: 1, -0.17077305912971497: 1, -1.1379677057266235: 1, -0.136034294962883: 1, 0.8696494698524475: 1, -1.0178923606872559: 1, -1.1375747919082642: 1, -0.19534148275852203: 1, -1.201613426208496: 1, -0.3075839579105377: 1, -0.32589226961135864: 1, 0.7587617635726929: 1, -0.16215069591999054: 1, -1.2009185552597046: 1, 0.043348729610443115: 1, -1.192414402961731: 1, -0.4258005619049072: 1, 0.7787987589836121: 1, 0.7848809361457825: 1, -0.6031686067581177: 1, 1.143233299255371: 1, -0.861803412437439: 1, -1.2001534700393677: 1, -0.6715388298034668: 1, -0.6006320714950562: 1, 0.3949566185474396: 1, 1.2888545989990234: 1, -0.012192374095320702: 1, 0.6273993253707886: 1, -0.04764068126678467: 1, -1.1882494688034058: 1, -0.6981508731842041: 1, -0.9249427914619446: 1, -0.17132940888404846: 1, -0.0996609777212143: 1, -1.20045006275177: 1, -0.14265145361423492: 1, -1.1139642000198364: 1, -0.03975825384259224: 1, -1.2010724544525146: 1, -1.1809542179107666: 1, 0.013616573065519333: 1, 0.00948107335716486: 1, -1.1074978113174438: 1, -1.2016228437423706: 1, -1.051085352897644: 1, -1.1774768829345703: 1, -0.5369265675544739: 1, -0.39080610871315: 1, -0.03033183142542839: 1, -0.10903797298669815: 1, -0.7482910752296448: 1, -0.07614605128765106: 1, -1.15325927734375: 1, 0.9220188856124878: 1, 1.3031054735183716: 1, -1.150406837463379: 1, -0.1598413586616516: 1, -1.1756606101989746: 1, 0.032617583870887756: 1, -1.1993433237075806: 1, 0.4001394212245941: 1, -1.0657732486724854: 1, -0.3311309218406677: 1, -1.201612949371338: 1, -0.06760650873184204: 1, -1.09720778465271: 1, -0.7073397040367126: 1, -0.4774172008037567: 1, 0.6389100551605225: 1, -1.0666824579238892: 1, -0.3503449857234955: 1, -1.2002339363098145: 1, -0.2299073189496994: 1, 0.9924389719963074: 1, -0.21156159043312073: 1, -0.39197561144828796: 1, -1.1276121139526367: 1, 0.004596139770001173: 1, -1.0416687726974487: 1, -0.6867349743843079: 1, -0.5342384576797485: 1, -1.2015101909637451: 1, -0.09844925999641418: 1, -0.5935716032981873: 1, 0.6907184720039368: 1, -1.1690752506256104: 1, -1.1992988586425781: 1, 0.821479320526123: 1, 0.5329916477203369: 1, 0.880368709564209: 1, -1.084214448928833: 1, -0.8594576716423035: 1, -0.35862404108047485: 1, 0.605282723903656: 1, -0.8211961388587952: 1, 0.369517058134079: 1, 1.2405850887298584: 1, 0.842113196849823: 1, -0.9499993324279785: 1, 0.48456287384033203: 1, -1.2016358375549316: 1, -0.5739816427230835: 1, -1.2016373872756958: 1, -0.576421856880188: 1, -1.195853590965271: 1, 1.0410280227661133: 1, -1.1941806077957153: 1, -0.7514510154724121: 1, 0.6366953253746033: 1, 1.1094199419021606: 1, 1.0952398777008057: 1, 1.258391261100769: 1, -0.07435453683137894: 1, 1.1807068586349487: 1, -0.15606260299682617: 1, -0.7601802349090576: 1, -0.5920382142066956: 1, 0.8677871227264404: 1, 0.669349730014801: 1, 0.5067287683486938: 1, 0.5890273451805115: 1, 0.8621184229850769: 1, -0.5897277593612671: 1, 0.050834186375141144: 1, -0.08644621819257736: 1, -0.7184330821037292: 1, -1.20154869556427: 1, -0.13237591087818146: 1, -0.3055903911590576: 1, 0.4334481656551361: 1, -1.1760457754135132: 1, -0.3122004270553589: 1, -0.8366453051567078: 1, -0.3642917275428772: 1, -0.5142630338668823: 1, 0.7907249331474304: 1, -0.13443662226200104: 1, -0.002722974168136716: 1, -1.1028809547424316: 1, -1.1580485105514526: 1, 0.8628989458084106: 1, -0.24963954091072083: 1, -0.5910298824310303: 1, 1.0215860605239868: 1, 0.9452794194221497: 1, 0.3778545558452606: 1, 1.1708914041519165: 1, -1.1908975839614868: 1, 0.4138732850551605: 1, -0.8764730095863342: 1, -0.6155209541320801: 1, -0.15946216881275177: 1, -0.42906203866004944: 1, -0.32544630765914917: 1, 1.30207359790802: 1, 0.6744810342788696: 1, 0.9348665475845337: 1, -1.0131398439407349: 1, 0.01674770377576351: 1, -0.003650385420769453: 1, -0.3348834216594696: 1, -0.06836508214473724: 1, -0.12709279358386993: 1, -0.7992537021636963: 1, -1.1966403722763062: 1, -0.13060888648033142: 1, -0.8245752453804016: 1, -0.5774872899055481: 1, -1.193916916847229: 1, 1.00320303440094: 1, 1.1278204917907715: 1, -0.6903147101402283: 1, 1.1092147827148438: 1, 0.6174435615539551: 1, -0.7010860443115234: 1, -0.01721060648560524: 1, 1.043671727180481: 1, -0.15634003281593323: 1, -0.34825557470321655: 1, -0.21185417473316193: 1, 0.7746310830116272: 1, -1.1586220264434814: 1, -1.2016310691833496: 1, -0.3917173445224762: 1, -0.02432066947221756: 1, 0.2524677515029907: 1, 0.2549906373023987: 1, 0.7206960916519165: 1, 0.6609118580818176: 1, -1.198868989944458: 1, 1.1562855243682861: 1, -0.016240552067756653: 1, 0.9105262160301208: 1, 0.15262554585933685: 1, 0.6296795010566711: 1, -1.1068792343139648: 1, -0.6869497895240784: 1, 0.07662157714366913: 1, -0.09314227104187012: 1, 1.196900725364685: 1, -0.09158548712730408: 1, -0.1383906453847885: 1, -0.17342792451381683: 1, 1.2882169485092163: 1, -1.1826090812683105: 1, 0.7786849141120911: 1, -0.19042982161045074: 1, -1.200330376625061: 1, 1.0692790746688843: 1, -0.257107138633728: 1, 0.9495259523391724: 1, -1.2016348838806152: 1, -0.21517714858055115: 1, 0.5923774242401123: 1, 0.9726781249046326: 1, -1.1697360277175903: 1, 0.008013189770281315: 1, -0.5649014711380005: 1, -0.21699510514736176: 1, -0.1227283924818039: 1, -0.7380070686340332: 1, 0.8076177835464478: 1, -0.0404001921415329: 1, 0.6649335622787476: 1, -0.028692159801721573: 1, -0.11562295258045197: 1, 1.1690500974655151: 1, -0.618209183216095: 1, 1.0350605249404907: 1, 1.1997345685958862: 1, -0.26567548513412476: 1, 0.7346874475479126: 1, -0.6019781231880188: 1, -0.24605818092823029: 1, -1.192280888557434: 1, 0.5382747054100037: 1, 1.0084635019302368: 1, 0.810942530632019: 1, -0.0674244612455368: 1, -1.2016351222991943: 1, 0.6161129474639893: 1, -1.006115436553955: 1, -0.3076569437980652: 1, -1.1914066076278687: 1, -1.0579359531402588: 1, -0.16474246978759766: 1, 1.1363762617111206: 1, -1.2016372680664062: 1, 0.8519235253334045: 1, 1.3037681579589844: 1, -0.03992554917931557: 1, 1.0595874786376953: 1, -1.2016328573226929: 1, -0.7893494963645935: 1, -1.0076838731765747: 1, 0.49005118012428284: 1, -0.171332448720932: 1, -0.8429122567176819: 1, 1.0700626373291016: 1, 1.257509708404541: 1, 0.9156388640403748: 1, -0.7673166990280151: 1, -0.14771254360675812: 1, -0.9743238091468811: 1, 0.68455970287323: 1, -0.05808640271425247: 1, -0.881252646446228: 1, -0.5050346255302429: 1, 0.17733901739120483: 1, -0.04230939969420433: 1, -0.03235474228858948: 1, -1.1322532892227173: 1, -0.8901136517524719: 1, -0.07969757169485092: 1, -0.2553582787513733: 1, -0.07029284536838531: 1, 0.762050449848175: 1, 0.5583640336990356: 1, 0.9881972074508667: 1, -1.109034538269043: 1, 0.5070648193359375: 1, -0.019765598699450493: 1, -0.29025569558143616: 1, -0.07907160371541977: 1, -0.0026253368705511093: 1, -1.0697368383407593: 1, -0.08915898948907852: 1, 0.004652172327041626: 1, -0.5897085666656494: 1, -0.23161835968494415: 1, 1.0157313346862793: 1, 1.1558300256729126: 1, 0.048983871936798096: 1, -1.0963338613510132: 1, 1.030333161354065: 1, 0.9011586904525757: 1, -0.2878003716468811: 1, 1.0766181945800781: 1, -1.155191421508789: 1, 1.023429274559021: 1, -0.7420557737350464: 1, -0.3634156584739685: 1, 1.1214849948883057: 1, -1.1934514045715332: 1, -0.002401667181402445: 1, 1.2993144989013672: 1, -1.1338447332382202: 1, -0.6266202926635742: 1, -0.3548189401626587: 1, 1.0808086395263672: 1, -0.455705851316452: 1, -0.9566242694854736: 1, 0.033837273716926575: 1, -0.999508798122406: 1, -0.05626079440116882: 1, 1.229008674621582: 1, -1.1707801818847656: 1, -1.182140827178955: 1, -1.1800310611724854: 1, -1.0668615102767944: 1, -0.63347989320755: 1, -1.0160771608352661: 1, -1.1520825624465942: 1, -1.167817234992981: 1, -0.9714952111244202: 1, -1.0810678005218506: 1, -0.6395750641822815: 1, -1.194927453994751: 1, -0.44190311431884766: 1, -1.1721937656402588: 1, -1.0433918237686157: 1, -1.162119746208191: 1, -1.2006478309631348: 1, -1.1339654922485352: 1, -0.8829452991485596: 1, -1.1954984664916992: 1, -1.1685314178466797: 1, -1.191677212715149: 1, -1.1582452058792114: 1, -1.0135008096694946: 1, -1.1722731590270996: 1, -1.193282127380371: 1, -1.157931923866272: 1, -0.6325321793556213: 1, -0.8867827653884888: 1, -1.1729844808578491: 1, -1.010536789894104: 1, -1.0467379093170166: 1, -1.0309724807739258: 1, -1.1989647150039673: 1, -0.8834558129310608: 1, -1.201562523841858: 1, -0.6635865569114685: 1, -0.5772318840026855: 1, -1.194190263748169: 1, -0.19914157688617706: 1, -1.17979097366333: 1, -1.1744723320007324: 1, -1.0831377506256104: 1, -1.161582589149475: 1, -1.0068711042404175: 1, -1.2014809846878052: 1, -1.2012261152267456: 1, -0.6185922622680664: 1, -0.8899372816085815: 1, -0.7500604391098022: 1, -1.1552096605300903: 1, -1.1936330795288086: 1, -1.170555830001831: 1, -0.957834780216217: 1, -1.0740134716033936: 1, -1.2015275955200195: 1, -1.1300313472747803: 1, -1.1405699253082275: 1, -1.0521938800811768: 1, -0.6205415725708008: 1, -1.0014375448226929: 1, -1.2015451192855835: 1, -1.1066040992736816: 1, -1.1855055093765259: 1, -1.091170310974121: 1, -0.7897263169288635: 1, -0.40758535265922546: 1, -1.126016616821289: 1, -1.1558103561401367: 1, -1.2008585929870605: 1, -0.9933805465698242: 1, -0.832706868648529: 1, -1.1563661098480225: 1, -0.9732658267021179: 1, -1.1349726915359497: 1, -1.1859160661697388: 1, -1.1969208717346191: 1, -1.1333073377609253: 1, -1.1816785335540771: 1, -1.1568188667297363: 1, -0.5632508397102356: 1, -1.2016375064849854: 1, -1.1857632398605347: 1, -1.198327660560608: 1, -0.9300684928894043: 1, -0.5115338563919067: 1, -1.026742935180664: 1, 1.7253838777542114: 1, -1.0461647510528564: 1, -0.8267379403114319: 1, -1.1486388444900513: 1, -0.25539615750312805: 1, -1.1933567523956299: 1, -0.86516273021698: 1, -0.9225814342498779: 1, -1.200010895729065: 1, -1.087907314300537: 1, -0.3156169652938843: 1, -1.2006280422210693: 1, -1.1891672611236572: 1, -1.1703253984451294: 1, -1.144382119178772: 1, -0.9726890325546265: 1, -0.9449849724769592: 1, -1.0933367013931274: 1, -0.4444347023963928: 1, 5.037554740905762: 1, -1.1434556245803833: 1, -1.1754673719406128: 1, -0.622269868850708: 1, 5.001932144165039: 1, 4.900933742523193: 1, -0.7939543128013611: 1, 4.302996635437012: 1, 4.58308219909668: 1, -1.1364892721176147: 1, -1.1480247974395752: 1, 4.875144004821777: 1, 5.072229385375977: 1, 0.4937030076980591: 1, 5.05051851272583: 1, 3.830434560775757: 1, -1.1136521100997925: 1, 5.006033897399902: 1, -0.6635236740112305: 1, 5.012721538543701: 1, -1.1181161403656006: 1, -0.4437340795993805: 1, -0.2650972902774811: 1, -1.1150031089782715: 1, -0.75212162733078: 1, -0.7968862652778625: 1, -1.145255446434021: 1, -1.2005667686462402: 1, -0.9272821545600891: 1, -1.02364981174469: 1, -0.5409148931503296: 1, -0.9141108393669128: 1, -0.8772833347320557: 1, -0.9129625558853149: 1, -0.7736660242080688: 1, -0.41631850600242615: 1, -1.1062737703323364: 1, -0.8366922736167908: 1, -1.1463063955307007: 1, -0.4774998426437378: 1, -1.1344302892684937: 1, 0.18425099551677704: 1, -0.8758900761604309: 1, -1.0246316194534302: 1, -1.0620733499526978: 1, -0.1397843062877655: 1, -0.47614291310310364: 1, -0.8666650652885437: 1, -0.8599510192871094: 1, -1.0074002742767334: 1, -0.9824236631393433: 1, -0.5638068318367004: 1, -0.4648326635360718: 1, -1.00275719165802: 1, -0.6867276430130005: 1, -0.669135332107544: 1, -1.2014120817184448: 1, -0.6967374682426453: 1, -1.0416630506515503: 1, -1.1946135759353638: 1, -0.5497206449508667: 1, -0.6818874478340149: 1, -1.1595861911773682: 1, -1.1821529865264893: 1, -1.076475739479065: 1, 1.593440055847168: 1, 0.6101611256599426: 1, 1.6096405982971191: 1, 0.9798484444618225: 1, -0.6111128330230713: 1, -1.0877141952514648: 1, 0.4647965729236603: 1, 0.9053057432174683: 1, 1.6532078981399536: 1, 0.5944019556045532: 1, 1.8255465030670166: 1, -0.44575440883636475: 1, 1.2443398237228394: 1, 0.2436400204896927: 1, 1.036679744720459: 1, 0.2817704975605011: 1, 1.2748090028762817: 1, -0.750208854675293: 1, 1.0043728351593018: 1, -0.9092623591423035: 1, -1.201631784439087: 1, 1.094826102256775: 1, -0.31994664669036865: 1, 0.18603742122650146: 1, -0.9458178281784058: 1, -0.39169713854789734: 1, 0.7324214577674866: 1, -1.201418161392212: 1, -0.2950673997402191: 1, -0.15851348638534546: 1, -1.1721446514129639: 1, -0.23552395403385162: 1, 1.6066374778747559: 1, 0.8136993050575256: 1, 0.7108749151229858: 1, -0.726171612739563: 1, 0.7067909836769104: 1, 0.8852934837341309: 1, 1.1564749479293823: 1, 1.7609484195709229: 1, -0.7198786735534668: 1, -0.4477367699146271: 1, 0.8030492067337036: 1, -1.2016229629516602: 1, 0.5780434608459473: 1, -0.7776852250099182: 1, -0.32108673453330994: 1, -1.1028145551681519: 1, -0.1892606019973755: 1, -1.2016355991363525: 1, -0.7252834439277649: 1, -1.1959316730499268: 1, -0.9352316856384277: 1, -0.5334532856941223: 1, 1.0577486753463745: 1, -0.5885310769081116: 1, 1.2156920433044434: 1, 0.6975425481796265: 1, 0.9127581715583801: 1, -0.35478705167770386: 1, -1.2016305923461914: 1, 0.6509128212928772: 1, -0.1808653175830841: 1, -0.411021888256073: 1, -0.17489729821681976: 1, 1.0823159217834473: 1, 0.5601547360420227: 1, 0.6412140727043152: 1, 1.3093386888504028: 1, 1.5706232786178589: 1, -0.3561877906322479: 1, -0.8054187893867493: 1, 0.5863446593284607: 1, -0.34684932231903076: 1, 1.585684895515442: 1, -1.2015637159347534: 1, -1.201583743095398: 1, -1.199397087097168: 1, -1.171905517578125: 1, -1.1621276140213013: 1, 1.123262643814087: 1, -1.016434669494629: 1, -1.2015913724899292: 1, -1.2009553909301758: 1, -0.9010990262031555: 1, -1.1365838050842285: 1, -1.196357011795044: 1, -1.1999870538711548: 1, 5.645717620849609: 1, -1.1910632848739624: 1, -1.195172905921936: 1, -0.459964781999588: 1, -1.1054575443267822: 1, 0.741217315196991: 1, 1.8886953592300415: 1, 0.19575154781341553: 1, -1.1993547677993774: 1, -1.1929975748062134: 1, 0.3444092869758606: 1, -1.2014538049697876: 1, 0.7184975147247314: 1, -1.157366394996643: 1, -0.5026354193687439: 1, -1.1594713926315308: 1, 0.6086026430130005: 1, -1.201351284980774: 1, -1.1019858121871948: 1, -1.1624016761779785: 1, -1.1929867267608643: 1, -1.18364417552948: 1, -1.1857080459594727: 1, -1.0951330661773682: 1, -1.1485586166381836: 1, -1.0865159034729004: 1, -1.1643073558807373: 1, -1.1970044374465942: 1, -0.40432149171829224: 1, -1.1610828638076782: 1, -1.188031554222107: 1, -0.6870049834251404: 1, -1.1676386594772339: 1, -1.186979055404663: 1, -1.1963162422180176: 1, -1.194710373878479: 1, -1.175083041191101: 1, -1.1964715719223022: 1, -1.2008846998214722: 1, -0.9958106875419617: 1, -1.0542218685150146: 1, 0.4647902548313141: 1, -1.1924408674240112: 1, -1.1728081703186035: 1, -1.1948115825653076: 1, -1.1685289144515991: 1, -1.1994209289550781: 1, -1.173497200012207: 1, -0.638097882270813: 1, -0.6645460724830627: 1, -1.2015430927276611: 1, 0.23683039844036102: 1, 0.05473056063055992: 1, -1.201563835144043: 1, -1.188680648803711: 1, -1.1971783638000488: 1, -1.0349972248077393: 1, -0.9986592531204224: 1, 0.275499552488327: 1, -1.1862393617630005: 1, -0.23024950921535492: 1, -1.171040415763855: 1, -1.1539435386657715: 1, -1.1912822723388672: 1, -1.1865227222442627: 1, -0.4641222357749939: 1, -0.7029581665992737: 1, -0.6273359060287476: 1, -1.0972120761871338: 1, -0.40274444222450256: 1, -1.1986582279205322: 1, -1.2010711431503296: 1, -1.1966667175292969: 1, -1.198758602142334: 1, -1.196737289428711: 1, -1.1819733381271362: 1, -1.1612766981124878: 1, 0.606712281703949: 1, 0.028185075148940086: 1, 1.4735376834869385: 1, 1.5148382186889648: 1, 0.987504780292511: 1, 0.22794750332832336: 1, 0.9434877038002014: 1, -0.1031346470117569: 1, -0.22873257100582123: 1, 0.22853031754493713: 1, -0.6209532618522644: 1, -0.21601253747940063: 1, 0.33931559324264526: 1, -1.2016159296035767: 1, 0.11681299656629562: 1, 0.06557659059762955: 1, 0.3338538706302643: 1, 1.914624810218811: 1, 0.2561040222644806: 1, 0.6983891129493713: 1, 1.431997537612915: 1, 0.30889958143234253: 1, 1.0659617185592651: 1, 0.7782679200172424: 1, 1.282376766204834: 1, 0.858607292175293: 1, 0.9224774241447449: 1, 1.533963680267334: 1, 1.3359390497207642: 1, -0.03743843734264374: 1, 0.31844547390937805: 1, -0.1758507341146469: 1, 0.7778939008712769: 1, 1.5112932920455933: 1, -0.1960129290819168: 1, -1.1865193843841553: 1, 0.35881760716438293: 1, 0.753506600856781: 1, 1.4364150762557983: 1, 1.4786081314086914: 1, 1.3110779523849487: 1, 1.55558443069458: 1, 0.18870316445827484: 1, -0.632728099822998: 1, 0.3140702247619629: 1, -0.10326965153217316: 1, -0.1196289211511612: 1, -0.44012218713760376: 1, 1.4899855852127075: 1, -0.1709771454334259: 1, 1.5044559240341187: 1, -0.6113037467002869: 1, 1.1449819803237915: 1, -0.9236016869544983: 1, 0.7352478504180908: 1, 1.0590577125549316: 1, -0.31073465943336487: 1, 0.21012525260448456: 1, 0.592692494392395: 1, -0.6031805276870728: 1, 0.6647680401802063: 1, -0.9130086302757263: 1, -0.10372047126293182: 1, 1.1246896982192993: 1, 0.11391282081604004: 1, 0.14948004484176636: 1, -0.11514480412006378: 1, -0.13019442558288574: 1, 1.6200270652770996: 1, 0.12041562795639038: 1, 0.7718502283096313: 1, 0.05425111949443817: 1, 0.3626178801059723: 1, 0.4515918791294098: 1, 0.20913533866405487: 1, 1.622856616973877: 1, -0.40100592374801636: 1, -0.20442236959934235: 1, -0.10369612276554108: 1, 0.7984791398048401: 1, 1.3110328912734985: 1, 1.5503476858139038: 1, 1.561331033706665: 1, 0.23876602947711945: 1, 0.23767612874507904: 1, 0.5634517669677734: 1, 1.466456651687622: 1, -0.5199218392372131: 1, -1.2015061378479004: 1, -1.201634168624878: 1, -1.1903178691864014: 1, -1.171350359916687: 1, -1.201610803604126: 1, -1.1100589036941528: 1, 0.3900337219238281: 1, -0.9999420642852783: 1, -1.201622486114502: 1, -1.2016303539276123: 1, -1.2014681100845337: 1, -0.06736226379871368: 1, -1.2015819549560547: 1, 0.34270647168159485: 1, -0.5888482928276062: 1, -1.2015849351882935: 1, 0.3153885304927826: 1, -0.8935246467590332: 1, -1.2015372514724731: 1, -1.2016048431396484: 1, 0.10832516103982925: 1, -1.2016196250915527: 1, -0.006573406048119068: 1, -0.5612114071846008: 1, 0.07849415391683578: 1, 0.8498935103416443: 1, -1.201079249382019: 1, -0.9951181411743164: 1, -1.2003253698349: 1, 0.5647678971290588: 1, -1.1779894828796387: 1, -0.9857455492019653: 1, 0.15111742913722992: 1, -0.009843221865594387: 1, -1.2016316652297974: 1, -0.5475073456764221: 1, -1.2014092206954956: 1, -1.1383882761001587: 1, -1.2012838125228882: 1, 0.9252344369888306: 1, 0.31476086378097534: 1, -0.5517370104789734: 1, 1.466652274131775: 1, 1.3268651962280273: 1, -0.3219013214111328: 1, 0.8655160665512085: 1, 0.8642333745956421: 1, -0.5888111591339111: 1, 0.5367603302001953: 1, 1.3780083656311035: 1, 1.1870133876800537: 1, 1.126368761062622: 1, 1.2197668552398682: 1, 0.004675476811826229: 1, 0.7520656585693359: 1, 1.0209075212478638: 1, 1.5651975870132446: 1, 0.7781831622123718: 1, -0.11230547726154327: 1, 0.01178812701255083: 1, -0.014552570879459381: 1, -0.0920228511095047: 1, 0.05546194687485695: 1, 0.3591007590293884: 1, 1.189407229423523: 1, -1.0867854356765747: 1, 0.9222752451896667: 1, 1.0225938558578491: 1, -0.16844011843204498: 1, -0.22581757605075836: 1, -0.4890022277832031: 1, 0.8458276391029358: 1, 0.29691436886787415: 1, 1.0407052040100098: 1, 0.8794386982917786: 1, -0.20083148777484894: 1, 1.244690179824829: 1, -0.22985504567623138: 1, 1.5059622526168823: 1, 1.059990644454956: 1, 0.021963730454444885: 1, 1.260263442993164: 1, 1.4145740270614624: 1, 0.7438257932662964: 1, 1.5916389226913452: 1, 0.19180983304977417: 1, -0.43688738346099854: 1, 0.8997297883033752: 1, 0.9983642101287842: 1, 1.133412480354309: 1, 0.8847638368606567: 1, 1.3578754663467407: 1, -0.07051629573106766: 1, 0.11520501971244812: 1, 1.2778698205947876: 1, 1.3494865894317627: 1, 1.0432312488555908: 1, -1.1632437705993652: 1, 1.5597952604293823: 1, 1.5033998489379883: 1, 0.22126778960227966: 1, 0.09744428098201752: 1, 1.3772739171981812: 1, 0.5069756507873535: 1, -0.19494549930095673: 1, 1.5239653587341309: 1, -0.4737872779369354: 1, -0.2417261004447937: 1, 1.1731380224227905: 1, -0.20986565947532654: 1, 0.6668532490730286: 1, 1.0762102603912354: 1, 1.4793431758880615: 1, 0.5000193119049072: 1, 0.9953116178512573: 1, 1.302850365638733: 1, 1.5040947198867798: 1, 0.7159395813941956: 1, 1.0842758417129517: 1, 0.9064841866493225: 1, 1.3597513437271118: 1, 0.8933335542678833: 1, 0.0863155722618103: 1, -0.8828660249710083: 1, 0.05755604803562164: 1, 0.35628634691238403: 1, 0.3431006968021393: 1, 1.5550216436386108: 1, 0.80833899974823: 1, 2.0702569484710693: 1, 0.9344565272331238: 1, -0.2038322389125824: 1, -1.1040070056915283: 1, -0.004799619782716036: 1, 1.1921144723892212: 1, 0.0384686179459095: 1, -1.193596363067627: 1, -0.5194405913352966: 1, 0.009009350091218948: 1, 1.4580349922180176: 1, 1.4103199243545532: 1, 0.7922017574310303: 1, 1.5027039051055908: 1, 0.2692132890224457: 1, 0.35639217495918274: 1, -0.20943275094032288: 1, 0.9195832014083862: 1, 1.5179330110549927: 1, 1.5557059049606323: 1, -0.7541334629058838: 1, 0.2642950117588043: 1, 0.8805737495422363: 1, -0.16789886355400085: 1, -0.17545948922634125: 1, 0.263934850692749: 1, 0.6183062195777893: 1, 1.3463716506958008: 1, 1.3844947814941406: 1, 0.7684015035629272: 1, 0.22540217638015747: 1, -0.9639919996261597: 1, -0.025239035487174988: 1, 1.4127790927886963: 1, 0.8287729024887085: 1, -0.11932859569787979: 1, 0.30258285999298096: 1, 0.1563034951686859: 1, 0.18295887112617493: 1, 0.2826254069805145: 1, 0.3602719008922577: 1, 0.04298178479075432: 1, 0.30982303619384766: 1, 0.2596873939037323: 1, 1.4639532566070557: 1, 1.1571227312088013: 1, 1.540098786354065: 1, 1.0418422222137451: 1, 0.1144905686378479: 1, 1.2591054439544678: 1, -0.20360040664672852: 1, -0.15476621687412262: 1, -0.10053509473800659: 1, 1.5731940269470215: 1, 0.6104775071144104: 1, 0.39806419610977173: 1, 1.473071813583374: 1, -0.3440307080745697: 1, 0.9935461282730103: 1, -0.30856987833976746: 1, 1.3900312185287476: 1, 0.2222539335489273: 1, 0.276736855506897: 1, 0.35236239433288574: 1, 1.6643083095550537: 1, -0.10448039323091507: 1, -0.19414900243282318: 1, -1.1556631326675415: 1, 0.07517638802528381: 1, 0.2801852524280548: 1, -0.48821160197257996: 1, 1.4599251747131348: 1, 1.5357881784439087: 1, -0.05793027952313423: 1, 1.3920340538024902: 1, 0.44933900237083435: 1, -0.5775644183158875: 1, 1.215183138847351: 1, 1.069445013999939: 1, 0.1911333203315735: 1, 0.3683989942073822: 1, 0.06307864934206009: 1, -0.28925973176956177: 1, 1.5599995851516724: 1, 1.5686708688735962: 1, -0.08143828064203262: 1, 0.2925977110862732: 1, -0.2210041582584381: 1, 0.31707215309143066: 1, 0.9266675710678101: 1, -0.3739321231842041: 1, 0.8433452844619751: 1, 0.7008569240570068: 1, 0.8512904644012451: 1, 1.916335940361023: 1, 1.5063830614089966: 1, -0.4644731879234314: 1, -0.11862857639789581: 1, -0.11317183822393417: 1, 0.9952307939529419: 1, 0.29443448781967163: 1, 0.39437854290008545: 1, -0.09898030012845993: 1, 1.4644227027893066: 1, 0.2846507132053375: 1, 1.5425565242767334: 1, -0.41700541973114014: 1, 1.5978120565414429: 1, 0.881543755531311: 1, -1.1900941133499146: 1, 0.3661552369594574: 1, 1.5401815176010132: 1, 1.4411144256591797: 1, 1.622040867805481: 1, 1.1854524612426758: 1, 0.968934953212738: 1, 0.98076331615448: 1, 1.0041277408599854: 1, -1.1742432117462158: 1, 1.4056357145309448: 1, -1.2014601230621338: 1, -1.1257261037826538: 1, -0.27063482999801636: 1, -1.0489486455917358: 1, -1.2004797458648682: 1, -0.0411478653550148: 1, -0.42487016320228577: 1, -0.002975589595735073: 1, 1.0863690376281738: 1, 0.5795131325721741: 1, 0.7288377285003662: 1, 1.3004077672958374: 1, -0.4453357756137848: 1, -0.9676279425621033: 1, -0.12074612081050873: 1, 1.150854468345642: 1, 1.2192449569702148: 1, -0.8759819269180298: 1, 0.9730016589164734: 1, 0.889022946357727: 1, -0.24570585787296295: 1, -0.674019455909729: 1, -1.2016277313232422: 1, -0.24846623837947845: 1, 0.840314507484436: 1, -0.16393840312957764: 1, -0.29299479722976685: 1, -0.42109400033950806: 1, -0.6826592683792114: 1, 0.8838387131690979: 1, -0.9900954961776733: 1, -0.1855221837759018: 1, -1.1928194761276245: 1, 1.179612159729004: 1, -0.37873539328575134: 1, 0.9235488772392273: 1, 0.4542813301086426: 1, -0.22304323315620422: 1, -0.12660875916481018: 1, -0.24156887829303741: 1, 1.1694233417510986: 1, 0.7264453172683716: 1, 1.2736730575561523: 1, -0.28629931807518005: 1, 0.009196557104587555: 1, 1.256977915763855: 1, -1.201277732849121: 1, -1.1879688501358032: 1, 1.2350752353668213: 1, -0.2299586683511734: 1, 0.9413108229637146: 1, 0.8562594652175903: 1, 0.8952587246894836: 1, -0.4684484004974365: 1, 1.150696039199829: 1, 1.0972230434417725: 1, 0.7325264811515808: 1, -0.25068432092666626: 1, 0.8119110465049744: 1, -0.2664187550544739: 1, 1.2978951930999756: 1, 0.729964017868042: 1, -0.8322789072990417: 1, 0.4259852468967438: 1, -0.27776581048965454: 1, -0.2164342701435089: 1, 0.23997803032398224: 1, 0.008470889180898666: 1, 1.1922990083694458: 1, -0.8705235719680786: 1, 0.8834699392318726: 1, 0.23272329568862915: 1, -0.09275590628385544: 1, -0.06864805519580841: 1, 0.8631362915039062: 1, -0.20541705191135406: 1, 0.8532567620277405: 1, 0.6013088822364807: 1, -0.35842111706733704: 1, -1.126828908920288: 1, 1.185150146484375: 1, 0.9848727583885193: 1, -0.1916339248418808: 1, 0.938378632068634: 1, 0.9840258359909058: 1, 0.7244752645492554: 1, -0.19223442673683167: 1, -0.1759326308965683: 1, 1.1819933652877808: 1, 0.12959904968738556: 1, -1.1653438806533813: 1, 0.029728559777140617: 1, -0.35746997594833374: 1, 1.0611008405685425: 1, -0.025104349479079247: 1, -0.17582924664020538: 1, 0.32448264956474304: 1, 1.2197721004486084: 1, -0.6921688914299011: 1, -0.16533638536930084: 1, 0.31131237745285034: 1, 1.2513697147369385: 1, -1.1878859996795654: 1, -0.8494775891304016: 1, -1.1774789094924927: 1, -0.12005668878555298: 1, -1.194725751876831: 1, 0.167218878865242: 1, 1.1985303163528442: 1, -1.194933295249939: 1, 0.666642963886261: 1, 0.5615567564964294: 1, 0.09289468824863434: 1, 0.05186900869011879: 1, -0.5160067081451416: 1, -0.4923703670501709: 1, -1.0983095169067383: 1, 0.21761490404605865: 1, -0.809281051158905: 1, 1.1265980005264282: 1, 0.02654222585260868: 1, 0.017293494194746017: 1, -0.253052294254303: 1, -0.8852211833000183: 1, 0.5908839702606201: 1, 1.0087279081344604: 1, 1.2238743305206299: 1, 1.1650327444076538: 1, 0.0796559751033783: 1, 1.2327803373336792: 1, -0.35524412989616394: 1, -0.1827705055475235: 1, -1.2004859447479248: 1, -1.0583271980285645: 1, 0.41254743933677673: 1, 0.042822714895009995: 1, 0.15707539021968842: 1, 1.1279875040054321: 1, 0.7561059594154358: 1, -1.1981785297393799: 1, 0.1316474825143814: 1, -0.5372467637062073: 1, -0.24153171479701996: 1, -0.8713244199752808: 1, -0.14272816479206085: 1, 1.1865397691726685: 1, 0.02958042360842228: 1, -0.2098180055618286: 1, -0.39703771471977234: 1, -0.9307324290275574: 1, 0.004906347021460533: 1, -0.9308528304100037: 1, -0.19495585560798645: 1, -0.3641962707042694: 1, -1.2016282081604004: 1, -0.7914682626724243: 1, 1.0062413215637207: 1, -0.02250954695045948: 1, -0.23942992091178894: 1, -0.11078709363937378: 1, -1.095828652381897: 1, -0.586733877658844: 1, -0.3953478932380676: 1, 0.7884150743484497: 1, -0.4121406674385071: 1, -1.0349785089492798: 1, 1.0134633779525757: 1, 0.44458866119384766: 1, 1.0013794898986816: 1, -0.2597183287143707: 1, 0.6872115731239319: 1, 1.035197138786316: 1, -0.16572129726409912: 1, -1.201588749885559: 1, -1.201620101928711: 1, -1.2016350030899048: 1, 0.0891273021697998: 1, -1.201523780822754: 1, -1.1484540700912476: 1, -1.201629877090454: 1, -1.201595425605774: 1, -1.2015864849090576: 1, -0.3404213488101959: 1, -0.5937166213989258: 1, -1.2016299962997437: 1, 4.363720893859863: 1, -1.1943039894104004: 1, -1.2015916109085083: 1, -1.179785132408142: 1, -0.11716806888580322: 1, -1.2015702724456787: 1, -1.2016273736953735: 1, 0.9705496430397034: 1, -1.2015695571899414: 1, 0.8222996592521667: 1, 1.0808240175247192: 1, -0.21433545649051666: 1, 0.5694330334663391: 1, 1.0520529747009277: 1, 0.9786769151687622: 1, -0.10290281474590302: 1, -0.040736664086580276: 1, -0.2904500663280487: 1, 0.6481048464775085: 1, 1.4819786548614502: 1, -0.2126571536064148: 1, 1.018097996711731: 1, 0.2079305797815323: 1, -0.10643515735864639: 1, -1.1586899757385254: 1, -1.0591267347335815: 1, -0.14289136230945587: 1, 0.36895880103111267: 1, 1.1995047330856323: 1, 0.2464127391576767: 1, -0.4132004976272583: 1, -1.2011888027191162: 1, -0.4687884449958801: 1, -1.1849600076675415: 1, -0.4057319462299347: 1, -1.2015478610992432: 1, -0.08719541132450104: 1, 0.1322988122701645: 1, -1.2016332149505615: 1, -1.0628424882888794: 1, -0.6676098704338074: 1, -0.6758065819740295: 1, -1.2016379833221436: 1, -1.194821834564209: 1, -0.045158740133047104: 1, -0.11703558266162872: 1, 0.1581522524356842: 1, -1.201630711555481: 1, -1.0203382968902588: 1, -1.2016286849975586: 1, -0.053435858339071274: 1, -1.2016292810440063: 1, -1.2015831470489502: 1, -1.2016255855560303: 1, -1.201633095741272: 1, -1.1987295150756836: 1, -1.2016326189041138: 1, -0.6771114468574524: 1, -1.1967262029647827: 1, -0.07099064439535141: 1, -1.2016361951828003: 1, -1.2015197277069092: 1, -1.201634407043457: 1, -1.201407790184021: 1, -1.2016335725784302: 1, -0.7989376187324524: 1, -1.193373441696167: 1, 0.3015405535697937: 1, 0.09893729537725449: 1, 0.005373222753405571: 1, -0.1517976075410843: 1, -0.14243346452713013: 1, 0.637002170085907: 1, -0.05695538595318794: 1, -0.28555259108543396: 1, 0.8218473196029663: 1, 0.2588636577129364: 1, -0.5884501934051514: 1, -0.1427413374185562: 1, -0.0332360677421093: 1, -0.34523066878318787: 1, 0.01664043217897415: 1, -0.6421114802360535: 1, -0.3469093143939972: 1, -0.029267514124512672: 1, -0.21881107985973358: 1, 0.33690160512924194: 1, 0.04192548990249634: 1, -1.1634924411773682: 1, 0.5468651056289673: 1, -0.5104274749755859: 1, -0.3879204988479614: 1, 0.5997416377067566: 1, 1.196738839149475: 1, -0.09905305504798889: 1, 1.0347987413406372: 1, 1.2749443054199219: 1, 1.2229245901107788: 1, -1.2015588283538818: 1, -0.7430113554000854: 1, 1.3061707019805908: 1, -1.0537559986114502: 1, -1.1755220890045166: 1, -1.0965174436569214: 1, -0.08886344730854034: 1, -1.1905653476715088: 1, -0.6103007793426514: 1, 1.310013771057129: 1, 0.029840881004929543: 1, 0.5362502932548523: 1, 0.5252094268798828: 1, -0.4360010027885437: 1, -0.04390183836221695: 1, -0.463926762342453: 1, 1.0769490003585815: 1, -0.11945565789937973: 1, 1.2526917457580566: 1, 0.3951069414615631: 1, 1.0069524049758911: 1, 0.8965467214584351: 1, -1.0558182001113892: 1, 0.047311048954725266: 1, 1.1649004220962524: 1, -0.34107181429862976: 1, 1.295539140701294: 1, 1.1717408895492554: 1, 1.2726079225540161: 1, 0.30879271030426025: 1, -0.10037212073802948: 1, 0.9712859988212585: 1, 1.176633596420288: 1, 0.7298784255981445: 1, -1.175829291343689: 1, -0.0580214224755764: 1, 0.3886134922504425: 1, 0.8548043966293335: 1, 1.2064505815505981: 1, 0.18457916378974915: 1, -0.84548020362854: 1, 0.9529565572738647: 1, 0.01943967677652836: 1, 0.04932570457458496: 1, -0.9601404666900635: 1, -0.8751402497291565: 1, -1.1107620000839233: 1, 0.9265954494476318: 1, -0.9734629392623901: 1, -1.193730115890503: 1, -0.19123347103595734: 1, 0.8673886060714722: 1, -0.011501064524054527: 1, -0.3390061855316162: 1, -1.2003904581069946: 1} test data: {-1.2016383409500122: 4, -1.2016351222991943: 2, -1.201621651649475: 2, -1.2016315460205078: 2, -1.2016254663467407: 2, -1.2016295194625854: 2, -1.2016366720199585: 2, -1.2016345262527466: 2, -1.2016324996948242: 2, -1.2016355991363525: 2, 1.5667779445648193: 1, -1.201623558998108: 1, -0.297276109457016: 1, 0.2470504194498062: 1, -0.008235386572778225: 1, 0.2669691741466522: 1, 1.1950836181640625: 1, -0.24468590319156647: 1, 1.478103518486023: 1, -1.1840753555297852: 1, 1.621954083442688: 1, 1.5633379220962524: 1, 1.593016266822815: 1, -0.5330178141593933: 1, -1.2015975713729858: 1, -1.2016290426254272: 1, 0.9703378081321716: 1, -1.2013576030731201: 1, -1.1526696681976318: 1, -0.6465518474578857: 1, -0.3525408208370209: 1, -0.4260459542274475: 1, 0.031881630420684814: 1, -0.8519163131713867: 1, -1.1730265617370605: 1, -1.092740535736084: 1, -0.9931707978248596: 1, 0.0860099047422409: 1, -0.916256844997406: 1, -1.2013626098632812: 1, -1.1647279262542725: 1, -1.1937233209609985: 1, 0.670230507850647: 1, 1.5627902746200562: 1, 1.6047093868255615: 1, -0.1421377956867218: 1, 0.6573249697685242: 1, -0.16630633175373077: 1, 0.7218993902206421: 1, 1.5149681568145752: 1, 0.8825770616531372: 1, 1.8254425525665283: 1, 0.5247108340263367: 1, 0.31109386682510376: 1, -0.06838630884885788: 1, 0.19487899541854858: 1, 0.9998847246170044: 1, -0.18418414890766144: 1, 0.25592291355133057: 1, 0.2954026460647583: 1, 1.6063342094421387: 1, 1.595760464668274: 1, -0.17784874141216278: 1, 0.2610865831375122: 1, 0.04832748696208: 1, 0.17204155027866364: 1, 1.6950387954711914: 1, 0.20332399010658264: 1, -0.7750424146652222: 1, 0.5299685597419739: 1, 0.8313724994659424: 1, -1.2015955448150635: 1, -0.16336557269096375: 1, 1.5455868244171143: 1, -0.4970245659351349: 1, -1.201635479927063: 1, 1.0667099952697754: 1, 0.02149348333477974: 1, -0.9010018706321716: 1, 0.002237366745248437: 1, -0.2558962106704712: 1, 0.7778733968734741: 1, 1.2952117919921875: 1, 1.4535586833953857: 1, 1.1870161294937134: 1, 0.2900018095970154: 1, -1.0348321199417114: 1, -0.5828713178634644: 1, -1.194837212562561: 1, -0.4268537759780884: 1, 1.2996296882629395: 1, 0.18236172199249268: 1, 1.4387927055358887: 1, -0.2865978181362152: 1, 1.603068470954895: 1, -0.3579169511795044: 1, 1.4029184579849243: 1, 0.16395387053489685: 1, 0.28549933433532715: 1, -1.2008297443389893: 1, 1.4342314004898071: 1, 0.8374537825584412: 1, 1.1079081296920776: 1, -0.8842195272445679: 1, -0.15980762243270874: 1, -1.1961579322814941: 1, -1.1269394159317017: 1, -1.1956098079681396: 1, 1.3062021732330322: 1, -0.37773171067237854: 1, -0.9769929647445679: 1, -0.4147615134716034: 1, -0.47762712836265564: 1, 1.3883335590362549: 1, -0.8472578525543213: 1, -0.5717604756355286: 1, -1.1897622346878052: 1, -0.408608615398407: 1, 1.0165932178497314: 1, 1.0860453844070435: 1, -0.8897131681442261: 1, 1.5550175905227661: 1, -1.2016119956970215: 1, 0.7517246007919312: 1, -1.009895920753479: 1, 1.8398889303207397: 1, 1.8768579959869385: 1, -0.6085047125816345: 1, -0.19289743900299072: 1, -1.1962995529174805: 1, -1.2016159296035767: 1, -0.9153497219085693: 1, -1.201633095741272: 1, 0.1561044156551361: 1, -1.201622486114502: 1, -1.201596975326538: 1, -1.2016369104385376: 1, -1.201564908027649: 1, -1.1506352424621582: 1, -1.201627254486084: 1, -1.2016280889511108: 1, -1.2015992403030396: 1, -1.2009947299957275: 1, -1.1999688148498535: 1, 0.21984633803367615: 1, -0.7713357210159302: 1, -1.2007629871368408: 1, -0.6686071157455444: 1, -0.5025617480278015: 1, -1.2015149593353271: 1, -1.2016363143920898: 1, -0.09781666100025177: 1, 0.7523799538612366: 1, -0.8503693342208862: 1, -1.2014657258987427: 1, -1.1237342357635498: 1, -1.1871466636657715: 1, -0.34237241744995117: 1, -1.1929398775100708: 1, -1.1852269172668457: 1, -1.196489930152893: 1, -1.1987890005111694: 1, -1.1976145505905151: 1, -1.2015410661697388: 1, -1.1482487916946411: 1, -0.37911587953567505: 1, -0.33821895718574524: 1, -0.6864959597587585: 1, -1.1830931901931763: 1, -1.1972260475158691: 1, 0.5888639092445374: 1, -1.2014594078063965: 1, 0.3422311544418335: 1, 1.3042255640029907: 1, 0.15129715204238892: 1, 1.9090871810913086: 1, 1.3982725143432617: 1, -1.1569617986679077: 1, -0.43377333879470825: 1, 0.05307941138744354: 1, -0.9736310243606567: 1, -0.9981153011322021: 1, -0.5620161294937134: 1, 0.006390336435288191: 1, 0.19889964163303375: 1, 0.7257186770439148: 1, -1.0622771978378296: 1, -0.4610443115234375: 1, 0.4990178942680359: 1, -0.04164140671491623: 1, -1.1138004064559937: 1, -1.0634658336639404: 1, -1.111975073814392: 1, -0.2154475301504135: 1, -0.5144169330596924: 1, 1.2280430793762207: 1, 0.16367900371551514: 1, 0.5357815027236938: 1, 1.287703037261963: 1, 1.425097107887268: 1, -0.9859256744384766: 1, 0.342952162027359: 1, -0.0960833728313446: 1, -0.27164560556411743: 1, -0.38419657945632935: 1, -1.0024443864822388: 1, -0.8622487187385559: 1, -0.9035985469818115: 1, 0.40835040807724: 1, -1.1544640064239502: 1, 1.1575602293014526: 1, 1.4774726629257202: 1, 0.9346560835838318: 1, 0.4252239763736725: 1, -0.03840658441185951: 1, -0.41735291481018066: 1, 1.5516252517700195: 1, -0.8234555125236511: 1, 1.114488959312439: 1, -0.9642779231071472: 1, -1.194373369216919: 1, -1.1659823656082153: 1, -0.36594024300575256: 1, -0.9982122778892517: 1, -1.199107050895691: 1, 3.259727716445923: 1, 1.0554637908935547: 1, -0.1391475349664688: 1, 0.6965684294700623: 1, -1.0221163034439087: 1, 1.6904795169830322: 1, -1.078048825263977: 1, 0.2662027180194855: 1, -0.981871485710144: 1, 1.4842547178268433: 1, 0.17207567393779755: 1, 0.0663938894867897: 1, -1.2012841701507568: 1, -0.19042661786079407: 1, -0.2147151529788971: 1, -0.9940837025642395: 1, -0.896551251411438: 1, -0.556195080280304: 1, 0.7494425773620605: 1, -1.1412583589553833: 1, -0.9288858771324158: 1, -0.6311256289482117: 1, -0.6980454921722412: 1, -0.930094301700592: 1, 0.2903974652290344: 1, -0.35407769680023193: 1, -1.0751264095306396: 1, -1.0768063068389893: 1, -0.8935588002204895: 1, -0.923246443271637: 1, -1.0411947965621948: 1, -0.1989370882511139: 1, 1.6431117057800293: 1, 1.4413039684295654: 1, -1.1940946578979492: 1, -0.7269378900527954: 1, -0.004194003064185381: 1, -0.7282882928848267: 1, 1.5277636051177979: 1, 1.7721431255340576: 1, 1.4308977127075195: 1, 0.4048600494861603: 1, 0.03207547590136528: 1, 0.2726168930530548: 1, -0.22176611423492432: 1, 0.3604428768157959: 1, 1.2427196502685547: 1, 1.573736548423767: 1, 1.2712597846984863: 1, 1.2730098962783813: 1, 0.22981515526771545: 1, 1.0202414989471436: 1, -0.37734130024909973: 1, 0.15925046801567078: 1, -1.2016338109970093: 1, -1.1950759887695312: 1, -0.993471086025238: 1, 0.17100460827350616: 1, 1.286658525466919: 1, 1.4653488397598267: 1, 0.643775999546051: 1, 0.34191834926605225: 1, 0.015656888484954834: 1, 0.15692748129367828: 1, -1.201636791229248: 1, -0.19042591750621796: 1, 0.2743425965309143: 1, 0.31522658467292786: 1, -1.160044550895691: 1, 0.6820655465126038: 1, 0.0010552277090027928: 1, 1.2379846572875977: 1, 0.16810350120067596: 1, -0.2711203992366791: 1, 1.4500402212142944: 1, 0.6386443972587585: 1, 1.6178103685379028: 1, 1.3024239540100098: 1, -0.9339156150817871: 1, 0.2775695323944092: 1, 0.9666041731834412: 1, 1.3437533378601074: 1, 0.1467430293560028: 1, -1.031872034072876: 1, 1.4644434452056885: 1, -0.32526895403862: 1, -0.589444637298584: 1, 0.28860634565353394: 1, -0.539240837097168: 1, -0.39828142523765564: 1, -0.16177639365196228: 1, 0.36046868562698364: 1, 0.9138351678848267: 1, -0.514695405960083: 1, 1.0683897733688354: 1, -0.1799832135438919: 1, 0.7724436521530151: 1, 0.739153265953064: 1, 0.9499619007110596: 1, 1.7417840957641602: 1, -0.6611031889915466: 1, 1.5755736827850342: 1, 1.427193522453308: 1, -0.24819114804267883: 1, 1.2588475942611694: 1, 0.2308363914489746: 1, 1.3250715732574463: 1, -0.027011625468730927: 1, 1.5811641216278076: 1, -1.1914112567901611: 1, -0.8986467123031616: 1, 1.2984728813171387: 1, 0.1011820137500763: 1, 0.7110782265663147: 1, 0.27488669753074646: 1, 0.11351441591978073: 1, 0.9129876494407654: 1, 0.5222756862640381: 1, 0.4746771454811096: 1, -0.9396678805351257: 1, -0.3907565474510193: 1, 1.6493046283721924: 1, -1.201619267463684: 1, 1.2604477405548096: 1, 1.5992790460586548: 1, -0.9704957008361816: 1, -1.1416743993759155: 1, 0.4860861599445343: 1, 1.3301595449447632: 1, -1.2016263008117676: 1, -0.807515025138855: 1, -0.41689032316207886: 1, 0.523788332939148: 1, 0.42848649621009827: 1, 1.5793328285217285: 1, 1.8360271453857422: 1, -1.1663979291915894: 1, -1.0099024772644043: 1, 0.3117649555206299: 1, -0.2495342493057251: 1, -1.2016352415084839: 1, 1.633592128753662: 1, 1.3466922044754028: 1, 1.5722275972366333: 1, 0.5880253314971924: 1, 1.7429335117340088: 1, -1.174880027770996: 1, -0.009660118259489536: 1, 0.23050570487976074: 1, -0.9877029061317444: 1, 0.5326334834098816: 1, 1.9196945428848267: 1, 1.5698747634887695: 1, 1.0374422073364258: 1, 1.5665374994277954: 1, 1.7350994348526: 1, -0.10222127288579941: 1, 1.8529928922653198: 1, 0.14208731055259705: 1, 0.6003371477127075: 1, -1.201629877090454: 1, -1.042400598526001: 1, -0.962668240070343: 1, 1.6464146375656128: 1, -0.5880576372146606: 1, 0.3740043342113495: 1, 1.4337563514709473: 1, -0.08275699615478516: 1, 0.04922454059123993: 1, 0.7481110692024231: 1, -0.7948459982872009: 1, 1.6735926866531372: 1, 0.056893277913331985: 1, -0.22299611568450928: 1, 1.6179100275039673: 1, 0.08808748424053192: 1, 1.6278821229934692: 1, 1.9158005714416504: 1, 1.5299123525619507: 1, -0.8965012431144714: 1, 1.231010913848877: 1, -0.4275071322917938: 1, -0.23398016393184662: 1, -0.4433523118495941: 1, 0.7045637369155884: 1, 1.0254307985305786: 1, 1.8846700191497803: 1, 0.6905592679977417: 1, 0.5497986674308777: 1, 0.19680047035217285: 1, 0.9832457304000854: 1, 1.0101338624954224: 1, 1.7298698425292969: 1, 1.5315228700637817: 1, 0.26564425230026245: 1, 0.521833598613739: 1, 0.9454357028007507: 1, 1.6500415802001953: 1, 0.7741647958755493: 1, -0.2079317569732666: 1, 0.8423707485198975: 1, -0.31705647706985474: 1, 1.6153100728988647: 1, -0.5665901303291321: 1, 1.1628241539001465: 1, 1.92928946018219: 1, 1.7397531270980835: 1, 1.2302463054656982: 1, 0.562964141368866: 1, 0.8005363941192627: 1, -1.0813920497894287: 1, 1.610649824142456: 1, 1.0590068101882935: 1, 0.9784976243972778: 1, 1.4854387044906616: 1, 0.6071187853813171: 1, -1.2015928030014038: 1, 1.271135687828064: 1, -0.8916471600532532: 1, 0.261216938495636: 1, 0.5681759119033813: 1, 0.8646785616874695: 1, 1.519827961921692: 1, -0.538144052028656: 1, -1.2015867233276367: 1, 0.3816104531288147: 1, 1.127722144126892: 1, 0.10180643945932388: 1, -1.012891411781311: 1, -1.2016215324401855: 1, 0.963599681854248: 1, 0.8969712257385254: 1, -1.2016236782073975: 1, 0.8895251154899597: 1, 0.9158507585525513: 1, -0.9954119920730591: 1, 1.2369016408920288: 1, 1.3847272396087646: 1, 1.1129239797592163: 1, 1.339892864227295: 1, -1.2011404037475586: 1, -0.6586742401123047: 1, 1.3510494232177734: 1, -0.9789384603500366: 1, 1.3490053415298462: 1, -0.2255825698375702: 1, 0.6801310777664185: 1, 0.6376197934150696: 1, 0.46223318576812744: 1, 0.12914451956748962: 1, 0.8198267817497253: 1, -0.8602240085601807: 1, 0.09368380159139633: 1, 1.887890100479126: 1, 0.8861773610115051: 1, 0.8217967748641968: 1, -1.1562846899032593: 1, 1.7338974475860596: 1, -0.2059621661901474: 1, -0.22918304800987244: 1, 1.8285820484161377: 1, 0.05316608399152756: 1, 1.0329307317733765: 1, -0.9175146222114563: 1, -0.4143541157245636: 1, 0.8007993698120117: 1, -1.2014458179473877: 1, 1.8896540403366089: 1, 1.4848576784133911: 1, -0.796614408493042: 1, 1.703546404838562: 1, -1.1579643487930298: 1, 1.746630072593689: 1, -0.21755054593086243: 1, 1.455495834350586: 1, 1.6787821054458618: 1, -0.420527845621109: 1, 1.2849032878875732: 1, 1.5742621421813965: 1, 1.0331618785858154: 1, -0.11489463597536087: 1, 1.9223994016647339: 1, 0.13984030485153198: 1, -0.7876200675964355: 1, 1.7499927282333374: 1, 0.32922476530075073: 1, -0.1335328370332718: 1, 1.0119178295135498: 1, 0.7037346363067627: 1, 5.270293235778809: 1, 1.8530430793762207: 1, -0.9310538172721863: 1, -1.2016310691833496: 1, 1.456301212310791: 1, 1.9232004880905151: 1, 0.2143784463405609: 1, -0.4622204601764679: 1, -1.1957098245620728: 1, -0.558110237121582: 1, -0.29665860533714294: 1, -1.0387167930603027: 1, 1.2142442464828491: 1, 0.7785534858703613: 1, 0.16830426454544067: 1, 1.8227369785308838: 1, -0.23732632398605347: 1, -0.92970210313797: 1, -1.2009780406951904: 1, -0.466978520154953: 1, 1.2964091300964355: 1, 1.052090048789978: 1, 1.4788439273834229: 1, -0.3439752459526062: 1, -0.07709828019142151: 1, -0.7822737693786621: 1, -0.28641819953918457: 1, -0.21501778066158295: 1, -0.5378462672233582: 1, -0.2761753499507904: 1, 0.43118155002593994: 1, -1.0356733798980713: 1, 1.5919677019119263: 1, -0.46113380789756775: 1, 1.878305196762085: 1, 1.3953920602798462: 1, -0.3858093321323395: 1, -1.193153738975525: 1, -1.2014148235321045: 1, 1.6845048666000366: 1, 0.6026607155799866: 1, 1.8707987070083618: 1, -1.1991149187088013: 1, -0.9299617409706116: 1, 0.012689988128840923: 1, -0.8769359588623047: 1, -0.3702247440814972: 1, 1.187366247177124: 1, 3.323413610458374: 1, -0.2670007050037384: 1, 0.06790906190872192: 1, -1.1109116077423096: 1, -0.29989245533943176: 1, -0.960394024848938: 1, -0.5378177762031555: 1, -0.35491982102394104: 1, 0.16428887844085693: 1, 0.47734835743904114: 1, 0.9746946096420288: 1, -0.5843047499656677: 1, -0.46731990575790405: 1, -0.9987406134605408: 1, -1.1868388652801514: 1, 1.491416096687317: 1, 1.1045739650726318: 1, 1.9010276794433594: 1, -0.5713714957237244: 1, -0.014453819021582603: 1, -1.0160380601882935: 1, -1.0031712055206299: 1, -0.8708242177963257: 1, -0.29448574781417847: 1, 1.6146701574325562: 1, -0.2992294430732727: 1, -1.0713788270950317: 1, 0.37810415029525757: 1, 1.6888245344161987: 1, -0.3349834084510803: 1, -0.8232764601707458: 1, 0.3796524703502655: 1, 0.3571058511734009: 1, -0.6655375957489014: 1, 1.6648616790771484: 1, -0.4548023045063019: 1, 1.8017815351486206: 1, 0.9851498603820801: 1, 1.6570682525634766: 1, 0.1589841991662979: 1, -0.27935805916786194: 1, 0.2097160518169403: 1, -0.9834045767784119: 1, 0.3213244378566742: 1, 1.6034691333770752: 1, -0.03495830297470093: 1, -1.0319366455078125: 1, -0.39972129464149475: 1, 0.8109627962112427: 1, -0.9147067666053772: 1, -0.13741447031497955: 1, 0.7185215353965759: 1, -1.1760764122009277: 1, 0.5790113210678101: 1, 0.5123543739318848: 1, -1.2016146183013916: 1, -0.3992172181606293: 1, -1.201583981513977: 1, -0.8602992296218872: 1, -1.1236215829849243: 1, 1.7555813789367676: 1, -1.1569743156433105: 1, -0.28438881039619446: 1, 0.15644948184490204: 1, 0.11937177926301956: 1, 0.611980676651001: 1, -0.14373785257339478: 1, 1.905008316040039: 1, -0.09393322467803955: 1, -0.3426608741283417: 1, 0.04079057276248932: 1, 0.21555371582508087: 1, 0.21643884479999542: 1, -0.6562255024909973: 1, 1.525660514831543: 1, -1.0813374519348145: 1, 0.956155002117157: 1, 1.4984081983566284: 1, -0.01187801081687212: 1, 1.5747997760772705: 1, 1.8505216836929321: 1, -1.2015869617462158: 1, 3.341240167617798: 1, -1.1207038164138794: 1, -1.1422733068466187: 1, -0.7939702272415161: 1, -0.9730278849601746: 1, -0.539626955986023: 1, 0.4937743842601776: 1, -0.30786851048469543: 1, -0.9930511116981506: 1, -0.8952922821044922: 1, 0.18035785853862762: 1, -1.201271653175354: 1, -1.1617506742477417: 1, -0.7995052933692932: 1, -0.4168057143688202: 1, -1.1991721391677856: 1, -0.43086937069892883: 1, 1.629822015762329: 1, 0.13262629508972168: 1, 1.8601784706115723: 1, -0.5884281992912292: 1, 0.4647309482097626: 1, -1.1694631576538086: 1, -0.6015652418136597: 1, -0.7289202809333801: 1, -1.199577808380127: 1, -0.9751223921775818: 1, -1.1969398260116577: 1, -1.2015608549118042: 1, 0.9462845921516418: 1, -1.1475187540054321: 1, -1.0460647344589233: 1, -1.1108695268630981: 1, -1.1953339576721191: 1, 4.282702445983887: 1, -1.201569676399231: 1, -1.2002716064453125: 1, -1.1925454139709473: 1, -1.2016171216964722: 1, -0.36108481884002686: 1, 0.012895430438220501: 1, 0.04404761642217636: 1, 1.4876383543014526: 1, -0.8890491724014282: 1, -1.1747305393218994: 1, -0.40984249114990234: 1, -1.1906999349594116: 1, 0.8944936394691467: 1, -1.1091188192367554: 1, -0.4976504147052765: 1, -0.4824763238430023: 1, -0.7597726583480835: 1, -0.20172715187072754: 1, -1.1632294654846191: 1, -1.1991511583328247: 1, -1.0164505243301392: 1, -0.4707425832748413: 1, -1.1992580890655518: 1, -1.0079307556152344: 1, -0.5657321810722351: 1, -0.12521179020404816: 1, -0.35893186926841736: 1, -0.8982016444206238: 1, -0.01552529539912939: 1, 0.944926917552948: 1, 0.5761882066726685: 1, -0.5374748706817627: 1, -0.7650251388549805: 1, -0.9430715441703796: 1, 0.0014178809942677617: 1, -0.27813780307769775: 1, 1.0866621732711792: 1, -0.25587567687034607: 1, -1.2016326189041138: 1, -0.3301823139190674: 1, -0.35624369978904724: 1, -1.200080394744873: 1, -1.1873303651809692: 1, 0.03431294485926628: 1, 1.1114397048950195: 1, -0.10418325662612915: 1, -0.7642948627471924: 1, -0.15097910165786743: 1, 0.040903303772211075: 1, -1.2011059522628784: 1, 0.7875488996505737: 1, 0.5111210942268372: 1, -1.0825824737548828: 1, -1.083876609802246: 1, -0.7811260223388672: 1, -1.192929744720459: 1, -0.8395327925682068: 1, 1.0160198211669922: 1, -1.1549781560897827: 1, 1.177069067955017: 1, 1.1708778142929077: 1, -0.5807573795318604: 1, -1.1866058111190796: 1, 0.03998654708266258: 1, -0.005674127489328384: 1, -0.41031748056411743: 1, 0.8209275603294373: 1, 1.2313082218170166: 1, -0.39157962799072266: 1, -0.042717207223176956: 1, -0.5272284746170044: 1, -0.29541367292404175: 1, -1.2016023397445679: 1, -1.1735186576843262: 1, -0.8984062075614929: 1, 0.264765202999115: 1, -1.169080138206482: 1, -1.186597228050232: 1, -1.193078875541687: 1, 0.8018452525138855: 1, -0.7683218121528625: 1, -0.7864221334457397: 1, -0.671938955783844: 1, -1.187011957168579: 1, 0.9907589554786682: 1, 0.3299405574798584: 1, 1.25115966796875: 1, 1.1413462162017822: 1, 0.19746625423431396: 1, 0.6585915088653564: 1, 0.3790889084339142: 1, -1.1513574123382568: 1, -0.16514527797698975: 1, 0.525627076625824: 1, -0.3780987560749054: 1, -0.5744653940200806: 1, 0.2788977324962616: 1, -0.4809620976448059: 1, 1.0399528741836548: 1, 1.274375557899475: 1, 0.41225412487983704: 1, -0.19212104380130768: 1, -0.36745402216911316: 1, 0.022439440712332726: 1, -1.0054869651794434: 1, -0.7266088128089905: 1, -0.11206144839525223: 1, 1.0150052309036255: 1, -0.01294313371181488: 1, -0.1045512706041336: 1, 1.3108209371566772: 1, -0.2588912844657898: 1, -0.12876513600349426: 1, -1.201636552810669: 1, 0.5745441317558289: 1, 0.2699461281299591: 1, -0.024909501895308495: 1, -1.1852235794067383: 1, -0.019019143655896187: 1, 0.6605957746505737: 1, -0.7624772787094116: 1, 1.0467203855514526: 1, -0.34817975759506226: 1, 1.2926610708236694: 1, -0.5622422695159912: 1, -0.12913857400417328: 1, -0.32669803500175476: 1, 0.34432777762413025: 1, -1.1104997396469116: 1, 0.43160757422447205: 1, 1.2668349742889404: 1, -0.27802959084510803: 1, 0.8297379016876221: 1, 1.2464758157730103: 1, 0.2748369872570038: 1, 1.1462621688842773: 1, 0.05971863865852356: 1, -0.32367783784866333: 1, 0.12906195223331451: 1, -0.15591250360012054: 1, -0.5908447504043579: 1, 0.7100340127944946: 1, 0.03461417928338051: 1, -0.23146884143352509: 1, 1.2829649448394775: 1, -0.22990889847278595: 1, -0.5931430459022522: 1, -1.2008949518203735: 1, 1.2035483121871948: 1, -0.5088394284248352: 1, 0.05442709103226662: 1, -0.26975223422050476: 1, -1.2016304731369019: 1, -1.1654343605041504: 1, -0.11618001013994217: 1, 0.021945519372820854: 1, 0.9278587102890015: 1, -0.15388239920139313: 1, -0.19999556243419647: 1, 1.182131290435791: 1, 0.6152291297912598: 1, -0.2519146203994751: 1, -0.024461327120661736: 1, -0.06410756707191467: 1, -0.048064157366752625: 1, -0.24131079018115997: 1, -0.06601618975400925: 1, -0.3034926950931549: 1, -0.18796367943286896: 1, -0.9494366645812988: 1, -0.2265346348285675: 1, -0.9567206501960754: 1, -1.1260875463485718: 1, -0.056251220405101776: 1, 0.053155358880758286: 1, 0.26051849126815796: 1, -1.144519329071045: 1, -1.2016303539276123: 1, -1.0224525928497314: 1, -0.4338090121746063: 1, -0.4475323557853699: 1, -1.200749397277832: 1, -0.2236318439245224: 1, -1.1501390933990479: 1, -0.6588079929351807: 1, -1.194840431213379: 1, -0.7443411946296692: 1, -1.1509727239608765: 1, -1.2015254497528076: 1, 0.9692907333374023: 1, -0.6411266922950745: 1, -1.05448579788208: 1, -0.694696843624115: 1, -1.0281414985656738: 1, -0.9656466245651245: 1, -0.7228063344955444: 1, -1.0399388074874878: 1, -1.0223268270492554: 1, -0.5631559491157532: 1, -1.1923733949661255: 1, -1.1858601570129395: 1, -0.6137503981590271: 1, -1.1981604099273682: 1, -0.6249680519104004: 1, -1.2013036012649536: 1, -0.9966712594032288: 1, -1.1832531690597534: 1, -1.0323843955993652: 1, -1.1691778898239136: 1, -1.175873041152954: 1, -0.17838822305202484: 1, -1.2016000747680664: 1, -0.5804309248924255: 1, -1.201621413230896: 1, -1.180148720741272: 1, -0.862193763256073: 1, -1.1350377798080444: 1, -1.1470420360565186: 1, -0.788809597492218: 1, -0.5520062446594238: 1, -0.5385211110115051: 1, -0.712040364742279: 1, -1.1977088451385498: 1, -1.1981457471847534: 1, -1.0501618385314941: 1, -1.1036909818649292: 1, -1.0589720010757446: 1, -0.9021695256233215: 1, -0.7001152634620667: 1, -0.21827441453933716: 1, -0.7420345544815063: 1, -0.4824954569339752: 1, -1.1785725355148315: 1, 5.066896438598633: 1, -0.07361169159412384: 1, -1.1445417404174805: 1, 0.939198911190033: 1, 4.722275257110596: 1, -0.9046985507011414: 1, 4.863577365875244: 1, -0.9485399723052979: 1, -1.1986464262008667: 1, -1.0538722276687622: 1, -1.0186684131622314: 1, -1.0699774026870728: 1, -1.1212965250015259: 1, 0.29684698581695557: 1, -1.1940333843231201: 1, -1.167614459991455: 1, -1.1790684461593628: 1, -0.9539603590965271: 1, -0.38882899284362793: 1, 0.216363787651062: 1, -1.0434380769729614: 1, -1.189503788948059: 1, -1.1503796577453613: 1, -0.8791208267211914: 1, -0.9453350901603699: 1, -0.967963457107544: 1, -1.1468044519424438: 1, -0.4794164001941681: 1, -1.1905823945999146: 1, -1.2016135454177856: 1, -1.1961623430252075: 1, -0.5816406607627869: 1, -0.8111313581466675: 1, 0.6591589450836182: 1, -0.4003660976886749: 1, -1.2016377449035645: 1, -0.45371508598327637: 1, 0.009732205420732498: 1, -1.154268741607666: 1, 1.1463862657546997: 1, 1.0962333679199219: 1, 0.05797763168811798: 1, -0.8635501265525818: 1, -1.1626108884811401: 1, 1.5002496242523193: 1, -0.433001846075058: 1, 0.7936035394668579: 1, -0.1682627946138382: 1, -0.15451399981975555: 1, -0.8244988322257996: 1, -0.4005826711654663: 1, -1.2010202407836914: 1, -0.6496835947036743: 1, 1.282943606376648: 1, 1.0670119524002075: 1, 1.2671806812286377: 1, -0.5976389050483704: 1, -1.0180860757827759: 1, -0.4656817615032196: 1, 0.7670885920524597: 1, -0.30494359135627747: 1, 0.5858985185623169: 1, -1.1689379215240479: 1, 0.21256738901138306: 1, -0.3316475749015808: 1, 1.2256038188934326: 1, 1.0147548913955688: 1, 0.4960012137889862: 1, -0.8736492395401001: 1, -0.7019102573394775: 1, -0.23251420259475708: 1, 1.5164122581481934: 1, -1.1871360540390015: 1, -1.1995155811309814: 1, -1.1970906257629395: 1, -1.1950762271881104: 1, 1.6846264600753784: 1, -1.1811505556106567: 1, -1.1973918676376343: 1, -1.164048194885254: 1, -1.2002415657043457: 1, -0.7359880805015564: 1, -1.0963668823242188: 1, -0.6259949207305908: 1, -0.5079992413520813: 1, -1.1482324600219727: 1, -0.7622874975204468: 1, -1.1784441471099854: 1, -1.0944702625274658: 1, -0.7456731200218201: 1, -1.0986745357513428: 1, 1.7719837427139282: 1, -0.6460915207862854: 1, -1.1882411241531372: 1, -1.0081939697265625: 1, -0.6900844573974609: 1, 0.5599294900894165: 1, 0.8697875142097473: 1, -0.16204535961151123: 1, 0.5490202307701111: 1, -0.7970835566520691: 1, 0.29118791222572327: 1, 1.6113003492355347: 1, 0.8645955324172974: 1, 0.6052579283714294: 1, 0.6880602836608887: 1, -0.7167530059814453: 1, 0.3594037592411041: 1, -0.17054051160812378: 1, -1.1990872621536255: 1, -1.1948130130767822: 1, -0.04312499612569809: 1, 1.104429006576538: 1, -0.6482374668121338: 1, -1.0539400577545166: 1, 0.7825908064842224: 1, 0.5573470592498779: 1, 1.0482161045074463: 1, 0.31122344732284546: 1, -0.2946179509162903: 1, 1.4752575159072876: 1, 0.3024345636367798: 1, -0.2681446373462677: 1, 1.375723958015442: 1, 1.1509000062942505: 1, 0.02118554152548313: 1, 1.556430697441101: 1, 1.3231751918792725: 1, 1.0554699897766113: 1, 1.4972656965255737: 1, 0.6854439377784729: 1, 0.3157300651073456: 1, -0.812441349029541: 1, 1.2999117374420166: 1, 0.81052166223526: 1, 1.233654499053955: 1, 0.3554361164569855: 1, 1.3589857816696167: 1, 1.2155990600585938: 1, 1.3862555027008057: 1, 1.2513844966888428: 1, 1.5127235651016235: 1, 0.4243623912334442: 1, 0.8098611235618591: 1, 0.5611000061035156: 1, 0.2568971812725067: 1, 0.5472216606140137: 1, 0.11328306794166565: 1, 0.339995801448822: 1, 0.2816910445690155: 1, 1.4051384925842285: 1, -0.30866673588752747: 1, 0.822748601436615: 1, 0.340713769197464: 1, -0.11455459892749786: 1, 0.1928955614566803: 1, 0.35902467370033264: 1, 0.3425867557525635: 1, -0.11516252905130386: 1, 0.19674475491046906: 1, 1.533202886581421: 1, 1.3192821741104126: 1, 1.5988372564315796: 1, 1.4527193307876587: 1, 1.256608247756958: 1, -0.03138621151447296: 1, 0.21266861259937286: 1, 0.36868804693222046: 1, 0.19260334968566895: 1, 1.4282907247543335: 1, 0.11682117730379105: 1, 0.7675086855888367: 1, 1.3918044567108154: 1, -0.06798223406076431: 1, 0.2615222632884979: 1, 1.7802213430404663: 1, -0.3147757053375244: 1, 0.7623692750930786: 1, 0.24058645963668823: 1, -1.1871042251586914: 1, 0.5713070034980774: 1, 1.481839656829834: 1, 1.4727312326431274: 1, 0.18826737999916077: 1, 0.06706182658672333: 1, -0.0526600144803524: 1, 1.379611611366272: 1, 1.1887938976287842: 1, 0.3319496512413025: 1, 1.4751214981079102: 1, -0.16230207681655884: 1, 0.002965901279821992: 1, 0.2539007067680359: 1, -0.6454114317893982: 1, 0.8502970337867737: 1, -1.0057076215744019: 1, 0.9693878293037415: 1, 0.8500742316246033: 1, 0.8342987895011902: 1, -0.13838951289653778: 1, 0.7092652916908264: 1, -0.19083698093891144: 1, 1.5217971801757812: 1, -0.870884895324707: 1, 0.6448225975036621: 1, 0.7280606031417847: 1, 0.8865175843238831: 1, 1.3759030103683472: 1, 0.27865079045295715: 1, 0.7161386013031006: 1, 1.007346510887146: 1, -0.56696617603302: 1, 1.1847658157348633: 1, 0.7253832817077637: 1, 1.405086636543274: 1, -1.1504056453704834: 1, 0.041503969579935074: 1, 0.9053120613098145: 1, 0.8618729114532471: 1, -0.091583751142025: 1, 0.2824403941631317: 1, 0.3697844445705414: 1, -1.086830973625183: 1, -0.3465023636817932: 1, 0.6749281883239746: 1, 0.057195477187633514: 1, 0.39953649044036865: 1, 0.6570013761520386: 1, 0.638069212436676: 1, 0.49092090129852295: 1, -0.9460977911949158: 1, 1.0112850666046143: 1, -1.1999285221099854: 1, 0.8999701142311096: 1, 0.3074534237384796: 1, 1.556864857673645: 1, 0.3124358654022217: 1, 0.7275790572166443: 1, 0.31239357590675354: 1, 0.6463801264762878: 1, 1.6108869314193726: 1, 0.29415011405944824: 1, -0.16245944797992706: 1, -0.040548212826251984: 1, -0.2168547362089157: 1, -1.0716415643692017: 1, -0.1884830892086029: 1, 1.4151798486709595: 1, 0.5769325494766235: 1, 0.5241292119026184: 1, 1.7404301166534424: 1, -1.0314160585403442: 1, 0.8222253322601318: 1, -0.1565595418214798: 1, 0.3499395549297333: 1, -0.20047886669635773: 1, 2.4050354957580566: 1, -0.6223658919334412: 1, 0.9422100782394409: 1, 0.23464715480804443: 1, -0.6390724778175354: 1, 0.6513680219650269: 1, -1.024586796760559: 1, 1.5512200593948364: 1, 0.8943637013435364: 1, 1.1072840690612793: 1, 0.2389289289712906: 1, -1.1837621927261353: 1, 1.4069277048110962: 1, 0.24872112274169922: 1, -0.6971253156661987: 1, 1.4821670055389404: 1, 1.471611738204956: 1, 1.547389030456543: 1, 0.8533394932746887: 1, 1.5009726285934448: 1, 0.6865427494049072: 1, 0.3126457929611206: 1, -0.11868320405483246: 1, 1.368375539779663: 1, 0.8744557499885559: 1, 1.4529069662094116: 1, 0.3231067657470703: 1, 0.853833794593811: 1, 0.017385760322213173: 1, 0.23268936574459076: 1, -0.5749701261520386: 1, -0.37779542803764343: 1, -0.9053927063941956: 1, 0.6657525897026062: 1, 0.008748067542910576: 1, 1.2413678169250488: 1, 1.2822530269622803: 1, -0.39580973982810974: 1, 0.008864369243383408: 1, 1.5909359455108643: 1, -0.8882886171340942: 1, -0.9168434739112854: 1, 1.0945255756378174: 1, -0.621107280254364: 1, -0.007039392367005348: 1, -1.1247143745422363: 1, 0.24701066315174103: 1, -0.2671576142311096: 1, 0.9305616617202759: 1, 0.6777730584144592: 1, 0.4251300096511841: 1, -0.33581042289733887: 1, -0.14633353054523468: 1, -1.1963841915130615: 1, 0.08744630217552185: 1, 1.1960792541503906: 1, -0.8740671277046204: 1, 0.9394524693489075: 1, -1.1812138557434082: 1, -1.2005233764648438: 1, 0.6232529878616333: 1, 0.36468705534935: 1, 0.09383262693881989: 1, 0.015011060051620007: 1, 1.113309621810913: 1, 0.6233600974082947: 1, -1.2016253471374512: 1, -0.9032037854194641: 1, -0.16826894879341125: 1, 0.055386364459991455: 1, -1.1972460746765137: 1, 1.009254813194275: 1, -1.1656997203826904: 1, 0.033690646290779114: 1, 0.4205377995967865: 1, -0.816495954990387: 1, 0.4987215995788574: 1, -0.04175446555018425: 1, -1.2016297578811646: 1, -0.0449078269302845: 1, 1.0578463077545166: 1, 1.039072036743164: 1, 0.29174771904945374: 1, -0.11954380571842194: 1, 0.02054119110107422: 1, 1.2373515367507935: 1, -0.15267345309257507: 1, -0.23531334102153778: 1, -0.20500345528125763: 1, -0.11709770560264587: 1, -0.20062661170959473: 1, 1.0187321901321411: 1, -0.012521528638899326: 1, 1.1683013439178467: 1, 0.8318881988525391: 1, 0.5979693531990051: 1, -1.1211071014404297: 1, 0.23339834809303284: 1, 0.654328465461731: 1, -0.41674312949180603: 1, 0.838370680809021: 1, -0.18254651129245758: 1, 0.8476697206497192: 1, -0.5479841232299805: 1, 1.2188843488693237: 1, 1.2529374361038208: 1, 0.6535213589668274: 1, -1.1798354387283325: 1, -0.22866979241371155: 1, 1.2479115724563599: 1, 1.1942393779754639: 1, -1.2016375064849854: 1, 0.27843114733695984: 1, 0.6842910647392273: 1, 0.20136801898479462: 1, -0.41403406858444214: 1, -1.2016206979751587: 1, -0.21048426628112793: 1, 0.86496901512146: 1, -1.2015115022659302: 1, -0.5834662318229675: 1, -0.518195390701294: 1, -0.28742867708206177: 1, 0.9015107750892639: 1, -1.1490250825881958: 1, -1.2013384103775024: 1, -1.1954847574234009: 1, -1.2015154361724854: 1, -1.2016382217407227: 1, -1.1363192796707153: 1, -1.2015819549560547: 1, -1.1979888677597046: 1, -1.180970549583435: 1, -0.416104793548584: 1, -0.8910970091819763: 1, -0.11705746501684189: 1, 1.111115574836731: 1, 0.11165464669466019: 1, 0.7346283793449402: 1, 1.5724915266036987: 1, -0.06609977036714554: 1, -0.4757806956768036: 1, 0.26258811354637146: 1, -1.1736985445022583: 1, 0.021630888804793358: 1, 0.4845307767391205: 1, -1.140577793121338: 1, -0.0922759622335434: 1, -0.5093275308609009: 1, -0.8209242224693298: 1, -0.6426911950111389: 1, -1.1623584032058716: 1, -1.150158166885376: 1, -1.1923046112060547: 1, -1.201493740081787: 1, -0.14631414413452148: 1, -0.16829612851142883: 1, 1.0674824714660645: 1, -1.0237786769866943: 1, -0.24580752849578857: 1, -0.33075013756752014: 1, 0.29349300265312195: 1, 1.28579580783844: 1, 0.8610304594039917: 1, -0.37206733226776123: 1, 3.9672465324401855: 1, -1.1947468519210815: 1, -0.015705665573477745: 1, 0.9066123366355896: 1, 1.2916063070297241: 1, 1.2879470586776733: 1, -0.30726683139801025: 1, -0.9972583055496216: 1, 1.200035810470581: 1, -0.47594985365867615: 1, -0.1688508540391922: 1, -1.2016342878341675: 1, -0.3213060796260834: 1, -1.2014774084091187: 1, 0.1497194468975067: 1, -1.2013877630233765: 1, -1.1318936347961426: 1, -1.2015341520309448: 1, -1.2016384601593018: 1, 0.469992071390152: 1, -1.2016327381134033: 1, -1.1948002576828003: 1, -0.848010778427124: 1, 2.26233172416687: 1, 0.19668835401535034: 1, -1.201473593711853: 1, -1.2015316486358643: 1, -1.2015215158462524: 1, 0.1331777125597: 1, -1.1992485523223877: 1, -0.8653848767280579: 1}
INFO     Network type: REG
INFO     Model: "sequential_1"
INFO     _________________________________________________________________
INFO     Layer (type)                 Output Shape              Param #
INFO     =================================================================
INFO     dense_5 (Dense)              (None, 1024)              2098176
INFO     _________________________________________________________________
INFO     dropout_4 (Dropout)          (None, 1024)              0
INFO     _________________________________________________________________
INFO     dense_6 (Dense)              (None, 512)               524800
INFO     _________________________________________________________________
INFO     dropout_5 (Dropout)          (None, 512)               0
INFO     _________________________________________________________________
INFO     dense_7 (Dense)              (None, 256)               131328
INFO     _________________________________________________________________
INFO     dropout_6 (Dropout)          (None, 256)               0
INFO     _________________________________________________________________
INFO     dense_8 (Dense)              (None, 128)               32896
INFO     _________________________________________________________________
INFO     dropout_7 (Dropout)          (None, 128)               0
INFO     _________________________________________________________________
INFO     dense_9 (Dense)              (None, 1)                 129
INFO     =================================================================
INFO     Total params: 2,787,329
INFO     Trainable params: 2,787,329
INFO     Non-trainable params: 0
INFO     _________________________________________________________________
WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.
WARNING  `period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.
2024-07-15 16:21:47.509264: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0
2024-07-15 16:21:47.509404: I tensorflow/core/grappler/clusters/single_machine.cc:357] Starting new session
2024-07-15 16:21:47.510784: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:1137] Optimization results for grappler item: graph_to_optimize
  function_optimizer: function_optimizer did nothing. time = 0.005ms.
  function_optimizer: function_optimizer did nothing. time = 0.001ms.
22/22 - 2s - loss: 4.8443 - val_loss: 4.8194
Epoch 2/2000
22/22 - 1s - loss: 4.7471 - val_loss: 4.7399
Epoch 3/2000
22/22 - 1s - loss: 4.6863 - val_loss: 4.6894
Epoch 4/2000
22/22 - 1s - loss: 4.6484 - val_loss: 4.6579
Epoch 5/2000
22/22 - 1s - loss: 4.6219 - val_loss: 4.6345
Epoch 6/2000
22/22 - 1s - loss: 4.5993 - val_loss: 4.6137
Epoch 7/2000
22/22 - 1s - loss: 4.5770 - val_loss: 4.5930
Epoch 8/2000
22/22 - 1s - loss: 4.5570 - val_loss: 4.5738
Epoch 9/2000
22/22 - 1s - loss: 4.5356 - val_loss: 4.5556
Epoch 10/2000
22/22 - 1s - loss: 4.5170 - val_loss: 4.5382
Epoch 00010: val_loss improved from inf to 4.53821, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 11/2000
22/22 - 1s - loss: 4.4960 - val_loss: 4.5214
Epoch 12/2000
22/22 - 1s - loss: 4.4783 - val_loss: 4.5065
Epoch 13/2000
22/22 - 1s - loss: 4.4606 - val_loss: 4.4923
Epoch 14/2000
22/22 - 1s - loss: 4.4435 - val_loss: 4.4795
Epoch 15/2000
22/22 - 1s - loss: 4.4257 - val_loss: 4.4667
Epoch 16/2000
22/22 - 1s - loss: 4.4125 - val_loss: 4.4550
Epoch 17/2000
22/22 - 1s - loss: 4.3957 - val_loss: 4.4428
Epoch 18/2000
22/22 - 1s - loss: 4.3796 - val_loss: 4.4314
Epoch 19/2000
22/22 - 1s - loss: 4.3642 - val_loss: 4.4196
Epoch 20/2000
22/22 - 1s - loss: 4.3522 - val_loss: 4.4090
Epoch 00020: val_loss improved from 4.53821 to 4.40899, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 21/2000
22/22 - 1s - loss: 4.3349 - val_loss: 4.3981
Epoch 22/2000
22/22 - 1s - loss: 4.3220 - val_loss: 4.3883
Epoch 23/2000
22/22 - 1s - loss: 4.3051 - val_loss: 4.3770
Epoch 24/2000
22/22 - 1s - loss: 4.2912 - val_loss: 4.3667
Epoch 25/2000
22/22 - 1s - loss: 4.2759 - val_loss: 4.3577
Epoch 26/2000
22/22 - 1s - loss: 4.2638 - val_loss: 4.3468
Epoch 27/2000
22/22 - 1s - loss: 4.2509 - val_loss: 4.3380
Epoch 28/2000
22/22 - 1s - loss: 4.2393 - val_loss: 4.3296
Epoch 29/2000
22/22 - 1s - loss: 4.2235 - val_loss: 4.3186
Epoch 30/2000
22/22 - 1s - loss: 4.2119 - val_loss: 4.3093
Epoch 00030: val_loss improved from 4.40899 to 4.30932, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 31/2000
22/22 - 1s - loss: 4.1972 - val_loss: 4.3023
Epoch 32/2000
22/22 - 1s - loss: 4.1831 - val_loss: 4.2925
Epoch 33/2000
22/22 - 1s - loss: 4.1699 - val_loss: 4.2838
Epoch 34/2000
22/22 - 1s - loss: 4.1566 - val_loss: 4.2773
Epoch 35/2000
22/22 - 1s - loss: 4.1453 - val_loss: 4.2675
Epoch 36/2000
22/22 - 1s - loss: 4.1306 - val_loss: 4.2608
Epoch 37/2000
22/22 - 1s - loss: 4.1184 - val_loss: 4.2505
Epoch 38/2000
22/22 - 1s - loss: 4.1071 - val_loss: 4.2450
Epoch 39/2000
22/22 - 1s - loss: 4.0936 - val_loss: 4.2373
Epoch 40/2000
22/22 - 1s - loss: 4.0843 - val_loss: 4.2275
Epoch 00040: val_loss improved from 4.30932 to 4.22749, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 41/2000
22/22 - 1s - loss: 4.0690 - val_loss: 4.2210
Epoch 42/2000
22/22 - 1s - loss: 4.0576 - val_loss: 4.2130
Epoch 43/2000
22/22 - 1s - loss: 4.0455 - val_loss: 4.2058
Epoch 44/2000
22/22 - 1s - loss: 4.0360 - val_loss: 4.1994
Epoch 45/2000
22/22 - 1s - loss: 4.0238 - val_loss: 4.1910
Epoch 46/2000
22/22 - 1s - loss: 4.0152 - val_loss: 4.1842
Epoch 47/2000
22/22 - 1s - loss: 4.0027 - val_loss: 4.1765
Epoch 48/2000
22/22 - 1s - loss: 3.9907 - val_loss: 4.1699
Epoch 49/2000
22/22 - 1s - loss: 3.9808 - val_loss: 4.1641
Epoch 50/2000
22/22 - 1s - loss: 3.9693 - val_loss: 4.1574
Epoch 00050: val_loss improved from 4.22749 to 4.15742, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 51/2000
22/22 - 1s - loss: 3.9566 - val_loss: 4.1507
Epoch 52/2000
22/22 - 1s - loss: 3.9495 - val_loss: 4.1433
Epoch 53/2000
22/22 - 1s - loss: 3.9401 - val_loss: 4.1378
Epoch 54/2000
22/22 - 1s - loss: 3.9262 - val_loss: 4.1312
Epoch 55/2000
22/22 - 1s - loss: 3.9174 - val_loss: 4.1256
Epoch 56/2000
22/22 - 1s - loss: 3.9072 - val_loss: 4.1185
Epoch 57/2000
22/22 - 1s - loss: 3.8986 - val_loss: 4.1120
Epoch 58/2000
22/22 - 1s - loss: 3.8885 - val_loss: 4.1065
Epoch 59/2000
22/22 - 1s - loss: 3.8782 - val_loss: 4.1001
Epoch 60/2000
22/22 - 1s - loss: 3.8706 - val_loss: 4.0947
Epoch 00060: val_loss improved from 4.15742 to 4.09466, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 61/2000
22/22 - 1s - loss: 3.8621 - val_loss: 4.0875
Epoch 62/2000
22/22 - 1s - loss: 3.8511 - val_loss: 4.0815
Epoch 63/2000
22/22 - 1s - loss: 3.8419 - val_loss: 4.0758
Epoch 64/2000
22/22 - 1s - loss: 3.8301 - val_loss: 4.0698
Epoch 65/2000
22/22 - 1s - loss: 3.8251 - val_loss: 4.0641
Epoch 66/2000
22/22 - 1s - loss: 3.8146 - val_loss: 4.0564
Epoch 67/2000
22/22 - 1s - loss: 3.8042 - val_loss: 4.0514
Epoch 68/2000
22/22 - 1s - loss: 3.7980 - val_loss: 4.0460
Epoch 69/2000
22/22 - 1s - loss: 3.7908 - val_loss: 4.0405
Epoch 70/2000
22/22 - 1s - loss: 3.7837 - val_loss: 4.0341
Epoch 00070: val_loss improved from 4.09466 to 4.03407, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 71/2000
22/22 - 1s - loss: 3.7762 - val_loss: 4.0275
Epoch 72/2000
22/22 - 1s - loss: 3.7653 - val_loss: 4.0223
Epoch 73/2000
22/22 - 1s - loss: 3.7536 - val_loss: 4.0176
Epoch 74/2000
22/22 - 1s - loss: 3.7507 - val_loss: 4.0122
Epoch 75/2000
22/22 - 1s - loss: 3.7376 - val_loss: 4.0064
Epoch 76/2000
22/22 - 1s - loss: 3.7285 - val_loss: 4.0018
Epoch 77/2000
22/22 - 1s - loss: 3.7249 - val_loss: 3.9964
Epoch 78/2000
22/22 - 1s - loss: 3.7184 - val_loss: 3.9900
Epoch 79/2000
22/22 - 1s - loss: 3.7088 - val_loss: 3.9847
Epoch 80/2000
22/22 - 1s - loss: 3.7026 - val_loss: 3.9791
Epoch 00080: val_loss improved from 4.03407 to 3.97908, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 81/2000
22/22 - 1s - loss: 3.6933 - val_loss: 3.9729
Epoch 82/2000
22/22 - 1s - loss: 3.6863 - val_loss: 3.9686
Epoch 83/2000
22/22 - 1s - loss: 3.6801 - val_loss: 3.9618
Epoch 84/2000
22/22 - 1s - loss: 3.6720 - val_loss: 3.9559
Epoch 85/2000
22/22 - 1s - loss: 3.6630 - val_loss: 3.9513
Epoch 86/2000
22/22 - 1s - loss: 3.6556 - val_loss: 3.9460
Epoch 87/2000
22/22 - 1s - loss: 3.6485 - val_loss: 3.9419
Epoch 88/2000
22/22 - 1s - loss: 3.6443 - val_loss: 3.9365
Epoch 89/2000
22/22 - 1s - loss: 3.6378 - val_loss: 3.9321
Epoch 90/2000
22/22 - 1s - loss: 3.6279 - val_loss: 3.9267
Epoch 00090: val_loss improved from 3.97908 to 3.92666, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 91/2000
22/22 - 1s - loss: 3.6228 - val_loss: 3.9216
Epoch 92/2000
22/22 - 1s - loss: 3.6145 - val_loss: 3.9161
Epoch 93/2000
22/22 - 1s - loss: 3.6083 - val_loss: 3.9113
Epoch 94/2000
22/22 - 1s - loss: 3.6020 - val_loss: 3.9062
Epoch 95/2000
22/22 - 1s - loss: 3.5966 - val_loss: 3.9017
Epoch 96/2000
22/22 - 1s - loss: 3.5907 - val_loss: 3.8973
Epoch 97/2000
22/22 - 1s - loss: 3.5847 - val_loss: 3.8924
Epoch 98/2000
22/22 - 1s - loss: 3.5756 - val_loss: 3.8881
Epoch 99/2000
22/22 - 1s - loss: 3.5720 - val_loss: 3.8827
Epoch 100/2000
22/22 - 1s - loss: 3.5597 - val_loss: 3.8765
Epoch 00100: val_loss improved from 3.92666 to 3.87647, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 101/2000
22/22 - 1s - loss: 3.5568 - val_loss: 3.8714
Epoch 102/2000
22/22 - 1s - loss: 3.5495 - val_loss: 3.8662
Epoch 103/2000
22/22 - 1s - loss: 3.5454 - val_loss: 3.8620
Epoch 104/2000
22/22 - 1s - loss: 3.5408 - val_loss: 3.8581
Epoch 105/2000
22/22 - 1s - loss: 3.5327 - val_loss: 3.8533
Epoch 106/2000
22/22 - 1s - loss: 3.5267 - val_loss: 3.8492
Epoch 107/2000
22/22 - 1s - loss: 3.5195 - val_loss: 3.8453
Epoch 108/2000
22/22 - 1s - loss: 3.5170 - val_loss: 3.8398
Epoch 109/2000
22/22 - 1s - loss: 3.5080 - val_loss: 3.8365
Epoch 110/2000
22/22 - 1s - loss: 3.5036 - val_loss: 3.8327
Epoch 00110: val_loss improved from 3.87647 to 3.83269, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 111/2000
22/22 - 1s - loss: 3.4995 - val_loss: 3.8262
Epoch 112/2000
22/22 - 1s - loss: 3.4920 - val_loss: 3.8208
Epoch 113/2000
22/22 - 1s - loss: 3.4865 - val_loss: 3.8168
Epoch 114/2000
22/22 - 1s - loss: 3.4810 - val_loss: 3.8132
Epoch 115/2000
22/22 - 1s - loss: 3.4769 - val_loss: 3.8083
Epoch 116/2000
22/22 - 1s - loss: 3.4680 - val_loss: 3.8047
Epoch 117/2000
22/22 - 1s - loss: 3.4651 - val_loss: 3.7991
Epoch 118/2000
22/22 - 1s - loss: 3.4561 - val_loss: 3.7945
Epoch 119/2000
22/22 - 1s - loss: 3.4543 - val_loss: 3.7904
Epoch 120/2000
22/22 - 1s - loss: 3.4462 - val_loss: 3.7861
Epoch 00120: val_loss improved from 3.83269 to 3.78611, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 121/2000
22/22 - 1s - loss: 3.4423 - val_loss: 3.7814
Epoch 122/2000
22/22 - 1s - loss: 3.4371 - val_loss: 3.7765
Epoch 123/2000
22/22 - 1s - loss: 3.4320 - val_loss: 3.7719
Epoch 124/2000
22/22 - 1s - loss: 3.4241 - val_loss: 3.7664
Epoch 125/2000
22/22 - 1s - loss: 3.4205 - val_loss: 3.7628
Epoch 126/2000
22/22 - 1s - loss: 3.4145 - val_loss: 3.7587
Epoch 127/2000
22/22 - 1s - loss: 3.4102 - val_loss: 3.7540
Epoch 128/2000
22/22 - 1s - loss: 3.4023 - val_loss: 3.7507
Epoch 129/2000
22/22 - 1s - loss: 3.3980 - val_loss: 3.7479
Epoch 130/2000
22/22 - 1s - loss: 3.3964 - val_loss: 3.7432
Epoch 00130: val_loss improved from 3.78611 to 3.74317, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 131/2000
22/22 - 1s - loss: 3.3906 - val_loss: 3.7387
Epoch 132/2000
22/22 - 1s - loss: 3.3884 - val_loss: 3.7343
Epoch 133/2000
22/22 - 1s - loss: 3.3795 - val_loss: 3.7309
Epoch 134/2000
22/22 - 1s - loss: 3.3751 - val_loss: 3.7259
Epoch 135/2000
22/22 - 1s - loss: 3.3715 - val_loss: 3.7218
Epoch 136/2000
22/22 - 1s - loss: 3.3687 - val_loss: 3.7182
Epoch 137/2000
22/22 - 1s - loss: 3.3611 - val_loss: 3.7147
Epoch 138/2000
22/22 - 1s - loss: 3.3530 - val_loss: 3.7091
Epoch 139/2000
22/22 - 1s - loss: 3.3472 - val_loss: 3.7053
Epoch 140/2000
22/22 - 1s - loss: 3.3468 - val_loss: 3.7004
Epoch 00140: val_loss improved from 3.74317 to 3.70037, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 141/2000
22/22 - 1s - loss: 3.3405 - val_loss: 3.6971
Epoch 142/2000
22/22 - 1s - loss: 3.3345 - val_loss: 3.6933
Epoch 143/2000
22/22 - 1s - loss: 3.3312 - val_loss: 3.6889
Epoch 144/2000
22/22 - 1s - loss: 3.3268 - val_loss: 3.6846
Epoch 145/2000
22/22 - 1s - loss: 3.3248 - val_loss: 3.6801
Epoch 146/2000
22/22 - 1s - loss: 3.3170 - val_loss: 3.6759
Epoch 147/2000
22/22 - 1s - loss: 3.3122 - val_loss: 3.6716
Epoch 148/2000
22/22 - 1s - loss: 3.3061 - val_loss: 3.6677
Epoch 149/2000
22/22 - 1s - loss: 3.3035 - val_loss: 3.6632
Epoch 150/2000
22/22 - 1s - loss: 3.3002 - val_loss: 3.6595
Epoch 00150: val_loss improved from 3.70037 to 3.65955, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 151/2000
22/22 - 1s - loss: 3.2933 - val_loss: 3.6560
Epoch 152/2000
22/22 - 1s - loss: 3.2883 - val_loss: 3.6522
Epoch 153/2000
22/22 - 1s - loss: 3.2845 - val_loss: 3.6489
Epoch 154/2000
22/22 - 1s - loss: 3.2801 - val_loss: 3.6445
Epoch 155/2000
22/22 - 1s - loss: 3.2769 - val_loss: 3.6409
Epoch 156/2000
22/22 - 1s - loss: 3.2736 - val_loss: 3.6375
Epoch 157/2000
22/22 - 1s - loss: 3.2718 - val_loss: 3.6341
Epoch 158/2000
22/22 - 1s - loss: 3.2625 - val_loss: 3.6314
Epoch 159/2000
22/22 - 1s - loss: 3.2606 - val_loss: 3.6270
Epoch 160/2000
22/22 - 1s - loss: 3.2522 - val_loss: 3.6223
Epoch 00160: val_loss improved from 3.65955 to 3.62228, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 161/2000
22/22 - 1s - loss: 3.2517 - val_loss: 3.6188
Epoch 162/2000
22/22 - 1s - loss: 3.2475 - val_loss: 3.6147
Epoch 163/2000
22/22 - 1s - loss: 3.2417 - val_loss: 3.6116
Epoch 164/2000
22/22 - 1s - loss: 3.2359 - val_loss: 3.6076
Epoch 165/2000
22/22 - 1s - loss: 3.2340 - val_loss: 3.6044
Epoch 166/2000
22/22 - 1s - loss: 3.2260 - val_loss: 3.6003
Epoch 167/2000
22/22 - 1s - loss: 3.2245 - val_loss: 3.5968
Epoch 168/2000
22/22 - 1s - loss: 3.2233 - val_loss: 3.5933
Epoch 169/2000
22/22 - 1s - loss: 3.2153 - val_loss: 3.5896
Epoch 170/2000
22/22 - 1s - loss: 3.2119 - val_loss: 3.5854
Epoch 00170: val_loss improved from 3.62228 to 3.58542, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 171/2000
22/22 - 1s - loss: 3.2082 - val_loss: 3.5814
Epoch 172/2000
22/22 - 1s - loss: 3.2043 - val_loss: 3.5782
Epoch 173/2000
22/22 - 1s - loss: 3.1964 - val_loss: 3.5744
Epoch 174/2000
22/22 - 1s - loss: 3.1977 - val_loss: 3.5716
Epoch 175/2000
22/22 - 1s - loss: 3.1917 - val_loss: 3.5684
Epoch 176/2000
22/22 - 1s - loss: 3.1852 - val_loss: 3.5645
Epoch 177/2000
22/22 - 1s - loss: 3.1826 - val_loss: 3.5605
Epoch 178/2000
22/22 - 1s - loss: 3.1799 - val_loss: 3.5553
Epoch 179/2000
22/22 - 1s - loss: 3.1756 - val_loss: 3.5518
Epoch 180/2000
22/22 - 1s - loss: 3.1709 - val_loss: 3.5483
Epoch 00180: val_loss improved from 3.58542 to 3.54832, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 181/2000
22/22 - 1s - loss: 3.1687 - val_loss: 3.5460
Epoch 182/2000
22/22 - 1s - loss: 3.1651 - val_loss: 3.5426
Epoch 183/2000
22/22 - 1s - loss: 3.1629 - val_loss: 3.5387
Epoch 184/2000
22/22 - 1s - loss: 3.1562 - val_loss: 3.5344
Epoch 185/2000
22/22 - 1s - loss: 3.1506 - val_loss: 3.5307
Epoch 186/2000
22/22 - 1s - loss: 3.1476 - val_loss: 3.5284
Epoch 187/2000
22/22 - 1s - loss: 3.1417 - val_loss: 3.5247
Epoch 188/2000
22/22 - 1s - loss: 3.1398 - val_loss: 3.5212
Epoch 189/2000
22/22 - 1s - loss: 3.1377 - val_loss: 3.5179
Epoch 190/2000
22/22 - 1s - loss: 3.1336 - val_loss: 3.5133
Epoch 00190: val_loss improved from 3.54832 to 3.51325, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 191/2000
22/22 - 1s - loss: 3.1320 - val_loss: 3.5099
Epoch 192/2000
22/22 - 1s - loss: 3.1243 - val_loss: 3.5068
Epoch 193/2000
22/22 - 1s - loss: 3.1239 - val_loss: 3.5031
Epoch 194/2000
22/22 - 1s - loss: 3.1159 - val_loss: 3.5000
Epoch 195/2000
22/22 - 1s - loss: 3.1122 - val_loss: 3.4966
Epoch 196/2000
22/22 - 1s - loss: 3.1106 - val_loss: 3.4936
Epoch 197/2000
22/22 - 1s - loss: 3.1058 - val_loss: 3.4891
Epoch 198/2000
22/22 - 1s - loss: 3.1002 - val_loss: 3.4850
Epoch 199/2000
22/22 - 1s - loss: 3.0973 - val_loss: 3.4827
Epoch 200/2000
22/22 - 1s - loss: 3.0966 - val_loss: 3.4797
Epoch 00200: val_loss improved from 3.51325 to 3.47974, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 201/2000
22/22 - 1s - loss: 3.0918 - val_loss: 3.4761
Epoch 202/2000
22/22 - 1s - loss: 3.0872 - val_loss: 3.4721
Epoch 203/2000
22/22 - 1s - loss: 3.0833 - val_loss: 3.4686
Epoch 204/2000
22/22 - 1s - loss: 3.0805 - val_loss: 3.4656
Epoch 205/2000
22/22 - 1s - loss: 3.0762 - val_loss: 3.4627
Epoch 206/2000
22/22 - 1s - loss: 3.0731 - val_loss: 3.4586
Epoch 207/2000
22/22 - 1s - loss: 3.0668 - val_loss: 3.4552
Epoch 208/2000
22/22 - 1s - loss: 3.0658 - val_loss: 3.4518
Epoch 209/2000
22/22 - 1s - loss: 3.0606 - val_loss: 3.4491
Epoch 210/2000
22/22 - 1s - loss: 3.0575 - val_loss: 3.4465
Epoch 00210: val_loss improved from 3.47974 to 3.44654, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 211/2000
22/22 - 1s - loss: 3.0548 - val_loss: 3.4432
Epoch 212/2000
22/22 - 1s - loss: 3.0495 - val_loss: 3.4403
Epoch 213/2000
22/22 - 1s - loss: 3.0459 - val_loss: 3.4372
Epoch 214/2000
22/22 - 1s - loss: 3.0433 - val_loss: 3.4344
Epoch 215/2000
22/22 - 1s - loss: 3.0400 - val_loss: 3.4311
Epoch 216/2000
22/22 - 1s - loss: 3.0354 - val_loss: 3.4268
Epoch 217/2000
22/22 - 1s - loss: 3.0341 - val_loss: 3.4235
Epoch 218/2000
22/22 - 1s - loss: 3.0304 - val_loss: 3.4206
Epoch 219/2000
22/22 - 1s - loss: 3.0269 - val_loss: 3.4179
Epoch 220/2000
22/22 - 1s - loss: 3.0228 - val_loss: 3.4148
Epoch 00220: val_loss improved from 3.44654 to 3.41482, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 221/2000
22/22 - 1s - loss: 3.0221 - val_loss: 3.4122
Epoch 222/2000
22/22 - 1s - loss: 3.0154 - val_loss: 3.4088
Epoch 223/2000
22/22 - 1s - loss: 3.0139 - val_loss: 3.4049
Epoch 224/2000
22/22 - 1s - loss: 3.0094 - val_loss: 3.4009
Epoch 225/2000
22/22 - 1s - loss: 3.0054 - val_loss: 3.3977
Epoch 226/2000
22/22 - 1s - loss: 3.0016 - val_loss: 3.3959
Epoch 227/2000
22/22 - 1s - loss: 3.0004 - val_loss: 3.3918
Epoch 228/2000
22/22 - 1s - loss: 2.9952 - val_loss: 3.3890
Epoch 229/2000
22/22 - 1s - loss: 2.9897 - val_loss: 3.3850
Epoch 230/2000
22/22 - 1s - loss: 2.9863 - val_loss: 3.3817
Epoch 00230: val_loss improved from 3.41482 to 3.38172, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 231/2000
22/22 - 1s - loss: 2.9809 - val_loss: 3.3778
Epoch 232/2000
22/22 - 1s - loss: 2.9794 - val_loss: 3.3756
Epoch 233/2000
22/22 - 1s - loss: 2.9809 - val_loss: 3.3733
Epoch 234/2000
22/22 - 1s - loss: 2.9744 - val_loss: 3.3705
Epoch 235/2000
22/22 - 1s - loss: 2.9684 - val_loss: 3.3677
Epoch 236/2000
22/22 - 1s - loss: 2.9696 - val_loss: 3.3634
Epoch 237/2000
22/22 - 1s - loss: 2.9660 - val_loss: 3.3598
Epoch 238/2000
22/22 - 1s - loss: 2.9609 - val_loss: 3.3570
Epoch 239/2000
22/22 - 1s - loss: 2.9591 - val_loss: 3.3546
Epoch 240/2000
22/22 - 1s - loss: 2.9521 - val_loss: 3.3515
Epoch 00240: val_loss improved from 3.38172 to 3.35154, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 241/2000
22/22 - 1s - loss: 2.9527 - val_loss: 3.3479
Epoch 242/2000
22/22 - 1s - loss: 2.9478 - val_loss: 3.3445
Epoch 243/2000
22/22 - 1s - loss: 2.9455 - val_loss: 3.3433
Epoch 244/2000
22/22 - 1s - loss: 2.9394 - val_loss: 3.3399
Epoch 245/2000
22/22 - 1s - loss: 2.9380 - val_loss: 3.3357
Epoch 246/2000
22/22 - 1s - loss: 2.9366 - val_loss: 3.3333
Epoch 247/2000
22/22 - 1s - loss: 2.9323 - val_loss: 3.3300
Epoch 248/2000
22/22 - 1s - loss: 2.9274 - val_loss: 3.3266
Epoch 249/2000
22/22 - 1s - loss: 2.9267 - val_loss: 3.3249
Epoch 250/2000
22/22 - 1s - loss: 2.9243 - val_loss: 3.3231
Epoch 00250: val_loss improved from 3.35154 to 3.32306, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 251/2000
22/22 - 1s - loss: 2.9191 - val_loss: 3.3196
Epoch 252/2000
22/22 - 1s - loss: 2.9161 - val_loss: 3.3159
Epoch 253/2000
22/22 - 1s - loss: 2.9101 - val_loss: 3.3137
Epoch 254/2000
22/22 - 1s - loss: 2.9089 - val_loss: 3.3104
Epoch 255/2000
22/22 - 1s - loss: 2.9078 - val_loss: 3.3066
Epoch 256/2000
22/22 - 1s - loss: 2.9033 - val_loss: 3.3032
Epoch 257/2000
22/22 - 1s - loss: 2.9017 - val_loss: 3.3010
Epoch 258/2000
22/22 - 1s - loss: 2.8958 - val_loss: 3.2980
Epoch 259/2000
22/22 - 1s - loss: 2.8956 - val_loss: 3.2946
Epoch 260/2000
22/22 - 1s - loss: 2.8899 - val_loss: 3.2919
Epoch 00260: val_loss improved from 3.32306 to 3.29192, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 261/2000
22/22 - 1s - loss: 2.8850 - val_loss: 3.2894
Epoch 262/2000
22/22 - 1s - loss: 2.8835 - val_loss: 3.2872
Epoch 263/2000
22/22 - 1s - loss: 2.8815 - val_loss: 3.2850
Epoch 264/2000
22/22 - 1s - loss: 2.8798 - val_loss: 3.2812
Epoch 265/2000
22/22 - 1s - loss: 2.8741 - val_loss: 3.2788
Epoch 266/2000
22/22 - 1s - loss: 2.8742 - val_loss: 3.2748
Epoch 267/2000
22/22 - 1s - loss: 2.8689 - val_loss: 3.2725
Epoch 268/2000
22/22 - 1s - loss: 2.8646 - val_loss: 3.2677
Epoch 269/2000
22/22 - 1s - loss: 2.8606 - val_loss: 3.2646
Epoch 270/2000
22/22 - 1s - loss: 2.8579 - val_loss: 3.2614
Epoch 00270: val_loss improved from 3.29192 to 3.26136, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 271/2000
22/22 - 1s - loss: 2.8591 - val_loss: 3.2591
Epoch 272/2000
22/22 - 1s - loss: 2.8525 - val_loss: 3.2568
Epoch 273/2000
22/22 - 1s - loss: 2.8531 - val_loss: 3.2539
Epoch 274/2000
22/22 - 1s - loss: 2.8486 - val_loss: 3.2503
Epoch 275/2000
22/22 - 1s - loss: 2.8419 - val_loss: 3.2477
Epoch 276/2000
22/22 - 1s - loss: 2.8392 - val_loss: 3.2452
Epoch 277/2000
22/22 - 1s - loss: 2.8422 - val_loss: 3.2428
Epoch 278/2000
22/22 - 1s - loss: 2.8349 - val_loss: 3.2406
Epoch 279/2000
22/22 - 1s - loss: 2.8323 - val_loss: 3.2388
Epoch 280/2000
22/22 - 1s - loss: 2.8279 - val_loss: 3.2358
Epoch 00280: val_loss improved from 3.26136 to 3.23576, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 281/2000
22/22 - 1s - loss: 2.8267 - val_loss: 3.2318
Epoch 282/2000
22/22 - 1s - loss: 2.8265 - val_loss: 3.2285
Epoch 283/2000
22/22 - 1s - loss: 2.8201 - val_loss: 3.2267
Epoch 284/2000
22/22 - 1s - loss: 2.8189 - val_loss: 3.2228
Epoch 285/2000
22/22 - 1s - loss: 2.8170 - val_loss: 3.2201
Epoch 286/2000
22/22 - 1s - loss: 2.8110 - val_loss: 3.2169
Epoch 287/2000
22/22 - 1s - loss: 2.8122 - val_loss: 3.2150
Epoch 288/2000
22/22 - 1s - loss: 2.8058 - val_loss: 3.2123
Epoch 289/2000
22/22 - 1s - loss: 2.8046 - val_loss: 3.2091
Epoch 290/2000
22/22 - 1s - loss: 2.7990 - val_loss: 3.2059
Epoch 00290: val_loss improved from 3.23576 to 3.20590, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 291/2000
22/22 - 1s - loss: 2.7959 - val_loss: 3.2035
Epoch 292/2000
22/22 - 1s - loss: 2.7938 - val_loss: 3.2019
Epoch 293/2000
22/22 - 1s - loss: 2.7920 - val_loss: 3.1989
Epoch 294/2000
22/22 - 1s - loss: 2.7855 - val_loss: 3.1949
Epoch 295/2000
22/22 - 1s - loss: 2.7828 - val_loss: 3.1930
Epoch 296/2000
22/22 - 1s - loss: 2.7820 - val_loss: 3.1907
Epoch 297/2000
22/22 - 1s - loss: 2.7780 - val_loss: 3.1878
Epoch 298/2000
22/22 - 1s - loss: 2.7762 - val_loss: 3.1844
Epoch 299/2000
22/22 - 1s - loss: 2.7771 - val_loss: 3.1811
Epoch 300/2000
22/22 - 1s - loss: 2.7671 - val_loss: 3.1779
Epoch 00300: val_loss improved from 3.20590 to 3.17794, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 301/2000
22/22 - 1s - loss: 2.7680 - val_loss: 3.1758
Epoch 302/2000
22/22 - 1s - loss: 2.7609 - val_loss: 3.1736
Epoch 303/2000
22/22 - 1s - loss: 2.7619 - val_loss: 3.1709
Epoch 304/2000
22/22 - 1s - loss: 2.7595 - val_loss: 3.1688
Epoch 305/2000
22/22 - 1s - loss: 2.7552 - val_loss: 3.1659
Epoch 306/2000
22/22 - 1s - loss: 2.7549 - val_loss: 3.1633
Epoch 307/2000
22/22 - 1s - loss: 2.7502 - val_loss: 3.1607
Epoch 308/2000
22/22 - 1s - loss: 2.7509 - val_loss: 3.1572
Epoch 309/2000
22/22 - 1s - loss: 2.7448 - val_loss: 3.1551
Epoch 310/2000
22/22 - 1s - loss: 2.7445 - val_loss: 3.1516
Epoch 00310: val_loss improved from 3.17794 to 3.15162, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 311/2000
22/22 - 1s - loss: 2.7406 - val_loss: 3.1486
Epoch 312/2000
22/22 - 1s - loss: 2.7388 - val_loss: 3.1474
Epoch 313/2000
22/22 - 1s - loss: 2.7332 - val_loss: 3.1440
Epoch 314/2000
22/22 - 1s - loss: 2.7322 - val_loss: 3.1420
Epoch 315/2000
22/22 - 1s - loss: 2.7282 - val_loss: 3.1374
Epoch 316/2000
22/22 - 1s - loss: 2.7261 - val_loss: 3.1351
Epoch 317/2000
22/22 - 1s - loss: 2.7217 - val_loss: 3.1318
Epoch 318/2000
22/22 - 1s - loss: 2.7212 - val_loss: 3.1299
Epoch 319/2000
22/22 - 1s - loss: 2.7196 - val_loss: 3.1282
Epoch 320/2000
22/22 - 1s - loss: 2.7152 - val_loss: 3.1255
Epoch 00320: val_loss improved from 3.15162 to 3.12554, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 321/2000
22/22 - 1s - loss: 2.7116 - val_loss: 3.1226
Epoch 322/2000
22/22 - 1s - loss: 2.7097 - val_loss: 3.1193
Epoch 323/2000
22/22 - 1s - loss: 2.7087 - val_loss: 3.1160
Epoch 324/2000
22/22 - 1s - loss: 2.7043 - val_loss: 3.1131
Epoch 325/2000
22/22 - 1s - loss: 2.7001 - val_loss: 3.1115
Epoch 326/2000
22/22 - 1s - loss: 2.6973 - val_loss: 3.1093
Epoch 327/2000
22/22 - 1s - loss: 2.6970 - val_loss: 3.1059
Epoch 328/2000
22/22 - 1s - loss: 2.6913 - val_loss: 3.1029
Epoch 329/2000
22/22 - 1s - loss: 2.6898 - val_loss: 3.1011
Epoch 330/2000
22/22 - 1s - loss: 2.6874 - val_loss: 3.0989
Epoch 00330: val_loss improved from 3.12554 to 3.09891, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 331/2000
22/22 - 1s - loss: 2.6830 - val_loss: 3.0963
Epoch 332/2000
22/22 - 1s - loss: 2.6812 - val_loss: 3.0941
Epoch 333/2000
22/22 - 1s - loss: 2.6805 - val_loss: 3.0907
Epoch 334/2000
22/22 - 1s - loss: 2.6780 - val_loss: 3.0877
Epoch 335/2000
22/22 - 1s - loss: 2.6751 - val_loss: 3.0856
Epoch 336/2000
22/22 - 1s - loss: 2.6715 - val_loss: 3.0830
Epoch 337/2000
22/22 - 1s - loss: 2.6696 - val_loss: 3.0812
Epoch 338/2000
22/22 - 1s - loss: 2.6679 - val_loss: 3.0806
Epoch 339/2000
22/22 - 1s - loss: 2.6625 - val_loss: 3.0767
Epoch 340/2000
22/22 - 1s - loss: 2.6594 - val_loss: 3.0734
Epoch 00340: val_loss improved from 3.09891 to 3.07336, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 341/2000
22/22 - 1s - loss: 2.6586 - val_loss: 3.0703
Epoch 342/2000
22/22 - 1s - loss: 2.6553 - val_loss: 3.0678
Epoch 343/2000
22/22 - 1s - loss: 2.6517 - val_loss: 3.0649
Epoch 344/2000
22/22 - 1s - loss: 2.6520 - val_loss: 3.0630
Epoch 345/2000
22/22 - 1s - loss: 2.6463 - val_loss: 3.0604
Epoch 346/2000
22/22 - 1s - loss: 2.6449 - val_loss: 3.0572
Epoch 347/2000
22/22 - 1s - loss: 2.6437 - val_loss: 3.0546
Epoch 348/2000
22/22 - 1s - loss: 2.6385 - val_loss: 3.0526
Epoch 349/2000
22/22 - 1s - loss: 2.6361 - val_loss: 3.0490
Epoch 350/2000
22/22 - 1s - loss: 2.6338 - val_loss: 3.0466
Epoch 00350: val_loss improved from 3.07336 to 3.04664, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 351/2000
22/22 - 1s - loss: 2.6325 - val_loss: 3.0448
Epoch 352/2000
22/22 - 1s - loss: 2.6295 - val_loss: 3.0426
Epoch 353/2000
22/22 - 1s - loss: 2.6275 - val_loss: 3.0407
Epoch 354/2000
22/22 - 1s - loss: 2.6220 - val_loss: 3.0390
Epoch 355/2000
22/22 - 1s - loss: 2.6200 - val_loss: 3.0358
Epoch 356/2000
22/22 - 1s - loss: 2.6211 - val_loss: 3.0330
Epoch 357/2000
22/22 - 1s - loss: 2.6137 - val_loss: 3.0300
Epoch 358/2000
22/22 - 1s - loss: 2.6124 - val_loss: 3.0271
Epoch 359/2000
22/22 - 1s - loss: 2.6100 - val_loss: 3.0251
Epoch 360/2000
22/22 - 1s - loss: 2.6070 - val_loss: 3.0218
Epoch 00360: val_loss improved from 3.04664 to 3.02178, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 361/2000
22/22 - 1s - loss: 2.6045 - val_loss: 3.0196
Epoch 362/2000
22/22 - 1s - loss: 2.6026 - val_loss: 3.0180
Epoch 363/2000
22/22 - 1s - loss: 2.6002 - val_loss: 3.0151
Epoch 364/2000
22/22 - 1s - loss: 2.5996 - val_loss: 3.0123
Epoch 365/2000
22/22 - 1s - loss: 2.5951 - val_loss: 3.0099
Epoch 366/2000
22/22 - 1s - loss: 2.5934 - val_loss: 3.0064
Epoch 367/2000
22/22 - 1s - loss: 2.5912 - val_loss: 3.0045
Epoch 368/2000
22/22 - 1s - loss: 2.5874 - val_loss: 3.0022
Epoch 369/2000
22/22 - 1s - loss: 2.5839 - val_loss: 3.0001
Epoch 370/2000
22/22 - 1s - loss: 2.5824 - val_loss: 2.9979
Epoch 00370: val_loss improved from 3.02178 to 2.99789, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 371/2000
22/22 - 1s - loss: 2.5789 - val_loss: 2.9957
Epoch 372/2000
22/22 - 1s - loss: 2.5768 - val_loss: 2.9934
Epoch 373/2000
22/22 - 1s - loss: 2.5747 - val_loss: 2.9913
Epoch 374/2000
22/22 - 1s - loss: 2.5741 - val_loss: 2.9885
Epoch 375/2000
22/22 - 1s - loss: 2.5695 - val_loss: 2.9861
Epoch 376/2000
22/22 - 1s - loss: 2.5664 - val_loss: 2.9833
Epoch 377/2000
22/22 - 1s - loss: 2.5673 - val_loss: 2.9804
Epoch 378/2000
22/22 - 1s - loss: 2.5632 - val_loss: 2.9778
Epoch 379/2000
22/22 - 1s - loss: 2.5615 - val_loss: 2.9763
Epoch 380/2000
22/22 - 1s - loss: 2.5597 - val_loss: 2.9744
Epoch 00380: val_loss improved from 2.99789 to 2.97436, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 381/2000
22/22 - 1s - loss: 2.5539 - val_loss: 2.9723
Epoch 382/2000
22/22 - 1s - loss: 2.5515 - val_loss: 2.9695
Epoch 383/2000
22/22 - 1s - loss: 2.5519 - val_loss: 2.9674
Epoch 384/2000
22/22 - 1s - loss: 2.5487 - val_loss: 2.9642
Epoch 385/2000
22/22 - 1s - loss: 2.5433 - val_loss: 2.9617
Epoch 386/2000
22/22 - 1s - loss: 2.5429 - val_loss: 2.9598
Epoch 387/2000
22/22 - 1s - loss: 2.5413 - val_loss: 2.9573
Epoch 388/2000
22/22 - 1s - loss: 2.5391 - val_loss: 2.9552
Epoch 389/2000
22/22 - 1s - loss: 2.5351 - val_loss: 2.9529
Epoch 390/2000
22/22 - 1s - loss: 2.5352 - val_loss: 2.9504
Epoch 00390: val_loss improved from 2.97436 to 2.95041, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 391/2000
22/22 - 1s - loss: 2.5292 - val_loss: 2.9487
Epoch 392/2000
22/22 - 1s - loss: 2.5283 - val_loss: 2.9459
Epoch 393/2000
22/22 - 1s - loss: 2.5252 - val_loss: 2.9431
Epoch 394/2000
22/22 - 1s - loss: 2.5225 - val_loss: 2.9408
Epoch 395/2000
22/22 - 1s - loss: 2.5210 - val_loss: 2.9376
Epoch 396/2000
22/22 - 1s - loss: 2.5172 - val_loss: 2.9347
Epoch 397/2000
22/22 - 1s - loss: 2.5152 - val_loss: 2.9319
Epoch 398/2000
22/22 - 1s - loss: 2.5136 - val_loss: 2.9308
Epoch 399/2000
22/22 - 1s - loss: 2.5099 - val_loss: 2.9279
Epoch 400/2000
22/22 - 1s - loss: 2.5085 - val_loss: 2.9264
Epoch 00400: val_loss improved from 2.95041 to 2.92644, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 401/2000
22/22 - 1s - loss: 2.5080 - val_loss: 2.9242
Epoch 402/2000
22/22 - 1s - loss: 2.5049 - val_loss: 2.9223
Epoch 403/2000
22/22 - 1s - loss: 2.5015 - val_loss: 2.9194
Epoch 404/2000
22/22 - 1s - loss: 2.4989 - val_loss: 2.9161
Epoch 405/2000
22/22 - 1s - loss: 2.4945 - val_loss: 2.9138
Epoch 406/2000
22/22 - 1s - loss: 2.4959 - val_loss: 2.9117
Epoch 407/2000
22/22 - 1s - loss: 2.4935 - val_loss: 2.9086
Epoch 408/2000
22/22 - 1s - loss: 2.4909 - val_loss: 2.9068
Epoch 409/2000
22/22 - 1s - loss: 2.4890 - val_loss: 2.9038
Epoch 410/2000
22/22 - 1s - loss: 2.4862 - val_loss: 2.9012
Epoch 00410: val_loss improved from 2.92644 to 2.90119, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 411/2000
22/22 - 1s - loss: 2.4813 - val_loss: 2.8985
Epoch 412/2000
22/22 - 1s - loss: 2.4799 - val_loss: 2.8955
Epoch 413/2000
22/22 - 1s - loss: 2.4753 - val_loss: 2.8930
Epoch 414/2000
22/22 - 1s - loss: 2.4747 - val_loss: 2.8911
Epoch 415/2000
22/22 - 1s - loss: 2.4736 - val_loss: 2.8908
Epoch 416/2000
22/22 - 1s - loss: 2.4698 - val_loss: 2.8886
Epoch 417/2000
22/22 - 1s - loss: 2.4659 - val_loss: 2.8862
Epoch 418/2000
22/22 - 1s - loss: 2.4637 - val_loss: 2.8847
Epoch 419/2000
22/22 - 1s - loss: 2.4611 - val_loss: 2.8825
Epoch 420/2000
22/22 - 1s - loss: 2.4633 - val_loss: 2.8799
Epoch 00420: val_loss improved from 2.90119 to 2.87991, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 421/2000
22/22 - 1s - loss: 2.4568 - val_loss: 2.8768
Epoch 422/2000
22/22 - 1s - loss: 2.4575 - val_loss: 2.8747
Epoch 423/2000
22/22 - 1s - loss: 2.4548 - val_loss: 2.8733
Epoch 424/2000
22/22 - 1s - loss: 2.4495 - val_loss: 2.8705
Epoch 425/2000
22/22 - 1s - loss: 2.4492 - val_loss: 2.8675
Epoch 426/2000
22/22 - 1s - loss: 2.4480 - val_loss: 2.8652
Epoch 427/2000
22/22 - 1s - loss: 2.4439 - val_loss: 2.8642
Epoch 428/2000
22/22 - 1s - loss: 2.4441 - val_loss: 2.8626
Epoch 429/2000
22/22 - 1s - loss: 2.4404 - val_loss: 2.8593
Epoch 430/2000
22/22 - 1s - loss: 2.4324 - val_loss: 2.8576
Epoch 00430: val_loss improved from 2.87991 to 2.85758, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 431/2000
22/22 - 1s - loss: 2.4341 - val_loss: 2.8547
Epoch 432/2000
22/22 - 1s - loss: 2.4308 - val_loss: 2.8535
Epoch 433/2000
22/22 - 1s - loss: 2.4305 - val_loss: 2.8517
Epoch 434/2000
22/22 - 1s - loss: 2.4267 - val_loss: 2.8486
Epoch 435/2000
22/22 - 1s - loss: 2.4253 - val_loss: 2.8460
Epoch 436/2000
22/22 - 1s - loss: 2.4225 - val_loss: 2.8431
Epoch 437/2000
22/22 - 1s - loss: 2.4238 - val_loss: 2.8404
Epoch 438/2000
22/22 - 1s - loss: 2.4181 - val_loss: 2.8391
Epoch 439/2000
22/22 - 1s - loss: 2.4162 - val_loss: 2.8362
Epoch 440/2000
22/22 - 1s - loss: 2.4127 - val_loss: 2.8343
Epoch 00440: val_loss improved from 2.85758 to 2.83430, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 441/2000
22/22 - 1s - loss: 2.4125 - val_loss: 2.8329
Epoch 442/2000
22/22 - 1s - loss: 2.4091 - val_loss: 2.8300
Epoch 443/2000
22/22 - 1s - loss: 2.4086 - val_loss: 2.8286
Epoch 444/2000
22/22 - 1s - loss: 2.4053 - val_loss: 2.8259
Epoch 445/2000
22/22 - 1s - loss: 2.4019 - val_loss: 2.8237
Epoch 446/2000
22/22 - 1s - loss: 2.4011 - val_loss: 2.8209
Epoch 447/2000
22/22 - 1s - loss: 2.3985 - val_loss: 2.8203
Epoch 448/2000
22/22 - 1s - loss: 2.3987 - val_loss: 2.8183
Epoch 449/2000
22/22 - 1s - loss: 2.3935 - val_loss: 2.8151
Epoch 450/2000
22/22 - 1s - loss: 2.3922 - val_loss: 2.8123
Epoch 00450: val_loss improved from 2.83430 to 2.81233, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 451/2000
22/22 - 1s - loss: 2.3885 - val_loss: 2.8093
Epoch 452/2000
22/22 - 1s - loss: 2.3886 - val_loss: 2.8078
Epoch 453/2000
22/22 - 1s - loss: 2.3843 - val_loss: 2.8055
Epoch 454/2000
22/22 - 1s - loss: 2.3842 - val_loss: 2.8035
Epoch 455/2000
22/22 - 1s - loss: 2.3812 - val_loss: 2.8015
Epoch 456/2000
22/22 - 1s - loss: 2.3783 - val_loss: 2.7996
Epoch 457/2000
22/22 - 1s - loss: 2.3748 - val_loss: 2.7975
Epoch 458/2000
22/22 - 1s - loss: 2.3734 - val_loss: 2.7943
Epoch 459/2000
22/22 - 1s - loss: 2.3728 - val_loss: 2.7927
Epoch 460/2000
22/22 - 1s - loss: 2.3677 - val_loss: 2.7914
Epoch 00460: val_loss improved from 2.81233 to 2.79136, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 461/2000
22/22 - 1s - loss: 2.3675 - val_loss: 2.7892
Epoch 462/2000
22/22 - 1s - loss: 2.3646 - val_loss: 2.7871
Epoch 463/2000
22/22 - 1s - loss: 2.3658 - val_loss: 2.7855
Epoch 464/2000
22/22 - 1s - loss: 2.3585 - val_loss: 2.7844
Epoch 465/2000
22/22 - 1s - loss: 2.3574 - val_loss: 2.7803
Epoch 466/2000
22/22 - 1s - loss: 2.3550 - val_loss: 2.7781
Epoch 467/2000
22/22 - 1s - loss: 2.3515 - val_loss: 2.7757
Epoch 468/2000
22/22 - 1s - loss: 2.3513 - val_loss: 2.7746
Epoch 469/2000
22/22 - 1s - loss: 2.3503 - val_loss: 2.7727
Epoch 470/2000
22/22 - 1s - loss: 2.3446 - val_loss: 2.7705
Epoch 00470: val_loss improved from 2.79136 to 2.77049, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 471/2000
22/22 - 1s - loss: 2.3448 - val_loss: 2.7691
Epoch 472/2000
22/22 - 1s - loss: 2.3442 - val_loss: 2.7667
Epoch 473/2000
22/22 - 1s - loss: 2.3391 - val_loss: 2.7629
Epoch 474/2000
22/22 - 1s - loss: 2.3392 - val_loss: 2.7620
Epoch 475/2000
22/22 - 1s - loss: 2.3399 - val_loss: 2.7602
Epoch 476/2000
22/22 - 1s - loss: 2.3360 - val_loss: 2.7577
Epoch 477/2000
22/22 - 1s - loss: 2.3324 - val_loss: 2.7556
Epoch 478/2000
22/22 - 1s - loss: 2.3302 - val_loss: 2.7535
Epoch 479/2000
22/22 - 1s - loss: 2.3266 - val_loss: 2.7502
Epoch 480/2000
22/22 - 1s - loss: 2.3245 - val_loss: 2.7487
Epoch 00480: val_loss improved from 2.77049 to 2.74870, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 481/2000
22/22 - 1s - loss: 2.3242 - val_loss: 2.7467
Epoch 482/2000
22/22 - 1s - loss: 2.3201 - val_loss: 2.7431
Epoch 483/2000
22/22 - 1s - loss: 2.3181 - val_loss: 2.7424
Epoch 484/2000
22/22 - 1s - loss: 2.3163 - val_loss: 2.7409
Epoch 485/2000
22/22 - 1s - loss: 2.3131 - val_loss: 2.7388
Epoch 486/2000
22/22 - 1s - loss: 2.3128 - val_loss: 2.7361
Epoch 487/2000
22/22 - 1s - loss: 2.3104 - val_loss: 2.7343
Epoch 488/2000
22/22 - 1s - loss: 2.3081 - val_loss: 2.7329
Epoch 489/2000
22/22 - 1s - loss: 2.3067 - val_loss: 2.7306
Epoch 490/2000
22/22 - 1s - loss: 2.3041 - val_loss: 2.7290
Epoch 00490: val_loss improved from 2.74870 to 2.72900, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 491/2000
22/22 - 1s - loss: 2.2997 - val_loss: 2.7262
Epoch 492/2000
22/22 - 1s - loss: 2.2999 - val_loss: 2.7241
Epoch 493/2000
22/22 - 1s - loss: 2.2955 - val_loss: 2.7220
Epoch 494/2000
22/22 - 1s - loss: 2.2962 - val_loss: 2.7209
Epoch 495/2000
22/22 - 1s - loss: 2.2931 - val_loss: 2.7186
Epoch 496/2000
22/22 - 1s - loss: 2.2915 - val_loss: 2.7173
Epoch 497/2000
22/22 - 1s - loss: 2.2919 - val_loss: 2.7145
Epoch 498/2000
22/22 - 1s - loss: 2.2873 - val_loss: 2.7121
Epoch 499/2000
22/22 - 1s - loss: 2.2856 - val_loss: 2.7101
Epoch 500/2000
22/22 - 1s - loss: 2.2846 - val_loss: 2.7078
Epoch 00500: val_loss improved from 2.72900 to 2.70785, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 501/2000
22/22 - 1s - loss: 2.2830 - val_loss: 2.7067
Epoch 502/2000
22/22 - 1s - loss: 2.2804 - val_loss: 2.7043
Epoch 503/2000
22/22 - 1s - loss: 2.2750 - val_loss: 2.7018
Epoch 504/2000
22/22 - 1s - loss: 2.2730 - val_loss: 2.6994
Epoch 505/2000
22/22 - 1s - loss: 2.2727 - val_loss: 2.6970
Epoch 506/2000
22/22 - 1s - loss: 2.2706 - val_loss: 2.6945
Epoch 507/2000
22/22 - 1s - loss: 2.2681 - val_loss: 2.6938
Epoch 508/2000
22/22 - 1s - loss: 2.2678 - val_loss: 2.6915
Epoch 509/2000
22/22 - 1s - loss: 2.2646 - val_loss: 2.6898
Epoch 510/2000
22/22 - 1s - loss: 2.2610 - val_loss: 2.6864
Epoch 00510: val_loss improved from 2.70785 to 2.68642, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 511/2000
22/22 - 1s - loss: 2.2579 - val_loss: 2.6852
Epoch 512/2000
22/22 - 1s - loss: 2.2579 - val_loss: 2.6832
Epoch 513/2000
22/22 - 1s - loss: 2.2546 - val_loss: 2.6812
Epoch 514/2000
22/22 - 1s - loss: 2.2546 - val_loss: 2.6785
Epoch 515/2000
22/22 - 1s - loss: 2.2512 - val_loss: 2.6765
Epoch 516/2000
22/22 - 1s - loss: 2.2487 - val_loss: 2.6737
Epoch 517/2000
22/22 - 1s - loss: 2.2462 - val_loss: 2.6725
Epoch 518/2000
22/22 - 1s - loss: 2.2444 - val_loss: 2.6706
Epoch 519/2000
22/22 - 1s - loss: 2.2420 - val_loss: 2.6685
Epoch 520/2000
22/22 - 1s - loss: 2.2405 - val_loss: 2.6659
Epoch 00520: val_loss improved from 2.68642 to 2.66592, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 521/2000
22/22 - 1s - loss: 2.2410 - val_loss: 2.6646
Epoch 522/2000
22/22 - 1s - loss: 2.2353 - val_loss: 2.6619
Epoch 523/2000
22/22 - 1s - loss: 2.2351 - val_loss: 2.6608
Epoch 524/2000
22/22 - 1s - loss: 2.2318 - val_loss: 2.6578
Epoch 525/2000
22/22 - 1s - loss: 2.2348 - val_loss: 2.6565
Epoch 526/2000
22/22 - 1s - loss: 2.2269 - val_loss: 2.6549
Epoch 527/2000
22/22 - 1s - loss: 2.2296 - val_loss: 2.6525
Epoch 528/2000
22/22 - 1s - loss: 2.2259 - val_loss: 2.6505
Epoch 529/2000
22/22 - 1s - loss: 2.2214 - val_loss: 2.6476
Epoch 530/2000
22/22 - 1s - loss: 2.2207 - val_loss: 2.6458
Epoch 00530: val_loss improved from 2.66592 to 2.64576, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 531/2000
22/22 - 1s - loss: 2.2182 - val_loss: 2.6435
Epoch 532/2000
22/22 - 1s - loss: 2.2168 - val_loss: 2.6424
Epoch 533/2000
22/22 - 1s - loss: 2.2163 - val_loss: 2.6395
Epoch 534/2000
22/22 - 1s - loss: 2.2153 - val_loss: 2.6373
Epoch 535/2000
22/22 - 1s - loss: 2.2110 - val_loss: 2.6351
Epoch 536/2000
22/22 - 1s - loss: 2.2089 - val_loss: 2.6334
Epoch 537/2000
22/22 - 1s - loss: 2.2070 - val_loss: 2.6312
Epoch 538/2000
22/22 - 1s - loss: 2.2067 - val_loss: 2.6300
Epoch 539/2000
22/22 - 1s - loss: 2.2021 - val_loss: 2.6286
Epoch 540/2000
22/22 - 1s - loss: 2.2021 - val_loss: 2.6260
Epoch 00540: val_loss improved from 2.64576 to 2.62603, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 541/2000
22/22 - 1s - loss: 2.1983 - val_loss: 2.6249
Epoch 542/2000
22/22 - 1s - loss: 2.1949 - val_loss: 2.6239
Epoch 543/2000
22/22 - 1s - loss: 2.1950 - val_loss: 2.6220
Epoch 544/2000
22/22 - 1s - loss: 2.1930 - val_loss: 2.6199
Epoch 545/2000
22/22 - 1s - loss: 2.1920 - val_loss: 2.6184
Epoch 546/2000
22/22 - 1s - loss: 2.1904 - val_loss: 2.6159
Epoch 547/2000
22/22 - 1s - loss: 2.1879 - val_loss: 2.6142
Epoch 548/2000
22/22 - 1s - loss: 2.1831 - val_loss: 2.6128
Epoch 549/2000
22/22 - 1s - loss: 2.1841 - val_loss: 2.6107
Epoch 550/2000
22/22 - 1s - loss: 2.1813 - val_loss: 2.6083
Epoch 00550: val_loss improved from 2.62603 to 2.60834, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 551/2000
22/22 - 1s - loss: 2.1793 - val_loss: 2.6052
Epoch 552/2000
22/22 - 1s - loss: 2.1758 - val_loss: 2.6047
Epoch 553/2000
22/22 - 1s - loss: 2.1767 - val_loss: 2.6022
Epoch 554/2000
22/22 - 1s - loss: 2.1740 - val_loss: 2.6002
Epoch 555/2000
22/22 - 1s - loss: 2.1707 - val_loss: 2.5985
Epoch 556/2000
22/22 - 1s - loss: 2.1707 - val_loss: 2.5964
Epoch 557/2000
22/22 - 1s - loss: 2.1692 - val_loss: 2.5940
Epoch 558/2000
22/22 - 1s - loss: 2.1649 - val_loss: 2.5912
Epoch 559/2000
22/22 - 1s - loss: 2.1630 - val_loss: 2.5893
Epoch 560/2000
22/22 - 1s - loss: 2.1605 - val_loss: 2.5859
Epoch 00560: val_loss improved from 2.60834 to 2.58589, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 561/2000
22/22 - 1s - loss: 2.1616 - val_loss: 2.5851
Epoch 562/2000
22/22 - 1s - loss: 2.1588 - val_loss: 2.5827
Epoch 563/2000
22/22 - 1s - loss: 2.1559 - val_loss: 2.5815
Epoch 564/2000
22/22 - 1s - loss: 2.1508 - val_loss: 2.5794
Epoch 565/2000
22/22 - 1s - loss: 2.1502 - val_loss: 2.5779
Epoch 566/2000
22/22 - 1s - loss: 2.1489 - val_loss: 2.5756
Epoch 567/2000
22/22 - 1s - loss: 2.1485 - val_loss: 2.5731
Epoch 568/2000
22/22 - 1s - loss: 2.1485 - val_loss: 2.5714
Epoch 569/2000
22/22 - 1s - loss: 2.1441 - val_loss: 2.5698
Epoch 570/2000
22/22 - 1s - loss: 2.1440 - val_loss: 2.5682
Epoch 00570: val_loss improved from 2.58589 to 2.56820, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 571/2000
22/22 - 1s - loss: 2.1417 - val_loss: 2.5664
Epoch 572/2000
22/22 - 1s - loss: 2.1394 - val_loss: 2.5654
Epoch 573/2000
22/22 - 1s - loss: 2.1358 - val_loss: 2.5636
Epoch 574/2000
22/22 - 1s - loss: 2.1362 - val_loss: 2.5611
Epoch 575/2000
22/22 - 1s - loss: 2.1322 - val_loss: 2.5602
Epoch 576/2000
22/22 - 1s - loss: 2.1321 - val_loss: 2.5579
Epoch 577/2000
22/22 - 1s - loss: 2.1281 - val_loss: 2.5563
Epoch 578/2000
22/22 - 1s - loss: 2.1265 - val_loss: 2.5550
Epoch 579/2000
22/22 - 1s - loss: 2.1249 - val_loss: 2.5531
Epoch 580/2000
22/22 - 1s - loss: 2.1239 - val_loss: 2.5511
Epoch 00580: val_loss improved from 2.56820 to 2.55111, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 581/2000
22/22 - 1s - loss: 2.1227 - val_loss: 2.5493
Epoch 582/2000
22/22 - 1s - loss: 2.1213 - val_loss: 2.5467
Epoch 583/2000
22/22 - 1s - loss: 2.1194 - val_loss: 2.5443
Epoch 584/2000
22/22 - 1s - loss: 2.1152 - val_loss: 2.5436
Epoch 585/2000
22/22 - 1s - loss: 2.1135 - val_loss: 2.5425
Epoch 586/2000
22/22 - 1s - loss: 2.1116 - val_loss: 2.5401
Epoch 587/2000
22/22 - 1s - loss: 2.1102 - val_loss: 2.5381
Epoch 588/2000
22/22 - 1s - loss: 2.1089 - val_loss: 2.5371
Epoch 589/2000
22/22 - 1s - loss: 2.1104 - val_loss: 2.5353
Epoch 590/2000
22/22 - 1s - loss: 2.1055 - val_loss: 2.5338
Epoch 00590: val_loss improved from 2.55111 to 2.53379, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 591/2000
22/22 - 1s - loss: 2.1047 - val_loss: 2.5310
Epoch 592/2000
22/22 - 1s - loss: 2.1015 - val_loss: 2.5290
Epoch 593/2000
22/22 - 1s - loss: 2.0980 - val_loss: 2.5287
Epoch 594/2000
22/22 - 1s - loss: 2.0964 - val_loss: 2.5257
Epoch 595/2000
22/22 - 1s - loss: 2.0959 - val_loss: 2.5242
Epoch 596/2000
22/22 - 1s - loss: 2.0958 - val_loss: 2.5225
Epoch 597/2000
22/22 - 1s - loss: 2.0940 - val_loss: 2.5196
Epoch 598/2000
22/22 - 1s - loss: 2.0908 - val_loss: 2.5174
Epoch 599/2000
22/22 - 1s - loss: 2.0907 - val_loss: 2.5158
Epoch 600/2000
22/22 - 1s - loss: 2.0856 - val_loss: 2.5137
Epoch 00600: val_loss improved from 2.53379 to 2.51369, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 601/2000
22/22 - 1s - loss: 2.0879 - val_loss: 2.5120
Epoch 602/2000
22/22 - 1s - loss: 2.0842 - val_loss: 2.5109
Epoch 603/2000
22/22 - 1s - loss: 2.0808 - val_loss: 2.5087
Epoch 604/2000
22/22 - 1s - loss: 2.0798 - val_loss: 2.5069
Epoch 605/2000
22/22 - 1s - loss: 2.0756 - val_loss: 2.5057
Epoch 606/2000
22/22 - 1s - loss: 2.0761 - val_loss: 2.5051
Epoch 607/2000
22/22 - 1s - loss: 2.0747 - val_loss: 2.5023
Epoch 608/2000
22/22 - 1s - loss: 2.0723 - val_loss: 2.5013
Epoch 609/2000
22/22 - 1s - loss: 2.0720 - val_loss: 2.4988
Epoch 610/2000
22/22 - 1s - loss: 2.0675 - val_loss: 2.4971
Epoch 00610: val_loss improved from 2.51369 to 2.49714, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 611/2000
22/22 - 1s - loss: 2.0663 - val_loss: 2.4961
Epoch 612/2000
22/22 - 1s - loss: 2.0637 - val_loss: 2.4940
Epoch 613/2000
22/22 - 1s - loss: 2.0627 - val_loss: 2.4921
Epoch 614/2000
22/22 - 1s - loss: 2.0614 - val_loss: 2.4901
Epoch 615/2000
22/22 - 1s - loss: 2.0584 - val_loss: 2.4869
Epoch 616/2000
22/22 - 1s - loss: 2.0575 - val_loss: 2.4858
Epoch 617/2000
22/22 - 1s - loss: 2.0552 - val_loss: 2.4838
Epoch 618/2000
22/22 - 1s - loss: 2.0531 - val_loss: 2.4819
Epoch 619/2000
22/22 - 1s - loss: 2.0511 - val_loss: 2.4806
Epoch 620/2000
22/22 - 1s - loss: 2.0503 - val_loss: 2.4786
Epoch 00620: val_loss improved from 2.49714 to 2.47858, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 621/2000
22/22 - 1s - loss: 2.0495 - val_loss: 2.4774
Epoch 622/2000
22/22 - 1s - loss: 2.0495 - val_loss: 2.4758
Epoch 623/2000
22/22 - 1s - loss: 2.0450 - val_loss: 2.4734
Epoch 624/2000
22/22 - 1s - loss: 2.0444 - val_loss: 2.4725
Epoch 625/2000
22/22 - 1s - loss: 2.0419 - val_loss: 2.4705
Epoch 626/2000
22/22 - 1s - loss: 2.0403 - val_loss: 2.4688
Epoch 627/2000
22/22 - 1s - loss: 2.0356 - val_loss: 2.4672
Epoch 628/2000
22/22 - 1s - loss: 2.0385 - val_loss: 2.4656
Epoch 629/2000
22/22 - 1s - loss: 2.0362 - val_loss: 2.4639
Epoch 630/2000
22/22 - 1s - loss: 2.0348 - val_loss: 2.4616
Epoch 00630: val_loss improved from 2.47858 to 2.46158, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 631/2000
22/22 - 1s - loss: 2.0310 - val_loss: 2.4604
Epoch 632/2000
22/22 - 1s - loss: 2.0278 - val_loss: 2.4580
Epoch 633/2000
22/22 - 1s - loss: 2.0278 - val_loss: 2.4572
Epoch 634/2000
22/22 - 1s - loss: 2.0257 - val_loss: 2.4555
Epoch 635/2000
22/22 - 1s - loss: 2.0252 - val_loss: 2.4538
Epoch 636/2000
22/22 - 1s - loss: 2.0230 - val_loss: 2.4519
Epoch 637/2000
22/22 - 1s - loss: 2.0208 - val_loss: 2.4502
Epoch 638/2000
22/22 - 1s - loss: 2.0170 - val_loss: 2.4489
Epoch 639/2000
22/22 - 1s - loss: 2.0161 - val_loss: 2.4469
Epoch 640/2000
22/22 - 1s - loss: 2.0153 - val_loss: 2.4448
Epoch 00640: val_loss improved from 2.46158 to 2.44484, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 641/2000
22/22 - 1s - loss: 2.0135 - val_loss: 2.4444
Epoch 642/2000
22/22 - 1s - loss: 2.0141 - val_loss: 2.4419
Epoch 643/2000
22/22 - 1s - loss: 2.0119 - val_loss: 2.4393
Epoch 644/2000
22/22 - 1s - loss: 2.0097 - val_loss: 2.4380
Epoch 645/2000
22/22 - 1s - loss: 2.0064 - val_loss: 2.4356
Epoch 646/2000
22/22 - 1s - loss: 2.0044 - val_loss: 2.4339
Epoch 647/2000
22/22 - 1s - loss: 2.0028 - val_loss: 2.4323
Epoch 648/2000
22/22 - 1s - loss: 2.0001 - val_loss: 2.4311
Epoch 649/2000
22/22 - 1s - loss: 2.0007 - val_loss: 2.4296
Epoch 650/2000
22/22 - 1s - loss: 1.9985 - val_loss: 2.4280
Epoch 00650: val_loss improved from 2.44484 to 2.42805, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 651/2000
22/22 - 1s - loss: 1.9995 - val_loss: 2.4256
Epoch 652/2000
22/22 - 1s - loss: 1.9954 - val_loss: 2.4232
Epoch 653/2000
22/22 - 1s - loss: 1.9943 - val_loss: 2.4223
Epoch 654/2000
22/22 - 1s - loss: 1.9919 - val_loss: 2.4203
Epoch 655/2000
22/22 - 1s - loss: 1.9902 - val_loss: 2.4199
Epoch 656/2000
22/22 - 1s - loss: 1.9883 - val_loss: 2.4172
Epoch 657/2000
22/22 - 1s - loss: 1.9849 - val_loss: 2.4152
Epoch 658/2000
22/22 - 1s - loss: 1.9864 - val_loss: 2.4136
Epoch 659/2000
22/22 - 1s - loss: 1.9832 - val_loss: 2.4121
Epoch 660/2000
22/22 - 1s - loss: 1.9828 - val_loss: 2.4097
Epoch 00660: val_loss improved from 2.42805 to 2.40966, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 661/2000
22/22 - 1s - loss: 1.9804 - val_loss: 2.4079
Epoch 662/2000
22/22 - 1s - loss: 1.9769 - val_loss: 2.4064
Epoch 663/2000
22/22 - 1s - loss: 1.9758 - val_loss: 2.4058
Epoch 664/2000
22/22 - 1s - loss: 1.9728 - val_loss: 2.4037
Epoch 665/2000
22/22 - 1s - loss: 1.9729 - val_loss: 2.4018
Epoch 666/2000
22/22 - 1s - loss: 1.9711 - val_loss: 2.4012
Epoch 667/2000
22/22 - 1s - loss: 1.9689 - val_loss: 2.3999
Epoch 668/2000
22/22 - 1s - loss: 1.9670 - val_loss: 2.3990
Epoch 669/2000
22/22 - 1s - loss: 1.9662 - val_loss: 2.3963
Epoch 670/2000
22/22 - 1s - loss: 1.9649 - val_loss: 2.3942
Epoch 00670: val_loss improved from 2.40966 to 2.39418, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 671/2000
22/22 - 1s - loss: 1.9639 - val_loss: 2.3935
Epoch 672/2000
22/22 - 1s - loss: 1.9596 - val_loss: 2.3929
Epoch 673/2000
22/22 - 1s - loss: 1.9582 - val_loss: 2.3897
Epoch 674/2000
22/22 - 1s - loss: 1.9585 - val_loss: 2.3878
Epoch 675/2000
22/22 - 1s - loss: 1.9541 - val_loss: 2.3855
Epoch 676/2000
22/22 - 1s - loss: 1.9528 - val_loss: 2.3838
Epoch 677/2000
22/22 - 1s - loss: 1.9545 - val_loss: 2.3821
Epoch 678/2000
22/22 - 1s - loss: 1.9503 - val_loss: 2.3800
Epoch 679/2000
22/22 - 1s - loss: 1.9523 - val_loss: 2.3779
Epoch 680/2000
22/22 - 1s - loss: 1.9468 - val_loss: 2.3777
Epoch 00680: val_loss improved from 2.39418 to 2.37766, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 681/2000
22/22 - 1s - loss: 1.9456 - val_loss: 2.3766
Epoch 682/2000
22/22 - 1s - loss: 1.9449 - val_loss: 2.3745
Epoch 683/2000
22/22 - 1s - loss: 1.9417 - val_loss: 2.3731
Epoch 684/2000
22/22 - 1s - loss: 1.9407 - val_loss: 2.3719
Epoch 685/2000
22/22 - 1s - loss: 1.9388 - val_loss: 2.3696
Epoch 686/2000
22/22 - 1s - loss: 1.9397 - val_loss: 2.3677
Epoch 687/2000
22/22 - 1s - loss: 1.9367 - val_loss: 2.3658
Epoch 688/2000
22/22 - 1s - loss: 1.9346 - val_loss: 2.3636
Epoch 689/2000
22/22 - 1s - loss: 1.9348 - val_loss: 2.3630
Epoch 690/2000
22/22 - 1s - loss: 1.9297 - val_loss: 2.3611
Epoch 00690: val_loss improved from 2.37766 to 2.36105, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 691/2000
22/22 - 1s - loss: 1.9280 - val_loss: 2.3598
Epoch 692/2000
22/22 - 1s - loss: 1.9280 - val_loss: 2.3576
Epoch 693/2000
22/22 - 1s - loss: 1.9262 - val_loss: 2.3560
Epoch 694/2000
22/22 - 1s - loss: 1.9252 - val_loss: 2.3536
Epoch 695/2000
22/22 - 1s - loss: 1.9230 - val_loss: 2.3529
Epoch 696/2000
22/22 - 1s - loss: 1.9216 - val_loss: 2.3522
Epoch 697/2000
22/22 - 1s - loss: 1.9190 - val_loss: 2.3502
Epoch 698/2000
22/22 - 1s - loss: 1.9193 - val_loss: 2.3480
Epoch 699/2000
22/22 - 1s - loss: 1.9171 - val_loss: 2.3467
Epoch 700/2000
22/22 - 1s - loss: 1.9142 - val_loss: 2.3448
Epoch 00700: val_loss improved from 2.36105 to 2.34479, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 701/2000
22/22 - 1s - loss: 1.9126 - val_loss: 2.3432
Epoch 702/2000
22/22 - 1s - loss: 1.9114 - val_loss: 2.3427
Epoch 703/2000
22/22 - 1s - loss: 1.9115 - val_loss: 2.3399
Epoch 704/2000
22/22 - 1s - loss: 1.9104 - val_loss: 2.3386
Epoch 705/2000
22/22 - 1s - loss: 1.9089 - val_loss: 2.3377
Epoch 706/2000
22/22 - 1s - loss: 1.9062 - val_loss: 2.3351
Epoch 707/2000
22/22 - 1s - loss: 1.9060 - val_loss: 2.3336
Epoch 708/2000
22/22 - 1s - loss: 1.9028 - val_loss: 2.3328
Epoch 709/2000
22/22 - 1s - loss: 1.9029 - val_loss: 2.3313
Epoch 710/2000
22/22 - 1s - loss: 1.8991 - val_loss: 2.3305
Epoch 00710: val_loss improved from 2.34479 to 2.33046, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 711/2000
22/22 - 1s - loss: 1.8957 - val_loss: 2.3296
Epoch 712/2000
22/22 - 1s - loss: 1.8957 - val_loss: 2.3276
Epoch 713/2000
22/22 - 1s - loss: 1.8956 - val_loss: 2.3255
Epoch 714/2000
22/22 - 1s - loss: 1.8927 - val_loss: 2.3245
Epoch 715/2000
22/22 - 1s - loss: 1.8936 - val_loss: 2.3218
Epoch 716/2000
22/22 - 1s - loss: 1.8892 - val_loss: 2.3204
Epoch 717/2000
22/22 - 1s - loss: 1.8894 - val_loss: 2.3199
Epoch 718/2000
22/22 - 1s - loss: 1.8873 - val_loss: 2.3185
Epoch 719/2000
22/22 - 1s - loss: 1.8846 - val_loss: 2.3167
Epoch 720/2000
22/22 - 1s - loss: 1.8838 - val_loss: 2.3147
Epoch 00720: val_loss improved from 2.33046 to 2.31474, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 721/2000
22/22 - 1s - loss: 1.8833 - val_loss: 2.3131
Epoch 722/2000
22/22 - 1s - loss: 1.8810 - val_loss: 2.3111
Epoch 723/2000
22/22 - 1s - loss: 1.8767 - val_loss: 2.3098
Epoch 724/2000
22/22 - 1s - loss: 1.8774 - val_loss: 2.3086
Epoch 725/2000
22/22 - 1s - loss: 1.8769 - val_loss: 2.3068
Epoch 726/2000
22/22 - 1s - loss: 1.8760 - val_loss: 2.3052
Epoch 727/2000
22/22 - 1s - loss: 1.8763 - val_loss: 2.3032
Epoch 728/2000
22/22 - 1s - loss: 1.8710 - val_loss: 2.3009
Epoch 729/2000
22/22 - 1s - loss: 1.8667 - val_loss: 2.2992
Epoch 730/2000
22/22 - 1s - loss: 1.8687 - val_loss: 2.2986
Epoch 00730: val_loss improved from 2.31474 to 2.29859, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 731/2000
22/22 - 1s - loss: 1.8685 - val_loss: 2.2979
Epoch 732/2000
22/22 - 1s - loss: 1.8644 - val_loss: 2.2961
Epoch 733/2000
22/22 - 1s - loss: 1.8633 - val_loss: 2.2942
Epoch 734/2000
22/22 - 1s - loss: 1.8622 - val_loss: 2.2932
Epoch 735/2000
22/22 - 1s - loss: 1.8585 - val_loss: 2.2914
Epoch 736/2000
22/22 - 1s - loss: 1.8568 - val_loss: 2.2902
Epoch 737/2000
22/22 - 1s - loss: 1.8588 - val_loss: 2.2885
Epoch 738/2000
22/22 - 1s - loss: 1.8560 - val_loss: 2.2877
Epoch 739/2000
22/22 - 1s - loss: 1.8544 - val_loss: 2.2846
Epoch 740/2000
22/22 - 1s - loss: 1.8510 - val_loss: 2.2839
Epoch 00740: val_loss improved from 2.29859 to 2.28395, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 741/2000
22/22 - 1s - loss: 1.8517 - val_loss: 2.2828
Epoch 742/2000
22/22 - 1s - loss: 1.8490 - val_loss: 2.2799
Epoch 743/2000
22/22 - 1s - loss: 1.8490 - val_loss: 2.2790
Epoch 744/2000
22/22 - 1s - loss: 1.8443 - val_loss: 2.2768
Epoch 745/2000
22/22 - 1s - loss: 1.8457 - val_loss: 2.2769
Epoch 746/2000
22/22 - 1s - loss: 1.8420 - val_loss: 2.2753
Epoch 747/2000
22/22 - 1s - loss: 1.8394 - val_loss: 2.2747
Epoch 748/2000
22/22 - 1s - loss: 1.8417 - val_loss: 2.2726
Epoch 749/2000
22/22 - 1s - loss: 1.8384 - val_loss: 2.2720
Epoch 750/2000
22/22 - 1s - loss: 1.8361 - val_loss: 2.2693
Epoch 00750: val_loss improved from 2.28395 to 2.26930, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 751/2000
22/22 - 1s - loss: 1.8347 - val_loss: 2.2673
Epoch 752/2000
22/22 - 1s - loss: 1.8340 - val_loss: 2.2658
Epoch 753/2000
22/22 - 1s - loss: 1.8341 - val_loss: 2.2651
Epoch 754/2000
22/22 - 1s - loss: 1.8325 - val_loss: 2.2639
Epoch 755/2000
22/22 - 1s - loss: 1.8289 - val_loss: 2.2614
Epoch 756/2000
22/22 - 1s - loss: 1.8295 - val_loss: 2.2601
Epoch 757/2000
22/22 - 1s - loss: 1.8248 - val_loss: 2.2585
Epoch 758/2000
22/22 - 1s - loss: 1.8231 - val_loss: 2.2574
Epoch 759/2000
22/22 - 1s - loss: 1.8237 - val_loss: 2.2557
Epoch 760/2000
22/22 - 1s - loss: 1.8246 - val_loss: 2.2540
Epoch 00760: val_loss improved from 2.26930 to 2.25398, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 761/2000
22/22 - 1s - loss: 1.8213 - val_loss: 2.2524
Epoch 762/2000
22/22 - 1s - loss: 1.8193 - val_loss: 2.2514
Epoch 763/2000
22/22 - 1s - loss: 1.8172 - val_loss: 2.2505
Epoch 764/2000
22/22 - 1s - loss: 1.8162 - val_loss: 2.2489
Epoch 765/2000
22/22 - 1s - loss: 1.8155 - val_loss: 2.2474
Epoch 766/2000
22/22 - 1s - loss: 1.8155 - val_loss: 2.2466
Epoch 767/2000
22/22 - 1s - loss: 1.8129 - val_loss: 2.2449
Epoch 768/2000
22/22 - 1s - loss: 1.8096 - val_loss: 2.2429
Epoch 769/2000
22/22 - 1s - loss: 1.8101 - val_loss: 2.2409
Epoch 770/2000
22/22 - 1s - loss: 1.8063 - val_loss: 2.2408
Epoch 00770: val_loss improved from 2.25398 to 2.24076, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 771/2000
22/22 - 1s - loss: 1.8072 - val_loss: 2.2388
Epoch 772/2000
22/22 - 1s - loss: 1.8050 - val_loss: 2.2373
Epoch 773/2000
22/22 - 1s - loss: 1.8040 - val_loss: 2.2367
Epoch 774/2000
22/22 - 1s - loss: 1.8017 - val_loss: 2.2359
Epoch 775/2000
22/22 - 1s - loss: 1.7995 - val_loss: 2.2343
Epoch 776/2000
22/22 - 1s - loss: 1.7982 - val_loss: 2.2312
Epoch 777/2000
22/22 - 1s - loss: 1.7977 - val_loss: 2.2302
Epoch 778/2000
22/22 - 1s - loss: 1.7963 - val_loss: 2.2287
Epoch 779/2000
22/22 - 1s - loss: 1.7948 - val_loss: 2.2258
Epoch 780/2000
22/22 - 1s - loss: 1.7932 - val_loss: 2.2248
Epoch 00780: val_loss improved from 2.24076 to 2.22478, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 781/2000
22/22 - 1s - loss: 1.7907 - val_loss: 2.2239
Epoch 782/2000
22/22 - 1s - loss: 1.7905 - val_loss: 2.2224
Epoch 783/2000
22/22 - 1s - loss: 1.7893 - val_loss: 2.2211
Epoch 784/2000
22/22 - 1s - loss: 1.7862 - val_loss: 2.2199
Epoch 785/2000
22/22 - 1s - loss: 1.7849 - val_loss: 2.2185
Epoch 786/2000
22/22 - 1s - loss: 1.7835 - val_loss: 2.2162
Epoch 787/2000
22/22 - 1s - loss: 1.7814 - val_loss: 2.2148
Epoch 788/2000
22/22 - 1s - loss: 1.7794 - val_loss: 2.2140
Epoch 789/2000
22/22 - 1s - loss: 1.7827 - val_loss: 2.2118
Epoch 790/2000
22/22 - 1s - loss: 1.7780 - val_loss: 2.2114
Epoch 00790: val_loss improved from 2.22478 to 2.21136, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 791/2000
22/22 - 1s - loss: 1.7772 - val_loss: 2.2095
Epoch 792/2000
22/22 - 1s - loss: 1.7756 - val_loss: 2.2089
Epoch 793/2000
22/22 - 1s - loss: 1.7736 - val_loss: 2.2061
Epoch 794/2000
22/22 - 1s - loss: 1.7742 - val_loss: 2.2052
Epoch 795/2000
22/22 - 1s - loss: 1.7716 - val_loss: 2.2035
Epoch 796/2000
22/22 - 1s - loss: 1.7716 - val_loss: 2.2026
Epoch 797/2000
22/22 - 1s - loss: 1.7700 - val_loss: 2.2004
Epoch 798/2000
22/22 - 1s - loss: 1.7687 - val_loss: 2.1999
Epoch 799/2000
22/22 - 1s - loss: 1.7630 - val_loss: 2.1982
Epoch 800/2000
22/22 - 1s - loss: 1.7653 - val_loss: 2.1954
Epoch 00800: val_loss improved from 2.21136 to 2.19540, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 801/2000
22/22 - 1s - loss: 1.7646 - val_loss: 2.1949
Epoch 802/2000
22/22 - 1s - loss: 1.7609 - val_loss: 2.1933
Epoch 803/2000
22/22 - 1s - loss: 1.7590 - val_loss: 2.1921
Epoch 804/2000
22/22 - 1s - loss: 1.7600 - val_loss: 2.1906
Epoch 805/2000
22/22 - 1s - loss: 1.7565 - val_loss: 2.1895
Epoch 806/2000
22/22 - 1s - loss: 1.7554 - val_loss: 2.1887
Epoch 807/2000
22/22 - 1s - loss: 1.7532 - val_loss: 2.1868
Epoch 808/2000
22/22 - 1s - loss: 1.7526 - val_loss: 2.1849
Epoch 809/2000
22/22 - 1s - loss: 1.7525 - val_loss: 2.1841
Epoch 810/2000
22/22 - 1s - loss: 1.7492 - val_loss: 2.1818
Epoch 00810: val_loss improved from 2.19540 to 2.18182, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 811/2000
22/22 - 1s - loss: 1.7486 - val_loss: 2.1804
Epoch 812/2000
22/22 - 1s - loss: 1.7475 - val_loss: 2.1791
Epoch 813/2000
22/22 - 1s - loss: 1.7458 - val_loss: 2.1776
Epoch 814/2000
22/22 - 1s - loss: 1.7443 - val_loss: 2.1769
Epoch 815/2000
22/22 - 1s - loss: 1.7431 - val_loss: 2.1764
Epoch 816/2000
22/22 - 1s - loss: 1.7415 - val_loss: 2.1759
Epoch 817/2000
22/22 - 1s - loss: 1.7400 - val_loss: 2.1727
Epoch 818/2000
22/22 - 1s - loss: 1.7390 - val_loss: 2.1715
Epoch 819/2000
22/22 - 1s - loss: 1.7365 - val_loss: 2.1696
Epoch 820/2000
22/22 - 1s - loss: 1.7345 - val_loss: 2.1695
Epoch 00820: val_loss improved from 2.18182 to 2.16946, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 821/2000
22/22 - 1s - loss: 1.7355 - val_loss: 2.1675
Epoch 822/2000
22/22 - 1s - loss: 1.7310 - val_loss: 2.1663
Epoch 823/2000
22/22 - 1s - loss: 1.7332 - val_loss: 2.1649
Epoch 824/2000
22/22 - 1s - loss: 1.7325 - val_loss: 2.1632
Epoch 825/2000
22/22 - 1s - loss: 1.7301 - val_loss: 2.1621
Epoch 826/2000
22/22 - 1s - loss: 1.7283 - val_loss: 2.1604
Epoch 827/2000
22/22 - 1s - loss: 1.7267 - val_loss: 2.1589
Epoch 828/2000
22/22 - 1s - loss: 1.7239 - val_loss: 2.1577
Epoch 829/2000
22/22 - 1s - loss: 1.7205 - val_loss: 2.1569
Epoch 830/2000
22/22 - 1s - loss: 1.7240 - val_loss: 2.1554
Epoch 00830: val_loss improved from 2.16946 to 2.15537, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 831/2000
22/22 - 1s - loss: 1.7197 - val_loss: 2.1548
Epoch 832/2000
22/22 - 1s - loss: 1.7174 - val_loss: 2.1538
Epoch 833/2000
22/22 - 1s - loss: 1.7184 - val_loss: 2.1524
Epoch 834/2000
22/22 - 1s - loss: 1.7163 - val_loss: 2.1505
Epoch 835/2000
22/22 - 1s - loss: 1.7148 - val_loss: 2.1488
Epoch 836/2000
22/22 - 1s - loss: 1.7132 - val_loss: 2.1475
Epoch 837/2000
22/22 - 1s - loss: 1.7112 - val_loss: 2.1463
Epoch 838/2000
22/22 - 1s - loss: 1.7105 - val_loss: 2.1453
Epoch 839/2000
22/22 - 1s - loss: 1.7094 - val_loss: 2.1433
Epoch 840/2000
22/22 - 1s - loss: 1.7093 - val_loss: 2.1419
Epoch 00840: val_loss improved from 2.15537 to 2.14189, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 841/2000
22/22 - 1s - loss: 1.7056 - val_loss: 2.1406
Epoch 842/2000
22/22 - 1s - loss: 1.7046 - val_loss: 2.1389
Epoch 843/2000
22/22 - 1s - loss: 1.7040 - val_loss: 2.1373
Epoch 844/2000
22/22 - 1s - loss: 1.6997 - val_loss: 2.1363
Epoch 845/2000
22/22 - 1s - loss: 1.7027 - val_loss: 2.1334
Epoch 846/2000
22/22 - 1s - loss: 1.7010 - val_loss: 2.1318
Epoch 847/2000
22/22 - 1s - loss: 1.7013 - val_loss: 2.1319
Epoch 848/2000
22/22 - 1s - loss: 1.6994 - val_loss: 2.1306
Epoch 849/2000
22/22 - 1s - loss: 1.6969 - val_loss: 2.1297
Epoch 850/2000
22/22 - 1s - loss: 1.6940 - val_loss: 2.1289
Epoch 00850: val_loss improved from 2.14189 to 2.12895, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 851/2000
22/22 - 1s - loss: 1.6933 - val_loss: 2.1288
Epoch 852/2000
22/22 - 1s - loss: 1.6935 - val_loss: 2.1265
Epoch 853/2000
22/22 - 1s - loss: 1.6902 - val_loss: 2.1249
Epoch 854/2000
22/22 - 1s - loss: 1.6906 - val_loss: 2.1239
Epoch 855/2000
22/22 - 1s - loss: 1.6907 - val_loss: 2.1226
Epoch 856/2000
22/22 - 1s - loss: 1.6890 - val_loss: 2.1208
Epoch 857/2000
22/22 - 1s - loss: 1.6867 - val_loss: 2.1187
Epoch 858/2000
22/22 - 1s - loss: 1.6853 - val_loss: 2.1179
Epoch 859/2000
22/22 - 1s - loss: 1.6826 - val_loss: 2.1168
Epoch 860/2000
22/22 - 1s - loss: 1.6814 - val_loss: 2.1151
Epoch 00860: val_loss improved from 2.12895 to 2.11508, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 861/2000
22/22 - 1s - loss: 1.6822 - val_loss: 2.1138
Epoch 862/2000
22/22 - 1s - loss: 1.6803 - val_loss: 2.1125
Epoch 863/2000
22/22 - 1s - loss: 1.6757 - val_loss: 2.1110
Epoch 864/2000
22/22 - 1s - loss: 1.6768 - val_loss: 2.1096
Epoch 865/2000
22/22 - 1s - loss: 1.6760 - val_loss: 2.1085
Epoch 866/2000
22/22 - 1s - loss: 1.6752 - val_loss: 2.1076
Epoch 867/2000
22/22 - 1s - loss: 1.6733 - val_loss: 2.1065
Epoch 868/2000
22/22 - 1s - loss: 1.6724 - val_loss: 2.1045
Epoch 869/2000
22/22 - 1s - loss: 1.6709 - val_loss: 2.1042
Epoch 870/2000
22/22 - 1s - loss: 1.6711 - val_loss: 2.1026
Epoch 00870: val_loss improved from 2.11508 to 2.10259, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 871/2000
22/22 - 1s - loss: 1.6679 - val_loss: 2.1014
Epoch 872/2000
22/22 - 1s - loss: 1.6655 - val_loss: 2.1007
Epoch 873/2000
22/22 - 1s - loss: 1.6660 - val_loss: 2.0984
Epoch 874/2000
22/22 - 1s - loss: 1.6616 - val_loss: 2.0969
Epoch 875/2000
22/22 - 1s - loss: 1.6623 - val_loss: 2.0955
Epoch 876/2000
22/22 - 1s - loss: 1.6604 - val_loss: 2.0947
Epoch 877/2000
22/22 - 1s - loss: 1.6608 - val_loss: 2.0935
Epoch 878/2000
22/22 - 1s - loss: 1.6582 - val_loss: 2.0915
Epoch 879/2000
22/22 - 1s - loss: 1.6582 - val_loss: 2.0905
Epoch 880/2000
22/22 - 1s - loss: 1.6555 - val_loss: 2.0896
Epoch 00880: val_loss improved from 2.10259 to 2.08962, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 881/2000
22/22 - 1s - loss: 1.6549 - val_loss: 2.0887
Epoch 882/2000
22/22 - 1s - loss: 1.6544 - val_loss: 2.0872
Epoch 883/2000
22/22 - 1s - loss: 1.6527 - val_loss: 2.0864
Epoch 884/2000
22/22 - 1s - loss: 1.6502 - val_loss: 2.0848
Epoch 885/2000
22/22 - 1s - loss: 1.6495 - val_loss: 2.0838
Epoch 886/2000
22/22 - 1s - loss: 1.6477 - val_loss: 2.0826
Epoch 887/2000
22/22 - 1s - loss: 1.6472 - val_loss: 2.0805
Epoch 888/2000
22/22 - 1s - loss: 1.6460 - val_loss: 2.0787
Epoch 889/2000
22/22 - 1s - loss: 1.6429 - val_loss: 2.0768
Epoch 890/2000
22/22 - 1s - loss: 1.6427 - val_loss: 2.0754
Epoch 00890: val_loss improved from 2.08962 to 2.07542, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 891/2000
22/22 - 1s - loss: 1.6417 - val_loss: 2.0755
Epoch 892/2000
22/22 - 1s - loss: 1.6413 - val_loss: 2.0734
Epoch 893/2000
22/22 - 1s - loss: 1.6373 - val_loss: 2.0732
Epoch 894/2000
22/22 - 1s - loss: 1.6392 - val_loss: 2.0715
Epoch 895/2000
22/22 - 1s - loss: 1.6362 - val_loss: 2.0702
Epoch 896/2000
22/22 - 1s - loss: 1.6351 - val_loss: 2.0686
Epoch 897/2000
22/22 - 1s - loss: 1.6349 - val_loss: 2.0674
Epoch 898/2000
22/22 - 1s - loss: 1.6333 - val_loss: 2.0673
Epoch 899/2000
22/22 - 1s - loss: 1.6297 - val_loss: 2.0662
Epoch 900/2000
22/22 - 1s - loss: 1.6293 - val_loss: 2.0649
Epoch 00900: val_loss improved from 2.07542 to 2.06490, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 901/2000
22/22 - 1s - loss: 1.6294 - val_loss: 2.0629
Epoch 902/2000
22/22 - 1s - loss: 1.6269 - val_loss: 2.0616
Epoch 903/2000
22/22 - 1s - loss: 1.6252 - val_loss: 2.0607
Epoch 904/2000
22/22 - 1s - loss: 1.6271 - val_loss: 2.0597
Epoch 905/2000
22/22 - 1s - loss: 1.6230 - val_loss: 2.0580
Epoch 906/2000
22/22 - 1s - loss: 1.6231 - val_loss: 2.0569
Epoch 907/2000
22/22 - 1s - loss: 1.6205 - val_loss: 2.0561
Epoch 908/2000
22/22 - 1s - loss: 1.6208 - val_loss: 2.0548
Epoch 909/2000
22/22 - 1s - loss: 1.6181 - val_loss: 2.0531
Epoch 910/2000
22/22 - 1s - loss: 1.6172 - val_loss: 2.0512
Epoch 00910: val_loss improved from 2.06490 to 2.05119, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 911/2000
22/22 - 1s - loss: 1.6171 - val_loss: 2.0504
Epoch 912/2000
22/22 - 1s - loss: 1.6148 - val_loss: 2.0487
Epoch 913/2000
22/22 - 1s - loss: 1.6137 - val_loss: 2.0479
Epoch 914/2000
22/22 - 1s - loss: 1.6150 - val_loss: 2.0482
Epoch 915/2000
22/22 - 1s - loss: 1.6125 - val_loss: 2.0457
Epoch 916/2000
22/22 - 1s - loss: 1.6116 - val_loss: 2.0438
Epoch 917/2000
22/22 - 1s - loss: 1.6079 - val_loss: 2.0434
Epoch 918/2000
22/22 - 1s - loss: 1.6055 - val_loss: 2.0416
Epoch 919/2000
22/22 - 1s - loss: 1.6059 - val_loss: 2.0397
Epoch 920/2000
22/22 - 1s - loss: 1.6066 - val_loss: 2.0388
Epoch 00920: val_loss improved from 2.05119 to 2.03880, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 921/2000
22/22 - 1s - loss: 1.6020 - val_loss: 2.0381
Epoch 922/2000
22/22 - 1s - loss: 1.6030 - val_loss: 2.0378
Epoch 923/2000
22/22 - 1s - loss: 1.6010 - val_loss: 2.0364
Epoch 924/2000
22/22 - 1s - loss: 1.6011 - val_loss: 2.0353
Epoch 925/2000
22/22 - 1s - loss: 1.5988 - val_loss: 2.0336
Epoch 926/2000
22/22 - 1s - loss: 1.5984 - val_loss: 2.0323
Epoch 927/2000
22/22 - 1s - loss: 1.5977 - val_loss: 2.0321
Epoch 928/2000
22/22 - 1s - loss: 1.5942 - val_loss: 2.0302
Epoch 929/2000
22/22 - 1s - loss: 1.5923 - val_loss: 2.0288
Epoch 930/2000
22/22 - 1s - loss: 1.5937 - val_loss: 2.0272
Epoch 00930: val_loss improved from 2.03880 to 2.02719, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 931/2000
22/22 - 1s - loss: 1.5917 - val_loss: 2.0250
Epoch 932/2000
22/22 - 1s - loss: 1.5914 - val_loss: 2.0252
Epoch 933/2000
22/22 - 1s - loss: 1.5906 - val_loss: 2.0230
Epoch 934/2000
22/22 - 1s - loss: 1.5877 - val_loss: 2.0223
Epoch 935/2000
22/22 - 1s - loss: 1.5875 - val_loss: 2.0215
Epoch 936/2000
22/22 - 1s - loss: 1.5864 - val_loss: 2.0195
Epoch 937/2000
22/22 - 1s - loss: 1.5864 - val_loss: 2.0167
Epoch 938/2000
22/22 - 1s - loss: 1.5819 - val_loss: 2.0174
Epoch 939/2000
22/22 - 1s - loss: 1.5805 - val_loss: 2.0164
Epoch 940/2000
22/22 - 1s - loss: 1.5828 - val_loss: 2.0153
Epoch 00940: val_loss improved from 2.02719 to 2.01529, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 941/2000
22/22 - 1s - loss: 1.5790 - val_loss: 2.0138
Epoch 942/2000
22/22 - 1s - loss: 1.5785 - val_loss: 2.0138
Epoch 943/2000
22/22 - 1s - loss: 1.5767 - val_loss: 2.0115
Epoch 944/2000
22/22 - 1s - loss: 1.5769 - val_loss: 2.0106
Epoch 945/2000
22/22 - 1s - loss: 1.5770 - val_loss: 2.0088
Epoch 946/2000
22/22 - 1s - loss: 1.5731 - val_loss: 2.0081
Epoch 947/2000
22/22 - 1s - loss: 1.5729 - val_loss: 2.0056
Epoch 948/2000
22/22 - 1s - loss: 1.5713 - val_loss: 2.0047
Epoch 949/2000
22/22 - 1s - loss: 1.5697 - val_loss: 2.0042
Epoch 950/2000
22/22 - 1s - loss: 1.5718 - val_loss: 2.0031
Epoch 00950: val_loss improved from 2.01529 to 2.00311, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 951/2000
22/22 - 1s - loss: 1.5695 - val_loss: 2.0016
Epoch 952/2000
22/22 - 1s - loss: 1.5657 - val_loss: 2.0008
Epoch 953/2000
22/22 - 1s - loss: 1.5635 - val_loss: 1.9992
Epoch 954/2000
22/22 - 1s - loss: 1.5638 - val_loss: 1.9990
Epoch 955/2000
22/22 - 1s - loss: 1.5642 - val_loss: 1.9972
Epoch 956/2000
22/22 - 1s - loss: 1.5604 - val_loss: 1.9974
Epoch 957/2000
22/22 - 1s - loss: 1.5589 - val_loss: 1.9954
Epoch 958/2000
22/22 - 1s - loss: 1.5583 - val_loss: 1.9934
Epoch 959/2000
22/22 - 1s - loss: 1.5592 - val_loss: 1.9932
Epoch 960/2000
22/22 - 1s - loss: 1.5575 - val_loss: 1.9919
Epoch 00960: val_loss improved from 2.00311 to 1.99193, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 961/2000
22/22 - 1s - loss: 1.5562 - val_loss: 1.9909
Epoch 962/2000
22/22 - 1s - loss: 1.5544 - val_loss: 1.9893
Epoch 963/2000
22/22 - 1s - loss: 1.5531 - val_loss: 1.9884
Epoch 964/2000
22/22 - 1s - loss: 1.5515 - val_loss: 1.9871
Epoch 965/2000
22/22 - 1s - loss: 1.5503 - val_loss: 1.9853
Epoch 966/2000
22/22 - 1s - loss: 1.5495 - val_loss: 1.9843
Epoch 967/2000
22/22 - 1s - loss: 1.5511 - val_loss: 1.9833
Epoch 968/2000
22/22 - 1s - loss: 1.5490 - val_loss: 1.9822
Epoch 969/2000
22/22 - 1s - loss: 1.5484 - val_loss: 1.9804
Epoch 970/2000
22/22 - 1s - loss: 1.5450 - val_loss: 1.9790
Epoch 00970: val_loss improved from 1.99193 to 1.97898, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 971/2000
22/22 - 1s - loss: 1.5447 - val_loss: 1.9790
Epoch 972/2000
22/22 - 1s - loss: 1.5437 - val_loss: 1.9776
Epoch 973/2000
22/22 - 1s - loss: 1.5418 - val_loss: 1.9777
Epoch 974/2000
22/22 - 1s - loss: 1.5414 - val_loss: 1.9758
Epoch 975/2000
22/22 - 1s - loss: 1.5404 - val_loss: 1.9739
Epoch 976/2000
22/22 - 1s - loss: 1.5368 - val_loss: 1.9731
Epoch 977/2000
22/22 - 1s - loss: 1.5358 - val_loss: 1.9722
Epoch 978/2000
22/22 - 1s - loss: 1.5383 - val_loss: 1.9710
Epoch 979/2000
22/22 - 1s - loss: 1.5337 - val_loss: 1.9698
Epoch 980/2000
22/22 - 1s - loss: 1.5337 - val_loss: 1.9684
Epoch 00980: val_loss improved from 1.97898 to 1.96835, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 981/2000
22/22 - 1s - loss: 1.5321 - val_loss: 1.9671
Epoch 982/2000
22/22 - 1s - loss: 1.5325 - val_loss: 1.9657
Epoch 983/2000
22/22 - 1s - loss: 1.5308 - val_loss: 1.9648
Epoch 984/2000
22/22 - 1s - loss: 1.5314 - val_loss: 1.9628
Epoch 985/2000
22/22 - 1s - loss: 1.5273 - val_loss: 1.9610
Epoch 986/2000
22/22 - 1s - loss: 1.5259 - val_loss: 1.9600
Epoch 987/2000
22/22 - 1s - loss: 1.5244 - val_loss: 1.9600
Epoch 988/2000
22/22 - 1s - loss: 1.5232 - val_loss: 1.9595
Epoch 989/2000
22/22 - 1s - loss: 1.5237 - val_loss: 1.9577
Epoch 990/2000
22/22 - 1s - loss: 1.5228 - val_loss: 1.9578
Epoch 00990: val_loss improved from 1.96835 to 1.95780, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 991/2000
22/22 - 1s - loss: 1.5215 - val_loss: 1.9559
Epoch 992/2000
22/22 - 1s - loss: 1.5200 - val_loss: 1.9555
Epoch 993/2000
22/22 - 1s - loss: 1.5196 - val_loss: 1.9550
Epoch 994/2000
22/22 - 1s - loss: 1.5176 - val_loss: 1.9535
Epoch 995/2000
22/22 - 1s - loss: 1.5170 - val_loss: 1.9523
Epoch 996/2000
22/22 - 1s - loss: 1.5158 - val_loss: 1.9507
Epoch 997/2000
22/22 - 1s - loss: 1.5144 - val_loss: 1.9501
Epoch 998/2000
22/22 - 1s - loss: 1.5142 - val_loss: 1.9476
Epoch 999/2000
22/22 - 1s - loss: 1.5129 - val_loss: 1.9471
Epoch 1000/2000
22/22 - 1s - loss: 1.5110 - val_loss: 1.9464
Epoch 01000: val_loss improved from 1.95780 to 1.94641, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1001/2000
22/22 - 1s - loss: 1.5115 - val_loss: 1.9449
Epoch 1002/2000
22/22 - 1s - loss: 1.5083 - val_loss: 1.9442
Epoch 1003/2000
22/22 - 1s - loss: 1.5080 - val_loss: 1.9437
Epoch 1004/2000
22/22 - 1s - loss: 1.5072 - val_loss: 1.9423
Epoch 1005/2000
22/22 - 1s - loss: 1.5051 - val_loss: 1.9419
Epoch 1006/2000
22/22 - 1s - loss: 1.5063 - val_loss: 1.9401
Epoch 1007/2000
22/22 - 1s - loss: 1.5032 - val_loss: 1.9391
Epoch 1008/2000
22/22 - 1s - loss: 1.5022 - val_loss: 1.9373
Epoch 1009/2000
22/22 - 1s - loss: 1.5022 - val_loss: 1.9368
Epoch 1010/2000
22/22 - 1s - loss: 1.5002 - val_loss: 1.9352
Epoch 01010: val_loss improved from 1.94641 to 1.93520, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1011/2000
22/22 - 1s - loss: 1.4976 - val_loss: 1.9348
Epoch 1012/2000
22/22 - 1s - loss: 1.4967 - val_loss: 1.9332
Epoch 1013/2000
22/22 - 1s - loss: 1.4958 - val_loss: 1.9322
Epoch 1014/2000
22/22 - 1s - loss: 1.4953 - val_loss: 1.9303
Epoch 1015/2000
22/22 - 1s - loss: 1.4943 - val_loss: 1.9291
Epoch 1016/2000
22/22 - 1s - loss: 1.4926 - val_loss: 1.9275
Epoch 1017/2000
22/22 - 1s - loss: 1.4909 - val_loss: 1.9269
Epoch 1018/2000
22/22 - 1s - loss: 1.4927 - val_loss: 1.9264
Epoch 1019/2000
22/22 - 1s - loss: 1.4910 - val_loss: 1.9253
Epoch 1020/2000
22/22 - 1s - loss: 1.4895 - val_loss: 1.9250
Epoch 01020: val_loss improved from 1.93520 to 1.92497, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1021/2000
22/22 - 1s - loss: 1.4898 - val_loss: 1.9224
Epoch 1022/2000
22/22 - 1s - loss: 1.4876 - val_loss: 1.9223
Epoch 1023/2000
22/22 - 1s - loss: 1.4834 - val_loss: 1.9200
Epoch 1024/2000
22/22 - 1s - loss: 1.4860 - val_loss: 1.9196
Epoch 1025/2000
22/22 - 1s - loss: 1.4863 - val_loss: 1.9175
Epoch 1026/2000
22/22 - 1s - loss: 1.4808 - val_loss: 1.9174
Epoch 1027/2000
22/22 - 1s - loss: 1.4816 - val_loss: 1.9169
Epoch 1028/2000
22/22 - 1s - loss: 1.4809 - val_loss: 1.9156
Epoch 1029/2000
22/22 - 1s - loss: 1.4777 - val_loss: 1.9153
Epoch 1030/2000
22/22 - 1s - loss: 1.4771 - val_loss: 1.9142
Epoch 01030: val_loss improved from 1.92497 to 1.91422, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1031/2000
22/22 - 1s - loss: 1.4770 - val_loss: 1.9135
Epoch 1032/2000
22/22 - 1s - loss: 1.4750 - val_loss: 1.9120
Epoch 1033/2000
22/22 - 1s - loss: 1.4751 - val_loss: 1.9098
Epoch 1034/2000
22/22 - 1s - loss: 1.4758 - val_loss: 1.9090
Epoch 1035/2000
22/22 - 1s - loss: 1.4721 - val_loss: 1.9078
Epoch 1036/2000
22/22 - 1s - loss: 1.4713 - val_loss: 1.9077
Epoch 1037/2000
22/22 - 1s - loss: 1.4712 - val_loss: 1.9075
Epoch 1038/2000
22/22 - 1s - loss: 1.4706 - val_loss: 1.9053
Epoch 1039/2000
22/22 - 1s - loss: 1.4676 - val_loss: 1.9042
Epoch 1040/2000
22/22 - 1s - loss: 1.4668 - val_loss: 1.9036
Epoch 01040: val_loss improved from 1.91422 to 1.90356, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1041/2000
22/22 - 1s - loss: 1.4654 - val_loss: 1.9018
Epoch 1042/2000
22/22 - 1s - loss: 1.4656 - val_loss: 1.9017
Epoch 1043/2000
22/22 - 1s - loss: 1.4643 - val_loss: 1.9011
Epoch 1044/2000
22/22 - 1s - loss: 1.4631 - val_loss: 1.9007
Epoch 1045/2000
22/22 - 1s - loss: 1.4650 - val_loss: 1.8983
Epoch 1046/2000
22/22 - 1s - loss: 1.4635 - val_loss: 1.8966
Epoch 1047/2000
22/22 - 1s - loss: 1.4596 - val_loss: 1.8958
Epoch 1048/2000
22/22 - 1s - loss: 1.4579 - val_loss: 1.8938
Epoch 1049/2000
22/22 - 1s - loss: 1.4578 - val_loss: 1.8933
Epoch 1050/2000
22/22 - 1s - loss: 1.4566 - val_loss: 1.8919
Epoch 01050: val_loss improved from 1.90356 to 1.89193, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1051/2000
22/22 - 1s - loss: 1.4554 - val_loss: 1.8907
Epoch 1052/2000
22/22 - 1s - loss: 1.4533 - val_loss: 1.8903
Epoch 1053/2000
22/22 - 1s - loss: 1.4531 - val_loss: 1.8897
Epoch 1054/2000
22/22 - 1s - loss: 1.4522 - val_loss: 1.8887
Epoch 1055/2000
22/22 - 1s - loss: 1.4530 - val_loss: 1.8875
Epoch 1056/2000
22/22 - 1s - loss: 1.4517 - val_loss: 1.8860
Epoch 1057/2000
22/22 - 1s - loss: 1.4510 - val_loss: 1.8854
Epoch 1058/2000
22/22 - 1s - loss: 1.4479 - val_loss: 1.8845
Epoch 1059/2000
22/22 - 1s - loss: 1.4472 - val_loss: 1.8838
Epoch 1060/2000
22/22 - 1s - loss: 1.4461 - val_loss: 1.8827
Epoch 01060: val_loss improved from 1.89193 to 1.88273, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1061/2000
22/22 - 1s - loss: 1.4462 - val_loss: 1.8809
Epoch 1062/2000
22/22 - 1s - loss: 1.4452 - val_loss: 1.8803
Epoch 1063/2000
22/22 - 1s - loss: 1.4418 - val_loss: 1.8797
Epoch 1064/2000
22/22 - 1s - loss: 1.4445 - val_loss: 1.8778
Epoch 1065/2000
22/22 - 1s - loss: 1.4394 - val_loss: 1.8763
Epoch 1066/2000
22/22 - 1s - loss: 1.4386 - val_loss: 1.8748
Epoch 1067/2000
22/22 - 1s - loss: 1.4388 - val_loss: 1.8745
Epoch 1068/2000
22/22 - 1s - loss: 1.4387 - val_loss: 1.8731
Epoch 1069/2000
22/22 - 1s - loss: 1.4365 - val_loss: 1.8730
Epoch 1070/2000
22/22 - 1s - loss: 1.4356 - val_loss: 1.8705
Epoch 01070: val_loss improved from 1.88273 to 1.87048, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1071/2000
22/22 - 1s - loss: 1.4357 - val_loss: 1.8707
Epoch 1072/2000
22/22 - 1s - loss: 1.4325 - val_loss: 1.8691
Epoch 1073/2000
22/22 - 1s - loss: 1.4320 - val_loss: 1.8692
Epoch 1074/2000
22/22 - 1s - loss: 1.4315 - val_loss: 1.8678
Epoch 1075/2000
22/22 - 1s - loss: 1.4293 - val_loss: 1.8669
Epoch 1076/2000
22/22 - 1s - loss: 1.4307 - val_loss: 1.8656
Epoch 1077/2000
22/22 - 1s - loss: 1.4269 - val_loss: 1.8646
Epoch 1078/2000
22/22 - 1s - loss: 1.4264 - val_loss: 1.8639
Epoch 1079/2000
22/22 - 1s - loss: 1.4257 - val_loss: 1.8637
Epoch 1080/2000
22/22 - 1s - loss: 1.4249 - val_loss: 1.8618
Epoch 01080: val_loss improved from 1.87048 to 1.86176, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1081/2000
22/22 - 1s - loss: 1.4234 - val_loss: 1.8610
Epoch 1082/2000
22/22 - 1s - loss: 1.4217 - val_loss: 1.8595
Epoch 1083/2000
22/22 - 1s - loss: 1.4219 - val_loss: 1.8589
Epoch 1084/2000
22/22 - 1s - loss: 1.4202 - val_loss: 1.8579
Epoch 1085/2000
22/22 - 1s - loss: 1.4209 - val_loss: 1.8575
Epoch 1086/2000
22/22 - 1s - loss: 1.4194 - val_loss: 1.8552
Epoch 1087/2000
22/22 - 1s - loss: 1.4148 - val_loss: 1.8545
Epoch 1088/2000
22/22 - 1s - loss: 1.4168 - val_loss: 1.8525
Epoch 1089/2000
22/22 - 1s - loss: 1.4166 - val_loss: 1.8524
Epoch 1090/2000
22/22 - 1s - loss: 1.4149 - val_loss: 1.8520
Epoch 01090: val_loss improved from 1.86176 to 1.85202, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1091/2000
22/22 - 1s - loss: 1.4141 - val_loss: 1.8498
Epoch 1092/2000
22/22 - 1s - loss: 1.4125 - val_loss: 1.8485
Epoch 1093/2000
22/22 - 1s - loss: 1.4131 - val_loss: 1.8484
Epoch 1094/2000
22/22 - 1s - loss: 1.4102 - val_loss: 1.8471
Epoch 1095/2000
22/22 - 1s - loss: 1.4076 - val_loss: 1.8462
Epoch 1096/2000
22/22 - 1s - loss: 1.4083 - val_loss: 1.8452
Epoch 1097/2000
22/22 - 1s - loss: 1.4080 - val_loss: 1.8447
Epoch 1098/2000
22/22 - 1s - loss: 1.4070 - val_loss: 1.8437
Epoch 1099/2000
22/22 - 1s - loss: 1.4064 - val_loss: 1.8435
Epoch 1100/2000
22/22 - 1s - loss: 1.4051 - val_loss: 1.8413
Epoch 01100: val_loss improved from 1.85202 to 1.84132, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1101/2000
22/22 - 1s - loss: 1.4043 - val_loss: 1.8408
Epoch 1102/2000
22/22 - 1s - loss: 1.4021 - val_loss: 1.8384
Epoch 1103/2000
22/22 - 1s - loss: 1.3999 - val_loss: 1.8383
Epoch 1104/2000
22/22 - 1s - loss: 1.4009 - val_loss: 1.8372
Epoch 1105/2000
22/22 - 1s - loss: 1.3990 - val_loss: 1.8356
Epoch 1106/2000
22/22 - 1s - loss: 1.3988 - val_loss: 1.8350
Epoch 1107/2000
22/22 - 1s - loss: 1.3963 - val_loss: 1.8338
Epoch 1108/2000
22/22 - 1s - loss: 1.3983 - val_loss: 1.8332
Epoch 1109/2000
22/22 - 1s - loss: 1.3964 - val_loss: 1.8322
Epoch 1110/2000
22/22 - 1s - loss: 1.3960 - val_loss: 1.8313
Epoch 01110: val_loss improved from 1.84132 to 1.83133, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1111/2000
22/22 - 1s - loss: 1.3932 - val_loss: 1.8306
Epoch 1112/2000
22/22 - 1s - loss: 1.3928 - val_loss: 1.8296
Epoch 1113/2000
22/22 - 1s - loss: 1.3925 - val_loss: 1.8284
Epoch 1114/2000
22/22 - 1s - loss: 1.3918 - val_loss: 1.8283
Epoch 1115/2000
22/22 - 1s - loss: 1.3893 - val_loss: 1.8269
Epoch 1116/2000
22/22 - 1s - loss: 1.3909 - val_loss: 1.8260
Epoch 1117/2000
22/22 - 1s - loss: 1.3874 - val_loss: 1.8245
Epoch 1118/2000
22/22 - 1s - loss: 1.3885 - val_loss: 1.8245
Epoch 1119/2000
22/22 - 1s - loss: 1.3851 - val_loss: 1.8240
Epoch 1120/2000
22/22 - 1s - loss: 1.3838 - val_loss: 1.8232
Epoch 01120: val_loss improved from 1.83133 to 1.82319, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1121/2000
22/22 - 1s - loss: 1.3829 - val_loss: 1.8221
Epoch 1122/2000
22/22 - 1s - loss: 1.3834 - val_loss: 1.8216
Epoch 1123/2000
22/22 - 1s - loss: 1.3811 - val_loss: 1.8204
Epoch 1124/2000
22/22 - 1s - loss: 1.3823 - val_loss: 1.8192
Epoch 1125/2000
22/22 - 1s - loss: 1.3806 - val_loss: 1.8176
Epoch 1126/2000
22/22 - 1s - loss: 1.3781 - val_loss: 1.8164
Epoch 1127/2000
22/22 - 1s - loss: 1.3771 - val_loss: 1.8160
Epoch 1128/2000
22/22 - 1s - loss: 1.3776 - val_loss: 1.8152
Epoch 1129/2000
22/22 - 1s - loss: 1.3765 - val_loss: 1.8137
Epoch 1130/2000
22/22 - 1s - loss: 1.3747 - val_loss: 1.8133
Epoch 01130: val_loss improved from 1.82319 to 1.81326, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1131/2000
22/22 - 1s - loss: 1.3744 - val_loss: 1.8124
Epoch 1132/2000
22/22 - 1s - loss: 1.3748 - val_loss: 1.8117
Epoch 1133/2000
22/22 - 1s - loss: 1.3736 - val_loss: 1.8092
Epoch 1134/2000
22/22 - 1s - loss: 1.3721 - val_loss: 1.8083
Epoch 1135/2000
22/22 - 1s - loss: 1.3696 - val_loss: 1.8084
Epoch 1136/2000
22/22 - 1s - loss: 1.3704 - val_loss: 1.8066
Epoch 1137/2000
22/22 - 1s - loss: 1.3680 - val_loss: 1.8058
Epoch 1138/2000
22/22 - 1s - loss: 1.3662 - val_loss: 1.8048
Epoch 1139/2000
22/22 - 1s - loss: 1.3663 - val_loss: 1.8036
Epoch 1140/2000
22/22 - 1s - loss: 1.3661 - val_loss: 1.8029
Epoch 01140: val_loss improved from 1.81326 to 1.80291, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1141/2000
22/22 - 1s - loss: 1.3668 - val_loss: 1.8024
Epoch 1142/2000
22/22 - 1s - loss: 1.3644 - val_loss: 1.8004
Epoch 1143/2000
22/22 - 1s - loss: 1.3641 - val_loss: 1.8009
Epoch 1144/2000
22/22 - 1s - loss: 1.3622 - val_loss: 1.7989
Epoch 1145/2000
22/22 - 1s - loss: 1.3603 - val_loss: 1.7969
Epoch 1146/2000
22/22 - 1s - loss: 1.3597 - val_loss: 1.7966
Epoch 1147/2000
22/22 - 1s - loss: 1.3582 - val_loss: 1.7957
Epoch 1148/2000
22/22 - 1s - loss: 1.3588 - val_loss: 1.7955
Epoch 1149/2000
22/22 - 1s - loss: 1.3555 - val_loss: 1.7944
Epoch 1150/2000
22/22 - 1s - loss: 1.3565 - val_loss: 1.7937
Epoch 01150: val_loss improved from 1.80291 to 1.79367, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1151/2000
22/22 - 1s - loss: 1.3557 - val_loss: 1.7921
Epoch 1152/2000
22/22 - 1s - loss: 1.3536 - val_loss: 1.7917
Epoch 1153/2000
22/22 - 1s - loss: 1.3532 - val_loss: 1.7905
Epoch 1154/2000
22/22 - 1s - loss: 1.3524 - val_loss: 1.7898
Epoch 1155/2000
22/22 - 1s - loss: 1.3522 - val_loss: 1.7887
Epoch 1156/2000
22/22 - 1s - loss: 1.3522 - val_loss: 1.7875
Epoch 1157/2000
22/22 - 1s - loss: 1.3501 - val_loss: 1.7877
Epoch 1158/2000
22/22 - 1s - loss: 1.3498 - val_loss: 1.7860
Epoch 1159/2000
22/22 - 1s - loss: 1.3478 - val_loss: 1.7853
Epoch 1160/2000
22/22 - 1s - loss: 1.3454 - val_loss: 1.7835
Epoch 01160: val_loss improved from 1.79367 to 1.78348, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1161/2000
22/22 - 1s - loss: 1.3457 - val_loss: 1.7818
Epoch 1162/2000
22/22 - 1s - loss: 1.3459 - val_loss: 1.7820
Epoch 1163/2000
22/22 - 1s - loss: 1.3424 - val_loss: 1.7811
Epoch 1164/2000
22/22 - 1s - loss: 1.3437 - val_loss: 1.7811
Epoch 1165/2000
22/22 - 1s - loss: 1.3413 - val_loss: 1.7793
Epoch 1166/2000
22/22 - 1s - loss: 1.3425 - val_loss: 1.7782
Epoch 1167/2000
22/22 - 1s - loss: 1.3382 - val_loss: 1.7773
Epoch 1168/2000
22/22 - 1s - loss: 1.3405 - val_loss: 1.7759
Epoch 1169/2000
22/22 - 1s - loss: 1.3374 - val_loss: 1.7757
Epoch 1170/2000
22/22 - 1s - loss: 1.3372 - val_loss: 1.7743
Epoch 01170: val_loss improved from 1.78348 to 1.77427, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1171/2000
22/22 - 1s - loss: 1.3344 - val_loss: 1.7742
Epoch 1172/2000
22/22 - 1s - loss: 1.3367 - val_loss: 1.7726
Epoch 1173/2000
22/22 - 1s - loss: 1.3362 - val_loss: 1.7724
Epoch 1174/2000
22/22 - 1s - loss: 1.3329 - val_loss: 1.7698
Epoch 1175/2000
22/22 - 1s - loss: 1.3320 - val_loss: 1.7686
Epoch 1176/2000
22/22 - 1s - loss: 1.3334 - val_loss: 1.7696
Epoch 1177/2000
22/22 - 1s - loss: 1.3314 - val_loss: 1.7685
Epoch 1178/2000
22/22 - 1s - loss: 1.3295 - val_loss: 1.7673
Epoch 1179/2000
22/22 - 1s - loss: 1.3319 - val_loss: 1.7666
Epoch 1180/2000
22/22 - 1s - loss: 1.3288 - val_loss: 1.7654
Epoch 01180: val_loss improved from 1.77427 to 1.76535, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1181/2000
22/22 - 1s - loss: 1.3276 - val_loss: 1.7649
Epoch 1182/2000
22/22 - 1s - loss: 1.3275 - val_loss: 1.7640
Epoch 1183/2000
22/22 - 1s - loss: 1.3267 - val_loss: 1.7635
Epoch 1184/2000
22/22 - 1s - loss: 1.3256 - val_loss: 1.7631
Epoch 1185/2000
22/22 - 1s - loss: 1.3235 - val_loss: 1.7628
Epoch 1186/2000
22/22 - 1s - loss: 1.3218 - val_loss: 1.7608
Epoch 1187/2000
22/22 - 1s - loss: 1.3222 - val_loss: 1.7593
Epoch 1188/2000
22/22 - 1s - loss: 1.3224 - val_loss: 1.7584
Epoch 1189/2000
22/22 - 1s - loss: 1.3214 - val_loss: 1.7565
Epoch 1190/2000
22/22 - 1s - loss: 1.3189 - val_loss: 1.7557
Epoch 01190: val_loss improved from 1.76535 to 1.75572, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1191/2000
22/22 - 1s - loss: 1.3171 - val_loss: 1.7556
Epoch 1192/2000
22/22 - 1s - loss: 1.3172 - val_loss: 1.7555
Epoch 1193/2000
22/22 - 1s - loss: 1.3174 - val_loss: 1.7536
Epoch 1194/2000
22/22 - 1s - loss: 1.3162 - val_loss: 1.7530
Epoch 1195/2000
22/22 - 1s - loss: 1.3145 - val_loss: 1.7525
Epoch 1196/2000
22/22 - 1s - loss: 1.3124 - val_loss: 1.7514
Epoch 1197/2000
22/22 - 1s - loss: 1.3112 - val_loss: 1.7516
Epoch 1198/2000
22/22 - 1s - loss: 1.3139 - val_loss: 1.7502
Epoch 1199/2000
22/22 - 1s - loss: 1.3111 - val_loss: 1.7487
Epoch 1200/2000
22/22 - 1s - loss: 1.3117 - val_loss: 1.7470
Epoch 01200: val_loss improved from 1.75572 to 1.74696, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1201/2000
22/22 - 1s - loss: 1.3095 - val_loss: 1.7471
Epoch 1202/2000
22/22 - 1s - loss: 1.3095 - val_loss: 1.7473
Epoch 1203/2000
22/22 - 1s - loss: 1.3060 - val_loss: 1.7458
Epoch 1204/2000
22/22 - 1s - loss: 1.3073 - val_loss: 1.7458
Epoch 1205/2000
22/22 - 1s - loss: 1.3054 - val_loss: 1.7446
Epoch 1206/2000
22/22 - 1s - loss: 1.3051 - val_loss: 1.7431
Epoch 1207/2000
22/22 - 1s - loss: 1.3045 - val_loss: 1.7424
Epoch 1208/2000
22/22 - 1s - loss: 1.3032 - val_loss: 1.7405
Epoch 1209/2000
22/22 - 1s - loss: 1.3019 - val_loss: 1.7403
Epoch 1210/2000
22/22 - 1s - loss: 1.3007 - val_loss: 1.7400
Epoch 01210: val_loss improved from 1.74696 to 1.74001, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1211/2000
22/22 - 1s - loss: 1.3015 - val_loss: 1.7384
Epoch 1212/2000
22/22 - 1s - loss: 1.2991 - val_loss: 1.7376
Epoch 1213/2000
22/22 - 1s - loss: 1.2982 - val_loss: 1.7370
Epoch 1214/2000
22/22 - 1s - loss: 1.2978 - val_loss: 1.7359
Epoch 1215/2000
22/22 - 1s - loss: 1.2975 - val_loss: 1.7352
Epoch 1216/2000
22/22 - 1s - loss: 1.2972 - val_loss: 1.7341
Epoch 1217/2000
22/22 - 1s - loss: 1.2954 - val_loss: 1.7336
Epoch 1218/2000
22/22 - 1s - loss: 1.2951 - val_loss: 1.7320
Epoch 1219/2000
22/22 - 1s - loss: 1.2938 - val_loss: 1.7308
Epoch 1220/2000
22/22 - 1s - loss: 1.2921 - val_loss: 1.7305
Epoch 01220: val_loss improved from 1.74001 to 1.73049, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1221/2000
22/22 - 1s - loss: 1.2907 - val_loss: 1.7298
Epoch 1222/2000
22/22 - 1s - loss: 1.2918 - val_loss: 1.7283
Epoch 1223/2000
22/22 - 1s - loss: 1.2897 - val_loss: 1.7267
Epoch 1224/2000
22/22 - 1s - loss: 1.2887 - val_loss: 1.7265
Epoch 1225/2000
22/22 - 1s - loss: 1.2849 - val_loss: 1.7244
Epoch 1226/2000
22/22 - 1s - loss: 1.2847 - val_loss: 1.7246
Epoch 1227/2000
22/22 - 1s - loss: 1.2860 - val_loss: 1.7239
Epoch 1228/2000
22/22 - 1s - loss: 1.2844 - val_loss: 1.7230
Epoch 1229/2000
22/22 - 1s - loss: 1.2859 - val_loss: 1.7232
Epoch 1230/2000
22/22 - 1s - loss: 1.2824 - val_loss: 1.7229
Epoch 01230: val_loss improved from 1.73049 to 1.72294, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1231/2000
22/22 - 1s - loss: 1.2841 - val_loss: 1.7210
Epoch 1232/2000
22/22 - 1s - loss: 1.2829 - val_loss: 1.7203
Epoch 1233/2000
22/22 - 1s - loss: 1.2823 - val_loss: 1.7190
Epoch 1234/2000
22/22 - 1s - loss: 1.2798 - val_loss: 1.7188
Epoch 1235/2000
22/22 - 1s - loss: 1.2783 - val_loss: 1.7183
Epoch 1236/2000
22/22 - 1s - loss: 1.2791 - val_loss: 1.7164
Epoch 1237/2000
22/22 - 1s - loss: 1.2776 - val_loss: 1.7162
Epoch 1238/2000
22/22 - 1s - loss: 1.2769 - val_loss: 1.7152
Epoch 1239/2000
22/22 - 1s - loss: 1.2760 - val_loss: 1.7141
Epoch 1240/2000
22/22 - 1s - loss: 1.2756 - val_loss: 1.7128
Epoch 01240: val_loss improved from 1.72294 to 1.71281, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1241/2000
22/22 - 1s - loss: 1.2742 - val_loss: 1.7129
Epoch 1242/2000
22/22 - 1s - loss: 1.2743 - val_loss: 1.7116
Epoch 1243/2000
22/22 - 1s - loss: 1.2750 - val_loss: 1.7097
Epoch 1244/2000
22/22 - 1s - loss: 1.2704 - val_loss: 1.7099
Epoch 1245/2000
22/22 - 1s - loss: 1.2697 - val_loss: 1.7090
Epoch 1246/2000
22/22 - 1s - loss: 1.2709 - val_loss: 1.7077
Epoch 1247/2000
22/22 - 1s - loss: 1.2688 - val_loss: 1.7078
Epoch 1248/2000
22/22 - 1s - loss: 1.2663 - val_loss: 1.7070
Epoch 1249/2000
22/22 - 1s - loss: 1.2683 - val_loss: 1.7061
Epoch 1250/2000
22/22 - 1s - loss: 1.2674 - val_loss: 1.7057
Epoch 01250: val_loss improved from 1.71281 to 1.70571, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1251/2000
22/22 - 1s - loss: 1.2674 - val_loss: 1.7048
Epoch 1252/2000
22/22 - 1s - loss: 1.2658 - val_loss: 1.7032
Epoch 1253/2000
22/22 - 1s - loss: 1.2650 - val_loss: 1.7015
Epoch 1254/2000
22/22 - 1s - loss: 1.2637 - val_loss: 1.7011
Epoch 1255/2000
22/22 - 1s - loss: 1.2636 - val_loss: 1.7005
Epoch 1256/2000
22/22 - 1s - loss: 1.2609 - val_loss: 1.6991
Epoch 1257/2000
22/22 - 1s - loss: 1.2614 - val_loss: 1.6982
Epoch 1258/2000
22/22 - 1s - loss: 1.2601 - val_loss: 1.6973
Epoch 1259/2000
22/22 - 1s - loss: 1.2583 - val_loss: 1.6969
Epoch 1260/2000
22/22 - 1s - loss: 1.2589 - val_loss: 1.6968
Epoch 01260: val_loss improved from 1.70571 to 1.69675, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1261/2000
22/22 - 1s - loss: 1.2593 - val_loss: 1.6958
Epoch 1262/2000
22/22 - 1s - loss: 1.2558 - val_loss: 1.6953
Epoch 1263/2000
22/22 - 1s - loss: 1.2547 - val_loss: 1.6939
Epoch 1264/2000
22/22 - 1s - loss: 1.2555 - val_loss: 1.6929
Epoch 1265/2000
22/22 - 1s - loss: 1.2547 - val_loss: 1.6918
Epoch 1266/2000
22/22 - 1s - loss: 1.2540 - val_loss: 1.6911
Epoch 1267/2000
22/22 - 1s - loss: 1.2519 - val_loss: 1.6897
Epoch 1268/2000
22/22 - 1s - loss: 1.2500 - val_loss: 1.6899
Epoch 1269/2000
22/22 - 1s - loss: 1.2508 - val_loss: 1.6898
Epoch 1270/2000
22/22 - 1s - loss: 1.2488 - val_loss: 1.6873
Epoch 01270: val_loss improved from 1.69675 to 1.68734, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1271/2000
22/22 - 1s - loss: 1.2485 - val_loss: 1.6866
Epoch 1272/2000
22/22 - 1s - loss: 1.2470 - val_loss: 1.6866
Epoch 1273/2000
22/22 - 1s - loss: 1.2492 - val_loss: 1.6849
Epoch 1274/2000
22/22 - 1s - loss: 1.2450 - val_loss: 1.6845
Epoch 1275/2000
22/22 - 1s - loss: 1.2446 - val_loss: 1.6836
Epoch 1276/2000
22/22 - 1s - loss: 1.2437 - val_loss: 1.6819
Epoch 1277/2000
22/22 - 1s - loss: 1.2436 - val_loss: 1.6815
Epoch 1278/2000
22/22 - 1s - loss: 1.2438 - val_loss: 1.6812
Epoch 1279/2000
22/22 - 1s - loss: 1.2431 - val_loss: 1.6800
Epoch 1280/2000
22/22 - 1s - loss: 1.2435 - val_loss: 1.6797
Epoch 01280: val_loss improved from 1.68734 to 1.67974, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1281/2000
22/22 - 1s - loss: 1.2398 - val_loss: 1.6790
Epoch 1282/2000
22/22 - 1s - loss: 1.2396 - val_loss: 1.6795
Epoch 1283/2000
22/22 - 1s - loss: 1.2378 - val_loss: 1.6784
Epoch 1284/2000
22/22 - 1s - loss: 1.2377 - val_loss: 1.6769
Epoch 1285/2000
22/22 - 1s - loss: 1.2383 - val_loss: 1.6761
Epoch 1286/2000
22/22 - 1s - loss: 1.2354 - val_loss: 1.6751
Epoch 1287/2000
22/22 - 1s - loss: 1.2369 - val_loss: 1.6742
Epoch 1288/2000
22/22 - 1s - loss: 1.2373 - val_loss: 1.6732
Epoch 1289/2000
22/22 - 1s - loss: 1.2345 - val_loss: 1.6723
Epoch 1290/2000
22/22 - 1s - loss: 1.2328 - val_loss: 1.6720
Epoch 01290: val_loss improved from 1.67974 to 1.67196, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1291/2000
22/22 - 1s - loss: 1.2332 - val_loss: 1.6704
Epoch 1292/2000
22/22 - 1s - loss: 1.2313 - val_loss: 1.6703
Epoch 1293/2000
22/22 - 1s - loss: 1.2298 - val_loss: 1.6699
Epoch 1294/2000
22/22 - 1s - loss: 1.2302 - val_loss: 1.6691
Epoch 1295/2000
22/22 - 1s - loss: 1.2284 - val_loss: 1.6685
Epoch 1296/2000
22/22 - 1s - loss: 1.2280 - val_loss: 1.6670
Epoch 1297/2000
22/22 - 1s - loss: 1.2279 - val_loss: 1.6668
Epoch 1298/2000
22/22 - 1s - loss: 1.2267 - val_loss: 1.6653
Epoch 1299/2000
22/22 - 1s - loss: 1.2262 - val_loss: 1.6646
Epoch 1300/2000
22/22 - 1s - loss: 1.2260 - val_loss: 1.6647
Epoch 01300: val_loss improved from 1.67196 to 1.66471, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1301/2000
22/22 - 1s - loss: 1.2257 - val_loss: 1.6640
Epoch 1302/2000
22/22 - 1s - loss: 1.2250 - val_loss: 1.6632
Epoch 1303/2000
22/22 - 1s - loss: 1.2238 - val_loss: 1.6622
Epoch 1304/2000
22/22 - 1s - loss: 1.2209 - val_loss: 1.6618
Epoch 1305/2000
22/22 - 1s - loss: 1.2214 - val_loss: 1.6605
Epoch 1306/2000
22/22 - 1s - loss: 1.2197 - val_loss: 1.6610
Epoch 1307/2000
22/22 - 1s - loss: 1.2199 - val_loss: 1.6592
Epoch 1308/2000
22/22 - 1s - loss: 1.2208 - val_loss: 1.6582
Epoch 1309/2000
22/22 - 1s - loss: 1.2178 - val_loss: 1.6580
Epoch 1310/2000
22/22 - 1s - loss: 1.2188 - val_loss: 1.6561
Epoch 01310: val_loss improved from 1.66471 to 1.65606, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1311/2000
22/22 - 1s - loss: 1.2165 - val_loss: 1.6559
Epoch 1312/2000
22/22 - 1s - loss: 1.2166 - val_loss: 1.6548
Epoch 1313/2000
22/22 - 1s - loss: 1.2143 - val_loss: 1.6546
Epoch 1314/2000
22/22 - 1s - loss: 1.2136 - val_loss: 1.6533
Epoch 1315/2000
22/22 - 1s - loss: 1.2124 - val_loss: 1.6523
Epoch 1316/2000
22/22 - 1s - loss: 1.2108 - val_loss: 1.6509
Epoch 1317/2000
22/22 - 1s - loss: 1.2104 - val_loss: 1.6506
Epoch 1318/2000
22/22 - 1s - loss: 1.2127 - val_loss: 1.6500
Epoch 1319/2000
22/22 - 1s - loss: 1.2099 - val_loss: 1.6487
Epoch 1320/2000
22/22 - 1s - loss: 1.2097 - val_loss: 1.6492
Epoch 01320: val_loss improved from 1.65606 to 1.64921, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1321/2000
22/22 - 1s - loss: 1.2088 - val_loss: 1.6477
Epoch 1322/2000
22/22 - 1s - loss: 1.2083 - val_loss: 1.6471
Epoch 1323/2000
22/22 - 1s - loss: 1.2076 - val_loss: 1.6466
Epoch 1324/2000
22/22 - 1s - loss: 1.2065 - val_loss: 1.6458
Epoch 1325/2000
22/22 - 1s - loss: 1.2058 - val_loss: 1.6449
Epoch 1326/2000
22/22 - 1s - loss: 1.2054 - val_loss: 1.6444
Epoch 1327/2000
22/22 - 1s - loss: 1.2036 - val_loss: 1.6424
Epoch 1328/2000
22/22 - 1s - loss: 1.2031 - val_loss: 1.6420
Epoch 1329/2000
22/22 - 1s - loss: 1.2026 - val_loss: 1.6407
Epoch 1330/2000
22/22 - 1s - loss: 1.2034 - val_loss: 1.6402
Epoch 01330: val_loss improved from 1.64921 to 1.64023, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1331/2000
22/22 - 1s - loss: 1.2001 - val_loss: 1.6393
Epoch 1332/2000
22/22 - 1s - loss: 1.1999 - val_loss: 1.6391
Epoch 1333/2000
22/22 - 1s - loss: 1.2002 - val_loss: 1.6378
Epoch 1334/2000
22/22 - 1s - loss: 1.1989 - val_loss: 1.6370
Epoch 1335/2000
22/22 - 1s - loss: 1.1985 - val_loss: 1.6370
Epoch 1336/2000
22/22 - 1s - loss: 1.1971 - val_loss: 1.6364
Epoch 1337/2000
22/22 - 1s - loss: 1.1963 - val_loss: 1.6352
Epoch 1338/2000
22/22 - 1s - loss: 1.1946 - val_loss: 1.6347
Epoch 1339/2000
22/22 - 1s - loss: 1.1955 - val_loss: 1.6344
Epoch 1340/2000
22/22 - 1s - loss: 1.1941 - val_loss: 1.6326
Epoch 01340: val_loss improved from 1.64023 to 1.63257, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1341/2000
22/22 - 1s - loss: 1.1944 - val_loss: 1.6313
Epoch 1342/2000
22/22 - 1s - loss: 1.1915 - val_loss: 1.6318
Epoch 1343/2000
22/22 - 1s - loss: 1.1916 - val_loss: 1.6306
Epoch 1344/2000
22/22 - 1s - loss: 1.1901 - val_loss: 1.6295
Epoch 1345/2000
22/22 - 1s - loss: 1.1901 - val_loss: 1.6291
Epoch 1346/2000
22/22 - 1s - loss: 1.1900 - val_loss: 1.6288
Epoch 1347/2000
22/22 - 1s - loss: 1.1882 - val_loss: 1.6283
Epoch 1348/2000
22/22 - 1s - loss: 1.1872 - val_loss: 1.6275
Epoch 1349/2000
22/22 - 1s - loss: 1.1876 - val_loss: 1.6257
Epoch 1350/2000
22/22 - 1s - loss: 1.1859 - val_loss: 1.6250
Epoch 01350: val_loss improved from 1.63257 to 1.62500, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1351/2000
22/22 - 1s - loss: 1.1857 - val_loss: 1.6240
Epoch 1352/2000
22/22 - 1s - loss: 1.1839 - val_loss: 1.6240
Epoch 1353/2000
22/22 - 1s - loss: 1.1838 - val_loss: 1.6225
Epoch 1354/2000
22/22 - 1s - loss: 1.1837 - val_loss: 1.6220
Epoch 1355/2000
22/22 - 1s - loss: 1.1828 - val_loss: 1.6205
Epoch 1356/2000
22/22 - 1s - loss: 1.1817 - val_loss: 1.6194
Epoch 1357/2000
22/22 - 1s - loss: 1.1816 - val_loss: 1.6188
Epoch 1358/2000
22/22 - 1s - loss: 1.1796 - val_loss: 1.6187
Epoch 1359/2000
22/22 - 1s - loss: 1.1774 - val_loss: 1.6190
Epoch 1360/2000
22/22 - 1s - loss: 1.1779 - val_loss: 1.6181
Epoch 01360: val_loss improved from 1.62500 to 1.61809, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1361/2000
22/22 - 1s - loss: 1.1773 - val_loss: 1.6173
Epoch 1362/2000
22/22 - 1s - loss: 1.1776 - val_loss: 1.6165
Epoch 1363/2000
22/22 - 1s - loss: 1.1763 - val_loss: 1.6164
Epoch 1364/2000
22/22 - 1s - loss: 1.1757 - val_loss: 1.6148
Epoch 1365/2000
22/22 - 1s - loss: 1.1766 - val_loss: 1.6153
Epoch 1366/2000
22/22 - 1s - loss: 1.1744 - val_loss: 1.6137
Epoch 1367/2000
22/22 - 1s - loss: 1.1719 - val_loss: 1.6135
Epoch 1368/2000
22/22 - 1s - loss: 1.1729 - val_loss: 1.6128
Epoch 1369/2000
22/22 - 1s - loss: 1.1712 - val_loss: 1.6124
Epoch 1370/2000
22/22 - 1s - loss: 1.1685 - val_loss: 1.6119
Epoch 01370: val_loss improved from 1.61809 to 1.61187, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1371/2000
22/22 - 1s - loss: 1.1703 - val_loss: 1.6102
Epoch 1372/2000
22/22 - 1s - loss: 1.1695 - val_loss: 1.6099
Epoch 1373/2000
22/22 - 1s - loss: 1.1690 - val_loss: 1.6089
Epoch 1374/2000
22/22 - 1s - loss: 1.1680 - val_loss: 1.6076
Epoch 1375/2000
22/22 - 1s - loss: 1.1670 - val_loss: 1.6072
Epoch 1376/2000
22/22 - 1s - loss: 1.1677 - val_loss: 1.6073
Epoch 1377/2000
22/22 - 1s - loss: 1.1657 - val_loss: 1.6059
Epoch 1378/2000
22/22 - 1s - loss: 1.1659 - val_loss: 1.6051
Epoch 1379/2000
22/22 - 1s - loss: 1.1620 - val_loss: 1.6045
Epoch 1380/2000
22/22 - 1s - loss: 1.1649 - val_loss: 1.6044
Epoch 01380: val_loss improved from 1.61187 to 1.60443, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1381/2000
22/22 - 1s - loss: 1.1631 - val_loss: 1.6029
Epoch 1382/2000
22/22 - 1s - loss: 1.1623 - val_loss: 1.6023
Epoch 1383/2000
22/22 - 1s - loss: 1.1612 - val_loss: 1.6012
Epoch 1384/2000
22/22 - 1s - loss: 1.1617 - val_loss: 1.6002
Epoch 1385/2000
22/22 - 1s - loss: 1.1597 - val_loss: 1.5989
Epoch 1386/2000
22/22 - 1s - loss: 1.1599 - val_loss: 1.5988
Epoch 1387/2000
22/22 - 1s - loss: 1.1593 - val_loss: 1.5988
Epoch 1388/2000
22/22 - 1s - loss: 1.1595 - val_loss: 1.5985
Epoch 1389/2000
22/22 - 1s - loss: 1.1578 - val_loss: 1.5977
Epoch 1390/2000
22/22 - 1s - loss: 1.1565 - val_loss: 1.5971
Epoch 01390: val_loss improved from 1.60443 to 1.59711, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1391/2000
22/22 - 1s - loss: 1.1553 - val_loss: 1.5960
Epoch 1392/2000
22/22 - 1s - loss: 1.1545 - val_loss: 1.5961
Epoch 1393/2000
22/22 - 1s - loss: 1.1535 - val_loss: 1.5931
Epoch 1394/2000
22/22 - 1s - loss: 1.1550 - val_loss: 1.5935
Epoch 1395/2000
22/22 - 1s - loss: 1.1552 - val_loss: 1.5929
Epoch 1396/2000
22/22 - 1s - loss: 1.1531 - val_loss: 1.5929
Epoch 1397/2000
22/22 - 1s - loss: 1.1502 - val_loss: 1.5919
Epoch 1398/2000
22/22 - 1s - loss: 1.1505 - val_loss: 1.5909
Epoch 1399/2000
22/22 - 1s - loss: 1.1503 - val_loss: 1.5902
Epoch 1400/2000
22/22 - 1s - loss: 1.1487 - val_loss: 1.5883
Epoch 01400: val_loss improved from 1.59711 to 1.58835, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1401/2000
22/22 - 1s - loss: 1.1471 - val_loss: 1.5872
Epoch 1402/2000
22/22 - 1s - loss: 1.1495 - val_loss: 1.5881
Epoch 1403/2000
22/22 - 1s - loss: 1.1483 - val_loss: 1.5867
Epoch 1404/2000
22/22 - 1s - loss: 1.1440 - val_loss: 1.5866
Epoch 1405/2000
22/22 - 1s - loss: 1.1456 - val_loss: 1.5857
Epoch 1406/2000
22/22 - 1s - loss: 1.1452 - val_loss: 1.5855
Epoch 1407/2000
22/22 - 1s - loss: 1.1455 - val_loss: 1.5841
Epoch 1408/2000
22/22 - 1s - loss: 1.1432 - val_loss: 1.5830
Epoch 1409/2000
22/22 - 1s - loss: 1.1422 - val_loss: 1.5816
Epoch 1410/2000
22/22 - 1s - loss: 1.1432 - val_loss: 1.5819
Epoch 01410: val_loss improved from 1.58835 to 1.58190, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1411/2000
22/22 - 1s - loss: 1.1407 - val_loss: 1.5816
Epoch 1412/2000
22/22 - 1s - loss: 1.1409 - val_loss: 1.5808
Epoch 1413/2000
22/22 - 1s - loss: 1.1414 - val_loss: 1.5799
Epoch 1414/2000
22/22 - 1s - loss: 1.1400 - val_loss: 1.5786
Epoch 1415/2000
22/22 - 1s - loss: 1.1382 - val_loss: 1.5785
Epoch 1416/2000
22/22 - 1s - loss: 1.1386 - val_loss: 1.5781
Epoch 1417/2000
22/22 - 1s - loss: 1.1378 - val_loss: 1.5770
Epoch 1418/2000
22/22 - 1s - loss: 1.1365 - val_loss: 1.5765
Epoch 1419/2000
22/22 - 1s - loss: 1.1356 - val_loss: 1.5753
Epoch 1420/2000
22/22 - 1s - loss: 1.1368 - val_loss: 1.5745
Epoch 01420: val_loss improved from 1.58190 to 1.57453, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1421/2000
22/22 - 1s - loss: 1.1345 - val_loss: 1.5746
Epoch 1422/2000
22/22 - 1s - loss: 1.1329 - val_loss: 1.5735
Epoch 1423/2000
22/22 - 1s - loss: 1.1328 - val_loss: 1.5724
Epoch 1424/2000
22/22 - 1s - loss: 1.1316 - val_loss: 1.5723
Epoch 1425/2000
22/22 - 1s - loss: 1.1317 - val_loss: 1.5710
Epoch 1426/2000
22/22 - 1s - loss: 1.1297 - val_loss: 1.5704
Epoch 1427/2000
22/22 - 1s - loss: 1.1292 - val_loss: 1.5707
Epoch 1428/2000
22/22 - 1s - loss: 1.1291 - val_loss: 1.5696
Epoch 1429/2000
22/22 - 1s - loss: 1.1290 - val_loss: 1.5694
Epoch 1430/2000
22/22 - 1s - loss: 1.1282 - val_loss: 1.5691
Epoch 01430: val_loss improved from 1.57453 to 1.56913, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1431/2000
22/22 - 1s - loss: 1.1281 - val_loss: 1.5666
Epoch 1432/2000
22/22 - 1s - loss: 1.1287 - val_loss: 1.5654
Epoch 1433/2000
22/22 - 1s - loss: 1.1250 - val_loss: 1.5657
Epoch 1434/2000
22/22 - 1s - loss: 1.1262 - val_loss: 1.5655
Epoch 1435/2000
22/22 - 1s - loss: 1.1221 - val_loss: 1.5640
Epoch 1436/2000
22/22 - 1s - loss: 1.1241 - val_loss: 1.5641
Epoch 1437/2000
22/22 - 1s - loss: 1.1227 - val_loss: 1.5637
Epoch 1438/2000
22/22 - 1s - loss: 1.1237 - val_loss: 1.5633
Epoch 1439/2000
22/22 - 1s - loss: 1.1221 - val_loss: 1.5619
Epoch 1440/2000
22/22 - 1s - loss: 1.1197 - val_loss: 1.5627
Epoch 01440: val_loss improved from 1.56913 to 1.56272, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1441/2000
22/22 - 1s - loss: 1.1210 - val_loss: 1.5606
Epoch 1442/2000
22/22 - 1s - loss: 1.1198 - val_loss: 1.5604
Epoch 1443/2000
22/22 - 1s - loss: 1.1184 - val_loss: 1.5595
Epoch 1444/2000
22/22 - 1s - loss: 1.1179 - val_loss: 1.5583
Epoch 1445/2000
22/22 - 1s - loss: 1.1194 - val_loss: 1.5582
Epoch 1446/2000
22/22 - 1s - loss: 1.1155 - val_loss: 1.5594
Epoch 1447/2000
22/22 - 1s - loss: 1.1170 - val_loss: 1.5575
Epoch 1448/2000
22/22 - 1s - loss: 1.1161 - val_loss: 1.5561
Epoch 1449/2000
22/22 - 1s - loss: 1.1148 - val_loss: 1.5560
Epoch 1450/2000
22/22 - 1s - loss: 1.1156 - val_loss: 1.5553
Epoch 01450: val_loss improved from 1.56272 to 1.55530, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1451/2000
22/22 - 1s - loss: 1.1133 - val_loss: 1.5541
Epoch 1452/2000
22/22 - 1s - loss: 1.1130 - val_loss: 1.5531
Epoch 1453/2000
22/22 - 1s - loss: 1.1114 - val_loss: 1.5521
Epoch 1454/2000
22/22 - 1s - loss: 1.1108 - val_loss: 1.5510
Epoch 1455/2000
22/22 - 1s - loss: 1.1102 - val_loss: 1.5504
Epoch 1456/2000
22/22 - 1s - loss: 1.1099 - val_loss: 1.5498
Epoch 1457/2000
22/22 - 2s - loss: 1.1064 - val_loss: 1.5500
Epoch 1458/2000
22/22 - 1s - loss: 1.1091 - val_loss: 1.5491
Epoch 1459/2000
22/22 - 1s - loss: 1.1094 - val_loss: 1.5483
Epoch 1460/2000
22/22 - 1s - loss: 1.1063 - val_loss: 1.5472
Epoch 01460: val_loss improved from 1.55530 to 1.54722, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1461/2000
22/22 - 1s - loss: 1.1072 - val_loss: 1.5480
Epoch 1462/2000
22/22 - 1s - loss: 1.1049 - val_loss: 1.5465
Epoch 1463/2000
22/22 - 1s - loss: 1.1032 - val_loss: 1.5460
Epoch 1464/2000
22/22 - 1s - loss: 1.1052 - val_loss: 1.5453
Epoch 1465/2000
22/22 - 1s - loss: 1.1042 - val_loss: 1.5449
Epoch 1466/2000
22/22 - 1s - loss: 1.1034 - val_loss: 1.5440
Epoch 1467/2000
22/22 - 1s - loss: 1.1021 - val_loss: 1.5444
Epoch 1468/2000
22/22 - 1s - loss: 1.1040 - val_loss: 1.5431
Epoch 1469/2000
22/22 - 1s - loss: 1.1019 - val_loss: 1.5423
Epoch 1470/2000
22/22 - 1s - loss: 1.1003 - val_loss: 1.5413
Epoch 01470: val_loss improved from 1.54722 to 1.54126, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1471/2000
22/22 - 1s - loss: 1.1001 - val_loss: 1.5406
Epoch 1472/2000
22/22 - 1s - loss: 1.0998 - val_loss: 1.5392
Epoch 1473/2000
22/22 - 1s - loss: 1.0969 - val_loss: 1.5397
Epoch 1474/2000
22/22 - 1s - loss: 1.0979 - val_loss: 1.5389
Epoch 1475/2000
22/22 - 1s - loss: 1.0963 - val_loss: 1.5385
Epoch 1476/2000
22/22 - 1s - loss: 1.0967 - val_loss: 1.5386
Epoch 1477/2000
22/22 - 1s - loss: 1.0960 - val_loss: 1.5374
Epoch 1478/2000
22/22 - 1s - loss: 1.0943 - val_loss: 1.5365
Epoch 1479/2000
22/22 - 1s - loss: 1.0954 - val_loss: 1.5353
Epoch 1480/2000
22/22 - 1s - loss: 1.0934 - val_loss: 1.5349
Epoch 01480: val_loss improved from 1.54126 to 1.53491, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1481/2000
22/22 - 1s - loss: 1.0938 - val_loss: 1.5334
Epoch 1482/2000
22/22 - 1s - loss: 1.0919 - val_loss: 1.5318
Epoch 1483/2000
22/22 - 1s - loss: 1.0914 - val_loss: 1.5314
Epoch 1484/2000
22/22 - 1s - loss: 1.0919 - val_loss: 1.5309
Epoch 1485/2000
22/22 - 1s - loss: 1.0899 - val_loss: 1.5310
Epoch 1486/2000
22/22 - 1s - loss: 1.0896 - val_loss: 1.5300
Epoch 1487/2000
22/22 - 1s - loss: 1.0895 - val_loss: 1.5296
Epoch 1488/2000
22/22 - 1s - loss: 1.0887 - val_loss: 1.5295
Epoch 1489/2000
22/22 - 1s - loss: 1.0884 - val_loss: 1.5277
Epoch 1490/2000
22/22 - 1s - loss: 1.0856 - val_loss: 1.5277
Epoch 01490: val_loss improved from 1.53491 to 1.52766, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1491/2000
22/22 - 1s - loss: 1.0860 - val_loss: 1.5272
Epoch 1492/2000
22/22 - 1s - loss: 1.0857 - val_loss: 1.5275
Epoch 1493/2000
22/22 - 1s - loss: 1.0868 - val_loss: 1.5267
Epoch 1494/2000
22/22 - 1s - loss: 1.0849 - val_loss: 1.5259
Epoch 1495/2000
22/22 - 1s - loss: 1.0850 - val_loss: 1.5248
Epoch 1496/2000
22/22 - 1s - loss: 1.0832 - val_loss: 1.5247
Epoch 1497/2000
22/22 - 1s - loss: 1.0838 - val_loss: 1.5237
Epoch 1498/2000
22/22 - 1s - loss: 1.0830 - val_loss: 1.5234
Epoch 1499/2000
22/22 - 1s - loss: 1.0816 - val_loss: 1.5227
Epoch 1500/2000
22/22 - 1s - loss: 1.0815 - val_loss: 1.5219
Epoch 01500: val_loss improved from 1.52766 to 1.52186, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1501/2000
22/22 - 1s - loss: 1.0807 - val_loss: 1.5216
Epoch 1502/2000
22/22 - 1s - loss: 1.0804 - val_loss: 1.5201
Epoch 1503/2000
22/22 - 1s - loss: 1.0786 - val_loss: 1.5197
Epoch 1504/2000
22/22 - 1s - loss: 1.0780 - val_loss: 1.5190
Epoch 1505/2000
22/22 - 1s - loss: 1.0787 - val_loss: 1.5191
Epoch 1506/2000
22/22 - 1s - loss: 1.0774 - val_loss: 1.5182
Epoch 1507/2000
22/22 - 1s - loss: 1.0764 - val_loss: 1.5178
Epoch 1508/2000
22/22 - 1s - loss: 1.0765 - val_loss: 1.5168
Epoch 1509/2000
22/22 - 1s - loss: 1.0751 - val_loss: 1.5161
Epoch 1510/2000
22/22 - 1s - loss: 1.0734 - val_loss: 1.5159
Epoch 01510: val_loss improved from 1.52186 to 1.51590, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1511/2000
22/22 - 1s - loss: 1.0736 - val_loss: 1.5153
Epoch 1512/2000
22/22 - 1s - loss: 1.0730 - val_loss: 1.5148
Epoch 1513/2000
22/22 - 1s - loss: 1.0722 - val_loss: 1.5137
Epoch 1514/2000
22/22 - 1s - loss: 1.0708 - val_loss: 1.5132
Epoch 1515/2000
22/22 - 1s - loss: 1.0719 - val_loss: 1.5125
Epoch 1516/2000
22/22 - 1s - loss: 1.0709 - val_loss: 1.5122
Epoch 1517/2000
22/22 - 1s - loss: 1.0703 - val_loss: 1.5115
Epoch 1518/2000
22/22 - 1s - loss: 1.0693 - val_loss: 1.5105
Epoch 1519/2000
22/22 - 1s - loss: 1.0679 - val_loss: 1.5109
Epoch 1520/2000
22/22 - 1s - loss: 1.0663 - val_loss: 1.5097
Epoch 01520: val_loss improved from 1.51590 to 1.50973, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1521/2000
22/22 - 1s - loss: 1.0676 - val_loss: 1.5080
Epoch 1522/2000
22/22 - 1s - loss: 1.0667 - val_loss: 1.5069
Epoch 1523/2000
22/22 - 1s - loss: 1.0638 - val_loss: 1.5071
Epoch 1524/2000
22/22 - 1s - loss: 1.0662 - val_loss: 1.5073
Epoch 1525/2000
22/22 - 1s - loss: 1.0646 - val_loss: 1.5056
Epoch 1526/2000
22/22 - 1s - loss: 1.0647 - val_loss: 1.5045
Epoch 1527/2000
22/22 - 1s - loss: 1.0615 - val_loss: 1.5044
Epoch 1528/2000
22/22 - 1s - loss: 1.0633 - val_loss: 1.5027
Epoch 1529/2000
22/22 - 1s - loss: 1.0625 - val_loss: 1.5030
Epoch 1530/2000
22/22 - 1s - loss: 1.0617 - val_loss: 1.5016
Epoch 01530: val_loss improved from 1.50973 to 1.50158, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1531/2000
22/22 - 1s - loss: 1.0623 - val_loss: 1.5016
Epoch 1532/2000
22/22 - 1s - loss: 1.0609 - val_loss: 1.5013
Epoch 1533/2000
22/22 - 1s - loss: 1.0599 - val_loss: 1.5005
Epoch 1534/2000
22/22 - 1s - loss: 1.0581 - val_loss: 1.4989
Epoch 1535/2000
22/22 - 1s - loss: 1.0603 - val_loss: 1.4990
Epoch 1536/2000
22/22 - 1s - loss: 1.0582 - val_loss: 1.4980
Epoch 1537/2000
22/22 - 1s - loss: 1.0576 - val_loss: 1.4976
Epoch 1538/2000
22/22 - 1s - loss: 1.0559 - val_loss: 1.4971
Epoch 1539/2000
22/22 - 1s - loss: 1.0548 - val_loss: 1.4971
Epoch 1540/2000
22/22 - 1s - loss: 1.0563 - val_loss: 1.4966
Epoch 01540: val_loss improved from 1.50158 to 1.49657, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1541/2000
22/22 - 1s - loss: 1.0549 - val_loss: 1.4963
Epoch 1542/2000
22/22 - 1s - loss: 1.0532 - val_loss: 1.4956
Epoch 1543/2000
22/22 - 1s - loss: 1.0552 - val_loss: 1.4947
Epoch 1544/2000
22/22 - 1s - loss: 1.0538 - val_loss: 1.4935
Epoch 1545/2000
22/22 - 1s - loss: 1.0522 - val_loss: 1.4927
Epoch 1546/2000
22/22 - 1s - loss: 1.0506 - val_loss: 1.4929
Epoch 1547/2000
22/22 - 1s - loss: 1.0515 - val_loss: 1.4923
Epoch 1548/2000
22/22 - 1s - loss: 1.0513 - val_loss: 1.4901
Epoch 1549/2000
22/22 - 1s - loss: 1.0500 - val_loss: 1.4912
Epoch 1550/2000
22/22 - 1s - loss: 1.0479 - val_loss: 1.4901
Epoch 01550: val_loss improved from 1.49657 to 1.49010, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1551/2000
22/22 - 1s - loss: 1.0482 - val_loss: 1.4895
Epoch 1552/2000
22/22 - 1s - loss: 1.0475 - val_loss: 1.4878
Epoch 1553/2000
22/22 - 1s - loss: 1.0476 - val_loss: 1.4882
Epoch 1554/2000
22/22 - 1s - loss: 1.0475 - val_loss: 1.4881
Epoch 1555/2000
22/22 - 1s - loss: 1.0459 - val_loss: 1.4881
Epoch 1556/2000
22/22 - 1s - loss: 1.0458 - val_loss: 1.4875
Epoch 1557/2000
22/22 - 1s - loss: 1.0465 - val_loss: 1.4872
Epoch 1558/2000
22/22 - 1s - loss: 1.0436 - val_loss: 1.4857
Epoch 1559/2000
22/22 - 1s - loss: 1.0438 - val_loss: 1.4855
Epoch 1560/2000
22/22 - 1s - loss: 1.0437 - val_loss: 1.4851
Epoch 01560: val_loss improved from 1.49010 to 1.48510, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1561/2000
22/22 - 1s - loss: 1.0435 - val_loss: 1.4843
Epoch 1562/2000
22/22 - 1s - loss: 1.0415 - val_loss: 1.4838
Epoch 1563/2000
22/22 - 1s - loss: 1.0412 - val_loss: 1.4827
Epoch 1564/2000
22/22 - 1s - loss: 1.0401 - val_loss: 1.4816
Epoch 1565/2000
22/22 - 1s - loss: 1.0397 - val_loss: 1.4818
Epoch 1566/2000
22/22 - 1s - loss: 1.0399 - val_loss: 1.4810
Epoch 1567/2000
22/22 - 1s - loss: 1.0382 - val_loss: 1.4798
Epoch 1568/2000
22/22 - 1s - loss: 1.0381 - val_loss: 1.4784
Epoch 1569/2000
22/22 - 1s - loss: 1.0391 - val_loss: 1.4772
Epoch 1570/2000
22/22 - 1s - loss: 1.0373 - val_loss: 1.4781
Epoch 01570: val_loss improved from 1.48510 to 1.47811, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1571/2000
22/22 - 1s - loss: 1.0381 - val_loss: 1.4763
Epoch 1572/2000
22/22 - 1s - loss: 1.0366 - val_loss: 1.4765
Epoch 1573/2000
22/22 - 1s - loss: 1.0344 - val_loss: 1.4762
Epoch 1574/2000
22/22 - 1s - loss: 1.0365 - val_loss: 1.4763
Epoch 1575/2000
22/22 - 1s - loss: 1.0343 - val_loss: 1.4737
Epoch 1576/2000
22/22 - 1s - loss: 1.0321 - val_loss: 1.4735
Epoch 1577/2000
22/22 - 1s - loss: 1.0341 - val_loss: 1.4736
Epoch 1578/2000
22/22 - 1s - loss: 1.0330 - val_loss: 1.4740
Epoch 1579/2000
22/22 - 1s - loss: 1.0310 - val_loss: 1.4728
Epoch 1580/2000
22/22 - 1s - loss: 1.0314 - val_loss: 1.4718
Epoch 01580: val_loss improved from 1.47811 to 1.47184, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1581/2000
22/22 - 1s - loss: 1.0303 - val_loss: 1.4708
Epoch 1582/2000
22/22 - 1s - loss: 1.0296 - val_loss: 1.4709
Epoch 1583/2000
22/22 - 1s - loss: 1.0292 - val_loss: 1.4703
Epoch 1584/2000
22/22 - 1s - loss: 1.0277 - val_loss: 1.4699
Epoch 1585/2000
22/22 - 1s - loss: 1.0271 - val_loss: 1.4694
Epoch 1586/2000
22/22 - 1s - loss: 1.0272 - val_loss: 1.4686
Epoch 1587/2000
22/22 - 1s - loss: 1.0257 - val_loss: 1.4688
Epoch 1588/2000
22/22 - 1s - loss: 1.0254 - val_loss: 1.4673
Epoch 1589/2000
22/22 - 1s - loss: 1.0243 - val_loss: 1.4668
Epoch 1590/2000
22/22 - 1s - loss: 1.0244 - val_loss: 1.4663
Epoch 01590: val_loss improved from 1.47184 to 1.46628, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1591/2000
22/22 - 1s - loss: 1.0239 - val_loss: 1.4663
Epoch 1592/2000
22/22 - 1s - loss: 1.0232 - val_loss: 1.4653
Epoch 1593/2000
22/22 - 1s - loss: 1.0216 - val_loss: 1.4649
Epoch 1594/2000
22/22 - 1s - loss: 1.0226 - val_loss: 1.4645
Epoch 1595/2000
22/22 - 1s - loss: 1.0214 - val_loss: 1.4632
Epoch 1596/2000
22/22 - 1s - loss: 1.0208 - val_loss: 1.4629
Epoch 1597/2000
22/22 - 1s - loss: 1.0210 - val_loss: 1.4617
Epoch 1598/2000
22/22 - 1s - loss: 1.0204 - val_loss: 1.4615
Epoch 1599/2000
22/22 - 1s - loss: 1.0187 - val_loss: 1.4618
Epoch 1600/2000
22/22 - 1s - loss: 1.0181 - val_loss: 1.4612
Epoch 01600: val_loss improved from 1.46628 to 1.46125, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1601/2000
22/22 - 1s - loss: 1.0168 - val_loss: 1.4610
Epoch 1602/2000
22/22 - 1s - loss: 1.0172 - val_loss: 1.4589
Epoch 1603/2000
22/22 - 1s - loss: 1.0177 - val_loss: 1.4575
Epoch 1604/2000
22/22 - 1s - loss: 1.0174 - val_loss: 1.4573
Epoch 1605/2000
22/22 - 1s - loss: 1.0169 - val_loss: 1.4576
Epoch 1606/2000
22/22 - 1s - loss: 1.0157 - val_loss: 1.4574
Epoch 1607/2000
22/22 - 1s - loss: 1.0157 - val_loss: 1.4570
Epoch 1608/2000
22/22 - 1s - loss: 1.0158 - val_loss: 1.4559
Epoch 1609/2000
22/22 - 1s - loss: 1.0130 - val_loss: 1.4553
Epoch 1610/2000
22/22 - 1s - loss: 1.0112 - val_loss: 1.4544
Epoch 01610: val_loss improved from 1.46125 to 1.45442, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1611/2000
22/22 - 1s - loss: 1.0127 - val_loss: 1.4549
Epoch 1612/2000
22/22 - 1s - loss: 1.0123 - val_loss: 1.4541
Epoch 1613/2000
22/22 - 1s - loss: 1.0122 - val_loss: 1.4537
Epoch 1614/2000
22/22 - 1s - loss: 1.0131 - val_loss: 1.4532
Epoch 1615/2000
22/22 - 1s - loss: 1.0089 - val_loss: 1.4516
Epoch 1616/2000
22/22 - 1s - loss: 1.0095 - val_loss: 1.4507
Epoch 1617/2000
22/22 - 1s - loss: 1.0092 - val_loss: 1.4506
Epoch 1618/2000
22/22 - 1s - loss: 1.0090 - val_loss: 1.4508
Epoch 1619/2000
22/22 - 1s - loss: 1.0082 - val_loss: 1.4499
Epoch 1620/2000
22/22 - 1s - loss: 1.0070 - val_loss: 1.4482
Epoch 01620: val_loss improved from 1.45442 to 1.44819, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1621/2000
22/22 - 1s - loss: 1.0051 - val_loss: 1.4484
Epoch 1622/2000
22/22 - 1s - loss: 1.0087 - val_loss: 1.4479
Epoch 1623/2000
22/22 - 1s - loss: 1.0069 - val_loss: 1.4480
Epoch 1624/2000
22/22 - 1s - loss: 1.0040 - val_loss: 1.4480
Epoch 1625/2000
22/22 - 1s - loss: 1.0051 - val_loss: 1.4473
Epoch 1626/2000
22/22 - 1s - loss: 1.0042 - val_loss: 1.4469
Epoch 1627/2000
22/22 - 1s - loss: 1.0040 - val_loss: 1.4455
Epoch 1628/2000
22/22 - 1s - loss: 1.0043 - val_loss: 1.4447
Epoch 1629/2000
22/22 - 1s - loss: 1.0014 - val_loss: 1.4441
Epoch 1630/2000
22/22 - 1s - loss: 1.0028 - val_loss: 1.4439
Epoch 01630: val_loss improved from 1.44819 to 1.44389, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1631/2000
22/22 - 1s - loss: 1.0004 - val_loss: 1.4434
Epoch 1632/2000
22/22 - 1s - loss: 0.9998 - val_loss: 1.4425
Epoch 1633/2000
22/22 - 1s - loss: 0.9978 - val_loss: 1.4410
Epoch 1634/2000
22/22 - 1s - loss: 0.9989 - val_loss: 1.4407
Epoch 1635/2000
22/22 - 1s - loss: 0.9995 - val_loss: 1.4397
Epoch 1636/2000
22/22 - 1s - loss: 0.9973 - val_loss: 1.4401
Epoch 1637/2000
22/22 - 1s - loss: 0.9971 - val_loss: 1.4401
Epoch 1638/2000
22/22 - 1s - loss: 0.9963 - val_loss: 1.4393
Epoch 1639/2000
22/22 - 1s - loss: 0.9986 - val_loss: 1.4377
Epoch 1640/2000
22/22 - 1s - loss: 0.9971 - val_loss: 1.4373
Epoch 01640: val_loss improved from 1.44389 to 1.43734, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1641/2000
22/22 - 1s - loss: 0.9962 - val_loss: 1.4363
Epoch 1642/2000
22/22 - 1s - loss: 0.9964 - val_loss: 1.4360
Epoch 1643/2000
22/22 - 1s - loss: 0.9922 - val_loss: 1.4359
Epoch 1644/2000
22/22 - 1s - loss: 0.9937 - val_loss: 1.4351
Epoch 1645/2000
22/22 - 1s - loss: 0.9934 - val_loss: 1.4348
Epoch 1646/2000
22/22 - 1s - loss: 0.9942 - val_loss: 1.4335
Epoch 1647/2000
22/22 - 1s - loss: 0.9915 - val_loss: 1.4324
Epoch 1648/2000
22/22 - 1s - loss: 0.9929 - val_loss: 1.4325
Epoch 1649/2000
22/22 - 1s - loss: 0.9914 - val_loss: 1.4324
Epoch 1650/2000
22/22 - 1s - loss: 0.9901 - val_loss: 1.4309
Epoch 01650: val_loss improved from 1.43734 to 1.43087, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1651/2000
22/22 - 1s - loss: 0.9916 - val_loss: 1.4309
Epoch 1652/2000
22/22 - 1s - loss: 0.9893 - val_loss: 1.4302
Epoch 1653/2000
22/22 - 1s - loss: 0.9911 - val_loss: 1.4298
Epoch 1654/2000
22/22 - 1s - loss: 0.9877 - val_loss: 1.4295
Epoch 1655/2000
22/22 - 1s - loss: 0.9852 - val_loss: 1.4288
Epoch 1656/2000
22/22 - 1s - loss: 0.9867 - val_loss: 1.4286
Epoch 1657/2000
22/22 - 1s - loss: 0.9860 - val_loss: 1.4278
Epoch 1658/2000
22/22 - 1s - loss: 0.9857 - val_loss: 1.4279
Epoch 1659/2000
22/22 - 1s - loss: 0.9865 - val_loss: 1.4271
Epoch 1660/2000
22/22 - 1s - loss: 0.9853 - val_loss: 1.4256
Epoch 01660: val_loss improved from 1.43087 to 1.42562, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1661/2000
22/22 - 1s - loss: 0.9850 - val_loss: 1.4246
Epoch 1662/2000
22/22 - 1s - loss: 0.9855 - val_loss: 1.4251
Epoch 1663/2000
22/22 - 1s - loss: 0.9841 - val_loss: 1.4245
Epoch 1664/2000
22/22 - 1s - loss: 0.9824 - val_loss: 1.4239
Epoch 1665/2000
22/22 - 1s - loss: 0.9837 - val_loss: 1.4236
Epoch 1666/2000
22/22 - 1s - loss: 0.9808 - val_loss: 1.4227
Epoch 1667/2000
22/22 - 1s - loss: 0.9814 - val_loss: 1.4214
Epoch 1668/2000
22/22 - 1s - loss: 0.9800 - val_loss: 1.4221
Epoch 1669/2000
22/22 - 1s - loss: 0.9805 - val_loss: 1.4211
Epoch 1670/2000
22/22 - 1s - loss: 0.9788 - val_loss: 1.4209
Epoch 01670: val_loss improved from 1.42562 to 1.42090, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1671/2000
22/22 - 1s - loss: 0.9800 - val_loss: 1.4200
Epoch 1672/2000
22/22 - 1s - loss: 0.9790 - val_loss: 1.4198
Epoch 1673/2000
22/22 - 1s - loss: 0.9789 - val_loss: 1.4194
Epoch 1674/2000
22/22 - 1s - loss: 0.9783 - val_loss: 1.4201
Epoch 1675/2000
22/22 - 1s - loss: 0.9763 - val_loss: 1.4194
Epoch 1676/2000
22/22 - 1s - loss: 0.9767 - val_loss: 1.4183
Epoch 1677/2000
22/22 - 1s - loss: 0.9743 - val_loss: 1.4173
Epoch 1678/2000
22/22 - 1s - loss: 0.9759 - val_loss: 1.4177
Epoch 1679/2000
22/22 - 1s - loss: 0.9749 - val_loss: 1.4169
Epoch 1680/2000
22/22 - 1s - loss: 0.9736 - val_loss: 1.4173
Epoch 01680: val_loss improved from 1.42090 to 1.41725, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1681/2000
22/22 - 1s - loss: 0.9737 - val_loss: 1.4158
Epoch 1682/2000
22/22 - 1s - loss: 0.9733 - val_loss: 1.4158
Epoch 1683/2000
22/22 - 1s - loss: 0.9732 - val_loss: 1.4151
Epoch 1684/2000
22/22 - 1s - loss: 0.9723 - val_loss: 1.4138
Epoch 1685/2000
22/22 - 1s - loss: 0.9729 - val_loss: 1.4132
Epoch 1686/2000
22/22 - 1s - loss: 0.9704 - val_loss: 1.4128
Epoch 1687/2000
22/22 - 1s - loss: 0.9701 - val_loss: 1.4119
Epoch 1688/2000
22/22 - 1s - loss: 0.9693 - val_loss: 1.4119
Epoch 1689/2000
22/22 - 1s - loss: 0.9692 - val_loss: 1.4106
Epoch 1690/2000
22/22 - 1s - loss: 0.9685 - val_loss: 1.4107
Epoch 01690: val_loss improved from 1.41725 to 1.41066, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1691/2000
22/22 - 1s - loss: 0.9681 - val_loss: 1.4102
Epoch 1692/2000
22/22 - 1s - loss: 0.9671 - val_loss: 1.4082
Epoch 1693/2000
22/22 - 1s - loss: 0.9661 - val_loss: 1.4081
Epoch 1694/2000
22/22 - 1s - loss: 0.9657 - val_loss: 1.4069
Epoch 1695/2000
22/22 - 1s - loss: 0.9650 - val_loss: 1.4075
Epoch 1696/2000
22/22 - 1s - loss: 0.9652 - val_loss: 1.4075
Epoch 1697/2000
22/22 - 1s - loss: 0.9637 - val_loss: 1.4067
Epoch 1698/2000
22/22 - 1s - loss: 0.9654 - val_loss: 1.4070
Epoch 1699/2000
22/22 - 1s - loss: 0.9634 - val_loss: 1.4056
Epoch 1700/2000
22/22 - 1s - loss: 0.9641 - val_loss: 1.4055
Epoch 01700: val_loss improved from 1.41066 to 1.40546, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1701/2000
22/22 - 1s - loss: 0.9630 - val_loss: 1.4038
Epoch 1702/2000
22/22 - 1s - loss: 0.9621 - val_loss: 1.4037
Epoch 1703/2000
22/22 - 1s - loss: 0.9609 - val_loss: 1.4038
Epoch 1704/2000
22/22 - 1s - loss: 0.9607 - val_loss: 1.4035
Epoch 1705/2000
22/22 - 1s - loss: 0.9616 - val_loss: 1.4032
Epoch 1706/2000
22/22 - 1s - loss: 0.9600 - val_loss: 1.4022
Epoch 1707/2000
22/22 - 1s - loss: 0.9590 - val_loss: 1.4021
Epoch 1708/2000
22/22 - 1s - loss: 0.9604 - val_loss: 1.4019
Epoch 1709/2000
22/22 - 1s - loss: 0.9599 - val_loss: 1.3997
Epoch 1710/2000
22/22 - 1s - loss: 0.9568 - val_loss: 1.3996
Epoch 01710: val_loss improved from 1.40546 to 1.39960, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1711/2000
22/22 - 1s - loss: 0.9565 - val_loss: 1.3990
Epoch 1712/2000
22/22 - 1s - loss: 0.9572 - val_loss: 1.3990
Epoch 1713/2000
22/22 - 1s - loss: 0.9541 - val_loss: 1.3985
Epoch 1714/2000
22/22 - 1s - loss: 0.9546 - val_loss: 1.3990
Epoch 1715/2000
22/22 - 1s - loss: 0.9545 - val_loss: 1.3982
Epoch 1716/2000
22/22 - 1s - loss: 0.9542 - val_loss: 1.3973
Epoch 1717/2000
22/22 - 1s - loss: 0.9531 - val_loss: 1.3964
Epoch 1718/2000
22/22 - 1s - loss: 0.9536 - val_loss: 1.3959
Epoch 1719/2000
22/22 - 1s - loss: 0.9517 - val_loss: 1.3959
Epoch 1720/2000
22/22 - 1s - loss: 0.9529 - val_loss: 1.3958
Epoch 01720: val_loss improved from 1.39960 to 1.39581, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1721/2000
22/22 - 1s - loss: 0.9525 - val_loss: 1.3950
Epoch 1722/2000
22/22 - 1s - loss: 0.9513 - val_loss: 1.3948
Epoch 1723/2000
22/22 - 1s - loss: 0.9516 - val_loss: 1.3946
Epoch 1724/2000
22/22 - 1s - loss: 0.9512 - val_loss: 1.3936
Epoch 1725/2000
22/22 - 1s - loss: 0.9488 - val_loss: 1.3925
Epoch 1726/2000
22/22 - 1s - loss: 0.9503 - val_loss: 1.3920
Epoch 1727/2000
22/22 - 1s - loss: 0.9509 - val_loss: 1.3903
Epoch 1728/2000
22/22 - 1s - loss: 0.9494 - val_loss: 1.3898
Epoch 1729/2000
22/22 - 1s - loss: 0.9497 - val_loss: 1.3897
Epoch 1730/2000
22/22 - 1s - loss: 0.9469 - val_loss: 1.3890
Epoch 01730: val_loss improved from 1.39581 to 1.38897, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1731/2000
22/22 - 1s - loss: 0.9472 - val_loss: 1.3888
Epoch 1732/2000
22/22 - 1s - loss: 0.9456 - val_loss: 1.3892
Epoch 1733/2000
22/22 - 1s - loss: 0.9464 - val_loss: 1.3886
Epoch 1734/2000
22/22 - 1s - loss: 0.9468 - val_loss: 1.3880
Epoch 1735/2000
22/22 - 1s - loss: 0.9457 - val_loss: 1.3871
Epoch 1736/2000
22/22 - 1s - loss: 0.9447 - val_loss: 1.3864
Epoch 1737/2000
22/22 - 1s - loss: 0.9438 - val_loss: 1.3865
Epoch 1738/2000
22/22 - 1s - loss: 0.9435 - val_loss: 1.3859
Epoch 1739/2000
22/22 - 1s - loss: 0.9424 - val_loss: 1.3865
Epoch 1740/2000
22/22 - 1s - loss: 0.9426 - val_loss: 1.3853
Epoch 01740: val_loss improved from 1.38897 to 1.38529, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1741/2000
22/22 - 1s - loss: 0.9430 - val_loss: 1.3855
Epoch 1742/2000
22/22 - 1s - loss: 0.9407 - val_loss: 1.3837
Epoch 1743/2000
22/22 - 1s - loss: 0.9401 - val_loss: 1.3834
Epoch 1744/2000
22/22 - 1s - loss: 0.9418 - val_loss: 1.3823
Epoch 1745/2000
22/22 - 1s - loss: 0.9408 - val_loss: 1.3822
Epoch 1746/2000
22/22 - 1s - loss: 0.9384 - val_loss: 1.3812
Epoch 1747/2000
22/22 - 1s - loss: 0.9400 - val_loss: 1.3814
Epoch 1748/2000
22/22 - 1s - loss: 0.9384 - val_loss: 1.3820
Epoch 1749/2000
22/22 - 1s - loss: 0.9379 - val_loss: 1.3813
Epoch 1750/2000
22/22 - 1s - loss: 0.9370 - val_loss: 1.3802
Epoch 01750: val_loss improved from 1.38529 to 1.38020, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1751/2000
22/22 - 1s - loss: 0.9365 - val_loss: 1.3792
Epoch 1752/2000
22/22 - 1s - loss: 0.9358 - val_loss: 1.3792
Epoch 1753/2000
22/22 - 1s - loss: 0.9364 - val_loss: 1.3783
Epoch 1754/2000
22/22 - 1s - loss: 0.9350 - val_loss: 1.3784
Epoch 1755/2000
22/22 - 1s - loss: 0.9353 - val_loss: 1.3779
Epoch 1756/2000
22/22 - 1s - loss: 0.9348 - val_loss: 1.3765
Epoch 1757/2000
22/22 - 1s - loss: 0.9336 - val_loss: 1.3758
Epoch 1758/2000
22/22 - 1s - loss: 0.9328 - val_loss: 1.3745
Epoch 1759/2000
22/22 - 1s - loss: 0.9324 - val_loss: 1.3751
Epoch 1760/2000
22/22 - 1s - loss: 0.9311 - val_loss: 1.3752
Epoch 01760: val_loss improved from 1.38020 to 1.37521, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1761/2000
22/22 - 1s - loss: 0.9321 - val_loss: 1.3739
Epoch 1762/2000
22/22 - 1s - loss: 0.9308 - val_loss: 1.3735
Epoch 1763/2000
22/22 - 1s - loss: 0.9303 - val_loss: 1.3733
Epoch 1764/2000
22/22 - 1s - loss: 0.9314 - val_loss: 1.3726
Epoch 1765/2000
22/22 - 1s - loss: 0.9301 - val_loss: 1.3739
Epoch 1766/2000
22/22 - 1s - loss: 0.9304 - val_loss: 1.3721
Epoch 1767/2000
22/22 - 1s - loss: 0.9291 - val_loss: 1.3715
Epoch 1768/2000
22/22 - 1s - loss: 0.9276 - val_loss: 1.3709
Epoch 1769/2000
22/22 - 1s - loss: 0.9286 - val_loss: 1.3702
Epoch 1770/2000
22/22 - 1s - loss: 0.9277 - val_loss: 1.3695
Epoch 01770: val_loss improved from 1.37521 to 1.36945, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1771/2000
22/22 - 1s - loss: 0.9260 - val_loss: 1.3696
Epoch 1772/2000
22/22 - 1s - loss: 0.9262 - val_loss: 1.3695
Epoch 1773/2000
22/22 - 1s - loss: 0.9277 - val_loss: 1.3698
Epoch 1774/2000
22/22 - 1s - loss: 0.9229 - val_loss: 1.3683
Epoch 1775/2000
22/22 - 1s - loss: 0.9235 - val_loss: 1.3676
Epoch 1776/2000
22/22 - 1s - loss: 0.9242 - val_loss: 1.3670
Epoch 1777/2000
22/22 - 1s - loss: 0.9244 - val_loss: 1.3672
Epoch 1778/2000
22/22 - 1s - loss: 0.9226 - val_loss: 1.3661
Epoch 1779/2000
22/22 - 1s - loss: 0.9217 - val_loss: 1.3651
Epoch 1780/2000
22/22 - 1s - loss: 0.9233 - val_loss: 1.3641
Epoch 01780: val_loss improved from 1.36945 to 1.36405, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1781/2000
22/22 - 1s - loss: 0.9204 - val_loss: 1.3643
Epoch 1782/2000
22/22 - 1s - loss: 0.9234 - val_loss: 1.3639
Epoch 1783/2000
22/22 - 1s - loss: 0.9210 - val_loss: 1.3633
Epoch 1784/2000
22/22 - 1s - loss: 0.9195 - val_loss: 1.3636
Epoch 1785/2000
22/22 - 1s - loss: 0.9197 - val_loss: 1.3623
Epoch 1786/2000
22/22 - 1s - loss: 0.9196 - val_loss: 1.3610
Epoch 1787/2000
22/22 - 1s - loss: 0.9200 - val_loss: 1.3611
Epoch 1788/2000
22/22 - 1s - loss: 0.9176 - val_loss: 1.3617
Epoch 1789/2000
22/22 - 1s - loss: 0.9176 - val_loss: 1.3606
Epoch 1790/2000
22/22 - 1s - loss: 0.9173 - val_loss: 1.3604
Epoch 01790: val_loss improved from 1.36405 to 1.36037, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1791/2000
22/22 - 1s - loss: 0.9160 - val_loss: 1.3596
Epoch 1792/2000
22/22 - 1s - loss: 0.9156 - val_loss: 1.3595
Epoch 1793/2000
22/22 - 1s - loss: 0.9153 - val_loss: 1.3594
Epoch 1794/2000
22/22 - 1s - loss: 0.9158 - val_loss: 1.3590
Epoch 1795/2000
22/22 - 1s - loss: 0.9160 - val_loss: 1.3579
Epoch 1796/2000
22/22 - 1s - loss: 0.9144 - val_loss: 1.3578
Epoch 1797/2000
22/22 - 1s - loss: 0.9139 - val_loss: 1.3564
Epoch 1798/2000
22/22 - 1s - loss: 0.9136 - val_loss: 1.3550
Epoch 1799/2000
22/22 - 1s - loss: 0.9151 - val_loss: 1.3558
Epoch 1800/2000
22/22 - 1s - loss: 0.9117 - val_loss: 1.3557
Epoch 01800: val_loss improved from 1.36037 to 1.35574, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1801/2000
22/22 - 1s - loss: 0.9105 - val_loss: 1.3553
Epoch 1802/2000
22/22 - 1s - loss: 0.9119 - val_loss: 1.3553
Epoch 1803/2000
22/22 - 1s - loss: 0.9115 - val_loss: 1.3545
Epoch 1804/2000
22/22 - 1s - loss: 0.9117 - val_loss: 1.3549
Epoch 1805/2000
22/22 - 1s - loss: 0.9099 - val_loss: 1.3540
Epoch 1806/2000
22/22 - 1s - loss: 0.9113 - val_loss: 1.3533
Epoch 1807/2000
22/22 - 1s - loss: 0.9088 - val_loss: 1.3518
Epoch 1808/2000
22/22 - 1s - loss: 0.9097 - val_loss: 1.3519
Epoch 1809/2000
22/22 - 1s - loss: 0.9089 - val_loss: 1.3500
Epoch 1810/2000
22/22 - 1s - loss: 0.9073 - val_loss: 1.3501
Epoch 01810: val_loss improved from 1.35574 to 1.35014, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1811/2000
22/22 - 1s - loss: 0.9073 - val_loss: 1.3489
Epoch 1812/2000
22/22 - 1s - loss: 0.9063 - val_loss: 1.3485
Epoch 1813/2000
22/22 - 1s - loss: 0.9064 - val_loss: 1.3487
Epoch 1814/2000
22/22 - 1s - loss: 0.9078 - val_loss: 1.3479
Epoch 1815/2000
22/22 - 1s - loss: 0.9066 - val_loss: 1.3471
Epoch 1816/2000
22/22 - 1s - loss: 0.9049 - val_loss: 1.3477
Epoch 1817/2000
22/22 - 1s - loss: 0.9034 - val_loss: 1.3477
Epoch 1818/2000
22/22 - 1s - loss: 0.9056 - val_loss: 1.3462
Epoch 1819/2000
22/22 - 1s - loss: 0.9042 - val_loss: 1.3464
Epoch 1820/2000
22/22 - 1s - loss: 0.9031 - val_loss: 1.3461
Epoch 01820: val_loss improved from 1.35014 to 1.34606, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1821/2000
22/22 - 1s - loss: 0.9025 - val_loss: 1.3447
Epoch 1822/2000
22/22 - 1s - loss: 0.9023 - val_loss: 1.3449
Epoch 1823/2000
22/22 - 1s - loss: 0.9004 - val_loss: 1.3441
Epoch 1824/2000
22/22 - 1s - loss: 0.9037 - val_loss: 1.3440
Epoch 1825/2000
22/22 - 1s - loss: 0.9000 - val_loss: 1.3431
Epoch 1826/2000
22/22 - 1s - loss: 0.8998 - val_loss: 1.3425
Epoch 1827/2000
22/22 - 1s - loss: 0.8993 - val_loss: 1.3415
Epoch 1828/2000
22/22 - 1s - loss: 0.8984 - val_loss: 1.3407
Epoch 1829/2000
22/22 - 1s - loss: 0.8986 - val_loss: 1.3417
Epoch 1830/2000
22/22 - 1s - loss: 0.8996 - val_loss: 1.3411
Epoch 01830: val_loss improved from 1.34606 to 1.34106, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1831/2000
22/22 - 1s - loss: 0.8980 - val_loss: 1.3411
Epoch 1832/2000
22/22 - 1s - loss: 0.9004 - val_loss: 1.3403
Epoch 1833/2000
22/22 - 1s - loss: 0.8971 - val_loss: 1.3404
Epoch 1834/2000
22/22 - 1s - loss: 0.8956 - val_loss: 1.3397
Epoch 1835/2000
22/22 - 1s - loss: 0.8965 - val_loss: 1.3397
Epoch 1836/2000
22/22 - 1s - loss: 0.8958 - val_loss: 1.3394
Epoch 1837/2000
22/22 - 1s - loss: 0.8954 - val_loss: 1.3387
Epoch 1838/2000
22/22 - 1s - loss: 0.8957 - val_loss: 1.3386
Epoch 1839/2000
22/22 - 1s - loss: 0.8949 - val_loss: 1.3366
Epoch 1840/2000
22/22 - 1s - loss: 0.8950 - val_loss: 1.3368
Epoch 01840: val_loss improved from 1.34106 to 1.33683, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1841/2000
22/22 - 1s - loss: 0.8938 - val_loss: 1.3358
Epoch 1842/2000
22/22 - 1s - loss: 0.8922 - val_loss: 1.3367
Epoch 1843/2000
22/22 - 1s - loss: 0.8924 - val_loss: 1.3359
Epoch 1844/2000
22/22 - 1s - loss: 0.8935 - val_loss: 1.3348
Epoch 1845/2000
22/22 - 1s - loss: 0.8911 - val_loss: 1.3343
Epoch 1846/2000
22/22 - 1s - loss: 0.8906 - val_loss: 1.3341
Epoch 1847/2000
22/22 - 1s - loss: 0.8903 - val_loss: 1.3332
Epoch 1848/2000
22/22 - 1s - loss: 0.8893 - val_loss: 1.3332
Epoch 1849/2000
22/22 - 1s - loss: 0.8893 - val_loss: 1.3322
Epoch 1850/2000
22/22 - 1s - loss: 0.8898 - val_loss: 1.3312
Epoch 01850: val_loss improved from 1.33683 to 1.33117, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1851/2000
22/22 - 1s - loss: 0.8885 - val_loss: 1.3309
Epoch 1852/2000
22/22 - 1s - loss: 0.8888 - val_loss: 1.3306
Epoch 1853/2000
22/22 - 1s - loss: 0.8886 - val_loss: 1.3296
Epoch 1854/2000
22/22 - 1s - loss: 0.8878 - val_loss: 1.3303
Epoch 1855/2000
22/22 - 1s - loss: 0.8876 - val_loss: 1.3293
Epoch 1856/2000
22/22 - 1s - loss: 0.8878 - val_loss: 1.3304
Epoch 1857/2000
22/22 - 1s - loss: 0.8855 - val_loss: 1.3288
Epoch 1858/2000
22/22 - 1s - loss: 0.8873 - val_loss: 1.3282
Epoch 1859/2000
22/22 - 1s - loss: 0.8845 - val_loss: 1.3278
Epoch 1860/2000
22/22 - 1s - loss: 0.8848 - val_loss: 1.3286
Epoch 01860: val_loss improved from 1.33117 to 1.32860, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1861/2000
22/22 - 1s - loss: 0.8854 - val_loss: 1.3270
Epoch 1862/2000
22/22 - 1s - loss: 0.8856 - val_loss: 1.3274
Epoch 1863/2000
22/22 - 1s - loss: 0.8824 - val_loss: 1.3267
Epoch 1864/2000
22/22 - 1s - loss: 0.8821 - val_loss: 1.3269
Epoch 1865/2000
22/22 - 1s - loss: 0.8804 - val_loss: 1.3268
Epoch 1866/2000
22/22 - 1s - loss: 0.8830 - val_loss: 1.3253
Epoch 1867/2000
22/22 - 1s - loss: 0.8822 - val_loss: 1.3237
Epoch 1868/2000
22/22 - 1s - loss: 0.8804 - val_loss: 1.3232
Epoch 1869/2000
22/22 - 1s - loss: 0.8805 - val_loss: 1.3241
Epoch 1870/2000
22/22 - 1s - loss: 0.8786 - val_loss: 1.3240
Epoch 01870: val_loss improved from 1.32860 to 1.32403, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1871/2000
22/22 - 1s - loss: 0.8806 - val_loss: 1.3232
Epoch 1872/2000
22/22 - 1s - loss: 0.8791 - val_loss: 1.3226
Epoch 1873/2000
22/22 - 1s - loss: 0.8809 - val_loss: 1.3226
Epoch 1874/2000
22/22 - 1s - loss: 0.8781 - val_loss: 1.3223
Epoch 1875/2000
22/22 - 1s - loss: 0.8791 - val_loss: 1.3207
Epoch 1876/2000
22/22 - 1s - loss: 0.8776 - val_loss: 1.3210
Epoch 1877/2000
22/22 - 1s - loss: 0.8781 - val_loss: 1.3195
Epoch 1878/2000
22/22 - 1s - loss: 0.8767 - val_loss: 1.3192
Epoch 1879/2000
22/22 - 1s - loss: 0.8770 - val_loss: 1.3187
Epoch 1880/2000
22/22 - 1s - loss: 0.8762 - val_loss: 1.3175
Epoch 01880: val_loss improved from 1.32403 to 1.31749, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1881/2000
22/22 - 1s - loss: 0.8778 - val_loss: 1.3173
Epoch 1882/2000
22/22 - 1s - loss: 0.8755 - val_loss: 1.3180
Epoch 1883/2000
22/22 - 1s - loss: 0.8733 - val_loss: 1.3180
Epoch 1884/2000
22/22 - 1s - loss: 0.8758 - val_loss: 1.3167
Epoch 1885/2000
22/22 - 1s - loss: 0.8734 - val_loss: 1.3156
Epoch 1886/2000
22/22 - 1s - loss: 0.8735 - val_loss: 1.3148
Epoch 1887/2000
22/22 - 1s - loss: 0.8733 - val_loss: 1.3148
Epoch 1888/2000
22/22 - 1s - loss: 0.8715 - val_loss: 1.3154
Epoch 1889/2000
22/22 - 1s - loss: 0.8717 - val_loss: 1.3146
Epoch 1890/2000
22/22 - 1s - loss: 0.8714 - val_loss: 1.3139
Epoch 01890: val_loss improved from 1.31749 to 1.31393, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1891/2000
22/22 - 1s - loss: 0.8719 - val_loss: 1.3146
Epoch 1892/2000
22/22 - 1s - loss: 0.8707 - val_loss: 1.3132
Epoch 1893/2000
22/22 - 1s - loss: 0.8690 - val_loss: 1.3132
Epoch 1894/2000
22/22 - 1s - loss: 0.8702 - val_loss: 1.3125
Epoch 1895/2000
22/22 - 1s - loss: 0.8685 - val_loss: 1.3121
Epoch 1896/2000
22/22 - 1s - loss: 0.8668 - val_loss: 1.3114
Epoch 1897/2000
22/22 - 1s - loss: 0.8667 - val_loss: 1.3103
Epoch 1898/2000
22/22 - 1s - loss: 0.8681 - val_loss: 1.3104
Epoch 1899/2000
22/22 - 1s - loss: 0.8663 - val_loss: 1.3103
Epoch 1900/2000
22/22 - 1s - loss: 0.8653 - val_loss: 1.3105
Epoch 01900: val_loss improved from 1.31393 to 1.31050, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1901/2000
22/22 - 1s - loss: 0.8672 - val_loss: 1.3092
Epoch 1902/2000
22/22 - 1s - loss: 0.8646 - val_loss: 1.3089
Epoch 1903/2000
22/22 - 1s - loss: 0.8660 - val_loss: 1.3084
Epoch 1904/2000
22/22 - 1s - loss: 0.8657 - val_loss: 1.3070
Epoch 1905/2000
22/22 - 1s - loss: 0.8641 - val_loss: 1.3067
Epoch 1906/2000
22/22 - 1s - loss: 0.8634 - val_loss: 1.3070
Epoch 1907/2000
22/22 - 1s - loss: 0.8646 - val_loss: 1.3071
Epoch 1908/2000
22/22 - 1s - loss: 0.8636 - val_loss: 1.3064
Epoch 1909/2000
22/22 - 1s - loss: 0.8645 - val_loss: 1.3070
Epoch 1910/2000
22/22 - 1s - loss: 0.8637 - val_loss: 1.3061
Epoch 01910: val_loss improved from 1.31050 to 1.30606, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1911/2000
22/22 - 1s - loss: 0.8606 - val_loss: 1.3055
Epoch 1912/2000
22/22 - 1s - loss: 0.8641 - val_loss: 1.3058
Epoch 1913/2000
22/22 - 1s - loss: 0.8624 - val_loss: 1.3060
Epoch 1914/2000
22/22 - 1s - loss: 0.8604 - val_loss: 1.3045
Epoch 1915/2000
22/22 - 1s - loss: 0.8616 - val_loss: 1.3040
Epoch 1916/2000
22/22 - 1s - loss: 0.8605 - val_loss: 1.3041
Epoch 1917/2000
22/22 - 1s - loss: 0.8603 - val_loss: 1.3042
Epoch 1918/2000
22/22 - 1s - loss: 0.8595 - val_loss: 1.3026
Epoch 1919/2000
22/22 - 1s - loss: 0.8586 - val_loss: 1.3033
Epoch 1920/2000
22/22 - 1s - loss: 0.8590 - val_loss: 1.3022
Epoch 01920: val_loss improved from 1.30606 to 1.30216, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1921/2000
22/22 - 1s - loss: 0.8578 - val_loss: 1.3017
Epoch 1922/2000
22/22 - 1s - loss: 0.8585 - val_loss: 1.3002
Epoch 1923/2000
22/22 - 1s - loss: 0.8573 - val_loss: 1.2998
Epoch 1924/2000
22/22 - 1s - loss: 0.8573 - val_loss: 1.2996
Epoch 1925/2000
22/22 - 1s - loss: 0.8563 - val_loss: 1.2985
Epoch 1926/2000
22/22 - 1s - loss: 0.8543 - val_loss: 1.2979
Epoch 1927/2000
22/22 - 1s - loss: 0.8545 - val_loss: 1.2977
Epoch 1928/2000
22/22 - 1s - loss: 0.8539 - val_loss: 1.2980
Epoch 1929/2000
22/22 - 1s - loss: 0.8565 - val_loss: 1.2967
Epoch 1930/2000
22/22 - 1s - loss: 0.8544 - val_loss: 1.2971
Epoch 01930: val_loss improved from 1.30216 to 1.29708, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1931/2000
22/22 - 1s - loss: 0.8539 - val_loss: 1.2972
Epoch 1932/2000
22/22 - 1s - loss: 0.8541 - val_loss: 1.2959
Epoch 1933/2000
22/22 - 1s - loss: 0.8523 - val_loss: 1.2961
Epoch 1934/2000
22/22 - 1s - loss: 0.8535 - val_loss: 1.2951
Epoch 1935/2000
22/22 - 1s - loss: 0.8522 - val_loss: 1.2955
Epoch 1936/2000
22/22 - 1s - loss: 0.8520 - val_loss: 1.2945
Epoch 1937/2000
22/22 - 1s - loss: 0.8502 - val_loss: 1.2943
Epoch 1938/2000
22/22 - 1s - loss: 0.8517 - val_loss: 1.2937
Epoch 1939/2000
22/22 - 1s - loss: 0.8512 - val_loss: 1.2937
Epoch 1940/2000
22/22 - 1s - loss: 0.8486 - val_loss: 1.2926
Epoch 01940: val_loss improved from 1.29708 to 1.29255, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1941/2000
22/22 - 1s - loss: 0.8499 - val_loss: 1.2918
Epoch 1942/2000
22/22 - 1s - loss: 0.8489 - val_loss: 1.2923
Epoch 1943/2000
22/22 - 1s - loss: 0.8499 - val_loss: 1.2914
Epoch 1944/2000
22/22 - 1s - loss: 0.8489 - val_loss: 1.2912
Epoch 1945/2000
22/22 - 1s - loss: 0.8474 - val_loss: 1.2907
Epoch 1946/2000
22/22 - 1s - loss: 0.8465 - val_loss: 1.2901
Epoch 1947/2000
22/22 - 1s - loss: 0.8476 - val_loss: 1.2894
Epoch 1948/2000
22/22 - 1s - loss: 0.8456 - val_loss: 1.2889
Epoch 1949/2000
22/22 - 1s - loss: 0.8466 - val_loss: 1.2893
Epoch 1950/2000
22/22 - 1s - loss: 0.8462 - val_loss: 1.2889
Epoch 01950: val_loss improved from 1.29255 to 1.28888, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1951/2000
22/22 - 1s - loss: 0.8460 - val_loss: 1.2880
Epoch 1952/2000
22/22 - 1s - loss: 0.8439 - val_loss: 1.2883
Epoch 1953/2000
22/22 - 1s - loss: 0.8455 - val_loss: 1.2875
Epoch 1954/2000
22/22 - 1s - loss: 0.8436 - val_loss: 1.2876
Epoch 1955/2000
22/22 - 1s - loss: 0.8431 - val_loss: 1.2878
Epoch 1956/2000
22/22 - 1s - loss: 0.8448 - val_loss: 1.2858
Epoch 1957/2000
22/22 - 1s - loss: 0.8431 - val_loss: 1.2862
Epoch 1958/2000
22/22 - 1s - loss: 0.8440 - val_loss: 1.2860
Epoch 1959/2000
22/22 - 1s - loss: 0.8420 - val_loss: 1.2860
Epoch 1960/2000
22/22 - 1s - loss: 0.8424 - val_loss: 1.2848
Epoch 01960: val_loss improved from 1.28888 to 1.28478, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1961/2000
22/22 - 1s - loss: 0.8402 - val_loss: 1.2828
Epoch 1962/2000
22/22 - 1s - loss: 0.8397 - val_loss: 1.2835
Epoch 1963/2000
22/22 - 1s - loss: 0.8405 - val_loss: 1.2830
Epoch 1964/2000
22/22 - 1s - loss: 0.8404 - val_loss: 1.2837
Epoch 1965/2000
22/22 - 1s - loss: 0.8395 - val_loss: 1.2832
Epoch 1966/2000
22/22 - 1s - loss: 0.8396 - val_loss: 1.2826
Epoch 1967/2000
22/22 - 1s - loss: 0.8376 - val_loss: 1.2814
Epoch 1968/2000
22/22 - 1s - loss: 0.8393 - val_loss: 1.2814
Epoch 1969/2000
22/22 - 1s - loss: 0.8382 - val_loss: 1.2809
Epoch 1970/2000
22/22 - 1s - loss: 0.8388 - val_loss: 1.2798
Epoch 01970: val_loss improved from 1.28478 to 1.27976, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1971/2000
22/22 - 1s - loss: 0.8370 - val_loss: 1.2804
Epoch 1972/2000
22/22 - 1s - loss: 0.8358 - val_loss: 1.2801
Epoch 1973/2000
22/22 - 1s - loss: 0.8363 - val_loss: 1.2786
Epoch 1974/2000
22/22 - 1s - loss: 0.8371 - val_loss: 1.2786
Epoch 1975/2000
22/22 - 1s - loss: 0.8357 - val_loss: 1.2780
Epoch 1976/2000
22/22 - 1s - loss: 0.8353 - val_loss: 1.2774
Epoch 1977/2000
22/22 - 1s - loss: 0.8345 - val_loss: 1.2771
Epoch 1978/2000
22/22 - 1s - loss: 0.8342 - val_loss: 1.2769
Epoch 1979/2000
22/22 - 1s - loss: 0.8341 - val_loss: 1.2769
Epoch 1980/2000
22/22 - 1s - loss: 0.8312 - val_loss: 1.2762
Epoch 01980: val_loss improved from 1.27976 to 1.27619, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1981/2000
22/22 - 1s - loss: 0.8330 - val_loss: 1.2774
Epoch 1982/2000
22/22 - 1s - loss: 0.8330 - val_loss: 1.2760
Epoch 1983/2000
22/22 - 1s - loss: 0.8348 - val_loss: 1.2773
Epoch 1984/2000
22/22 - 1s - loss: 0.8330 - val_loss: 1.2750
Epoch 1985/2000
22/22 - 1s - loss: 0.8337 - val_loss: 1.2747
Epoch 1986/2000
22/22 - 1s - loss: 0.8300 - val_loss: 1.2747
Epoch 1987/2000
22/22 - 1s - loss: 0.8306 - val_loss: 1.2735
Epoch 1988/2000
22/22 - 1s - loss: 0.8312 - val_loss: 1.2732
Epoch 1989/2000
22/22 - 1s - loss: 0.8302 - val_loss: 1.2716
Epoch 1990/2000
22/22 - 1s - loss: 0.8300 - val_loss: 1.2721
Epoch 01990: val_loss improved from 1.27619 to 1.27214, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1991/2000
22/22 - 1s - loss: 0.8286 - val_loss: 1.2719
Epoch 1992/2000
22/22 - 1s - loss: 0.8292 - val_loss: 1.2708
Epoch 1993/2000
22/22 - 1s - loss: 0.8288 - val_loss: 1.2709
Epoch 1994/2000
22/22 - 1s - loss: 0.8288 - val_loss: 1.2712
Epoch 1995/2000
22/22 - 1s - loss: 0.8275 - val_loss: 1.2702
Epoch 1996/2000
22/22 - 1s - loss: 0.8270 - val_loss: 1.2701
Epoch 1997/2000
22/22 - 1s - loss: 0.8263 - val_loss: 1.2699
Epoch 1998/2000
22/22 - 1s - loss: 0.8267 - val_loss: 1.2694
Epoch 1999/2000
22/22 - 1s - loss: 0.8268 - val_loss: 1.2683
Epoch 2000/2000
22/22 - 1s - loss: 0.8239 - val_loss: 1.2686
Epoch 02000: val_loss improved from 1.27214 to 1.26856, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
INFO     Computation time for training the single-label model for AR: 41.4 min
INFO     Network type: REG
INFO     Model: "sequential"
INFO     _________________________________________________________________
INFO     Layer (type)                 Output Shape              Param #
INFO     =================================================================
INFO     dense (Dense)                (None, 1024)              2098176
INFO     _________________________________________________________________
INFO     dropout (Dropout)            (None, 1024)              0
INFO     _________________________________________________________________
INFO     dense_1 (Dense)              (None, 512)               524800
INFO     _________________________________________________________________
INFO     dropout_1 (Dropout)          (None, 512)               0
INFO     _________________________________________________________________
INFO     dense_2 (Dense)              (None, 256)               131328
INFO     _________________________________________________________________
INFO     dropout_2 (Dropout)          (None, 256)               0
INFO     _________________________________________________________________
INFO     dense_3 (Dense)              (None, 128)               32896
INFO     _________________________________________________________________
INFO     dropout_3 (Dropout)          (None, 128)               0
INFO     _________________________________________________________________
INFO     dense_4 (Dense)              (None, 1)                 129
INFO     =================================================================
INFO     Total params: 2,787,329
INFO     Trainable params: 2,787,329
INFO     Non-trainable params: 0
INFO     _________________________________________________________________
INFO     Evaluating trained model 'AR single-labeled Fold-3' on test data
INFO     Network type: REG
INFO     Model: "sequential_1"
INFO     _________________________________________________________________
INFO     Layer (type)                 Output Shape              Param #
INFO     =================================================================
INFO     dense_5 (Dense)              (None, 1024)              2098176
INFO     _________________________________________________________________
INFO     dropout_4 (Dropout)          (None, 1024)              0
INFO     _________________________________________________________________
INFO     dense_6 (Dense)              (None, 512)               524800
INFO     _________________________________________________________________
INFO     dropout_5 (Dropout)          (None, 512)               0
INFO     _________________________________________________________________
INFO     dense_7 (Dense)              (None, 256)               131328
INFO     _________________________________________________________________
INFO     dropout_6 (Dropout)          (None, 256)               0
INFO     _________________________________________________________________
INFO     dense_8 (Dense)              (None, 128)               32896
INFO     _________________________________________________________________
INFO     dropout_7 (Dropout)          (None, 128)               0
INFO     _________________________________________________________________
INFO     dense_9 (Dense)              (None, 1)                 129
INFO     =================================================================
INFO     Total params: 2,787,329
INFO     Trainable params: 2,787,329
INFO     Non-trainable params: 0
INFO     _________________________________________________________________
2024-07-15 17:03:12.755271: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.
INFO:tensorflow:Assets written to: /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_saved_model/assets
INFO     Assets written to: /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_saved_model/assets