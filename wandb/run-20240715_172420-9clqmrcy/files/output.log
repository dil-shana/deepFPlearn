ERROR    The specified wabTarget for Weights & Biases tracking does not exist: ARR
INFO     X training matrix of shape (4128, 2048) and type float32
INFO     Y training matrix of shape (4128,) and type float32
INFO     Training of fold number: 1
INFO     Training sample distribution: train data: {-1.2016366720199585: 8, -1.2016383409500122: 5, -1.2016324996948242: 4, -1.2016351222991943: 3, -1.201636552810669: 3, -1.2016377449035645: 3, -1.2016327381134033: 3, -1.201635479927063: 3, -1.2016363143920898: 3, -1.2016353607177734: 3, -1.201636791229248: 3, -1.2016342878341675: 3, -1.2016339302062988: 2, -1.2016295194625854: 2, -1.201627254486084: 2, -1.201621651649475: 2, -1.201635718345642: 2, -1.2016254663467407: 2, -1.2016253471374512: 2, -1.2016302347183228: 2, -1.2016384601593018: 2, -1.201633095741272: 2, -1.201596975326538: 2, -1.2016304731369019: 2, -1.2016375064849854: 2, -1.2016290426254272: 2, -1.201622486114502: 2, -1.2016355991363525: 2, -1.2009780406951904: 2, -1.2016191482543945: 2, -1.2016315460205078: 2, -1.2016288042068481: 2, -1.2016310691833496: 2, -1.2016345262527466: 2, 0.02149348333477974: 1, 1.0667099952697754: 1, -1.2016195058822632: 1, 0.6829988956451416: 1, -0.2558962106704712: 1, -0.425843745470047: 1, 0.06667295098304749: 1, -0.4970245659351349: 1, 1.4458893537521362: 1, 0.5299685597419739: 1, -1.1963087320327759: 1, 0.002237366745248437: 1, 1.4535586833953857: 1, 0.7778733968734741: 1, 0.3664189279079437: 1, 0.20332399010658264: 1, -0.9010018706321716: 1, 0.2900018095970154: 1, 0.5601941347122192: 1, 1.6106586456298828: 1, 1.168498158454895: 1, -0.2516374886035919: 1, 0.10525540262460709: 1, -0.4459679424762726: 1, 1.2952117919921875: 1, 1.1870161294937134: 1, 0.5256680846214294: 1, 1.4836735725402832: 1, 1.3986883163452148: 1, 0.831580638885498: 1, 0.3489625155925751: 1, -0.6966411471366882: 1, 0.6442342400550842: 1, 1.5455868244171143: 1, 0.8313724994659424: 1, -0.16336557269096375: 1, -0.16630633175373077: 1, -0.09881063550710678: 1, 0.2669691741466522: 1, 1.5410983562469482: 1, 1.3818247318267822: 1, 0.21878471970558167: 1, 1.1950836181640625: 1, 1.3501269817352295: 1, 0.34516385197639465: 1, 0.11128426343202591: 1, -0.5620038509368896: 1, 0.007953275926411152: 1, -0.24468590319156647: 1, 1.478103518486023: 1, -1.1840753555297852: 1, 1.3625078201293945: 1, 0.12603989243507385: 1, 1.621954083442688: 1, 1.5633379220962524: 1, 1.593016266822815: 1, 0.9703378081321716: 1, 1.1153340339660645: 1, 1.5667779445648193: 1, -1.201623558998108: 1, -0.3203604817390442: 1, -1.201627492904663: 1, -0.31709083914756775: 1, 0.08119866997003555: 1, -0.35442060232162476: 1, 0.22109845280647278: 1, -0.3040960133075714: 1, -0.7750424146652222: 1, 0.7218993902206421: 1, -0.41311657428741455: 1, 0.0860099047422409: 1, 0.28807583451271057: 1, -0.297276109457016: 1, 0.6573249697685242: 1, 0.4933412969112396: 1, 0.3488617241382599: 1, -0.6442912220954895: 1, -0.5330178141593933: 1, 0.25783771276474: 1, -0.008235386572778225: 1, 0.2470504194498062: 1, -0.9223102331161499: 1, 0.25592291355133057: 1, -0.6546655297279358: 1, 1.3083291053771973: 1, 1.9054079055786133: 1, 0.31334978342056274: 1, -0.20045150816440582: 1, 0.8569538593292236: 1, -0.2393776774406433: 1, 0.6820655465126038: 1, 0.7402693033218384: 1, -1.0873202085494995: 1, 0.540539026260376: 1, 0.10738043487071991: 1, -1.160044550895691: 1, 1.2997812032699585: 1, 1.4923255443572998: 1, 0.5403873920440674: 1, 0.10213274508714676: 1, -0.9669303297996521: 1, 1.1683435440063477: 1, 0.31522658467292786: 1, 0.695344090461731: 1, 0.21625953912734985: 1, 0.2743425965309143: 1, -0.19042591750621796: 1, 1.4424492120742798: 1, -1.189130187034607: 1, 0.0010552277090027928: 1, 0.9285130500793457: 1, 0.3443826735019684: 1, 1.286658525466919: 1, 1.5370275974273682: 1, 0.7614589929580688: 1, 0.6386443972587585: 1, 1.4500402212142944: 1, -0.24653010070323944: 1, 1.088638186454773: 1, -0.22351212799549103: 1, -1.1701372861862183: 1, -0.10035426914691925: 1, -0.2711203992366791: 1, 1.457643747329712: 1, 0.3245508372783661: 1, -0.48075738549232483: 1, -0.13643299043178558: 1, 0.16810350120067596: 1, 0.49536043405532837: 1, 1.2379846572875977: 1, 0.23716896772384644: 1, 0.2500914931297302: 1, 0.15692748129367828: 1, 1.6950387954711914: 1, 0.36155056953430176: 1, 1.595760464668274: 1, 0.8440163731575012: 1, -0.5706648826599121: 1, -0.4307803809642792: 1, 0.26908448338508606: 1, 0.06883639097213745: 1, 1.6063342094421387: 1, 0.2661615014076233: 1, 0.22961212694644928: 1, 0.2954026460647583: 1, -0.18418414890766144: 1, 1.415509581565857: 1, 1.5149681568145752: 1, 0.9998847246170044: 1, 0.44327667355537415: 1, 0.21748410165309906: 1, -0.06838630884885788: 1, 0.31109386682510376: 1, 0.5247108340263367: 1, 1.8254425525665283: 1, 0.8825770616531372: 1, -0.17784874141216278: 1, 0.2610865831375122: 1, 0.04832748696208: 1, 1.4158756732940674: 1, -0.026365874335169792: 1, 0.7279888391494751: 1, 0.7412875294685364: 1, 0.015656888484954834: 1, 1.5035943984985352: 1, 0.34191834926605225: 1, -0.10739652067422867: 1, 0.643775999546051: 1, 1.4653488397598267: 1, 1.3042255640029907: 1, 0.17204155027866364: 1, 1.2753889560699463: 1, 1.4251669645309448: 1, -1.0201867818832397: 1, 0.15129715204238892: 1, 0.2609155476093292: 1, -1.194837212562561: 1, 0.3422311544418335: 1, 1.5088152885437012: 1, 0.36003923416137695: 1, 0.19487899541854858: 1, 0.007752139586955309: 1, -0.1421377956867218: 1, -1.1961579322814941: 1, -1.2016159296035767: 1, -1.2008297443389893: 1, -1.1980621814727783: 1, -1.198028802871704: 1, -1.1962871551513672: 1, -1.201625108718872: 1, -1.1962995529174805: 1, -1.1956098079681396: 1, -1.1269394159317017: 1, -0.7543148398399353: 1, -0.15980762243270874: 1, 1.4907145500183105: 1, -0.8842195272445679: 1, -0.7196366786956787: 1, -0.2302703857421875: 1, -0.5718616843223572: 1, -0.005905755329877138: 1, 1.1079081296920776: 1, -1.1402051448822021: 1, -0.07565759867429733: 1, 0.8374537825584412: 1, -0.26343750953674316: 1, -1.2009947299957275: 1, -1.1927686929702759: 1, -1.1939657926559448: 1, 0.005114047322422266: 1, -1.2000701427459717: 1, -1.1476235389709473: 1, -1.2007057666778564: 1, -1.1639471054077148: 1, -1.2007761001586914: 1, -1.2015410661697388: 1, -1.1976145505905151: 1, -1.197718858718872: 1, -0.9927355051040649: 1, -1.1987890005111694: 1, -1.196489930152893: 1, -1.1964974403381348: 1, -1.1852269172668457: 1, -1.1929398775100708: 1, -1.1945018768310547: 1, -0.34237241744995117: 1, -1.1871466636657715: 1, -1.197901725769043: 1, -1.1866750717163086: 1, -1.1237342357635498: 1, -1.1664562225341797: 1, 1.4295574426651: 1, -0.00951747503131628: 1, 1.4342314004898071: 1, 0.5139665007591248: 1, -0.37773171067237854: 1, -0.9769929647445679: 1, -0.4147615134716034: 1, 1.6281145811080933: 1, -0.19289743900299072: 1, -0.6085047125816345: 1, 1.0926192998886108: 1, 0.21325090527534485: 1, 0.6560998558998108: 1, 1.8768579959869385: 1, 0.6321052312850952: 1, 1.7107861042022705: 1, 1.8398889303207397: 1, -1.009895920753479: 1, 0.7517246007919312: 1, 1.3204209804534912: 1, -1.2016119956970215: 1, -0.4007585644721985: 1, 1.5550175905227661: 1, -0.8897131681442261: 1, 0.7365891337394714: 1, 1.3062021732330322: 1, -0.30772721767425537: 1, -0.09802207350730896: 1, 1.2996296882629395: 1, 0.9686956405639648: 1, 0.28549933433532715: 1, -0.5639210939407349: 1, -0.39423879981040955: 1, -0.4268537759780884: 1, 0.16395387053489685: 1, 0.5479292273521423: 1, -0.5222632884979248: 1, -0.27864202857017517: 1, -0.5042855143547058: 1, -0.32787415385246277: 1, 0.2115481197834015: 1, 1.4029184579849243: 1, -0.3579169511795044: 1, 1.603068470954895: 1, -0.8235211968421936: 1, 0.2787120044231415: 1, -0.2865978181362152: 1, -0.683555543422699: 1, 1.4387927055358887: 1, 0.18236172199249268: 1, -1.1323087215423584: 1, 1.520749807357788: 1, -1.1482487916946411: 1, -0.8519163131713867: 1, -1.2016046047210693: 1, -0.4260459542274475: 1, -0.3525408208370209: 1, -1.180529236793518: 1, -1.2015936374664307: 1, -0.6465518474578857: 1, 0.8243502378463745: 1, -1.1526696681976318: 1, -1.2014310359954834: 1, -1.0348321199417114: 1, -1.2015955448150635: 1, -0.5828713178634644: 1, -1.2016127109527588: 1, 0.10008653253316879: 1, -0.9153497219085693: 1, 0.1308048814535141: 1, -0.1883729249238968: 1, -1.2015149593353271: 1, -1.055851936340332: 1, -0.5025617480278015: 1, -1.2004907131195068: 1, 0.031881630420684814: 1, -1.1730265617370605: 1, -1.201509714126587: 1, -0.3419588804244995: 1, 1.6047093868255615: 1, 1.5627902746200562: 1, 0.670230507850647: 1, -0.040320102125406265: 1, -1.1937233209609985: 1, -1.2012096643447876: 1, -1.1647279262542725: 1, -1.2015366554260254: 1, 0.37413451075553894: 1, 0.16982176899909973: 1, -0.6204319000244141: 1, -1.2013626098632812: 1, -1.201349139213562: 1, -0.46576231718063354: 1, -0.916256844997406: 1, -1.201637625694275: 1, 0.339458167552948: 1, -0.9931707978248596: 1, -1.2013576030731201: 1, -1.092740535736084: 1, -1.2016162872314453: 1, -0.6686071157455444: 1, -1.1959139108657837: 1, -0.37911587953567505: 1, 0.1561044156551361: 1, -0.09781666100025177: 1, -0.06465810537338257: 1, -1.2015609741210938: 1, 0.7523799538612366: 1, -0.8503693342208862: 1, -1.2014657258987427: 1, -1.201615810394287: 1, -1.186226725578308: 1, -1.2014594078063965: 1, -0.27123570442199707: 1, 0.5888639092445374: 1, -1.1972260475158691: 1, -1.1996524333953857: 1, 0.39954620599746704: 1, -1.201310396194458: 1, -0.9657180905342102: 1, -1.1830931901931763: 1, -0.6864959597587585: 1, -0.33821895718574524: 1, -1.1987498998641968: 1, -0.512914776802063: 1, -0.4154701232910156: 1, -0.7784246206283569: 1, -1.2007629871368408: 1, -1.2014739513397217: 1, -0.7713357210159302: 1, -1.2016221284866333: 1, 0.812082827091217: 1, -1.201629400253296: 1, -0.21383443474769592: 1, 0.21984633803367615: 1, -1.1999688148498535: 1, -1.2006030082702637: 1, -1.201606273651123: 1, -1.2015992403030396: 1, -1.2016041278839111: 1, 0.46835482120513916: 1, -1.2016280889511108: 1, -1.201348900794983: 1, -1.1382324695587158: 1, -1.1506352424621582: 1, -0.8027163147926331: 1, -1.201614260673523: 1, -1.2002341747283936: 1, -1.201564908027649: 1, -1.2016369104385376: 1, -1.1746125221252441: 1, 0.34950539469718933: 1, 0.17100460827350616: 1, 0.15106460452079773: 1, -0.8241826295852661: 1, -1.0799649953842163: 1, -1.1919200420379639: 1, -0.04164140671491623: 1, -0.8634552955627441: 1, 0.4990178942680359: 1, -0.4610443115234375: 1, -0.9751180410385132: 1, -1.0622771978378296: 1, -0.5990430116653442: 1, 1.7476170063018799: 1, -1.1072322130203247: 1, 0.19889964163303375: 1, -0.12463472783565521: 1, 1.9090871810913086: 1, 1.5657020807266235: 1, 0.006390336435288191: 1, -0.5620161294937134: 1, -1.18631112575531: 1, -0.3605387806892395: 1, 0.292474627494812: 1, -0.9981153011322021: 1, -1.1329883337020874: 1, -1.1138004064559937: 1, -0.8920286297798157: 1, -1.1573199033737183: 1, 1.788353681564331: 1, -0.896551251411438: 1, 0.43905025720596313: 1, 0.6965684294700623: 1, -0.26542410254478455: 1, -1.0395952463150024: 1, -0.3497014045715332: 1, -0.9432769417762756: 1, 1.3899286985397339: 1, 1.5516252517700195: 1, 0.7257186770439148: 1, -0.8993450403213501: 1, -1.098894715309143: 1, -0.24214474856853485: 1, -0.3594827950000763: 1, -1.1312958002090454: 1, -1.2013144493103027: 1, -0.41735291481018066: 1, 1.5167564153671265: 1, -0.2154475301504135: 1, -1.111975073814392: 1, -1.0634658336639404: 1, -0.9736310243606567: 1, -0.5120261311531067: 1, 0.05307941138744354: 1, 0.9346560835838318: 1, -0.03840658441185951: 1, 1.324471354484558: 1, -1.2006902694702148: 1, -1.0523524284362793: 1, -0.5724524259567261: 1, -0.7811231017112732: 1, 0.4252239763736725: 1, -0.748826265335083: 1, 0.22341269254684448: 1, -1.199570655822754: 1, -1.1873440742492676: 1, -0.43377333879470825: 1, 1.4774726629257202: 1, -0.731924295425415: 1, -0.45086827874183655: 1, -1.0084080696105957: 1, -0.9050359129905701: 1, -0.5784834027290344: 1, 1.1575602293014526: 1, -1.1544640064239502: 1, 0.40835040807724: 1, -0.9035985469818115: 1, -1.1970343589782715: 1, -1.17500901222229: 1, -1.1196062564849854: 1, -0.5850008726119995: 1, 0.6492028832435608: 1, 0.5685755014419556: 1, 0.46832725405693054: 1, 0.20812031626701355: 1, -1.1569617986679077: 1, -1.0893759727478027: 1, -1.1004241704940796: 1, 0.5786248445510864: 1, -0.26597675681114197: 1, 1.3982725143432617: 1, -1.0213873386383057: 1, 1.281778335571289: 1, -0.4367877244949341: 1, -0.8407037854194641: 1, -1.1983946561813354: 1, -0.5144169330596924: 1, -0.7376867532730103: 1, 1.2280430793762207: 1, 0.16367900371551514: 1, -1.1570571660995483: 1, 0.5357815027236938: 1, -0.556195080280304: 1, 0.7494425773620605: 1, -0.993471086025238: 1, -0.8923998475074768: 1, -0.981871485710144: 1, 0.2662027180194855: 1, -1.078048825263977: 1, 1.536348581314087: 1, 1.6904795169830322: 1, -1.0221163034439087: 1, -0.1391475349664688: 1, -1.094030499458313: 1, -0.7628646492958069: 1, 1.114488959312439: 1, 1.0554637908935547: 1, 0.17207567393779755: 1, 3.259727716445923: 1, -1.1946264505386353: 1, 0.025435535237193108: 1, -0.09378552436828613: 1, -0.8962797522544861: 1, -1.199107050895691: 1, 0.9532740116119385: 1, 0.8790757060050964: 1, -0.9627416729927063: 1, -0.608140766620636: 1, 1.4842547178268433: 1, 1.4690353870391846: 1, 1.4887964725494385: 1, -0.7701697945594788: 1, 0.4876076281070709: 1, 1.0163378715515137: 1, -0.9919153451919556: 1, -1.1343045234680176: 1, -0.8108161091804504: 1, 1.4876383543014526: 1, -1.0858170986175537: 1, -0.90742427110672: 1, -1.024053692817688: 1, 0.6749281883239746: 1, -0.5982488989830017: 1, 0.0663938894867897: 1, -0.3465023636817932: 1, -0.3573164939880371: 1, 1.3890480995178223: 1, -1.0539400577545166: 1, -1.1605626344680786: 1, -0.8580590486526489: 1, -0.8890491724014282: 1, 0.6293842196464539: 1, 1.1045739650726318: 1, 0.04404761642217636: 1, -1.0076677799224854: 1, -0.9982122778892517: 1, -1.2001420259475708: 1, -0.930094301700592: 1, -0.35407769680023193: 1, -1.197619080543518: 1, -0.2589719295501709: 1, -1.2003082036972046: 1, -0.4400210678577423: 1, -1.1370965242385864: 1, -0.8680421113967896: 1, 0.2903974652290344: 1, -1.193503737449646: 1, -0.45158419013023376: 1, -1.1848548650741577: 1, -1.1845873594284058: 1, -0.6980454921722412: 1, -0.6311256289482117: 1, -1.188598394393921: 1, 1.192413568496704: 1, -1.1560314893722534: 1, -0.9288858771324158: 1, -0.5754680633544922: 1, 0.7908697128295898: 1, 0.10187211632728577: 1, -1.1412583589553833: 1, -1.0751264095306396: 1, -1.0768063068389893: 1, -1.18907630443573: 1, -1.0519613027572632: 1, -1.0507465600967407: 1, 1.8212419748306274: 1, -0.36594024300575256: 1, -0.9299888610839844: 1, -1.1659823656082153: 1, -1.0714704990386963: 1, -0.4354245066642761: 1, -1.1646332740783691: 1, -1.194373369216919: 1, -0.9642779231071472: 1, -1.1041064262390137: 1, -1.2012841701507568: 1, -0.19042661786079407: 1, -0.2147151529788971: 1, -0.9940837025642395: 1, 1.3862724304199219: 1, 1.6431117057800293: 1, -0.1989370882511139: 1, -1.0411947965621948: 1, -0.923246443271637: 1, -0.8935588002204895: 1, 1.7851945161819458: 1, -0.8622487187385559: 1, -1.0024443864822388: 1, 0.36215391755104065: 1, -0.9218448400497437: 1, 0.2308363914489746: 1, 1.4938230514526367: 1, 1.1383615732192993: 1, 0.8504006266593933: 1, 1.2588475942611694: 1, 0.4723914861679077: 1, -0.24819114804267883: 1, -0.08030800521373749: 1, -0.06996402144432068: 1, -0.5825971961021423: 1, 0.2403424084186554: 1, 1.427193522453308: 1, -0.16857680678367615: 1, -0.39828142523765564: 1, 0.7533062696456909: 1, 1.5823100805282593: 1, 1.3024239540100098: 1, 0.9470359086990356: 1, 1.2427196502685547: 1, -0.5881722569465637: 1, -0.8510425090789795: 1, 1.224327802658081: 1, 0.465168297290802: 1, -0.8754668831825256: 1, 1.3494455814361572: 1, 0.27488669753074646: 1, 1.2950940132141113: 1, 0.7110782265663147: 1, 1.210314393043518: 1, 0.5456136465072632: 1, -0.16512484848499298: 1, 0.1011820137500763: 1, 0.6780659556388855: 1, 1.495969295501709: 1, 1.2984728813171387: 1, 1.0170906782150269: 1, 1.9331234693527222: 1, 0.6696186065673828: 1, -0.6162902116775513: 1, 1.3002121448516846: 1, -0.8986467123031616: 1, -0.15746326744556427: 1, -1.1914112567901611: 1, 1.5811641216278076: 1, -0.027011625468730927: 1, 1.3250715732574463: 1, 0.12876805663108826: 1, -0.930620014667511: 1, 1.6178103685379028: 1, -1.2016338109970093: 1, -0.21738800406455994: 1, -0.4253336787223816: 1, 0.2726168930530548: 1, -0.02133699133992195: 1, 0.03207547590136528: 1, 0.4048600494861603: 1, 1.4308977127075195: 1, 0.637361466884613: 1, 1.3996703624725342: 1, 1.7721431255340576: 1, 1.0551327466964722: 1, 1.5277636051177979: 1, 0.15925046801567078: 1, 0.912930965423584: 1, 0.7471779584884644: 1, -1.115770697593689: 1, 1.090896725654602: 1, 1.1508636474609375: 1, 1.0995265245437622: 1, -0.7282882928848267: 1, -1.1950759887695312: 1, 1.0165932178497314: 1, 0.9149655103683472: 1, -0.22176611423492432: 1, -0.059458885341882706: 1, 0.7950616478919983: 1, -0.20539666712284088: 1, -0.37734130024909973: 1, 1.0202414989471436: 1, 1.1998642683029175: 1, 0.22981515526771545: 1, 1.2730098962783813: 1, 0.7611046433448792: 1, -1.113705039024353: 1, 1.2712597846984863: 1, 1.573736548423767: 1, 1.4694101810455322: 1, 0.3604428768157959: 1, 0.4633817672729492: 1, 0.35480692982673645: 1, 1.4536134004592896: 1, 0.699573814868927: 1, 0.3411409258842468: 1, 0.394859254360199: 1, -0.004194003064185381: 1, 0.23328566551208496: 1, -0.4297850430011749: 1, 1.6003168821334839: 1, 0.9657272696495056: 1, -0.16252407431602478: 1, -1.107023000717163: 1, -0.2782626748085022: 1, 1.9238320589065552: 1, -1.1969212293624878: 1, -1.07969331741333: 1, 0.1708119511604309: 1, -0.23679472506046295: 1, 0.2964378595352173: 1, 0.7527873516082764: 1, 0.739153265953064: 1, 0.03332117572426796: 1, 1.4062000513076782: 1, -0.047685928642749786: 1, 0.32160520553588867: 1, -0.19720852375030518: 1, 0.3313300311565399: 1, 0.7724436521530151: 1, 1.3262649774551392: 1, 0.3607413172721863: 1, -0.10225572437047958: 1, 1.366266131401062: 1, 0.9568199515342712: 1, -0.16261765360832214: 1, -0.1799832135438919: 1, -0.9339156150817871: 1, -1.1940946578979492: 1, -0.8234555125236511: 1, 1.4413039684295654: 1, -0.2101057469844818: 1, -0.38419657945632935: 1, -0.6876721978187561: 1, -0.27164560556411743: 1, 1.6284458637237549: 1, -0.09434226900339127: 1, -0.0960833728313446: 1, 1.1112544536590576: 1, 1.9067574739456177: 1, 0.7951827049255371: 1, 0.07571198046207428: 1, -0.8877851366996765: 1, 1.6006011962890625: 1, 0.342952162027359: 1, -0.9859256744384766: 1, -1.1895103454589844: 1, 1.425097107887268: 1, -1.1913617849349976: 1, -1.0719594955444336: 1, 1.287703037261963: 1, 0.35307395458221436: 1, 0.900846004486084: 1, 1.0683897733688354: 1, 0.11351441591978073: 1, 1.0402084589004517: 1, -0.26631277799606323: 1, 0.9666041731834412: 1, 1.4656307697296143: 1, 0.9499619007110596: 1, 0.02830575592815876: 1, 0.6718612909317017: 1, 1.3615977764129639: 1, -0.9881011247634888: 1, 0.8268551230430603: 1, 1.7417840957641602: 1, -0.6611031889915466: 1, -0.514695405960083: 1, 0.6606081128120422: 1, 1.5755736827850342: 1, 0.8417104482650757: 1, 0.4746771454811096: 1, 1.0541952848434448: 1, 0.5222756862640381: 1, 0.9129876494407654: 1, -0.01062939316034317: 1, 1.0303751230239868: 1, 1.5870535373687744: 1, 1.3437533378601074: 1, 0.2640135586261749: 1, 0.2808535397052765: 1, 0.16384904086589813: 1, 0.9138351678848267: 1, 0.36046868562698364: 1, 0.12815426290035248: 1, 1.7614071369171143: 1, -0.16177639365196228: 1, 0.9005318284034729: 1, -0.18478137254714966: 1, -0.539240837097168: 1, 0.2775695323944092: 1, 0.28860634565353394: 1, 0.2634084224700928: 1, -0.021469445899128914: 1, -0.589444637298584: 1, -0.32526895403862: 1, 1.4644434452056885: 1, 0.6039040088653564: 1, -1.031872034072876: 1, 0.1467430293560028: 1, -0.5022063255310059: 1, 0.31799715757369995: 1, 0.1881372481584549: 1, 1.0860453844070435: 1, -1.1767117977142334: 1, -0.6538054943084717: 1, 1.2202951908111572: 1, -1.1264077425003052: 1, 1.0540943145751953: 1, 1.5793328285217285: 1, 0.42848649621009827: 1, -0.41689032316207886: 1, -0.2686515748500824: 1, 1.8881990909576416: 1, -1.061113953590393: 1, 1.7208062410354614: 1, -1.201619267463684: 1, -0.4105451703071594: 1, 1.5992790460586548: 1, 1.280470848083496: 1, -0.807515025138855: 1, -1.2013123035430908: 1, -1.2016263008117676: 1, 1.3301595449447632: 1, 0.4860861599445343: 1, 1.6493014097213745: 1, -1.1416743993759155: 1, 1.1998889446258545: 1, -0.9704957008361816: 1, -1.2016189098358154: 1, 1.5100682973861694: 1, 1.8360271453857422: 1, -1.1663979291915894: 1, 0.014584558084607124: 1, 1.891126036643982: 1, 0.19680047035217285: 1, 0.8425688147544861: 1, 0.5497986674308777: 1, 0.08808748424053192: 1, 1.6464146375656128: 1, 1.9098440408706665: 1, 0.523788332939148: 1, 1.4320223331451416: 1, 1.7558945417404175: 1, -0.4567924439907074: 1, -0.962668240070343: 1, -1.2016352415084839: 1, -0.2495342493057251: 1, 0.3117649555206299: 1, 0.5131713151931763: 1, 1.76934015750885: 1, -1.2016271352767944: 1, 0.9834553003311157: 1, -1.0099024772644043: 1, -1.102870225906372: 1, -0.9806917309761047: 1, -0.94952791929245: 1, 1.901644229888916: 1, 0.6003371477127075: 1, 0.9758270978927612: 1, 0.14208731055259705: 1, 1.8529928922653198: 1, -0.10222127288579941: 1, 1.7350994348526: 1, 1.7166484594345093: 1, 1.5665374994277954: 1, -1.0994056463241577: 1, 1.0374422073364258: 1, 0.5713223814964294: 1, -1.2016212940216064: 1, 1.5698747634887695: 1, 1.3496158123016357: 1, 1.9196945428848267: 1, 1.9020731449127197: 1, -1.2016111612319946: 1, -0.5369133949279785: 1, -0.6532332301139832: 1, 0.9807740449905396: 1, 1.1523829698562622: 1, 1.796111822128296: 1, -1.201629877090454: 1, 0.804813027381897: 1, 1.2709132432937622: 1, -0.5811882019042969: 1, 0.454349547624588: 1, -0.5300003886222839: 1, 1.2604477405548096: 1, 1.2889325618743896: 1, 0.35712626576423645: 1, -0.864342451095581: 1, 1.9024626016616821: 1, -1.1658029556274414: 1, -1.2016136646270752: 1, -1.0555497407913208: 1, 1.633592128753662: 1, 1.3466922044754028: 1, -1.2002416849136353: 1, 1.5722275972366333: 1, -0.8662946820259094: 1, 0.5880253314971924: 1, -1.088794231414795: 1, -1.042400598526001: 1, 0.10260368138551712: 1, 1.4280599355697632: 1, -0.2810556888580322: 1, 0.9832457304000854: 1, 1.0101338624954224: 1, -0.2232077419757843: 1, 1.8893579244613647: 1, -0.22210338711738586: 1, 0.9173278212547302: 1, 0.14515815675258636: 1, 0.7045637369155884: 1, -0.03429622948169708: 1, 0.03225273638963699: 1, 1.545008897781372: 1, 1.590468406677246: 1, 1.3033519983291626: 1, 0.19334031641483307: 1, -1.0097246170043945: 1, 1.7298698425292969: 1, -0.03317539766430855: 1, 1.7539278268814087: 1, 1.0263571739196777: 1, -0.4220041036605835: 1, 0.29674577713012695: 1, 0.7860816717147827: 1, 0.48093563318252563: 1, 1.0254307985305786: 1, -0.12864308059215546: 1, -0.6814844608306885: 1, 0.766596257686615: 1, 1.4337563514709473: 1, -0.08275699615478516: 1, 0.04922454059123993: 1, 1.8799982070922852: 1, -0.8965012431144714: 1, 1.5299123525619507: 1, 1.9158005714416504: 1, 0.4917255938053131: 1, 1.6278821229934692: 1, 1.6179100275039673: 1, 0.16001862287521362: 1, 0.3740043342113495: 1, -0.22299611568450928: 1, 0.056893277913331985: 1, 1.6735926866531372: 1, 0.8739101886749268: 1, 0.24924464523792267: 1, -0.7589280009269714: 1, -0.03557446971535683: 1, -0.7948459982872009: 1, 0.81379234790802: 1, -0.7293344736099243: 1, 1.6143009662628174: 1, 0.7481110692024231: 1, 0.4076603651046753: 1, 1.6906383037567139: 1, 1.8846700191497803: 1, 0.1083393469452858: 1, 0.9554869532585144: 1, 0.7741647958755493: 1, 1.6500415802001953: 1, 0.6886505484580994: 1, 0.5059344172477722: 1, -0.04758370667695999: 1, 1.7850080728530884: 1, -0.2531147599220276: 1, 0.9454357028007507: 1, 0.9327585697174072: 1, 0.521833598613739: 1, 0.26564425230026245: 1, 0.5438616275787354: 1, 1.5315228700637817: 1, 0.591416597366333: 1, 0.27008119225502014: 1, 0.24442099034786224: 1, 0.7021965980529785: 1, 0.27857378125190735: 1, 1.1086541414260864: 1, -0.025178229436278343: 1, -0.3399538993835449: 1, 1.612230896949768: 1, 0.6905592679977417: 1, -0.2079317569732666: 1, 1.5815212726593018: 1, 1.1628241539001465: 1, 1.7223035097122192: 1, 0.34220972657203674: 1, 0.6320856809616089: 1, 1.9327584505081177: 1, 0.6905925273895264: 1, 1.4504951238632202: 1, -0.10395042598247528: 1, -0.5665901303291321: 1, 1.6153100728988647: 1, 1.865704894065857: 1, 1.7069745063781738: 1, 1.7395148277282715: 1, 1.0135881900787354: 1, -0.31705647706985474: 1, 0.9889004230499268: 1, 0.8423707485198975: 1, 0.5330069065093994: 1, 1.0885628461837769: 1, -0.19816404581069946: 1, 1.6013163328170776: 1, 0.3571586012840271: 1, -0.408608615398407: 1, 1.3225599527359009: 1, -1.201594352722168: 1, 0.5681759119033813: 1, 0.261216938495636: 1, -0.7838892936706543: 1, 1.271135687828064: 1, 0.562964141368866: 1, 1.6581584215164185: 1, 0.8071705102920532: 1, -1.2015928030014038: 1, 0.6071187853813171: 1, 0.38025134801864624: 1, 1.2595564126968384: 1, 1.4854387044906616: 1, 0.9784976243972778: 1, 1.7376213073730469: 1, 1.9389169216156006: 1, 1.0590068101882935: 1, 1.610649824142456: 1, -1.0813920497894287: 1, -1.0571757555007935: 1, -0.9450681209564209: 1, 0.8005363941192627: 1, 0.2591017186641693: 1, 1.0987391471862793: 1, 0.8646785616874695: 1, -1.085170865058899: 1, 0.8198267817497253: 1, 1.8067305088043213: 1, 0.12914451956748962: 1, -0.24759358167648315: 1, -0.655949056148529: 1, 1.5291143655776978: 1, 1.7461694478988647: 1, 0.46223318576812744: 1, 1.127722144126892: 1, 1.8749902248382568: 1, -1.201613187789917: 1, 0.3816104531288147: 1, -1.002498984336853: 1, 1.8291547298431396: 1, -1.2015867233276367: 1, -0.538144052028656: 1, -1.040601134300232: 1, -0.8136187195777893: 1, 1.519827961921692: 1, 1.3174397945404053: 1, 1.16111421585083: 1, 0.10180643945932388: 1, -1.012891411781311: 1, 1.8800233602523804: 1, 0.9158507585525513: 1, 1.3335411548614502: 1, 1.3847272396087646: 1, 1.110692024230957: 1, -0.8793452382087708: 1, 1.7776927947998047: 1, 1.2369016408920288: 1, -0.9954119920730591: 1, 1.8261455297470093: 1, -0.3587249517440796: 1, -0.20700129866600037: 1, -0.6896651983261108: 1, -0.13542917370796204: 1, 0.8895251154899597: 1, 1.5496872663497925: 1, -1.1936933994293213: 1, 0.4616207182407379: 1, -0.7448302507400513: 1, -1.2016236782073975: 1, 0.8969712257385254: 1, -0.8653848767280579: 1, 1.8480027914047241: 1, -0.7542659640312195: 1, 1.1129239797592163: 1, -0.5476839542388916: 1, -0.7379482984542847: 1, -0.24342182278633118: 1, -1.2016215324401855: 1, 0.963599681854248: 1, 0.6376197934150696: 1, 0.6801310777664185: 1, 0.7080846428871155: 1, 0.4253986179828644: 1, -0.2255825698375702: 1, 1.3490053415298462: 1, 1.5920872688293457: 1, 1.7425659894943237: 1, 1.0934252738952637: 1, -0.9789384603500366: 1, -0.6306192278862: 1, 1.3510494232177734: 1, 1.6034055948257446: 1, -0.6586742401123047: 1, 0.1718989461660385: 1, -1.200726866722107: 1, -1.2011404037475586: 1, 1.339892864227295: 1, -0.13886263966560364: 1, -1.1579643487930298: 1, -0.21755054593086243: 1, 1.455495834350586: 1, 1.8285820484161377: 1, 0.25442907214164734: 1, 1.636014699935913: 1, 1.6676998138427734: 1, -0.9175146222114563: 1, 0.9006003737449646: 1, 1.0329307317733765: 1, 1.0343732833862305: 1, 0.05316608399152756: 1, 1.6221997737884521: 1, 1.9172451496124268: 1, 0.43482455611228943: 1, 1.8354456424713135: 1, 1.8706549406051636: 1, -0.22918304800987244: 1, -0.2059621661901474: 1, 1.7338974475860596: 1, -1.1562846899032593: 1, 1.760259985923767: 1, -0.11584310233592987: 1, -0.9069592952728271: 1, 1.8416829109191895: 1, 1.7511584758758545: 1, -0.4143541157245636: 1, 0.8007993698120117: 1, 0.6508381366729736: 1, -1.2014458179473877: 1, -0.5870760679244995: 1, 0.5326334834098816: 1, -0.4422439932823181: 1, -0.9877029061317444: 1, -1.2016080617904663: 1, 0.23050570487976074: 1, -0.7923315763473511: 1, -0.009660118259489536: 1, 1.114802360534668: 1, -1.174880027770996: 1, 1.7429335117340088: 1, 1.92928946018219: 1, 1.7397531270980835: 1, -0.8557604551315308: 1, -0.08804576843976974: 1, 1.2302463054656982: 1, 0.8335886001586914: 1, -0.8916471600532532: 1, 0.04224063828587532: 1, 1.4848576784133911: 1, 1.8896540403366089: 1, 0.8217967748641968: 1, 0.08071509003639221: 1, 0.8788592219352722: 1, 1.0276689529418945: 1, 0.019096076488494873: 1, -0.1335328370332718: 1, 1.7602899074554443: 1, 1.8266348838806152: 1, 1.058468222618103: 1, 0.32922476530075073: 1, 1.7661612033843994: 1, 1.7499927282333374: 1, -0.7876200675964355: 1, 0.13984030485153198: 1, 1.9223994016647339: 1, -0.11489463597536087: 1, 1.0331618785858154: 1, -1.156775951385498: 1, -0.4194781482219696: 1, 1.5742621421813965: 1, 0.1478143036365509: 1, 1.2849032878875732: 1, -0.420527845621109: 1, 1.6787821054458618: 1, 1.0356202125549316: 1, 1.0119178295135498: 1, -0.7692103385925293: 1, 1.5984208583831787: 1, 1.7267848253250122: 1, 0.63859623670578: 1, 1.7614209651947021: 1, 0.8861773610115051: 1, 1.887890100479126: 1, 0.09368380159139633: 1, -0.907404899597168: 1, 1.2843722105026245: 1, -0.796614408493042: 1, 1.7114263772964478: 1, -0.8602240085601807: 1, 1.473099708557129: 1, 0.9352988600730896: 1, 1.597861409187317: 1, 0.9059203267097473: 1, 1.0176738500595093: 1, 1.703546404838562: 1, 0.9797016382217407: 1, 1.746630072593689: 1, 0.9968640804290771: 1, 5.270293235778809: 1, 0.7037346363067627: 1, 1.692647099494934: 1, -0.3100615441799164: 1, 1.231010913848877: 1, 0.017721591517329216: 1, -0.3422335684299469: 1, -0.29665860533714294: 1, -0.4729682505130768: 1, -0.8232591152191162: 1, -0.032225385308265686: 1, -0.558110237121582: 1, 0.9276106357574463: 1, -0.014680324122309685: 1, -0.6007823944091797: 1, -0.7505127787590027: 1, -1.1957098245620728: 1, -0.33291712403297424: 1, 1.3759360313415527: 1, 1.1735796928405762: 1, -0.4622204601764679: 1, -0.03591597080230713: 1, -0.14395083487033844: 1, 0.2143784463405609: 1, 0.5354000329971313: 1, 0.171223446726799: 1, -0.34326422214508057: 1, 1.4956955909729004: 1, -1.0387167930603027: 1, -0.9877877235412598: 1, -0.4592617452144623: 1, 1.2142442464828491: 1, 1.052090048789978: 1, 1.2964091300964355: 1, 1.5233625173568726: 1, 1.3913298845291138: 1, -0.466978520154953: 1, 1.569981575012207: 1, -0.92970210313797: 1, 0.9121710062026978: 1, -0.34284013509750366: 1, -0.47152507305145264: 1, -0.23732632398605347: 1, 1.8227369785308838: 1, 0.7785534858703613: 1, -0.3081098794937134: 1, 0.006228437647223473: 1, -0.8751227855682373: 1, 1.456301212310791: 1, 1.542358636856079: 1, -0.46590861678123474: 1, -0.6440175771713257: 1, -0.7915438413619995: 1, 1.9232004880905151: 1, -0.3439752459526062: 1, -0.4275071322917938: 1, -0.5378462672233582: 1, 1.5919677019119263: 1, 1.7606215476989746: 1, 1.3363116979599: 1, -0.9762966632843018: 1, -0.8456059098243713: 1, -1.0356733798980713: 1, 0.22667939960956573: 1, 0.43118155002593994: 1, 0.6444940567016602: 1, -0.2761753499507904: 1, -0.21501778066158295: 1, -0.07709828019142151: 1, -0.0690179392695427: 1, -0.06596078723669052: 1, 1.058951735496521: 1, 1.1691573858261108: 1, 1.6570682525634766: 1, 1.4193427562713623: 1, 1.8792171478271484: 1, -0.0695866122841835: 1, 1.93668532371521: 1, 0.16830426454544067: 1, -0.46113380789756775: 1, 1.878305196762085: 1, 1.3953920602798462: 1, -0.3858093321323395: 1, -0.48933231830596924: 1, -0.7822737693786621: 1, -0.28641819953918457: 1, -0.9299617409706116: 1, -1.1991149187088013: 1, 1.8707987070083618: 1, 0.6785101294517517: 1, 0.29917436838150024: 1, 0.6026607155799866: 1, -1.185569167137146: 1, 1.6845048666000366: 1, -1.2014148235321045: 1, -1.193153738975525: 1, -1.1891201734542847: 1, 1.3399251699447632: 1, -0.44544142484664917: 1, 0.15484686195850372: 1, -0.5857658982276917: 1, -1.1479105949401855: 1, -0.5629591941833496: 1, -0.24207068979740143: 1, 1.4788439273834229: 1, -0.06991042196750641: 1, -0.3753896951675415: 1, -0.5378177762031555: 1, 1.4534815549850464: 1, -0.30685290694236755: 1, -0.46731990575790405: 1, -0.5843047499656677: 1, -0.1967451572418213: 1, 0.9746946096420288: 1, 0.47734835743904114: 1, 0.16428887844085693: 1, -0.35491982102394104: 1, -0.5582360029220581: 1, -0.960394024848938: 1, 0.012689988128840923: 1, -0.200442373752594: 1, -0.29989245533943176: 1, -1.1109116077423096: 1, 1.7616217136383057: 1, -0.4605187475681305: 1, 0.06790906190872192: 1, -0.2670007050037384: 1, 3.323413610458374: 1, 0.3248298168182373: 1, 1.4998488426208496: 1, -0.9987406134605408: 1, -1.1868388652801514: 1, -0.17455770075321198: 1, 1.491416096687317: 1, 1.5324621200561523: 1, -1.1897622346878052: 1, -0.46823152899742126: 1, -0.5717604756355286: 1, -0.07945768535137177: 1, -0.8472578525543213: 1, 0.5575955510139465: 1, 0.6789939403533936: 1, 1.3883335590362549: 1, -0.47762712836265564: 1, -0.9396678805351257: 1, -0.1308683305978775: 1, -0.5413272380828857: 1, -0.7269378900527954: 1, 1.6493046283721924: 1, -0.2937408983707428: 1, 0.15043634176254272: 1, -0.5880576372146606: 1, 0.4303920269012451: 1, -0.13842415809631348: 1, -0.3907565474510193: 1, 1.9010276794433594: 1, -0.879592776298523: 1, -0.288830429315567: 1, -0.5174428224563599: 1, -0.21868036687374115: 1, 1.6888245344161987: 1, 1.2261123657226562: 1, 0.37810415029525757: 1, -1.0713788270950317: 1, -0.5210259556770325: 1, -0.49563685059547424: 1, -0.2992294430732727: 1, -1.179208755493164: 1, 1.3037792444229126: 1, 1.6146701574325562: 1, -0.36514076590538025: 1, -0.29448574781417847: 1, -0.8708242177963257: 1, 0.025362450629472733: 1, -0.023513898253440857: 1, -1.0031712055206299: 1, -0.014453819021582603: 1, -0.3702247440814972: 1, -0.8769359588623047: 1, 0.7159720659255981: 1, -0.3349834084510803: 1, -0.8232764601707458: 1, -0.8833669424057007: 1, 0.04851381108164787: 1, 0.6150393486022949: 1, 1.187366247177124: 1, -0.238590806722641: 1, -0.5713714957237244: 1, -1.0160380601882935: 1, -0.5338919758796692: 1, -0.4835919141769409: 1, 0.9851498603820801: 1, 0.273947536945343: 1, -1.1766016483306885: 1, 1.376474142074585: 1, 1.8017815351486206: 1, -0.4548023045063019: 1, 1.6648616790771484: 1, 0.9130324721336365: 1, 0.7212937474250793: 1, 0.48826223611831665: 1, -0.6655375957489014: 1, 0.3571058511734009: 1, 0.3796524703502655: 1, 0.5128249526023865: 1, 0.1589841991662979: 1, 0.7185215353965759: 1, -0.608808159828186: 1, -0.1501469612121582: 1, 0.2422785758972168: 1, -0.21730461716651917: 1, -0.03495830297470093: 1, 1.6034691333770752: 1, 0.3640252351760864: 1, 0.3213244378566742: 1, 0.5874748826026917: 1, -0.5802597403526306: 1, -1.0406304597854614: 1, -0.9834045767784119: 1, -0.3135724663734436: 1, -1.155177116394043: 1, 1.7555813789367676: 1, -1.1569743156433105: 1, -0.6055639982223511: 1, -0.28438881039619446: 1, 0.12007596343755722: 1, -0.4845651388168335: 1, 0.5530001521110535: 1, 1.0318180322647095: 1, 1.0491001605987549: 1, -0.25207433104515076: 1, -1.0319366455078125: 1, 0.2126206010580063: 1, 1.7796648740768433: 1, -0.39972129464149475: 1, -1.1232612133026123: 1, -0.392406165599823: 1, -1.103760838508606: 1, -1.188301682472229: 1, -0.8279891610145569: 1, -1.1760764122009277: 1, -1.0625981092453003: 1, -0.6904935240745544: 1, -1.1205850839614868: 1, 1.9273109436035156: 1, 0.3162490129470825: 1, 1.552185297012329: 1, -0.13741447031497955: 1, -1.1448076963424683: 1, 0.2097160518169403: 1, -0.04817575961351395: 1, 1.2531977891921997: 1, -0.9147067666053772: 1, 0.8109627962112427: 1, 2.0270323753356934: 1, 0.04068145155906677: 1, 0.6704513430595398: 1, 0.15644948184490204: 1, 1.8505216836929321: 1, -0.8678878545761108: 1, -0.14373785257339478: 1, 0.611980676651001: 1, 0.5500550270080566: 1, 0.11937177926301956: 1, -0.5307965278625488: 1, -0.27935805916786194: 1, -0.2874172031879425: 1, -0.9310538172721863: 1, 0.8171444535255432: 1, 1.8530430793762207: 1, 0.46222802996635437: 1, 0.1769435554742813: 1, 0.7334970831871033: 1, -0.2089931219816208: 1, -0.4171464443206787: 1, -0.024080123752355576: 1, -0.4433523118495941: 1, -0.6431723237037659: 1, -0.6276612877845764: 1, -0.23398016393184662: 1, 0.7566526532173157: 1, 1.905008316040039: 1, -0.09393322467803955: 1, 0.3744315505027771: 1, 1.6683679819107056: 1, -0.24634599685668945: 1, 1.852098822593689: 1, 1.5747997760772705: 1, 0.7721536755561829: 1, -0.01187801081687212: 1, 1.4984081983566284: 1, -1.2015986442565918: 1, 0.956155002117157: 1, -1.0813374519348145: 1, 1.525660514831543: 1, 1.6697852611541748: 1, 0.7304840087890625: 1, -0.6562255024909973: 1, 0.21643884479999542: 1, -0.26246213912963867: 1, 0.21555371582508087: 1, 0.3141234815120697: 1, 0.04079057276248932: 1, -0.3426608741283417: 1, 1.7097703218460083: 1, 0.6405824422836304: 1, 0.5790113210678101: 1, -1.1336802244186401: 1, 1.8601784706115723: 1, -0.5884281992912292: 1, -0.7983001470565796: 1, -0.25122812390327454: 1, -0.9730278849601746: 1, 1.5011951923370361: 1, -1.1689732074737549: 1, -0.44949105381965637: 1, -0.9465891718864441: 1, -1.1831696033477783: 1, -0.26783594489097595: 1, -0.7939702272415161: 1, -1.1422733068466187: 1, 0.5123543739318848: 1, -0.3383774757385254: 1, 0.4647309482097626: 1, -0.29477736353874207: 1, -0.6015652418136597: 1, 0.012895430438220501: 1, -0.36108481884002686: 1, -0.8523722290992737: 1, -0.7296833395957947: 1, 0.7230984568595886: 1, -1.2016171216964722: 1, -0.539626955986023: 1, -0.3189155161380768: 1, -0.364163339138031: 1, 0.4937743842601776: 1, 0.13262629508972168: 1, 1.629822015762329: 1, -0.43086937069892883: 1, -1.1991721391677856: 1, -0.4168057143688202: 1, -0.2183121144771576: 1, -0.7995052933692932: 1, -0.22242674231529236: 1, 1.7963730096817017: 1, -0.9981749057769775: 1, -1.1617506742477417: 1, -1.061142921447754: 1, -0.24450848996639252: 1, 1.896323323249817: 1, -0.3123464286327362: 1, -1.201271653175354: 1, 0.18035785853862762: 1, -0.37186357378959656: 1, -0.8952922821044922: 1, -0.9930511116981506: 1, -0.30786851048469543: 1, -1.1925454139709473: 1, -1.0483030080795288: 1, -1.2002716064453125: 1, 4.5118513107299805: 1, -1.199577808380127: 1, -0.7289202809333801: 1, -1.1515345573425293: 1, -1.1694631576538086: 1, -0.13113076984882355: 1, -1.1207038164138794: 1, 3.341240167617798: 1, -1.1624175310134888: 1, -1.1994960308074951: 1, -1.2015869617462158: 1, -0.2223142683506012: 1, 0.7078472375869751: 1, -0.5361114740371704: 1, -1.1236215829849243: 1, -1.2013746500015259: 1, -0.8602992296218872: 1, -1.201583981513977: 1, -0.3992172181606293: 1, -1.1680830717086792: 1, -0.5809049010276794: 1, -1.2016146183013916: 1, -0.9751223921775818: 1, -1.1448140144348145: 1, 0.5143935680389404: 1, -1.201536774635315: 1, -1.201569676399231: 1, -1.110012173652649: 1, -0.6403390765190125: 1, -1.1406184434890747: 1, -0.9465845823287964: 1, 4.282702445983887: 1, -1.1945738792419434: 1, -1.1494790315628052: 1, -1.201560139656067: 1, -1.2015953063964844: 1, -1.1635701656341553: 1, -1.1953339576721191: 1, -1.2015472650527954: 1, -1.1108695268630981: 1, -1.0460647344589233: 1, -1.1475187540054321: 1, 0.9462845921516418: 1, -1.1829675436019897: 1, 4.112330913543701: 1, -1.2015608549118042: 1, -1.1969398260116577: 1, 0.39953649044036865: 1, -0.49170398712158203: 1, -0.08619289845228195: 1, -1.0164505243301392: 1, -0.39080610871315: 1, -0.10903797298669815: 1, -0.7482910752296448: 1, -0.07614605128765106: 1, -1.15325927734375: 1, -1.0079307556152344: 1, -1.1992580890655518: 1, 0.9220188856124878: 1, 1.3031054735183716: 1, -0.4707425832748413: 1, -1.150406837463379: 1, -1.186597228050232: 1, -1.1756606101989746: 1, 0.032617583870887756: 1, -1.1632294654846191: 1, -1.1747305393218994: 1, -1.1993433237075806: 1, -0.20172715187072754: 1, 0.4001394212245941: 1, -0.7597726583480835: 1, -1.0657732486724854: 1, -0.3311309218406677: 1, -0.5369265675544739: 1, -0.5657321810722351: 1, -0.12521179020404816: 1, -0.6006320714950562: 1, -1.1992239952087402: 1, 0.02189079485833645: 1, -1.169080138206482: 1, -0.3552163541316986: 1, 0.264765202999115: 1, -0.8984062075614929: 1, -1.2008600234985352: 1, -0.11124473065137863: 1, -1.2016023397445679: 1, 1.2057219743728638: 1, 0.5111210942268372: 1, -0.7557051777839661: 1, 0.7875488996505737: 1, -1.1981743574142456: 1, -0.16389766335487366: 1, 1.488932728767395: 1, -1.2011059522628784: 1, -0.8982016444206238: 1, -0.35893186926841736: 1, -1.2001534700393677: 1, -0.6715388298034668: 1, -0.4824763238430023: 1, -0.4976504147052765: 1, -1.2016228437423706: 1, 1.0700626373291016: 1, -0.7673166990280151: 1, -0.15097910165786743: 1, -0.14771254360675812: 1, -0.7642948627471924: 1, -0.10418325662612915: 1, 1.1114397048950195: 1, -0.05808640271425247: 1, 0.03431294485926628: 1, -0.881252646446228: 1, -0.5050346255302429: 1, -1.1873303651809692: 1, -1.200080394744873: 1, 0.17733901739120483: 1, -0.04230939969420433: 1, -0.35624369978904724: 1, -0.03235474228858948: 1, -0.3301823139190674: 1, -0.8901136517524719: 1, 0.49005118012428284: 1, -1.2016326189041138: 1, -0.7893494963645935: 1, 0.9156388640403748: 1, 0.040903303772211075: 1, 1.2888545989990234: 1, -0.5374748706817627: 1, -1.1091188192367554: 1, 0.8944936394691467: 1, -1.1906999349594116: 1, -0.012192374095320702: 1, 0.6273993253707886: 1, -0.40984249114990234: 1, -1.1882494688034058: 1, -0.01552529539912939: 1, -0.0996609777212143: 1, -1.20045006275177: 1, -0.14265145361423492: 1, -1.1139642000198364: 1, 0.944926917552948: 1, 0.5761882066726685: 1, -0.03975825384259224: 1, -1.2010724544525146: 1, -1.1809542179107666: 1, 0.00948107335716486: 1, 0.4138732850551605: 1, -0.8764730095863342: 1, -0.24605818092823029: 1, -1.1420694589614868: 1, -1.193078875541687: 1, 1.2197721004486084: 1, -0.13237591087818146: 1, -1.1580485105514526: 1, -1.1866058111190796: 1, -0.24963954091072083: 1, -0.5807573795318604: 1, 1.0215860605239868: 1, 0.9452794194221497: 1, 1.1708778142929077: 1, 0.3778545558452606: 1, 1.1708914041519165: 1, 0.4334481656551361: 1, 0.6366953253746033: 1, 0.8018452525138855: 1, 1.177069067955017: 1, -1.20154869556427: 1, 1.1094199419021606: 1, 1.0952398777008057: 1, 1.258391261100769: 1, -0.07435453683137894: 1, 1.1807068586349487: 1, -0.15606260299682617: 1, -1.1549781560897827: 1, 0.8677871227264404: 1, -1.1028809547424316: 1, -0.002722974168136716: 1, 0.03998654708266258: 1, -0.005674127489328384: 1, -1.0224525928497314: 1, -1.2016303539276123: 1, -1.144519329071045: 1, -0.39197561144828796: 1, -1.1276121139526367: 1, -1.0416687726974487: 1, -0.042717207223176956: 1, -0.6867349743843079: 1, -0.5342384576797485: 1, -1.2015101909637451: 1, -0.39157962799072266: 1, -0.5935716032981873: 1, 1.0410280227661133: 1, -0.3055903911590576: 1, -1.1760457754135132: 1, 1.2313082218170166: 1, -0.3122004270553589: 1, -0.8366453051567078: 1, 0.8209275603294373: 1, -0.41031748056411743: 1, -0.5142630338668823: 1, 1.0160198211669922: 1, 0.5890273451805115: 1, 0.8621184229850769: 1, -0.16215069591999054: 1, 0.043348729610443115: 1, -1.192414402961731: 1, -0.4258005619049072: 1, 0.19746625423431396: 1, 0.7787987589836121: 1, 1.1413462162017822: 1, 0.7848809361457825: 1, -0.6031686067581177: 1, 1.25115966796875: 1, 0.3299405574798584: 1, 0.9907589554786682: 1, 1.143233299255371: 1, -0.861803412437439: 1, -1.187011957168579: 1, -0.671938955783844: 1, 0.8696494698524475: 1, -0.7864221334457397: 1, -0.136034294962883: 1, -1.1379677057266235: 1, -0.17077305912971497: 1, -0.7683218121528625: 1, -1.2009185552597046: 1, 0.6585915088653564: 1, -0.8395327925682068: 1, 0.7587617635726929: 1, -0.5897277593612671: 1, -1.192929744720459: 1, -0.08644621819257736: 1, -0.7811260223388672: 1, -0.7184330821037292: 1, -0.7514510154724121: 1, -1.09720778465271: 1, -0.06760650873184204: 1, -1.201612949371338: 1, -1.0178923606872559: 1, -1.083876609802246: 1, -0.5272284746170044: 1, -1.1375747919082642: 1, -0.19534148275852203: 1, -1.201613426208496: 1, -0.3075839579105377: 1, -0.32589226961135864: 1, -1.0825824737548828: 1, -0.29541367292404175: 1, -1.1735186576843262: 1, 0.3790889084339142: 1, -1.192280888557434: 1, -1.2016328573226929: 1, 0.5382747054100037: 1, 1.0350605249404907: 1, -0.7380070686340332: 1, 0.8076177835464478: 1, 0.6649335622787476: 1, -0.028692159801721573: 1, -0.11206144839525223: 1, -0.11562295258045197: 1, 1.1690500974655151: 1, -0.7266088128089905: 1, 0.022439440712332726: 1, -0.618209183216095: 1, -0.3780987560749054: 1, -0.25587567687034607: 1, -0.36745402216911316: 1, 0.5923774242401123: 1, -0.19212104380130768: 1, -1.2016348838806152: 1, 0.9105262160301208: 1, 0.9495259523391724: 1, 0.41225412487983704: 1, 0.15262554585933685: 1, 0.6296795010566711: 1, -1.1068792343139648: 1, -0.1227283924818039: 1, 1.0150052309036255: 1, -0.21699510514736176: 1, -0.01294313371181488: 1, 1.182131290435791: 1, -0.06836508214473724: 1, -0.15388239920139313: 1, 0.05971863865852356: 1, 1.1462621688842773: 1, -0.7992537021636963: 1, -1.1966403722763062: 1, -0.13060888648033142: 1, -0.8245752453804016: 1, -0.5774872899055481: 1, -1.198868989944458: 1, 1.1278204917907715: 1, 0.2748369872570038: 1, 1.1562855243682861: 1, -0.12876513600349426: 1, -1.1697360277175903: 1, 0.008013189770281315: 1, -0.2588912844657898: 1, 1.3108209371566772: 1, -0.5649014711380005: 1, -0.1045512706041336: 1, -0.6869497895240784: 1, 0.07662157714366913: 1, 1.274375557899475: 1, -0.5160067081451416: 1, -1.0983095169067383: 1, 1.2668349742889404: 1, 0.21761490404605865: 1, 0.43160757422447205: 1, -1.1104997396469116: 1, 0.34432777762413025: 1, 0.02654222585260868: 1, -0.32669803500175476: 1, -0.253052294254303: 1, -0.8852211833000183: 1, -0.12913857400417328: 1, 0.5908839702606201: 1, 1.0087279081344604: 1, 0.167218878865242: 1, -0.5622422695159912: 1, 1.2926610708236694: 1, 0.12959904968738556: 1, -0.34817975759506226: 1, -1.1653438806533813: 1, 1.0467203855514526: 1, -0.35746997594833374: 1, -0.4923703670501709: 1, -0.27802959084510803: 1, 1.0399528741836548: 1, 0.8297379016876221: 1, 1.196900725364685: 1, -0.09158548712730408: 1, -0.4809620976448059: 1, 0.2788977324962616: 1, -0.1383906453847885: 1, -0.17342792451381683: 1, -1.1826090812683105: 1, -0.19042982161045074: 1, -1.200330376625061: 1, 1.0692790746688843: 1, 0.606712281703949: 1, -0.5744653940200806: 1, -1.182140827178955: 1, 0.028185075148940086: 1, -0.27063482999801636: 1, 0.5745441317558289: 1, 0.666642963886261: 1, 0.2699461281299591: 1, -0.024909501895308495: 1, 0.5615567564964294: 1, 1.2464758157730103: 1, -0.003650385420769453: 1, 0.01674770377576351: 1, -1.0131398439407349: 1, 0.05442709103226662: 1, -0.5088394284248352: 1, -0.3548189401626587: 1, 1.2035483121871948: 1, 1.0808086395263672: 1, -1.2008949518203735: 1, -0.9566242694854736: 1, -0.999508798122406: 1, -0.05626079440116882: 1, -0.5931430459022522: 1, 1.0766181945800781: 1, 0.9011586904525757: 1, 0.762050449848175: 1, -0.22990889847278595: 1, 1.2829649448394775: 1, -0.23146884143352509: 1, 0.03461417928338051: 1, 0.7100340127944946: 1, 1.030333161354065: 1, 0.5583640336990356: 1, -1.109034538269043: 1, 0.5070648193359375: 1, -0.6266202926635742: 1, -0.26975223422050476: 1, -0.019765598699450493: 1, -1.1654343605041504: 1, 0.810942530632019: 1, 0.6161129474639893: 1, 1.0866621732711792: 1, -0.27813780307769775: 1, 0.0014178809942677617: 1, -0.9430715441703796: 1, -0.7650251388549805: 1, -1.0579359531402588: 1, -0.16474246978759766: 1, -1.1513574123382568: 1, 0.8519235253334045: 1, 1.3037681579589844: 1, 1.0595874786376953: 1, -1.155191421508789: 1, -1.1991511583328247: 1, -0.16514527797698975: 1, 1.1214849948883057: 1, -1.0054869651794434: 1, -1.1934514045715332: 1, -0.002401667181402445: 1, -0.11618001013994217: 1, -0.5908447504043579: 1, -0.15591250360012054: 1, 0.6152291297912598: 1, -0.15634003281593323: 1, -0.34825557470321655: 1, -0.06601618975400925: 1, -0.21185417473316193: 1, -1.1586220264434814: 1, -0.02432066947221756: 1, 0.2524677515029907: 1, 0.7206960916519165: 1, -0.6903147101402283: 1, 1.00320303440094: 1, -1.193916916847229: 1, -0.15946216881275177: 1, -0.24131079018115997: 1, -0.048064157366752625: 1, -0.06410756707191467: 1, -0.42906203866004944: 1, -0.024461327120661736: 1, -0.32544630765914917: 1, -0.2519146203994751: 1, 1.30207359790802: 1, 0.6744810342788696: 1, 0.9348665475845337: 1, -0.3034926950931549: 1, -0.18796367943286896: 1, -0.0026253368705511093: 1, -0.9494366645812988: 1, -1.0697368383407593: 1, 0.12906195223331451: 1, 0.021945519372820854: 1, -0.32367783784866333: 1, -0.5897085666656494: 1, -0.23161835968494415: 1, 0.9278587102890015: 1, -0.19999556243419647: 1, 0.26051849126815796: 1, 0.053155358880758286: 1, -0.056251220405101776: 1, -1.1260875463485718: 1, 0.048983871936798096: 1, -0.9567206501960754: 1, -1.0963338613510132: 1, -0.07029284536838531: 1, -0.6019781231880188: 1, 1.1092147827148438: 1, 0.6174435615539551: 1, -0.7010860443115234: 1, -0.2265346348285675: 1, 0.9692907333374023: 1, 0.9924389719963074: 1, -1.2002339363098145: 1, -0.6137503981590271: 1, -1.0160771608352661: 1, -0.5631559491157532: 1, -0.9714952111244202: 1, -1.0810678005218506: 1, -1.1923733949661255: 1, -1.1721937656402588: 1, -1.0433918237686157: 1, -1.1339654922485352: 1, -1.1858601570129395: 1, -0.8829452991485596: 1, -0.788809597492218: 1, -1.1643073558807373: 1, -1.1685314178466797: 1, -1.1470420360565186: 1, -1.0068711042404175: 1, -1.1350377798080444: 1, -1.126016616821289: 1, -1.1558103561401367: 1, -0.862193763256073: 1, -1.180148720741272: 1, -1.201621413230896: 1, -0.5804309248924255: 1, -1.0668615102767944: 1, -1.191677212715149: 1, -1.1582452058792114: 1, -0.4475323557853699: 1, -0.4338090121746063: 1, -1.1509727239608765: 1, -0.9300684928894043: 1, -1.193282127380371: 1, -1.157931923866272: 1, -0.7443411946296692: 1, -0.8867827653884888: 1, -1.194840431213379: 1, -1.1729844808578491: 1, -1.010536789894104: 1, -1.1989647150039673: 1, -0.6588079929351807: 1, -1.1501390933990479: 1, -1.201562523841858: 1, -1.194190263748169: 1, -0.19914157688617706: 1, -0.2236318439245224: 1, -1.200749397277832: 1, -1.17979097366333: 1, -1.1744723320007324: 1, -1.1722731590270996: 1, -1.2016000747680664: 1, -0.832706868648529: 1, -1.1563661098480225: 1, -1.2015275955200195: 1, -1.2016135454177856: 1, -1.1405699253082275: 1, -1.0521938800811768: 1, -1.1961623430252075: 1, -1.1871360540390015: 1, -0.6205415725708008: 1, -1.2015451192855835: 1, -1.0986745357513428: 1, -1.1066040992736816: 1, -0.7456731200218201: 1, -1.076475739479065: 1, -1.2009553909301758: 1, -1.0944702625274658: 1, -1.1019858121871948: 1, -1.1784441471099854: 1, -0.7622874975204468: 1, -1.1929867267608643: 1, -1.1482324600219727: 1, -0.5079992413520813: 1, -1.1857080459594727: 1, -1.1485586166381836: 1, -1.1300313472747803: 1, -1.1905823945999146: 1, -0.9732658267021179: 1, -1.1981604099273682: 1, -1.1349726915359497: 1, -1.1969208717346191: 1, -1.1333073377609253: 1, -1.1568188667297363: 1, -0.17838822305202484: 1, -0.5632508397102356: 1, -1.1857632398605347: 1, -0.40758535265922546: 1, -1.175873041152954: 1, -1.1691778898239136: 1, -1.0323843955993652: 1, -1.2014809846878052: 1, -1.1832531690597534: 1, -0.6185922622680664: 1, -0.8899372816085815: 1, -0.9966712594032288: 1, -0.7500604391098022: 1, -1.2013036012649536: 1, -0.6249680519104004: 1, -1.1552096605300903: 1, -1.1936330795288086: 1, -1.026742935180664: 1, -0.2650972902774811: 1, -1.2015254497528076: 1, -1.0461647510528564: 1, -0.8267379403114319: 1, -1.0589720010757446: 1, -1.1486388444900513: 1, -1.1036909818649292: 1, -1.0501618385314941: 1, -1.1981457471847534: 1, -0.9225814342498779: 1, -1.200010895729065: 1, -0.9485399723052979: 1, -1.2006280422210693: 1, -1.1891672611236572: 1, -1.1977088451385498: 1, -1.144382119178772: 1, -0.9449849724769592: 1, -1.1986464262008667: 1, -1.0933367013931274: 1, -1.0186684131622314: 1, -0.4794164001941681: 1, -1.1434556245803833: 1, -1.1468044519424438: 1, -0.4437340795993805: 1, -0.9021695256233215: 1, -0.7001152634620667: 1, -0.47614291310310364: 1, -0.21827441453933716: 1, -1.193596363067627: 1, 0.057195477187633514: 1, 4.863577365875244: 1, -0.9046985507011414: 1, 5.001932144165039: 1, 4.722275257110596: 1, 0.939198911190033: 1, -1.1445417404174805: 1, 4.302996635437012: 1, -1.1364892721176147: 1, -0.07361169159412384: 1, 4.875144004821777: 1, 5.066896438598633: 1, 5.072229385375977: 1, 0.4937030076980591: 1, 5.05051851272583: 1, -1.1785725355148315: 1, -0.4824954569339752: 1, -0.6635236740112305: 1, 5.037554740905762: 1, -0.7420345544815063: 1, -0.967963457107544: 1, -0.8666650652885437: 1, -0.6411266922950745: 1, -0.7968862652778625: 1, -1.1212965250015259: 1, -1.2005667686462402: 1, -1.0699774026870728: 1, -0.5409148931503296: 1, -1.0538722276687622: 1, -0.9141108393669128: 1, -0.712040364742279: 1, -0.8772833347320557: 1, -0.5385211110115051: 1, -0.5520062446594238: 1, -0.9129625558853149: 1, -1.0223268270492554: 1, -0.41631850600242615: 1, -1.0399388074874878: 1, -1.1062737703323364: 1, -0.7228063344955444: 1, -0.9656466245651245: 1, -1.0281414985656738: 1, -1.1463063955307007: 1, -0.694696843624115: 1, -1.05448579788208: 1, -1.145255446434021: 1, 0.29684698581695557: 1, -0.9453350901603699: 1, 0.18425099551677704: 1, -0.8791208267211914: 1, -1.1503796577453613: 1, -0.9824236631393433: 1, -1.189503788948059: 1, -0.4648326635360718: 1, -1.0434380769729614: 1, -1.00275719165802: 1, -0.6867276430130005: 1, -0.669135332107544: 1, -1.2014120817184448: 1, -0.6967374682426453: 1, -1.0416630506515503: 1, 0.216363787651062: 1, -0.38882899284362793: 1, -1.1946135759353638: 1, -0.9539603590965271: 1, -1.1790684461593628: 1, -1.167614459991455: 1, -1.0620733499526978: 1, -1.1940333843231201: 1, -1.1150031089782715: 1, -1.0865159034729004: 1, -1.1970044374465942: 1, -0.3503449857234955: 1, -1.1621276140213013: 1, -0.3561877906322479: 1, -0.8054187893867493: 1, -0.34684932231903076: 1, 1.585684895515442: 1, 1.1463862657546997: 1, -1.2015637159347534: 1, -1.154268741607666: 1, -1.201583743095398: 1, -1.171905517578125: 1, 0.009732205420732498: 1, 1.123262643814087: 1, -0.40432149171829224: 1, -0.45371508598327637: 1, -0.411021888256073: 1, 0.8030492067337036: 1, -1.2016305923461914: 1, 0.5780434608459473: 1, -0.32108673453330994: 1, -0.4003660976886749: 1, 0.6591589450836182: 1, -0.1892606019973755: 1, -0.8111313581466675: 1, 1.5706232786178589: 1, 1.0962333679199219: 1, 0.5601547360420227: 1, 1.0823159217834473: 1, -0.9092623591423035: 1, 1.593440055847168: 1, 1.0043728351593018: 1, 1.6096405982971191: 1, -0.433001846075058: 1, -1.0877141952514648: 1, 1.6532078981399536: 1, 1.5002496242523193: 1, -1.1626108884811401: 1, -0.8635501265525818: 1, 0.5944019556045532: 1, 1.8255465030670166: 1, -0.44575440883636475: 1, 1.2443398237228394: 1, 0.2436400204896927: 1, 1.036679744720459: 1, 0.2817704975605011: 1, 1.2748090028762817: 1, -0.750208854675293: 1, -0.7198786735534668: 1, 0.05797763168811798: 1, -0.8244988322257996: 1, -0.5816406607627869: 1, -0.4005826711654663: 1, 1.2405850887298584: 1, -0.9499993324279785: 1, 0.5858985185623169: 1, -0.30494359135627747: 1, -1.2016358375549316: 1, -0.5739816427230835: 1, 0.7670885920524597: 1, -0.4656817615032196: 1, -1.0180860757827759: 1, -1.2016373872756958: 1, -0.576421856880188: 1, -1.195853590965271: 1, 0.821479320526123: 1, -1.1690752506256104: 1, -0.5976389050483704: 1, 1.2671806812286377: 1, 1.0670119524002075: 1, -0.7073397040367126: 1, -0.4774172008037567: 1, 0.6389100551605225: 1, -1.0666824579238892: 1, 1.282943606376648: 1, 0.842113196849823: 1, 0.369517058134079: 1, -0.6496835947036743: 1, -1.1689379215240479: 1, -0.7252834439277649: 1, -0.7019102573394775: 1, -0.9352316856384277: 1, -0.5334532856941223: 1, 1.0577486753463745: 1, -0.5885310769081116: 1, 1.2156920433044434: 1, -0.8736492395401001: 1, 0.6975425481796265: 1, 0.4960012137889862: 1, 1.0147548913955688: 1, -0.35478705167770386: 1, -1.1707801818847656: 1, 1.2256038188934326: 1, 0.9517895579338074: 1, -0.3316475749015808: 1, 0.880368709564209: 1, -1.084214448928833: 1, 0.21256738901138306: 1, -0.8594576716423035: 1, 0.605282723903656: 1, 0.7936035394668579: 1, -0.1682627946138382: 1, 1.1564749479293823: 1, -1.157366394996643: 1, 0.275499552488327: 1, -0.23024950921535492: 1, -1.171040415763855: 1, -1.1865227222442627: 1, -1.1973918676376343: 1, -1.1811505556106567: 1, 1.6846264600753784: 1, -1.0972120761871338: 1, -0.40274444222450256: 1, -1.2010711431503296: 1, -1.1966667175292969: 1, -1.198758602142334: 1, -1.196737289428711: 1, -1.1950762271881104: 1, -0.9986592531204224: 1, -1.1970906257629395: 1, 1.7719837427139282: 1, -1.1995155811309814: 1, -1.2008846998214722: 1, -1.0349972248077393: 1, -0.9958106875419617: 1, -1.175083041191101: 1, 0.7184975147247314: 1, 0.4647902548313141: 1, 0.3444092869758606: 1, -1.1610828638076782: 1, -1.188031554222107: 1, -0.6870049834251404: 1, -0.6259949207305908: 1, -1.186979055404663: 1, -1.1963162422180176: 1, -1.201351284980774: 1, -1.0963668823242188: 1, -1.1594713926315308: 1, -0.9010990262031555: 1, -0.5026354193687439: 1, -1.1365838050842285: 1, -1.1999870538711548: 1, -1.1910632848739624: 1, -0.7359880805015564: 1, -1.195172905921936: 1, 0.741217315196991: 1, -1.1993547677993774: 1, -1.2002415657043457: 1, -1.164048194885254: 1, -1.1929975748062134: 1, -1.0542218685150146: 1, -1.1924408674240112: 1, -0.15451399981975555: 1, 0.8645955324172974: 1, -1.201418161392212: 1, 1.6113003492355347: 1, 0.29118791222572327: 1, -0.2950673997402191: 1, -0.15851348638534546: 1, -1.1721446514129639: 1, -0.23552395403385162: 1, -0.7970835566520691: 1, 0.5490202307701111: 1, 1.6066374778747559: 1, -0.16204535961151123: 1, 0.8697875142097473: 1, 0.7108749151229858: 1, 0.5599294900894165: 1, -0.6900844573974609: 1, 0.7067909836769104: 1, -1.1882411241531372: 1, 1.5164122581481934: 1, -1.2010202407836914: 1, 0.8852934837341309: 1, -0.23251420259475708: 1, 0.7324214577674866: 1, -0.39169713854789734: 1, -1.1728081703186035: 1, 0.6052579283714294: 1, -0.6460915207862854: 1, -1.1685289144515991: 1, -1.1994209289550781: 1, -1.173497200012207: 1, -0.638097882270813: 1, -1.2015430927276611: 1, 0.23683039844036102: 1, -1.0081939697265625: 1, -0.04312499612569809: 1, -1.1948130130767822: 1, -1.1990872621536255: 1, -1.188680648803711: 1, -1.1971783638000488: 1, -0.17054051160812378: 1, -1.1964715719223022: 1, 0.3594037592411041: 1, -1.2015913724899292: 1, -1.016434669494629: 1, -0.7167530059814453: 1, 0.18603742122650146: 1, 0.6880602836608887: 1, 1.0611008405685425: 1, -0.7624772787094116: 1, 0.6570013761520386: 1, -0.9236016869544983: 1, 0.11391282081604004: 1, 1.2999117374420166: 1, 1.4899855852127075: 1, -0.1709771454334259: 1, -0.812441349029541: 1, 0.3157300651073456: 1, 0.6854439377784729: 1, 1.5044559240341187: 1, -0.6113037467002869: 1, 1.1449819803237915: 1, 0.7352478504180908: 1, 0.18826737999916077: 1, 1.0590577125549316: 1, 1.4972656965255737: 1, 1.0554699897766113: 1, 1.3231751918792725: 1, 0.592692494392395: 1, 1.556430697441101: 1, -0.6031805276870728: 1, -0.9130086302757263: 1, 1.1509000062942505: 1, 0.7825908064842224: 1, -0.1196289211511612: 1, 0.14948004484176636: 1, -0.13019442558288574: 1, 0.23767612874507904: 1, -0.632728099822998: 1, 0.3140702247619629: 1, -0.10326965153217316: 1, 0.12041562795639038: 1, 0.7718502283096313: 1, 0.05425111949443817: 1, 0.3626178801059723: 1, 0.4515918791294098: 1, 1.622856616973877: 1, -0.40100592374801636: 1, -0.10369612276554108: 1, 1.4727312326431274: 1, 1.481839656829834: 1, 1.3110328912734985: 1, 0.5713070034980774: 1, 0.24058645963668823: 1, 0.3425867557525635: 1, 0.35902467370033264: 1, 1.561331033706665: 1, 0.1928955614566803: 1, 0.23876602947711945: 1, 1.260263442993164: 1, 1.375723958015442: 1, 1.4145740270614624: 1, 0.3554361164569855: 1, 0.9935461282730103: 1, 0.2826254069805145: 1, -0.3440307080745697: 1, 0.04298178479075432: 1, -0.11455459892749786: 1, 0.30982303619384766: 1, 0.340713769197464: 1, 0.2596873939037323: 1, 0.822748601436615: 1, 1.4639532566070557: 1, -0.30866673588752747: 1, 1.4051384925842285: 1, 0.1144905686378479: 1, 1.2591054439544678: 1, -0.20360040664672852: 1, 0.2816910445690155: 1, -0.15476621687412262: 1, -0.10053509473800659: 1, 0.339995801448822: 1, 0.11328306794166565: 1, 0.5472216606140137: 1, 1.3589857816696167: 1, 1.215183138847351: 1, 0.7438257932662964: 1, 1.233654499053955: 1, -0.2681446373462677: 1, 0.18295887112617493: 1, 0.2222539335489273: 1, 0.3024345636367798: 1, 0.35236239433288574: 1, 1.6643083095550537: 1, 1.4752575159072876: 1, -0.19414900243282318: 1, -1.1556631326675415: 1, -0.2946179509162903: 1, 0.2801852524280548: 1, -0.48821160197257996: 1, 1.4599251747131348: 1, 0.31122344732284546: 1, -0.05793027952313423: 1, 1.3920340538024902: 1, 1.0482161045074463: 1, 0.44933900237083435: 1, 0.5573470592498779: 1, -0.5775644183158875: 1, 0.81052166223526: 1, 0.30889958143234253: 1, 0.06706182658672333: 1, 0.6605957746505737: 1, 0.7520656585693359: 1, -0.5888111591339111: 1, 1.7802213430404663: 1, 0.2615222632884979: 1, 0.5367603302001953: 1, -0.06798223406076431: 1, 1.3918044567108154: 1, 1.126368761062622: 1, 1.2197668552398682: 1, 0.7675086855888367: 1, 0.004675476811826229: 1, 1.0209075212478638: 1, 0.6983891129493713: 1, 0.11682117730379105: 1, 0.7781831622123718: 1, 1.4282907247543335: 1, 0.19260334968566895: 1, 0.36868804693222046: 1, 0.01178812701255083: 1, -0.5517370104789734: 1, -0.5199218392372131: 1, 0.21266861259937286: 1, -0.03138621151447296: 1, 0.3231067657470703: 1, 0.8642333745956421: 1, -0.3219013214111328: 1, 0.853833794593811: 1, 0.9222752451896667: 1, -0.16844011843204498: 1, -0.22581757605075836: 1, -0.4890022277832031: 1, -1.1736985445022583: 1, 0.26258811354637146: 1, -0.20083148777484894: 1, -0.22985504567623138: 1, 1.059990644454956: 1, -0.4757806956768036: 1, 0.021963730454444885: 1, -0.0920228511095047: 1, -0.06609977036714554: 1, 1.5724915266036987: 1, 0.7346283793449402: 1, 1.466652274131775: 1, 0.11165464669466019: 1, 1.111115574836731: 1, 1.5909359455108643: 1, 0.017385760322213173: 1, -0.014552570879459381: 1, 0.5634517669677734: 1, 1.256608247756958: 1, 1.4527193307876587: 1, 1.4751214981079102: 1, 0.18870316445827484: 1, 0.858607292175293: 1, 1.282376766204834: 1, 0.7782679200172424: 1, 1.0659617185592651: 1, 0.987504780292511: 1, 0.22794750332832336: 1, 1.1887938976287842: 1, 0.9434877038002014: 1, 1.379611611366272: 1, 0.22853031754493713: 1, -0.6209532618522644: 1, -0.0526600144803524: 1, -0.21601253747940063: 1, 1.5148382186889648: 1, 0.33931559324264526: 1, 0.11681299656629562: 1, 0.06557659059762955: 1, 0.3338538706302643: 1, 1.914624810218811: 1, 0.2561040222644806: 1, 0.3319496512413025: 1, -0.16230207681655884: 1, -0.03743843734264374: 1, 1.55558443069458: 1, 1.5988372564315796: 1, 1.3192821741104126: 1, 1.533202886581421: 1, 0.31844547390937805: 1, 0.19674475491046906: 1, -0.3147757053375244: 1, -0.1758507341146469: 1, -0.11516252905130386: 1, 0.7623692750930786: 1, -1.1871042251586914: 1, 0.7778939008712769: 1, -0.1960129290819168: 1, 0.9693878293037415: 1, -1.0057076215744019: 1, 0.8502970337867737: 1, -0.6454114317893982: 1, 0.753506600856781: 1, 0.2539007067680359: 1, 0.002965901279821992: 1, 1.4364150762557983: 1, 1.4786081314086914: 1, 0.2568971812725067: 1, 0.5611000061035156: 1, 0.8098611235618591: 1, 0.7161386013031006: 1, 0.7253832817077637: 1, 1.5550216436386108: 1, 0.80833899974823: 1, 2.0702569484710693: 1, 0.9344565272331238: 1, -0.2038322389125824: 1, -1.1040070056915283: 1, 1.1847658157348633: 1, -0.56696617603302: 1, 1.1921144723892212: 1, 0.7092652916908264: 1, 0.4243623912334442: 1, 0.0384686179459095: 1, 0.27865079045295715: 1, -0.5194405913352966: 1, 0.009009350091218948: 1, 1.4580349922180176: 1, 1.4103199243545532: 1, 1.3759030103683472: 1, 0.8865175843238831: 1, 0.7280606031417847: 1, 0.7922017574310303: 1, 1.405086636543274: 1, 0.3431006968021393: 1, -1.1504056453704834: 1, -0.11932859569787979: 1, 1.5512200593948364: 1, -0.19494549930095673: 1, -0.16245944797992706: 1, 0.29415011405944824: 1, 1.6108869314193726: 1, 0.05755604803562164: 1, 0.8618729114532471: 1, 0.9195832014083862: 1, 1.5557059049606323: 1, -0.7541334629058838: 1, 0.2642950117588043: 1, 0.8805737495422363: 1, 0.9053120613098145: 1, -0.16789886355400085: 1, -0.17545948922634125: 1, 0.263934850692749: 1, 1.3463716506958008: 1, 1.3844947814941406: 1, 0.7684015035629272: 1, 0.22540217638015747: 1, 0.041503969579935074: 1, 1.5027039051055908: 1, 0.6448225975036621: 1, 0.2692132890224457: 1, 0.9760450720787048: 1, 0.8999701142311096: 1, -1.1716605424880981: 1, -1.1579349040985107: 1, -1.1999285221099854: 1, 0.46070119738578796: 1, 1.4680920839309692: 1, 1.0112850666046143: 1, -0.9134752750396729: 1, -1.2015975713729858: 1, -0.8270519375801086: 1, -0.04211708903312683: 1, -0.974824070930481: 1, -0.9460977911949158: 1, -0.7166287899017334: 1, -1.19014310836792: 1, 1.8432141542434692: 1, -0.8671839237213135: 1, 0.49092090129852295: 1, 0.638069212436676: 1, -0.8936908841133118: 1, 1.4807751178741455: 1, 0.3074534237384796: 1, -0.04237562045454979: 1, 1.4735376834869385: 1, -0.20395949482917786: 1, -0.870884895324707: 1, 1.5217971801757812: 1, 0.008224710822105408: 1, 0.48444247245788574: 1, -0.30212122201919556: 1, -0.19083698093891144: 1, -0.091583751142025: 1, 0.2824403941631317: 1, -0.009973025880753994: 1, 0.3276282548904419: 1, 0.3697844445705414: 1, -1.1851680278778076: 1, 1.2598285675048828: 1, -1.086830973625183: 1, -0.6757361888885498: 1, 0.6463801264762878: 1, 1.4938876628875732: 1, 0.31239357590675354: 1, 0.7275790572166443: 1, 0.3124358654022217: 1, 1.556864857673645: 1, -0.8828660249710083: 1, 0.09744428098201752: 1, 1.1072840690612793: 1, 0.23464715480804443: 1, -0.6223658919334412: 1, 2.4050354957580566: 1, -0.09898030012845993: 1, 0.29443448781967163: 1, 0.06307864934206009: 1, -0.20047886669635773: 1, 0.3499395549297333: 1, -0.1565595418214798: 1, 0.9952307939529419: 1, 0.8222253322601318: 1, -0.28925973176956177: 1, 1.5599995851516724: 1, -1.0314160585403442: 1, 1.5686708688735962: 1, 1.7404301166534424: 1, 0.2925977110862732: 1, -0.2210041582584381: 1, 0.9266675710678101: 1, -0.3739321231842041: 1, 0.8433452844619751: 1, 0.5241292119026184: 1, 0.9422100782394409: 1, 1.0041277408599854: 1, 1.916335940361023: 1, -0.6390724778175354: 1, 1.5731940269470215: 1, 0.39806419610977173: 1, 1.473071813583374: 1, 1.5127235651016235: 1, 1.2513844966888428: 1, 0.39437854290008545: 1, 1.4644227027893066: 1, 0.2846507132053375: 1, 1.3862555027008057: 1, 1.2155990600585938: 1, -1.1900941133499146: 1, 0.3661552369594574: 1, 0.8500742316246033: 1, 1.4411144256591797: 1, 0.02118554152548313: 1, 1.622040867805481: 1, 1.1854524612426758: 1, 0.8342987895011902: 1, 0.968934953212738: 1, 0.98076331615448: 1, 1.007346510887146: 1, 0.5769325494766235: 1, 1.5063830614089966: 1, 0.2389289289712906: 1, -0.11868320405483246: 1, 1.5239653587341309: 1, 0.5069756507873535: 1, 1.3772739171981812: 1, -0.43688738346099854: 1, 0.8997297883033752: 1, 0.6865427494049072: 1, 1.5009726285934448: 1, 0.8533394932746887: 1, 0.8847638368606567: 1, 1.547389030456543: 1, 1.471611738204956: 1, 1.4821670055389404: 1, -0.07051629573106766: 1, 1.2778698205947876: 1, 1.3494865894317627: 1, -0.6971253156661987: 1, 0.24872112274169922: 1, 1.4069277048110962: 1, -1.1632437705993652: 1, -1.1837621927261353: 1, 1.5597952604293823: 1, 0.3126457929611206: 1, 0.8933335542678833: 1, -0.4644731879234314: 1, 1.368375539779663: 1, 1.4151798486709595: 1, -0.11317183822393417: 1, 0.3683989942073822: 1, 0.1563034951686859: 1, -0.1884830892086029: 1, 0.35628634691238403: 1, -1.0716415643692017: 1, -0.2168547362089157: 1, 0.30258285999298096: 1, 0.6513680219650269: 1, -0.040548212826251984: 1, 1.1731380224227905: 1, -0.20986565947532654: 1, -1.024586796760559: 1, 0.8943637013435364: 1, 1.4529069662094116: 1, 0.8744557499885559: 1, 0.9953116178512573: 1, 1.302850365638733: 1, 0.7159395813941956: 1, 1.3597513437271118: 1, -1.0867854356765747: 1, 1.189407229423523: 1, 0.05546194687485695: 1, -0.39580973982810974: 1, 0.8532567620277405: 1, 0.23268936574459076: 1, -0.35842111706733704: 1, -1.126828908920288: 1, 1.185150146484375: 1, 0.9848727583885193: 1, 0.938378632068634: 1, 0.9840258359909058: 1, 0.008470889180898666: 1, -0.2164342701435089: 1, -1.201277732849121: 1, -0.34523066878318787: 1, 1.2822530269622803: 1, -0.27776581048965454: 1, 1.2413678169250488: 1, 1.2350752353668213: 1, -0.2299586683511734: 1, 0.008748067542910576: 1, 0.9413108229637146: 1, 0.6657525897026062: 1, 0.8562594652175903: 1, -0.9053927063941956: 1, -0.20541705191135406: 1, 0.008864369243383408: 1, 0.8631362915039062: 1, -0.8882886171340942: 1, -0.12074612081050873: 1, 1.150854468345642: 1, 1.2192449569702148: 1, -0.8759819269180298: 1, 1.009254813194275: 1, 0.9730016589164734: 1, 0.889022946357727: 1, -0.674019455909729: 1, -1.1972460746765137: 1, 0.055386364459991455: 1, -0.2671576142311096: 1, 0.009196557104587555: 1, 0.23997803032398224: 1, 0.24701066315174103: 1, -0.8705235719680786: 1, -1.1247143745422363: 1, -0.007039392367005348: 1, -0.621107280254364: 1, 0.23272329568862915: 1, 1.0945255756378174: 1, -0.9168434739112854: 1, 0.8952587246894836: 1, -0.37779542803764343: 1, -0.4684484004974365: 1, -1.0965174436569214: 1, 0.015011060051620007: 1, -1.1905653476715088: 1, -0.6103007793426514: 1, 1.310013771057129: 1, 0.029840881004929543: 1, 0.09383262693881989: 1, 0.36468705534935: 1, 0.6232529878616333: 1, -0.5104274749755859: 1, -0.14243346452713013: 1, 0.5468651056289673: 1, 0.637002170085907: 1, -1.2005233764648438: 1, -1.1812138557434082: 1, -0.05695538595318794: 1, -0.28555259108543396: 1, 0.8218473196029663: 1, 0.2588636577129364: 1, -0.5884501934051514: 1, 0.9394524693489075: 1, -0.1427413374185562: 1, -0.08886344730854034: 1, -1.0537559986114502: 1, -0.5749701261520386: 1, 1.3061707019805908: 1, 0.9305616617202759: 1, 0.6777730584144592: 1, 0.7325264811515808: 1, 0.4251300096511841: 1, 1.2978951930999756: 1, 0.729964017868042: 1, -0.8322789072990417: 1, -0.33581042289733887: 1, 0.4259852468967438: 1, -0.2597183287143707: 1, 0.6872115731239319: 1, 1.035197138786316: 1, 1.196738839149475: 1, -0.16826894879341125: 1, -0.9032037854194641: 1, 1.0347987413406372: 1, 0.6233600974082947: 1, 1.113309621810913: 1, 1.2229245901107788: 1, -1.2015588283538818: 1, -0.7430113554000854: 1, -0.012521528638899326: 1, -1.0489486455917358: 1, 0.5795131325721741: 1, -0.11078709363937378: 1, 1.2373515367507935: 1, -0.586733877658844: 1, -1.0349785089492798: 1, 1.0134633779525757: 1, 0.44458866119384766: 1, 1.0013794898986816: 1, 0.02054119110107422: 1, -0.9308528304100037: 1, -0.9307324290275574: 1, -0.39703771471977234: 1, -1.2004859447479248: 1, -1.0583271980285645: 1, 0.41254743933677673: 1, -1.20163094997406: 1, 0.15707539021968842: 1, -0.11954380571842194: 1, 0.1316474825143814: 1, 0.29174771904945374: 1, 1.039072036743164: 1, 1.0578463077545166: 1, -0.5372467637062073: 1, -1.095828652381897: 1, -0.23942992091178894: 1, -0.24153171479701996: 1, -0.02250954695045948: 1, -0.019019143655896187: 1, -1.1852235794067383: 1, 1.104429006576538: 1, 0.525627076625824: 1, 0.31131237745285034: 1, -0.6482374668121338: 1, -1.1878859996795654: 1, -1.1774789094924927: 1, 1.1650327444076538: 1, 1.1985303163528442: 1, -0.13838951289653778: 1, -0.11709770560264587: 1, 0.0796559751033783: 1, -0.20500345528125763: 1, 0.004906347021460533: 1, -0.23531334102153778: 1, -0.15267345309257507: 1, -0.19495585560798645: 1, -1.2016282081604004: 1, -0.7914682626724243: 1, 1.0062413215637207: 1, -0.0449078269302845: 1, -0.8713244199752808: 1, 0.8318881988525391: 1, 1.2479115724563599: 1, -1.1798354387283325: 1, 0.6535213589668274: 1, 1.2529374361038208: 1, 1.2188843488693237: 1, -0.5479841232299805: 1, 0.4542813301086426: 1, 0.8476697206497192: 1, -0.24156887829303741: 1, -0.18254651129245758: 1, 0.838370680809021: 1, -0.41674312949180603: 1, 0.654328465461731: 1, 1.1694233417510986: 1, 0.23339834809303284: 1, -1.2016277313232422: 1, -1.2004797458648682: 1, -0.0411478653550148: 1, -1.1211071014404297: 1, -0.42487016320228577: 1, -0.002975589595735073: 1, 0.5979693531990051: 1, -0.22866979241371155: 1, 1.179612159729004: 1, -0.14272816479206085: 1, 1.1942393779754639: 1, -1.2016297578811646: 1, 1.1865397691726685: 1, -0.04175446555018425: 1, -0.2098180055618286: 1, 1.2327803373336792: 1, -0.1759326308965683: 1, -0.19223442673683167: 1, 0.7244752645492554: 1, 0.4987215995788574: 1, -0.42109400033950806: 1, -0.816495954990387: 1, 0.4205377995967865: 1, 0.033690646290779114: 1, 0.8838387131690979: 1, -0.9900954961776733: 1, -0.20062661170959473: 1, -1.1656997203826904: 1, -0.1855221837759018: 1, 1.0187321901321411: 1, 1.1683013439178467: 1, -1.1928194761276245: 1, -0.8740671277046204: 1, 1.1960792541503906: 1, 0.021630888804793358: 1, -0.4057319462299347: 1, -1.2015819549560547: 1, -1.1363192796707153: 1, 0.2464127391576767: 1, -1.2016382217407227: 1, -1.201630711555481: 1, -0.4132004976272583: 1, -1.2015154361724854: 1, -1.2011888027191162: 1, -0.4687884449958801: 1, -1.1849600076675415: 1, -0.08719541132450104: 1, 0.08744630217552185: 1, 0.1322988122701645: 1, -1.2016332149505615: 1, -1.0628424882888794: 1, -0.6758065819740295: 1, -1.1954847574234009: 1, -1.2016379833221436: 1, -1.194821834564209: 1, -1.2013384103775024: 1, -0.045158740133047104: 1, -1.1490250825881958: 1, -0.053435858339071274: 1, -1.1979888677597046: 1, -1.1923046112060547: 1, 0.3015405535697937: 1, 0.9705496430397034: 1, 0.469992071390152: 1, -1.2016286849975586: 1, -1.2016292810440063: 1, -1.2015831470489502: 1, -1.2015341520309448: 1, -1.1318936347961426: 1, -1.1987295150756836: 1, -1.2013877630233765: 1, -0.6771114468574524: 1, -1.1967262029647827: 1, -0.07099064439535141: 1, 0.1497194468975067: 1, -1.2014774084091187: 1, -1.2016361951828003: 1, -1.2015197277069092: 1, -1.201493740081787: 1, -1.201407790184021: 1, -1.2016335725784302: 1, -0.8910970091819763: 1, -1.193373441696167: 1, -0.11703558266162872: 1, 0.1581522524356842: 1, 0.9015107750892639: 1, -0.416104793548584: 1, -1.201634168624878: 1, -1.1903178691864014: 1, -1.171350359916687: 1, -1.150158166885376: 1, -1.1623584032058716: 1, -1.201610803604126: 1, 0.3900337219238281: 1, -0.6426911950111389: 1, -0.8209242224693298: 1, -1.2014681100845337: 1, -0.06736226379871368: 1, -0.5888482928276062: 1, -0.5093275308609009: 1, -1.2015849351882935: 1, -0.0922759622335434: 1, 0.3153885304927826: 1, -0.8935246467590332: 1, 0.10832516103982925: 1, 0.31476086378097534: 1, -1.140577793121338: 1, 0.4845307767391205: 1, -0.11705746501684189: 1, 0.20136801898479462: 1, -0.28742867708206177: 1, -1.180970549583435: 1, -0.518195390701294: 1, -0.5834662318229675: 1, -0.9676279425621033: 1, -1.2015115022659302: 1, -1.2014601230621338: 1, 0.07849415391683578: 1, 0.86496901512146: 1, -0.21048426628112793: 1, 0.8498935103416443: 1, -0.9951181411743164: 1, -1.2016206979751587: 1, -0.9857455492019653: 1, 0.15111742913722992: 1, -0.009843221865594387: 1, -0.5475073456764221: 1, -1.2014092206954956: 1, -1.1383882761001587: 1, -1.2012838125228882: 1, -1.2016196250915527: 1, -1.2016048431396484: 1, -0.41403406858444214: 1, -0.14289136230945587: 1, -1.179785132408142: 1, -1.2015916109085083: 1, 0.9066123366355896: 1, -0.19123347103595734: 1, -0.015705665573477745: 1, 0.8673886060714722: 1, -0.011501064524054527: 1, -0.3390061855316162: 1, -1.1947468519210815: 1, -1.175829291343689: 1, 3.9672465324401855: 1, -0.37206733226776123: 1, 0.7298784255981445: 1, 0.8610304594039917: 1, 1.0769490003585815: 1, 1.28579580783844: 1, 0.29349300265312195: 1, 1.2526917457580566: 1, 0.3951069414615631: 1, 1.0069524049758911: 1, 0.8965467214584351: 1, -1.0558182001113892: 1, 0.047311048954725266: 1, 1.1649004220962524: 1, -0.9734629392623901: 1, 1.2916063070297241: 1, 1.295539140701294: 1, 0.9265954494476318: 1, -0.029267514124512672: 1, -1.1963841915130615: 1, -0.21881107985973358: 1, 0.33690160512924194: 1, -0.14633353054523468: 1, 0.04192548990249634: 1, 0.27843114733695984: 1, 0.6842910647392273: 1, 0.5252094268798828: 1, -0.14631414413452148: 1, 1.2064505815505981: 1, 0.18457916378974915: 1, -0.84548020362854: 1, 1.200035810470581: 1, 0.9529565572738647: 1, 0.04932570457458496: 1, -0.9972583055496216: 1, -0.30726683139801025: 1, 1.2879470586776733: 1, -0.9601404666900635: 1, -1.1107620000839233: 1, -0.34107181429862976: 1, -0.33075013756752014: 1, -1.1943039894104004: 1, -1.1586899757385254: 1, -0.16572129726409912: 1, -1.2015702724456787: 1, -1.201588749885559: 1, -1.201620101928711: 1, 0.1331777125597: 1, 0.0891273021697998: 1, -1.201523780822754: 1, -1.2015215158462524: 1, -1.2015316486358643: 1, -1.1484540700912476: 1, -1.201595425605774: 1, -1.201473593711853: 1, 0.19668835401535034: 1, 2.26233172416687: 1, -1.2015864849090576: 1, -1.2016347646713257: 1, -0.848010778427124: 1, -1.1948002576828003: 1, -0.5937166213989258: 1, -1.2016299962997437: 1, 4.363720893859863: 1, -1.2016273736953735: 1, -0.10643515735864639: 1, 1.1717408895492554: 1, -1.1992485523223877: 1, 1.2726079225540161: 1, 0.30879271030426025: 1, -0.24580752849578857: 1, -0.10037212073802948: 1, -1.0237786769866943: 1, 0.005373222753405571: 1, 1.1995047330856323: 1, 1.0674824714660645: 1, 0.09893729537725449: 1, 1.0808240175247192: 1, -0.47594985365867615: 1, -0.21433545649051666: 1, 1.0520529747009277: 1, 0.9786769151687622: 1, -0.10290281474590302: 1, -0.2904500663280487: 1, -0.16829612851142883: 1, -0.1688508540391922: 1, 0.6481048464775085: 1, -0.2126571536064148: 1, -0.3213060796260834: 1, -1.1320221424102783: 1} test data: {-1.20163094997406: 3, -1.201637625694275: 2, -1.2016369104385376: 2, -0.622269868850708: 1, -0.3344474732875824: 1, 0.6465133428573608: 1, 1.4755654335021973: 1, 1.166581153869629: 1, 1.2994223833084106: 1, -0.17338967323303223: 1, 0.9650787115097046: 1, -0.31671836972236633: 1, 0.29451489448547363: 1, -0.4993174076080322: 1, 0.663663923740387: 1, 1.571059226989746: 1, 0.8274267315864563: 1, -0.30124133825302124: 1, 0.1385408341884613: 1, 1.605971336364746: 1, 0.8664619326591492: 1, 0.32629087567329407: 1, 1.6089627742767334: 1, 0.8396581411361694: 1, -0.37317758798599243: 1, -0.5348793864250183: 1, 0.8028292059898376: 1, 0.5170465111732483: 1, 0.6158161163330078: 1, 1.5124294757843018: 1, 0.16298139095306396: 1, -1.2016340494155884: 1, -0.7180438041687012: 1, -0.7170498371124268: 1, 0.8741052150726318: 1, 0.8892757296562195: 1, 1.2882025241851807: 1, 1.0355088710784912: 1, 1.4811328649520874: 1, -0.25767362117767334: 1, -0.5288078188896179: 1, 1.1008855104446411: 1, 0.71575528383255: 1, 1.3784377574920654: 1, 0.7825468182563782: 1, 1.427628755569458: 1, -0.030014334246516228: 1, 0.13578376173973083: 1, 1.5805106163024902: 1, 0.20390741527080536: 1, 0.7171676754951477: 1, -1.0873279571533203: 1, 1.4237861633300781: 1, 0.10751932114362717: 1, -1.1972938776016235: 1, -1.1205617189407349: 1, -1.201635479927063: 1, -0.2831217646598816: 1, -1.1674489974975586: 1, 1.14208984375: 1, 0.4001854956150055: 1, 0.034827083349227905: 1, 0.5436715483665466: 1, -1.045175313949585: 1, -0.7745821475982666: 1, -1.1962217092514038: 1, -1.2014697790145874: 1, -1.2016384601593018: 1, 1.7539664506912231: 1, -0.29486238956451416: 1, -1.177890419960022: 1, -0.3334684669971466: 1, 0.7675377726554871: 1, -0.883184015750885: 1, 0.0290671493858099: 1, 0.2407364845275879: 1, 1.8953936100006104: 1, -1.0070439577102661: 1, 0.6647039651870728: 1, 1.5720155239105225: 1, -0.399366557598114: 1, 0.5732383131980896: 1, 0.31096193194389343: 1, -0.16027171909809113: 1, 0.2713952362537384: 1, -0.3774075210094452: 1, 1.5978947877883911: 1, 1.1970174312591553: 1, -0.29657527804374695: 1, 0.21768306195735931: 1, -0.5027830004692078: 1, 0.32263097167015076: 1, -1.2015589475631714: 1, -0.8657602667808533: 1, -1.137963891029358: 1, -0.9888867139816284: 1, 0.6741006970405579: 1, 0.9611315131187439: 1, -0.506174623966217: 1, 0.603252649307251: 1, -0.4781683385372162: 1, -1.201597809791565: 1, -1.2015974521636963: 1, 0.003300704760476947: 1, -0.016411839053034782: 1, -0.116549052298069: 1, 1.0025136470794678: 1, -0.9803085327148438: 1, -0.233070969581604: 1, 0.5211433172225952: 1, -0.9951556921005249: 1, -0.7244925498962402: 1, -1.1898142099380493: 1, -0.8917420506477356: 1, -1.1212905645370483: 1, -1.1791499853134155: 1, 1.8480533361434937: 1, 0.5474697947502136: 1, -0.9047161340713501: 1, 0.3200169503688812: 1, 0.30057549476623535: 1, 1.0116826295852661: 1, -1.2009141445159912: 1, -1.1325860023498535: 1, -1.1712636947631836: 1, -1.0393953323364258: 1, 1.6304298639297485: 1, -0.7096079587936401: 1, -0.18136551976203918: 1, -1.0776159763336182: 1, -0.67027348279953: 1, -0.13872799277305603: 1, 0.7332795858383179: 1, 1.0842758417129517: 1, 0.9064841866493225: 1, 0.0863155722618103: 1, 1.5916389226913452: 1, 0.19180983304977417: 1, 0.9983642101287842: 1, 1.133412480354309: 1, 1.3578754663467407: 1, 0.11520501971244812: 1, 1.0432312488555908: 1, 1.5033998489379883: 1, 0.22126778960227966: 1, 0.6183062195777893: 1, -0.9639919996261597: 1, -0.025239035487174988: 1, 1.4127790927886963: 1, 0.8287729024887085: 1, 1.5179330110549927: 1, -0.20943275094032288: 1, 0.35639217495918274: 1, -0.004799619782716036: 1, -0.5884833335876465: 1, -1.0422825813293457: 1, 0.46373531222343445: 1, -0.14438967406749725: 1, -0.05734042823314667: 1, -0.9131625890731812: 1, 1.2659807205200195: 1, -0.8956496715545654: 1, -0.48030614852905273: 1, -0.9158876538276672: 1, -1.1507015228271484: 1, -1.1607003211975098: 1, 1.1217374801635742: 1, -0.13197508454322815: 1, -0.4185396134853363: 1, -0.6316778063774109: 1, 0.002877143444493413: 1, -0.11414719372987747: 1, -0.18094922602176666: 1, 0.1897212415933609: 1, -0.8486149311065674: 1, 0.3255096673965454: 1, 0.3698660731315613: 1, 1.053705096244812: 1, 1.001185417175293: 1, 0.3286954462528229: 1, -0.8747767210006714: 1, -0.5242934823036194: 1, 0.784543514251709: 1, -0.9793020486831665: 1, 1.8447388410568237: 1, -1.1907743215560913: 1, -1.1941906213760376: 1, -1.0090559720993042: 1, 1.3791615962982178: 1, 1.577986240386963: 1, 1.8358889818191528: 1, -0.07864277809858322: 1, -1.1442792415618896: 1, 0.2268732339143753: 1, -1.0407896041870117: 1, -1.1999518871307373: 1, 0.06941241025924683: 1, -1.1800106763839722: 1, -1.1358855962753296: 1, 0.9790754318237305: 1, 0.20924636721611023: 1, 0.8691079020500183: 1, -1.1798655986785889: 1, -0.700495719909668: 1, -0.9755853414535522: 1, -0.30134135484695435: 1, -0.47023993730545044: 1, 0.5000193119049072: 1, -0.35984915494918823: 1, 0.2675780653953552: 1, 1.6441566944122314: 1, -0.7435175180435181: 1, -0.26848453283309937: 1, -1.2006548643112183: 1, 0.11243647336959839: 1, -1.2014974355697632: 1, 0.28526046872138977: 1, -0.9635469913482666: 1, -0.7447215914726257: 1, 1.6134487390518188: 1, -1.0590986013412476: 1, 1.5396013259887695: 1, -0.7490406036376953: 1, -0.253825306892395: 1, -0.44384631514549255: 1, 0.5419895052909851: 1, -1.0791629552841187: 1, -0.7931265234947205: 1, 0.5422061085700989: 1, 1.5719680786132812: 1, 0.8521577715873718: 1, 1.6939563751220703: 1, 1.4182209968566895: 1, 0.0023630079813301563: 1, -0.2709258496761322: 1, 1.1236602067947388: 1, 0.26557838916778564: 1, 0.43645578622817993: 1, -0.2772659659385681: 1, 0.2889866232872009: 1, 0.5755087733268738: 1, 0.39559683203697205: 1, -0.1754150390625: 1, -0.4791189730167389: 1, 0.9761800765991211: 1, -0.180640310049057: 1, 0.9952530264854431: 1, -0.22097758948802948: 1, 1.4221618175506592: 1, -1.054260492324829: 1, -0.0515512116253376: 1, 1.3716888427734375: 1, 0.949316143989563: 1, 0.723430871963501: 1, 1.8263726234436035: 1, 0.8505056500434875: 1, 0.18436919152736664: 1, -0.7990305423736572: 1, -1.201361894607544: 1, 0.9216548800468445: 1, -1.1302224397659302: 1, 1.2727433443069458: 1, -0.43114355206489563: 1, -0.0861414223909378: 1, -1.2015763521194458: 1, 1.2539737224578857: 1, -1.2015223503112793: 1, 0.9156394600868225: 1, -1.2016057968139648: 1, -1.14544677734375: 1, 0.9750880599021912: 1, 0.5805153250694275: 1, -1.2014168500900269: 1, 0.4620521366596222: 1, -1.0611162185668945: 1, -0.34224218130111694: 1, -0.06900987029075623: 1, -0.40272125601768494: 1, -1.130736231803894: 1, -0.24564200639724731: 1, -1.2007230520248413: 1, 1.3129916191101074: 1, -1.2016278505325317: 1, -0.7696746587753296: 1, 1.896134853363037: 1, 1.7389863729476929: 1, 1.0316096544265747: 1, 0.9160022139549255: 1, 0.10331395268440247: 1, 1.4475048780441284: 1, 1.446770191192627: 1, 1.788615107536316: 1, -0.010684509761631489: 1, 1.4813575744628906: 1, 1.5016316175460815: 1, 1.6772441864013672: 1, -0.5458275079727173: 1, 1.8225407600402832: 1, 0.35647323727607727: 1, 1.5142070055007935: 1, -0.5466570258140564: 1, 1.8180814981460571: 1, 1.4667671918869019: 1, 1.8303353786468506: 1, -0.20698606967926025: 1, 0.20394694805145264: 1, 0.780617356300354: 1, 1.6759966611862183: 1, 0.7833142876625061: 1, 0.11675674468278885: 1, 1.477781891822815: 1, -0.37577909231185913: 1, -0.9003047943115234: 1, -1.093284010887146: 1, -0.16766004264354706: 1, -0.5345551371574402: 1, -0.9143604040145874: 1, 1.2115764617919922: 1, -0.47733497619628906: 1, -0.24040797352790833: 1, 0.3244344890117645: 1, -0.7031784653663635: 1, -0.6786049008369446: 1, 0.2551988363265991: 1, -0.4369107186794281: 1, -1.0817426443099976: 1, -0.31910526752471924: 1, 0.6444322466850281: 1, -0.3169466555118561: 1, -0.8443267941474915: 1, -1.1333893537521362: 1, 1.6555308103561401: 1, -0.42767956852912903: 1, 0.3824501037597656: 1, 1.483720302581787: 1, 0.19413244724273682: 1, 1.7392624616622925: 1, 0.672914445400238: 1, -0.03796708956360817: 1, 1.5701237916946411: 1, 1.0024393796920776: 1, -0.3424704372882843: 1, -0.7417653203010559: 1, 0.8612715601921082: 1, -0.007546336855739355: 1, -0.766767144203186: 1, -0.13618627190589905: 1, -0.5030357241630554: 1, 1.5933094024658203: 1, -0.0211151335388422: 1, -0.5381679534912109: 1, 0.5300026535987854: 1, 0.6790488958358765: 1, -0.8521113991737366: 1, 0.17652635276317596: 1, 0.31867286562919617: 1, 1.7650138139724731: 1, -0.6234576106071472: 1, -0.0832226425409317: 1, 1.5722923278808594: 1, -0.3237372636795044: 1, -0.8671026229858398: 1, 1.8596769571304321: 1, -0.3255411982536316: 1, -0.9720814824104309: 1, -0.18857857584953308: 1, 1.5428107976913452: 1, -1.1350090503692627: 1, 1.6552691459655762: 1, 0.2092854529619217: 1, 0.3915187418460846: 1, 0.49887171387672424: 1, 0.10666077584028244: 1, -0.9726563692092896: 1, -0.061833277344703674: 1, 1.2151012420654297: 1, -0.22874142229557037: 1, -1.2016154527664185: 1, -1.2016232013702393: 1, 1.4624748229980469: 1, 1.2401331663131714: 1, -1.2016234397888184: 1, -0.2558027505874634: 1, -0.22727444767951965: 1, 1.2042120695114136: 1, -0.15924076735973358: 1, -1.1006834506988525: 1, -0.5899453163146973: 1, -0.46826034784317017: 1, -0.9971945881843567: 1, 1.7544053792953491: 1, -0.5209143161773682: 1, -0.906280517578125: 1, 1.5116616487503052: 1, -0.3431845009326935: 1, -0.563433825969696: 1, -0.4653182625770569: 1, -1.1008306741714478: 1, -0.27475300431251526: 1, -0.7234905958175659: 1, 1.571593999862671: 1, -0.26545509696006775: 1, -0.6922621726989746: 1, -0.8816695213317871: 1, -1.1498054265975952: 1, 0.8056516647338867: 1, -1.201555848121643: 1, -1.1971925497055054: 1, -0.8350648880004883: 1, -0.2970311641693115: 1, -0.7790790796279907: 1, 1.5040947198867798: 1, 1.4793431758880615: 1, 4.900933742523193: 1, -0.5920382142066956: 1, 0.004596139770001173: 1, -0.09844925999641418: 1, -1.1992988586425781: 1, -1.1941806077957153: 1, -0.3642917275428772: 1, 0.7907249331474304: 1, -0.13443662226200104: 1, 0.8628989458084106: 1, -0.5910298824310303: 1, -0.7601802349090576: 1, 0.669349730014801: 1, -0.2299073189496994: 1, 0.5067287683486938: 1, 0.050834186375141144: 1, -1.2016347646713257: 1, 0.1461368203163147: 1, 1.2778337001800537: 1, -0.9202075004577637: 1, 0.7242860198020935: 1, 0.8666924238204956: 1, 0.8550933599472046: 1, -0.05180063098669052: 1, -0.21156159043312073: 1, 0.6907184720039368: 1, 0.2549906373023987: 1, 0.6509128212928772: 1, 0.4647965729236603: 1, 0.9053057432174683: 1, 1.7609484195709229: 1, -1.201631784439087: 1, -0.1808653175830841: 1, -0.17489729821681976: 1, 0.6412140727043152: 1, 1.3093386888504028: 1, 0.5863446593284607: 1, -1.199397087097168: 1, -1.2016229629516602: 1, 0.48456287384033203: 1, -0.7776852250099182: 1, -1.1028145551681519: 1, -1.2016355991363525: 1, -1.1959316730499268: 1, 0.9127581715583801: 1, -0.44190311431884766: 1, 1.229008674621582: 1, 0.5329916477203369: 1, -0.35862404108047485: 1, -0.8211961388587952: 1, -1.051085352897644: 1, -0.03033183142542839: 1, -0.1598413586616516: 1, -0.29025569558143616: 1, -0.2878003716468811: 1, 1.023429274559021: 1, -0.7420557737350464: 1, -0.3634156584739685: 1, 1.2993144989013672: 1, -1.1338447332382202: 1, -0.455705851316452: 1, -1.2016353607177734: 1, 0.033837273716926575: 1, 0.9881972074508667: 1, -0.07907160371541977: 1, -1.1774768829345703: 1, -1.2016366720199585: 1, -0.08915898948907852: 1, 0.004652172327041626: 1, 1.0157313346862793: 1, 1.1558300256729126: 1, -0.016240552067756653: 1, 0.7346874475479126: 1, -0.01721060648560524: 1, 1.043671727180481: 1, 0.7746310830116272: 1, -0.2553582787513733: 1, -1.0076838731765747: 1, -0.07969757169485092: 1, -0.03992554917931557: 1, 0.3949566185474396: 1, -1.1074978113174438: 1, -0.04764068126678467: 1, -0.6981508731842041: 1, -0.9249427914619446: 1, -0.17132940888404846: 1, 0.013616573065519333: 1, -1.1908975839614868: 1, -0.171332448720932: 1, -0.8429122567176819: 1, 1.257509708404541: 1, -0.9743238091468811: 1, 0.68455970287323: 1, -1.1322532892227173: 1, 1.0084635019302368: 1, -0.0674244612455368: 1, -1.006115436553955: 1, -0.3076569437980652: 1, -1.1914066076278687: 1, 1.1363762617111206: 1, -1.2016372680664062: 1, -0.6111128330230713: 1, 0.9798484444618225: 1, 0.6101611256599426: 1, -1.0467379093170166: 1, -0.8758900761604309: 1, -0.75212162733078: 1, -0.9272821545600891: 1, -1.02364981174469: 1, -0.7736660242080688: 1, -0.8366922736167908: 1, -0.4774998426437378: 1, -1.1344302892684937: 1, -0.5115338563919067: 1, -0.6325321793556213: 1, -1.0309724807739258: 1, -1.2006478309631348: 1, -0.8834558129310608: 1, -0.6635865569114685: 1, -0.5772318840026855: 1, -1.0135008096694946: 1, -0.63347989320755: 1, -1.1520825624465942: 1, -1.167817234992981: 1, -0.6395750641822815: 1, -1.1800310611724854: 1, -1.194927453994751: 1, -0.6818874478340149: 1, -0.5497206449508667: 1, -0.5638068318367004: 1, -1.0074002742767334: 1, -0.7939543128013611: 1, 4.58308219909668: 1, -1.1480247974395752: 1, 3.830434560775757: 1, -1.1136521100997925: 1, 5.006033897399902: 1, 5.012721538543701: 1, -1.1754673719406128: 1, 1.7253838777542114: 1, -0.4444347023963928: 1, -0.25539615750312805: 1, -1.1933567523956299: 1, -0.86516273021698: 1, -1.087907314300537: 1, -0.3156169652938843: 1, -1.1703253984451294: 1, -0.9726890325546265: 1, -1.1181161403656006: 1, -1.0246316194534302: 1, -0.1397843062877655: 1, -0.8599510192871094: 1, -1.162119746208191: 1, -1.1954984664916992: 1, 1.094826102256775: 1, -1.1819733381271362: 1, 0.19575154781341553: 1, -1.2014538049697876: 1, -1.194710373878479: 1, 0.6086026430130005: 1, -1.1539435386657715: 1, -1.1912822723388672: 1, -0.4641222357749939: 1, -0.7029581665992737: 1, -0.6273359060287476: 1, -1.1986582279205322: 1, -1.1612766981124878: 1, -1.0831377506256104: 1, -1.1862393617630005: 1, -1.1948115825653076: 1, -0.6645460724830627: 1, 0.05473056063055992: 1, -1.201563835144043: 1, -0.4477367699146271: 1, -0.31994664669036865: 1, -0.9458178281784058: 1, 0.8136993050575256: 1, -0.726171612739563: 1, 1.8886953592300415: 1, -1.1054575443267822: 1, -0.459964781999588: 1, 5.645717620849609: 1, -1.161582589149475: 1, -0.7897263169288635: 1, -1.2008585929870605: 1, -0.9933805465698242: 1, -1.1859160661697388: 1, -1.1816785335540771: 1, -1.198327660560608: 1, -1.091170310974121: 1, -1.1855055093765259: 1, -1.2012261152267456: 1, -1.170555830001831: 1, -0.957834780216217: 1, -1.0740134716033936: 1, -1.0014375448226929: 1, -1.1595861911773682: 1, -1.1821529865264893: 1, -1.1624016761779785: 1, -1.18364417552948: 1, -1.0951330661773682: 1, -1.1676386594772339: 1, -1.196357011795044: 1, -0.3917173445224762: 1, 0.6609118580818176: 1, 1.0762102603912354: 1, 0.29691436886787415: 1, -1.2015372514724731: 1, -1.2016377449035645: 1, -1.1100589036941528: 1, -0.9999420642852783: 1, -1.2016303539276123: 1, -1.2015819549560547: 1, 0.34270647168159485: 1, 0.9252344369888306: 1, 1.0225938558578491: 1, 0.8458276391029358: 1, 1.0407052040100098: 1, -1.2016316652297974: 1, 0.8794386982917786: 1, 1.244690179824829: 1, 1.5059622526168823: 1, 0.3591007590293884: 1, 1.3268651962280273: 1, 0.8655160665512085: 1, 1.3780083656311035: 1, 1.1870133876800537: 1, 1.5651975870132446: 1, -0.11230547726154327: 1, -1.2015061378479004: 1, -1.1779894828796387: 1, -0.6155209541320801: 1, -1.201634407043457: 1, -1.0591267347335815: 1, -1.2015695571899414: 1, -1.2016041278839111: 1, -1.2016350030899048: 1, -1.201629877090454: 1, -0.3404213488101959: 1, -0.11716806888580322: 1, 0.36895880103111267: 1, -1.2016255855560303: 1, -1.2016326189041138: 1, -0.7989376187324524: 1, 0.5647678971290588: 1, -1.0203382968902588: 1, -1.2015478610992432: 1, -0.6676098704338074: 1, -1.1257261037826538: 1, -1.2016159296035767: 1, -0.006573406048119068: 1, -0.5612114071846008: 1, -1.201079249382019: 1, -1.2003253698349: 1, -1.2016363143920898: 1, 1.466456651687622: 1, 0.9224774241447449: 1, 1.533963680267334: 1, 0.881543755531311: 1, 1.1571227312088013: 1, 1.540098786354065: 1, 1.0418422222137451: 1, 0.6104775071144104: 1, 1.069445013999939: 1, -0.30856987833976746: 1, 0.1911333203315735: 1, 1.5425565242767334: 1, -0.41700541973114014: 1, 1.5978120565414429: 1, 1.5401815176010132: 1, 1.3359390497207642: 1, -1.1742432117462158: 1, 1.4056357145309448: 1, -0.08143828064203262: 1, 0.31707215309143066: 1, 0.7008569240570068: 1, 0.8512904644012451: 1, -0.11862857639789581: 1, -0.4737872779369354: 1, -0.2417261004447937: 1, 0.6668532490730286: 1, 0.3602719008922577: 1, 1.3900312185287476: 1, 1.5357881784439087: 1, 0.07517638802528381: 1, 1.5112932920455933: 1, -1.1865193843841553: 1, 0.35881760716438293: 1, 1.3110779523849487: 1, -0.1031346470117569: 1, -0.22873257100582123: 1, 1.431997537612915: 1, -0.11514480412006378: 1, 1.6200270652770996: 1, 0.20913533866405487: 1, -0.20442236959934235: 1, 0.7984791398048401: 1, 1.5503476858139038: 1, -0.44012218713760376: 1, -0.31073465943336487: 1, 0.21012525260448456: 1, 0.6647680401802063: 1, -0.10372047126293182: 1, 1.1246896982192993: 1, 0.276736855506897: 1, -0.10448039323091507: 1, 0.2079305797815323: 1, 1.018097996711731: 1, 1.4819786548614502: 1, -0.1827705055475235: 1, -0.6921688914299011: 1, -0.16533638536930084: 1, 1.2513697147369385: 1, -0.8494775891304016: 1, -0.12005668878555298: 1, -0.3641962707042694: 1, -0.3953478932380676: 1, 0.7884150743484497: 1, -0.4121406674385071: 1, -0.35524412989616394: 1, 0.042822714895009995: 1, 1.2736730575561523: 1, 1.1279875040054321: 1, 0.7561059594154358: 1, -1.1981785297393799: 1, 0.02958042360842228: 1, -0.29299479722976685: 1, -0.6826592683792114: 1, -0.37873539328575134: 1, 0.9235488772392273: 1, -0.22304323315620422: 1, -0.12660875916481018: 1, 0.32448264956474304: 1, -0.17582924664020538: 1, -0.025104349479079247: 1, 0.029728559777140617: 1, -0.3348834216594696: 1, -0.12709279358386993: 1, -0.21517714858055115: 1, 0.9726781249046326: 1, -0.0404001921415329: 1, -1.201636552810669: 1, 1.1997345685958862: 1, -0.26567548513412476: 1, -0.09314227104187012: 1, 1.2882169485092163: 1, 0.7786849141120911: 1, -0.257107138633728: 1, 0.09289468824863434: 1, 0.05186900869011879: 1, -0.809281051158905: 1, 1.1265980005264282: 1, 0.017293494194746017: 1, 1.2238743305206299: 1, -1.194933295249939: 1, 1.1819933652877808: 1, -1.194725751876831: 1, 0.7264453172683716: 1, -0.16393840312957764: 1, -0.040736664086580276: 1, 0.8548043966293335: 1, 0.5362502932548523: 1, 0.5997416377067566: 1, -0.0332360677421093: 1, 0.01664043217897415: 1, -0.6421114802360535: 1, -0.3469093143939972: 1, -1.1634924411773682: 1, -0.3879204988479614: 1, -0.4360010027885437: 1, -0.0580214224755764: 1, 0.01943967677652836: 1, 0.840314507484436: 1, -0.8751402497291565: 1, -1.193730115890503: 1, 0.3886134922504425: 1, -0.463926762342453: 1, -0.11945565789937973: 1, 0.9712859988212585: 1, 1.176633596420288: 1, -0.04390183836221695: 1, 0.8222996592521667: 1, 0.5694330334663391: 1, -1.1755220890045166: 1, 1.2749443054199219: 1, -0.09905305504798889: 1, -0.1517976075410843: 1, -0.24846623837947845: 1, 1.0863690376281738: 1, 0.7288377285003662: 1, 1.3004077672958374: 1, -0.4453357756137848: 1, -0.24570585787296295: 1, -0.28629931807518005: 1, 1.256977915763855: 1, 1.1922990083694458: 1, 0.8834699392318726: 1, -0.09275590628385544: 1, -0.06864805519580841: 1, -1.2016304731369019: 1, 0.6013088822364807: 1, -0.1916339248418808: 1, -1.1879688501358032: 1, 1.150696039199829: 1, 1.0972230434417725: 1, -0.25068432092666626: 1, 0.8119110465049744: 1, -0.2664187550544739: 1, -1.2003904581069946: 1}
2024-07-15 17:24:26.388539: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory
2024-07-15 17:24:26.388606: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)
2024-07-15 17:24:26.388661: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (UFZ544049): /proc/driver/nvidia/version does not exist
2024-07-15 17:24:26.389527: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
INFO     Network type: REG
INFO     Model: "sequential"
INFO     _________________________________________________________________
INFO     Layer (type)                 Output Shape              Param #
INFO     =================================================================
INFO     dense (Dense)                (None, 1024)              2098176
INFO     _________________________________________________________________
INFO     dropout (Dropout)            (None, 1024)              0
INFO     _________________________________________________________________
INFO     dense_1 (Dense)              (None, 512)               524800
INFO     _________________________________________________________________
INFO     dropout_1 (Dropout)          (None, 512)               0
INFO     _________________________________________________________________
INFO     dense_2 (Dense)              (None, 256)               131328
INFO     _________________________________________________________________
INFO     dropout_2 (Dropout)          (None, 256)               0
INFO     _________________________________________________________________
INFO     dense_3 (Dense)              (None, 128)               32896
INFO     _________________________________________________________________
INFO     dropout_3 (Dropout)          (None, 128)               0
INFO     _________________________________________________________________
INFO     dense_4 (Dense)              (None, 1)                 129
INFO     =================================================================
INFO     Total params: 2,787,329
INFO     Trainable params: 2,787,329
INFO     Non-trainable params: 0
INFO     _________________________________________________________________
WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.
WARNING  `period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.
2024-07-15 17:24:27.246850: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0
2024-07-15 17:24:27.247013: I tensorflow/core/grappler/clusters/single_machine.cc:357] Starting new session
2024-07-15 17:24:27.249614: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:1137] Optimization results for grappler item: graph_to_optimize
  function_optimizer: function_optimizer did nothing. time = 0.005ms.
  function_optimizer: function_optimizer did nothing. time = 0.001ms.
WARNING:tensorflow:From /home/shanavas/miniforge3/envs/dfpl_env/lib/python3.9/site-packages/tensorflow/python/ops/math_ops.py:3777: tensor_shape_from_node_def_name (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.graph_util.tensor_shape_from_node_def_name`
WARNING  From /home/shanavas/miniforge3/envs/dfpl_env/lib/python3.9/site-packages/tensorflow/python/ops/math_ops.py:3777: tensor_shape_from_node_def_name (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.graph_util.tensor_shape_from_node_def_name`
2024-07-15 17:24:27.411597: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)
Epoch 1/5000
26/26 - 1s - loss: 4.7609 - val_loss: 4.7101
Epoch 2/5000
26/26 - 1s - loss: 4.6811 - val_loss: 4.6516
Epoch 3/5000
26/26 - 1s - loss: 4.6322 - val_loss: 4.6147
Epoch 4/5000
26/26 - 1s - loss: 4.5983 - val_loss: 4.5895
Epoch 5/5000
26/26 - 1s - loss: 4.5746 - val_loss: 4.5691
Epoch 6/5000
26/26 - 1s - loss: 4.5505 - val_loss: 4.5503
Epoch 7/5000
26/26 - 1s - loss: 4.5263 - val_loss: 4.5327
Epoch 8/5000
26/26 - 1s - loss: 4.5036 - val_loss: 4.5150
Epoch 9/5000
26/26 - 1s - loss: 4.4811 - val_loss: 4.4974
Epoch 10/5000
26/26 - 1s - loss: 4.4588 - val_loss: 4.4814
Epoch 00010: val_loss improved from inf to 4.48143, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 11/5000
26/26 - 1s - loss: 4.4366 - val_loss: 4.4670
Epoch 12/5000
26/26 - 1s - loss: 4.4173 - val_loss: 4.4532
Epoch 13/5000
26/26 - 1s - loss: 4.3968 - val_loss: 4.4391
Epoch 14/5000
26/26 - 1s - loss: 4.3791 - val_loss: 4.4257
Epoch 15/5000
26/26 - 1s - loss: 4.3610 - val_loss: 4.4136
Epoch 16/5000
26/26 - 1s - loss: 4.3436 - val_loss: 4.4000
Epoch 17/5000
26/26 - 1s - loss: 4.3276 - val_loss: 4.3877
Epoch 18/5000
26/26 - 1s - loss: 4.3118 - val_loss: 4.3755
Epoch 19/5000
26/26 - 1s - loss: 4.2940 - val_loss: 4.3631
Epoch 20/5000
26/26 - 1s - loss: 4.2777 - val_loss: 4.3518
Epoch 00020: val_loss improved from 4.48143 to 4.35177, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 21/5000
26/26 - 1s - loss: 4.2646 - val_loss: 4.3397
Epoch 22/5000
26/26 - 1s - loss: 4.2466 - val_loss: 4.3282
Epoch 23/5000
26/26 - 1s - loss: 4.2298 - val_loss: 4.3172
Epoch 24/5000
26/26 - 1s - loss: 4.2145 - val_loss: 4.3055
Epoch 25/5000
26/26 - 1s - loss: 4.1979 - val_loss: 4.2947
Epoch 26/5000
26/26 - 1s - loss: 4.1852 - val_loss: 4.2840
Epoch 27/5000
26/26 - 1s - loss: 4.1676 - val_loss: 4.2732
Epoch 28/5000
26/26 - 1s - loss: 4.1541 - val_loss: 4.2636
Epoch 29/5000
26/26 - 1s - loss: 4.1379 - val_loss: 4.2525
Epoch 30/5000
26/26 - 1s - loss: 4.1246 - val_loss: 4.2436
Epoch 00030: val_loss improved from 4.35177 to 4.24360, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 31/5000
26/26 - 1s - loss: 4.1085 - val_loss: 4.2331
Epoch 32/5000
26/26 - 1s - loss: 4.0940 - val_loss: 4.2238
Epoch 33/5000
26/26 - 1s - loss: 4.0805 - val_loss: 4.2138
Epoch 34/5000
26/26 - 1s - loss: 4.0666 - val_loss: 4.2040
Epoch 35/5000
26/26 - 1s - loss: 4.0518 - val_loss: 4.1954
Epoch 36/5000
26/26 - 1s - loss: 4.0376 - val_loss: 4.1864
Epoch 37/5000
26/26 - 1s - loss: 4.0234 - val_loss: 4.1777
Epoch 38/5000
26/26 - 1s - loss: 4.0119 - val_loss: 4.1692
Epoch 39/5000
26/26 - 1s - loss: 3.9975 - val_loss: 4.1613
Epoch 40/5000
26/26 - 1s - loss: 3.9829 - val_loss: 4.1513
Epoch 00040: val_loss improved from 4.24360 to 4.15130, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 41/5000
26/26 - 1s - loss: 3.9690 - val_loss: 4.1439
Epoch 42/5000
26/26 - 1s - loss: 3.9602 - val_loss: 4.1367
Epoch 43/5000
26/26 - 1s - loss: 3.9451 - val_loss: 4.1287
Epoch 44/5000
26/26 - 1s - loss: 3.9324 - val_loss: 4.1196
Epoch 45/5000
26/26 - 1s - loss: 3.9204 - val_loss: 4.1132
Epoch 46/5000
26/26 - 1s - loss: 3.9091 - val_loss: 4.1054
Epoch 47/5000
26/26 - 1s - loss: 3.8976 - val_loss: 4.0998
Epoch 48/5000
26/26 - 1s - loss: 3.8836 - val_loss: 4.0926
Epoch 49/5000
26/26 - 1s - loss: 3.8749 - val_loss: 4.0847
Epoch 50/5000
26/26 - 1s - loss: 3.8605 - val_loss: 4.0766
Epoch 00050: val_loss improved from 4.15130 to 4.07662, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 51/5000
26/26 - 1s - loss: 3.8504 - val_loss: 4.0690
Epoch 52/5000
26/26 - 1s - loss: 3.8389 - val_loss: 4.0630
Epoch 53/5000
26/26 - 1s - loss: 3.8274 - val_loss: 4.0568
Epoch 54/5000
26/26 - 1s - loss: 3.8177 - val_loss: 4.0472
Epoch 55/5000
26/26 - 1s - loss: 3.8056 - val_loss: 4.0411
Epoch 56/5000
26/26 - 1s - loss: 3.7965 - val_loss: 4.0331
Epoch 57/5000
26/26 - 1s - loss: 3.7882 - val_loss: 4.0272
Epoch 58/5000
26/26 - 1s - loss: 3.7767 - val_loss: 4.0207
Epoch 59/5000
26/26 - 1s - loss: 3.7633 - val_loss: 4.0134
Epoch 60/5000
26/26 - 1s - loss: 3.7560 - val_loss: 4.0068
Epoch 00060: val_loss improved from 4.07662 to 4.00680, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 61/5000
26/26 - 1s - loss: 3.7435 - val_loss: 3.9999
Epoch 62/5000
26/26 - 1s - loss: 3.7358 - val_loss: 3.9941
Epoch 63/5000
26/26 - 1s - loss: 3.7248 - val_loss: 3.9871
Epoch 64/5000
26/26 - 1s - loss: 3.7141 - val_loss: 3.9806
Epoch 65/5000
26/26 - 1s - loss: 3.7075 - val_loss: 3.9740
Epoch 66/5000
26/26 - 1s - loss: 3.6963 - val_loss: 3.9660
Epoch 67/5000
26/26 - 1s - loss: 3.6887 - val_loss: 3.9633
Epoch 68/5000
26/26 - 1s - loss: 3.6769 - val_loss: 3.9540
Epoch 69/5000
26/26 - 1s - loss: 3.6713 - val_loss: 3.9467
Epoch 70/5000
26/26 - 1s - loss: 3.6591 - val_loss: 3.9423
Epoch 00070: val_loss improved from 4.00680 to 3.94234, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 71/5000
26/26 - 1s - loss: 3.6515 - val_loss: 3.9367
Epoch 72/5000
26/26 - 1s - loss: 3.6421 - val_loss: 3.9319
Epoch 73/5000
26/26 - 1s - loss: 3.6327 - val_loss: 3.9254
Epoch 74/5000
26/26 - 1s - loss: 3.6239 - val_loss: 3.9205
Epoch 75/5000
26/26 - 1s - loss: 3.6188 - val_loss: 3.9153
Epoch 76/5000
26/26 - 1s - loss: 3.6071 - val_loss: 3.9080
Epoch 77/5000
26/26 - 1s - loss: 3.5986 - val_loss: 3.9028
Epoch 78/5000
26/26 - 1s - loss: 3.5902 - val_loss: 3.8953
Epoch 79/5000
26/26 - 1s - loss: 3.5840 - val_loss: 3.8887
Epoch 80/5000
26/26 - 1s - loss: 3.5764 - val_loss: 3.8853
Epoch 00080: val_loss improved from 3.94234 to 3.88526, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 81/5000
26/26 - 1s - loss: 3.5688 - val_loss: 3.8809
Epoch 82/5000
26/26 - 1s - loss: 3.5582 - val_loss: 3.8725
Epoch 83/5000
26/26 - 1s - loss: 3.5528 - val_loss: 3.8666
Epoch 84/5000
26/26 - 1s - loss: 3.5413 - val_loss: 3.8612
Epoch 85/5000
26/26 - 1s - loss: 3.5362 - val_loss: 3.8574
Epoch 86/5000
26/26 - 1s - loss: 3.5296 - val_loss: 3.8509
Epoch 87/5000
26/26 - 1s - loss: 3.5209 - val_loss: 3.8437
Epoch 88/5000
26/26 - 1s - loss: 3.5119 - val_loss: 3.8394
Epoch 89/5000
26/26 - 1s - loss: 3.5056 - val_loss: 3.8336
Epoch 90/5000
26/26 - 1s - loss: 3.4973 - val_loss: 3.8293
Epoch 00090: val_loss improved from 3.88526 to 3.82932, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 91/5000
26/26 - 1s - loss: 3.4898 - val_loss: 3.8244
Epoch 92/5000
26/26 - 1s - loss: 3.4830 - val_loss: 3.8199
Epoch 93/5000
26/26 - 1s - loss: 3.4766 - val_loss: 3.8174
Epoch 94/5000
26/26 - 1s - loss: 3.4705 - val_loss: 3.8087
Epoch 95/5000
26/26 - 1s - loss: 3.4612 - val_loss: 3.8059
Epoch 96/5000
26/26 - 1s - loss: 3.4558 - val_loss: 3.7972
Epoch 97/5000
26/26 - 1s - loss: 3.4508 - val_loss: 3.7924
Epoch 98/5000
26/26 - 1s - loss: 3.4409 - val_loss: 3.7873
Epoch 99/5000
26/26 - 1s - loss: 3.4380 - val_loss: 3.7800
Epoch 100/5000
26/26 - 1s - loss: 3.4291 - val_loss: 3.7761
Epoch 00100: val_loss improved from 3.82932 to 3.77614, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 101/5000
26/26 - 1s - loss: 3.4222 - val_loss: 3.7718
Epoch 102/5000
26/26 - 1s - loss: 3.4159 - val_loss: 3.7660
Epoch 103/5000
26/26 - 1s - loss: 3.4088 - val_loss: 3.7606
Epoch 104/5000
26/26 - 1s - loss: 3.4020 - val_loss: 3.7547
Epoch 105/5000
26/26 - 1s - loss: 3.3959 - val_loss: 3.7518
Epoch 106/5000
26/26 - 1s - loss: 3.3918 - val_loss: 3.7462
Epoch 107/5000
26/26 - 1s - loss: 3.3825 - val_loss: 3.7422
Epoch 108/5000
26/26 - 1s - loss: 3.3787 - val_loss: 3.7365
Epoch 109/5000
26/26 - 1s - loss: 3.3701 - val_loss: 3.7327
Epoch 110/5000
26/26 - 1s - loss: 3.3670 - val_loss: 3.7314
Epoch 00110: val_loss improved from 3.77614 to 3.73144, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 111/5000
26/26 - 1s - loss: 3.3591 - val_loss: 3.7248
Epoch 112/5000
26/26 - 1s - loss: 3.3523 - val_loss: 3.7166
Epoch 113/5000
26/26 - 1s - loss: 3.3448 - val_loss: 3.7146
Epoch 114/5000
26/26 - 1s - loss: 3.3399 - val_loss: 3.7084
Epoch 115/5000
26/26 - 1s - loss: 3.3334 - val_loss: 3.7051
Epoch 116/5000
26/26 - 1s - loss: 3.3272 - val_loss: 3.6992
Epoch 117/5000
26/26 - 1s - loss: 3.3231 - val_loss: 3.6952
Epoch 118/5000
26/26 - 1s - loss: 3.3171 - val_loss: 3.6887
Epoch 119/5000
26/26 - 1s - loss: 3.3118 - val_loss: 3.6847
Epoch 120/5000
26/26 - 1s - loss: 3.3069 - val_loss: 3.6793
Epoch 00120: val_loss improved from 3.73144 to 3.67925, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 121/5000
26/26 - 1s - loss: 3.2964 - val_loss: 3.6732
Epoch 122/5000
26/26 - 1s - loss: 3.2944 - val_loss: 3.6686
Epoch 123/5000
26/26 - 1s - loss: 3.2882 - val_loss: 3.6627
Epoch 124/5000
26/26 - 1s - loss: 3.2817 - val_loss: 3.6610
Epoch 125/5000
26/26 - 1s - loss: 3.2769 - val_loss: 3.6554
Epoch 126/5000
26/26 - 1s - loss: 3.2750 - val_loss: 3.6502
Epoch 127/5000
26/26 - 1s - loss: 3.2658 - val_loss: 3.6447
Epoch 128/5000
26/26 - 1s - loss: 3.2607 - val_loss: 3.6415
Epoch 129/5000
26/26 - 1s - loss: 3.2558 - val_loss: 3.6382
Epoch 130/5000
26/26 - 1s - loss: 3.2509 - val_loss: 3.6343
Epoch 00130: val_loss improved from 3.67925 to 3.63426, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 131/5000
26/26 - 1s - loss: 3.2419 - val_loss: 3.6279
Epoch 132/5000
26/26 - 1s - loss: 3.2385 - val_loss: 3.6234
Epoch 133/5000
26/26 - 1s - loss: 3.2350 - val_loss: 3.6171
Epoch 134/5000
26/26 - 1s - loss: 3.2308 - val_loss: 3.6101
Epoch 135/5000
26/26 - 1s - loss: 3.2248 - val_loss: 3.6088
Epoch 136/5000
26/26 - 1s - loss: 3.2170 - val_loss: 3.6054
Epoch 137/5000
26/26 - 1s - loss: 3.2115 - val_loss: 3.6015
Epoch 138/5000
26/26 - 1s - loss: 3.2082 - val_loss: 3.5979
Epoch 139/5000
26/26 - 1s - loss: 3.2027 - val_loss: 3.5925
Epoch 140/5000
26/26 - 1s - loss: 3.1984 - val_loss: 3.5916
Epoch 00140: val_loss improved from 3.63426 to 3.59158, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 141/5000
26/26 - 1s - loss: 3.1913 - val_loss: 3.5912
Epoch 142/5000
26/26 - 1s - loss: 3.1877 - val_loss: 3.5828
Epoch 143/5000
26/26 - 1s - loss: 3.1831 - val_loss: 3.5785
Epoch 144/5000
26/26 - 1s - loss: 3.1767 - val_loss: 3.5720
Epoch 145/5000
26/26 - 1s - loss: 3.1697 - val_loss: 3.5693
Epoch 146/5000
26/26 - 1s - loss: 3.1680 - val_loss: 3.5643
Epoch 147/5000
26/26 - 1s - loss: 3.1613 - val_loss: 3.5594
Epoch 148/5000
26/26 - 1s - loss: 3.1591 - val_loss: 3.5566
Epoch 149/5000
26/26 - 1s - loss: 3.1523 - val_loss: 3.5527
Epoch 150/5000
26/26 - 1s - loss: 3.1485 - val_loss: 3.5472
Epoch 00150: val_loss improved from 3.59158 to 3.54720, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 151/5000
26/26 - 1s - loss: 3.1430 - val_loss: 3.5432
Epoch 152/5000
26/26 - 1s - loss: 3.1386 - val_loss: 3.5388
Epoch 153/5000
26/26 - 1s - loss: 3.1343 - val_loss: 3.5358
Epoch 154/5000
26/26 - 1s - loss: 3.1305 - val_loss: 3.5309
Epoch 155/5000
26/26 - 1s - loss: 3.1234 - val_loss: 3.5278
Epoch 156/5000
26/26 - 1s - loss: 3.1214 - val_loss: 3.5231
Epoch 157/5000
26/26 - 1s - loss: 3.1160 - val_loss: 3.5181
Epoch 158/5000
26/26 - 1s - loss: 3.1125 - val_loss: 3.5150
Epoch 159/5000
26/26 - 1s - loss: 3.1064 - val_loss: 3.5123
Epoch 160/5000
26/26 - 1s - loss: 3.1042 - val_loss: 3.5120
Epoch 00160: val_loss improved from 3.54720 to 3.51200, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 161/5000
26/26 - 1s - loss: 3.0987 - val_loss: 3.5058
Epoch 162/5000
26/26 - 1s - loss: 3.0942 - val_loss: 3.5000
Epoch 163/5000
26/26 - 1s - loss: 3.0846 - val_loss: 3.4937
Epoch 164/5000
26/26 - 1s - loss: 3.0805 - val_loss: 3.4905
Epoch 165/5000
26/26 - 1s - loss: 3.0773 - val_loss: 3.4882
Epoch 166/5000
26/26 - 1s - loss: 3.0750 - val_loss: 3.4862
Epoch 167/5000
26/26 - 1s - loss: 3.0729 - val_loss: 3.4809
Epoch 168/5000
26/26 - 1s - loss: 3.0689 - val_loss: 3.4758
Epoch 169/5000
26/26 - 1s - loss: 3.0605 - val_loss: 3.4714
Epoch 170/5000
26/26 - 1s - loss: 3.0556 - val_loss: 3.4670
Epoch 00170: val_loss improved from 3.51200 to 3.46700, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 171/5000
26/26 - 1s - loss: 3.0553 - val_loss: 3.4650
Epoch 172/5000
26/26 - 1s - loss: 3.0470 - val_loss: 3.4616
Epoch 173/5000
26/26 - 1s - loss: 3.0462 - val_loss: 3.4555
Epoch 174/5000
26/26 - 1s - loss: 3.0400 - val_loss: 3.4519
Epoch 175/5000
26/26 - 1s - loss: 3.0369 - val_loss: 3.4500
Epoch 176/5000
26/26 - 1s - loss: 3.0339 - val_loss: 3.4437
Epoch 177/5000
26/26 - 1s - loss: 3.0303 - val_loss: 3.4445
Epoch 178/5000
26/26 - 1s - loss: 3.0244 - val_loss: 3.4396
Epoch 179/5000
26/26 - 1s - loss: 3.0179 - val_loss: 3.4343
Epoch 180/5000
26/26 - 1s - loss: 3.0169 - val_loss: 3.4309
Epoch 00180: val_loss improved from 3.46700 to 3.43085, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 181/5000
26/26 - 1s - loss: 3.0111 - val_loss: 3.4263
Epoch 182/5000
26/26 - 1s - loss: 3.0071 - val_loss: 3.4263
Epoch 183/5000
26/26 - 1s - loss: 3.0019 - val_loss: 3.4199
Epoch 184/5000
26/26 - 1s - loss: 3.0004 - val_loss: 3.4144
Epoch 185/5000
26/26 - 1s - loss: 2.9951 - val_loss: 3.4108
Epoch 186/5000
26/26 - 1s - loss: 2.9918 - val_loss: 3.4078
Epoch 187/5000
26/26 - 1s - loss: 2.9877 - val_loss: 3.4077
Epoch 188/5000
26/26 - 1s - loss: 2.9852 - val_loss: 3.4037
Epoch 189/5000
26/26 - 1s - loss: 2.9779 - val_loss: 3.4010
Epoch 190/5000
26/26 - 1s - loss: 2.9737 - val_loss: 3.3959
Epoch 00190: val_loss improved from 3.43085 to 3.39591, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 191/5000
26/26 - 1s - loss: 2.9707 - val_loss: 3.3896
Epoch 192/5000
26/26 - 1s - loss: 2.9684 - val_loss: 3.3872
Epoch 193/5000
26/26 - 1s - loss: 2.9631 - val_loss: 3.3841
Epoch 194/5000
26/26 - 1s - loss: 2.9630 - val_loss: 3.3794
Epoch 195/5000
26/26 - 1s - loss: 2.9564 - val_loss: 3.3766
Epoch 196/5000
26/26 - 1s - loss: 2.9521 - val_loss: 3.3756
Epoch 197/5000
26/26 - 1s - loss: 2.9463 - val_loss: 3.3718
Epoch 198/5000
26/26 - 1s - loss: 2.9413 - val_loss: 3.3657
Epoch 199/5000
26/26 - 1s - loss: 2.9407 - val_loss: 3.3658
Epoch 200/5000
26/26 - 1s - loss: 2.9376 - val_loss: 3.3619
Epoch 00200: val_loss improved from 3.39591 to 3.36194, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 201/5000
26/26 - 1s - loss: 2.9322 - val_loss: 3.3573
Epoch 202/5000
26/26 - 1s - loss: 2.9266 - val_loss: 3.3516
Epoch 203/5000
26/26 - 1s - loss: 2.9243 - val_loss: 3.3481
Epoch 204/5000
26/26 - 1s - loss: 2.9209 - val_loss: 3.3467
Epoch 205/5000
26/26 - 1s - loss: 2.9172 - val_loss: 3.3430
Epoch 206/5000
26/26 - 1s - loss: 2.9130 - val_loss: 3.3397
Epoch 207/5000
26/26 - 1s - loss: 2.9067 - val_loss: 3.3367
Epoch 208/5000
26/26 - 1s - loss: 2.9061 - val_loss: 3.3333
Epoch 209/5000
26/26 - 1s - loss: 2.9037 - val_loss: 3.3263
Epoch 210/5000
26/26 - 1s - loss: 2.8993 - val_loss: 3.3228
Epoch 00210: val_loss improved from 3.36194 to 3.32281, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 211/5000
26/26 - 1s - loss: 2.8966 - val_loss: 3.3210
Epoch 212/5000
26/26 - 1s - loss: 2.8886 - val_loss: 3.3182
Epoch 213/5000
26/26 - 1s - loss: 2.8848 - val_loss: 3.3136
Epoch 214/5000
26/26 - 1s - loss: 2.8827 - val_loss: 3.3122
Epoch 215/5000
26/26 - 1s - loss: 2.8816 - val_loss: 3.3086
Epoch 216/5000
26/26 - 1s - loss: 2.8774 - val_loss: 3.3061
Epoch 217/5000
26/26 - 1s - loss: 2.8759 - val_loss: 3.3027
Epoch 218/5000
26/26 - 1s - loss: 2.8697 - val_loss: 3.2994
Epoch 219/5000
26/26 - 1s - loss: 2.8667 - val_loss: 3.2936
Epoch 220/5000
26/26 - 1s - loss: 2.8606 - val_loss: 3.2914
Epoch 00220: val_loss improved from 3.32281 to 3.29136, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 221/5000
26/26 - 1s - loss: 2.8581 - val_loss: 3.2875
Epoch 222/5000
26/26 - 1s - loss: 2.8535 - val_loss: 3.2850
Epoch 223/5000
26/26 - 1s - loss: 2.8513 - val_loss: 3.2811
Epoch 224/5000
26/26 - 1s - loss: 2.8480 - val_loss: 3.2769
Epoch 225/5000
26/26 - 1s - loss: 2.8463 - val_loss: 3.2755
Epoch 226/5000
26/26 - 1s - loss: 2.8419 - val_loss: 3.2735
Epoch 227/5000
26/26 - 1s - loss: 2.8353 - val_loss: 3.2692
Epoch 228/5000
26/26 - 1s - loss: 2.8347 - val_loss: 3.2665
Epoch 229/5000
26/26 - 1s - loss: 2.8310 - val_loss: 3.2642
Epoch 230/5000
26/26 - 1s - loss: 2.8272 - val_loss: 3.2622
Epoch 00230: val_loss improved from 3.29136 to 3.26216, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 231/5000
26/26 - 1s - loss: 2.8239 - val_loss: 3.2561
Epoch 232/5000
26/26 - 1s - loss: 2.8230 - val_loss: 3.2549
Epoch 233/5000
26/26 - 1s - loss: 2.8184 - val_loss: 3.2480
Epoch 234/5000
26/26 - 1s - loss: 2.8154 - val_loss: 3.2457
Epoch 235/5000
26/26 - 1s - loss: 2.8091 - val_loss: 3.2430
Epoch 236/5000
26/26 - 1s - loss: 2.8069 - val_loss: 3.2397
Epoch 237/5000
26/26 - 1s - loss: 2.8021 - val_loss: 3.2380
Epoch 238/5000
26/26 - 1s - loss: 2.7988 - val_loss: 3.2343
Epoch 239/5000
26/26 - 1s - loss: 2.7970 - val_loss: 3.2287
Epoch 240/5000
26/26 - 1s - loss: 2.7951 - val_loss: 3.2266
Epoch 00240: val_loss improved from 3.26216 to 3.22662, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 241/5000
26/26 - 1s - loss: 2.7908 - val_loss: 3.2258
Epoch 242/5000
26/26 - 1s - loss: 2.7851 - val_loss: 3.2250
Epoch 243/5000
26/26 - 1s - loss: 2.7842 - val_loss: 3.2211
Epoch 244/5000
26/26 - 1s - loss: 2.7804 - val_loss: 3.2161
Epoch 245/5000
26/26 - 1s - loss: 2.7761 - val_loss: 3.2135
Epoch 246/5000
26/26 - 1s - loss: 2.7726 - val_loss: 3.2094
Epoch 247/5000
26/26 - 1s - loss: 2.7682 - val_loss: 3.2057
Epoch 248/5000
26/26 - 1s - loss: 2.7655 - val_loss: 3.2029
Epoch 249/5000
26/26 - 1s - loss: 2.7625 - val_loss: 3.2002
Epoch 250/5000
26/26 - 1s - loss: 2.7591 - val_loss: 3.1986
Epoch 00250: val_loss improved from 3.22662 to 3.19859, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 251/5000
26/26 - 1s - loss: 2.7571 - val_loss: 3.1949
Epoch 252/5000
26/26 - 1s - loss: 2.7535 - val_loss: 3.1912
Epoch 253/5000
26/26 - 1s - loss: 2.7499 - val_loss: 3.1905
Epoch 254/5000
26/26 - 1s - loss: 2.7479 - val_loss: 3.1859
Epoch 255/5000
26/26 - 1s - loss: 2.7437 - val_loss: 3.1815
Epoch 256/5000
26/26 - 1s - loss: 2.7398 - val_loss: 3.1783
Epoch 257/5000
26/26 - 1s - loss: 2.7368 - val_loss: 3.1745
Epoch 258/5000
26/26 - 1s - loss: 2.7326 - val_loss: 3.1739
Epoch 259/5000
26/26 - 1s - loss: 2.7299 - val_loss: 3.1704
Epoch 260/5000
26/26 - 1s - loss: 2.7261 - val_loss: 3.1667
Epoch 00260: val_loss improved from 3.19859 to 3.16666, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 261/5000
26/26 - 1s - loss: 2.7225 - val_loss: 3.1616
Epoch 262/5000
26/26 - 1s - loss: 2.7208 - val_loss: 3.1593
Epoch 263/5000
26/26 - 1s - loss: 2.7200 - val_loss: 3.1591
Epoch 264/5000
26/26 - 1s - loss: 2.7133 - val_loss: 3.1563
Epoch 265/5000
26/26 - 1s - loss: 2.7106 - val_loss: 3.1514
Epoch 266/5000
26/26 - 1s - loss: 2.7090 - val_loss: 3.1508
Epoch 267/5000
26/26 - 1s - loss: 2.7041 - val_loss: 3.1480
Epoch 268/5000
26/26 - 1s - loss: 2.7027 - val_loss: 3.1465
Epoch 269/5000
26/26 - 1s - loss: 2.6994 - val_loss: 3.1420
Epoch 270/5000
26/26 - 1s - loss: 2.6954 - val_loss: 3.1388
Epoch 00270: val_loss improved from 3.16666 to 3.13878, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 271/5000
26/26 - 1s - loss: 2.6932 - val_loss: 3.1362
Epoch 272/5000
26/26 - 1s - loss: 2.6898 - val_loss: 3.1337
Epoch 273/5000
26/26 - 1s - loss: 2.6874 - val_loss: 3.1302
Epoch 274/5000
26/26 - 1s - loss: 2.6861 - val_loss: 3.1250
Epoch 275/5000
26/26 - 1s - loss: 2.6807 - val_loss: 3.1223
Epoch 276/5000
26/26 - 1s - loss: 2.6777 - val_loss: 3.1191
Epoch 277/5000
26/26 - 1s - loss: 2.6726 - val_loss: 3.1186
Epoch 278/5000
26/26 - 1s - loss: 2.6752 - val_loss: 3.1142
Epoch 279/5000
26/26 - 1s - loss: 2.6647 - val_loss: 3.1097
Epoch 280/5000
26/26 - 1s - loss: 2.6643 - val_loss: 3.1085
Epoch 00280: val_loss improved from 3.13878 to 3.10848, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 281/5000
26/26 - 1s - loss: 2.6609 - val_loss: 3.1060
Epoch 282/5000
26/26 - 1s - loss: 2.6612 - val_loss: 3.1054
Epoch 283/5000
26/26 - 1s - loss: 2.6564 - val_loss: 3.1032
Epoch 284/5000
26/26 - 1s - loss: 2.6522 - val_loss: 3.0995
Epoch 285/5000
26/26 - 1s - loss: 2.6485 - val_loss: 3.0925
Epoch 286/5000
26/26 - 1s - loss: 2.6488 - val_loss: 3.0928
Epoch 287/5000
26/26 - 1s - loss: 2.6447 - val_loss: 3.0883
Epoch 288/5000
26/26 - 1s - loss: 2.6434 - val_loss: 3.0856
Epoch 289/5000
26/26 - 1s - loss: 2.6392 - val_loss: 3.0848
Epoch 290/5000
26/26 - 1s - loss: 2.6339 - val_loss: 3.0835
Epoch 00290: val_loss improved from 3.10848 to 3.08349, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 291/5000
26/26 - 1s - loss: 2.6340 - val_loss: 3.0788
Epoch 292/5000
26/26 - 1s - loss: 2.6293 - val_loss: 3.0735
Epoch 293/5000
26/26 - 1s - loss: 2.6251 - val_loss: 3.0708
Epoch 294/5000
26/26 - 1s - loss: 2.6212 - val_loss: 3.0679
Epoch 295/5000
26/26 - 1s - loss: 2.6207 - val_loss: 3.0663
Epoch 296/5000
26/26 - 1s - loss: 2.6182 - val_loss: 3.0643
Epoch 297/5000
26/26 - 1s - loss: 2.6145 - val_loss: 3.0603
Epoch 298/5000
26/26 - 1s - loss: 2.6118 - val_loss: 3.0577
Epoch 299/5000
26/26 - 1s - loss: 2.6100 - val_loss: 3.0553
Epoch 300/5000
26/26 - 1s - loss: 2.6068 - val_loss: 3.0523
Epoch 00300: val_loss improved from 3.08349 to 3.05230, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 301/5000
26/26 - 1s - loss: 2.6051 - val_loss: 3.0503
Epoch 302/5000
26/26 - 1s - loss: 2.5995 - val_loss: 3.0501
Epoch 303/5000
26/26 - 1s - loss: 2.5996 - val_loss: 3.0448
Epoch 304/5000
26/26 - 1s - loss: 2.5938 - val_loss: 3.0412
Epoch 305/5000
26/26 - 1s - loss: 2.5924 - val_loss: 3.0398
Epoch 306/5000
26/26 - 1s - loss: 2.5869 - val_loss: 3.0372
Epoch 307/5000
26/26 - 1s - loss: 2.5838 - val_loss: 3.0335
Epoch 308/5000
26/26 - 1s - loss: 2.5837 - val_loss: 3.0297
Epoch 309/5000
26/26 - 1s - loss: 2.5805 - val_loss: 3.0288
Epoch 310/5000
26/26 - 1s - loss: 2.5774 - val_loss: 3.0272
Epoch 00310: val_loss improved from 3.05230 to 3.02722, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 311/5000
26/26 - 1s - loss: 2.5746 - val_loss: 3.0226
Epoch 312/5000
26/26 - 1s - loss: 2.5695 - val_loss: 3.0184
Epoch 313/5000
26/26 - 1s - loss: 2.5693 - val_loss: 3.0192
Epoch 314/5000
26/26 - 1s - loss: 2.5633 - val_loss: 3.0137
Epoch 315/5000
26/26 - 1s - loss: 2.5626 - val_loss: 3.0117
Epoch 316/5000
26/26 - 1s - loss: 2.5615 - val_loss: 3.0124
Epoch 317/5000
26/26 - 1s - loss: 2.5585 - val_loss: 3.0082
Epoch 318/5000
26/26 - 1s - loss: 2.5557 - val_loss: 3.0049
Epoch 319/5000
26/26 - 1s - loss: 2.5532 - val_loss: 3.0028
Epoch 320/5000
26/26 - 1s - loss: 2.5498 - val_loss: 2.9992
Epoch 00320: val_loss improved from 3.02722 to 2.99922, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 321/5000
26/26 - 1s - loss: 2.5473 - val_loss: 2.9974
Epoch 322/5000
26/26 - 1s - loss: 2.5426 - val_loss: 2.9945
Epoch 323/5000
26/26 - 1s - loss: 2.5406 - val_loss: 2.9906
Epoch 324/5000
26/26 - 1s - loss: 2.5377 - val_loss: 2.9880
Epoch 325/5000
26/26 - 1s - loss: 2.5336 - val_loss: 2.9882
Epoch 326/5000
26/26 - 1s - loss: 2.5303 - val_loss: 2.9857
Epoch 327/5000
26/26 - 1s - loss: 2.5294 - val_loss: 2.9827
Epoch 328/5000
26/26 - 1s - loss: 2.5266 - val_loss: 2.9774
Epoch 329/5000
26/26 - 1s - loss: 2.5249 - val_loss: 2.9768
Epoch 330/5000
26/26 - 1s - loss: 2.5217 - val_loss: 2.9696
Epoch 00330: val_loss improved from 2.99922 to 2.96964, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 331/5000
26/26 - 1s - loss: 2.5178 - val_loss: 2.9677
Epoch 332/5000
26/26 - 1s - loss: 2.5165 - val_loss: 2.9668
Epoch 333/5000
26/26 - 1s - loss: 2.5121 - val_loss: 2.9656
Epoch 334/5000
26/26 - 2s - loss: 2.5083 - val_loss: 2.9623
Epoch 335/5000
26/26 - 1s - loss: 2.5066 - val_loss: 2.9612
Epoch 336/5000
26/26 - 1s - loss: 2.5065 - val_loss: 2.9584
Epoch 337/5000
26/26 - 1s - loss: 2.4988 - val_loss: 2.9547
Epoch 338/5000
26/26 - 1s - loss: 2.4990 - val_loss: 2.9556
Epoch 339/5000
26/26 - 1s - loss: 2.4956 - val_loss: 2.9482
Epoch 340/5000
26/26 - 1s - loss: 2.4940 - val_loss: 2.9466
Epoch 00340: val_loss improved from 2.96964 to 2.94659, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 341/5000
26/26 - 1s - loss: 2.4922 - val_loss: 2.9440
Epoch 342/5000
26/26 - 1s - loss: 2.4874 - val_loss: 2.9427
Epoch 343/5000
26/26 - 1s - loss: 2.4859 - val_loss: 2.9391
Epoch 344/5000
26/26 - 1s - loss: 2.4838 - val_loss: 2.9365
Epoch 345/5000
26/26 - 1s - loss: 2.4803 - val_loss: 2.9331
Epoch 346/5000
26/26 - 1s - loss: 2.4790 - val_loss: 2.9302
Epoch 347/5000
26/26 - 1s - loss: 2.4744 - val_loss: 2.9293
Epoch 348/5000
26/26 - 1s - loss: 2.4741 - val_loss: 2.9251
Epoch 349/5000
26/26 - 1s - loss: 2.4714 - val_loss: 2.9227
Epoch 350/5000
26/26 - 1s - loss: 2.4671 - val_loss: 2.9201
Epoch 00350: val_loss improved from 2.94659 to 2.92014, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 351/5000
26/26 - 1s - loss: 2.4658 - val_loss: 2.9194
Epoch 352/5000
26/26 - 1s - loss: 2.4627 - val_loss: 2.9175
Epoch 353/5000
26/26 - 1s - loss: 2.4619 - val_loss: 2.9144
Epoch 354/5000
26/26 - 1s - loss: 2.4564 - val_loss: 2.9115
Epoch 355/5000
26/26 - 1s - loss: 2.4514 - val_loss: 2.9080
Epoch 356/5000
26/26 - 1s - loss: 2.4529 - val_loss: 2.9084
Epoch 357/5000
26/26 - 1s - loss: 2.4488 - val_loss: 2.9064
Epoch 358/5000
26/26 - 1s - loss: 2.4463 - val_loss: 2.9022
Epoch 359/5000
26/26 - 1s - loss: 2.4446 - val_loss: 2.9009
Epoch 360/5000
26/26 - 1s - loss: 2.4415 - val_loss: 2.9006
Epoch 00360: val_loss improved from 2.92014 to 2.90065, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 361/5000
26/26 - 1s - loss: 2.4391 - val_loss: 2.8955
Epoch 362/5000
26/26 - 1s - loss: 2.4385 - val_loss: 2.8945
Epoch 363/5000
26/26 - 1s - loss: 2.4354 - val_loss: 2.8923
Epoch 364/5000
26/26 - 1s - loss: 2.4304 - val_loss: 2.8876
Epoch 365/5000
26/26 - 1s - loss: 2.4288 - val_loss: 2.8850
Epoch 366/5000
26/26 - 1s - loss: 2.4259 - val_loss: 2.8828
Epoch 367/5000
26/26 - 1s - loss: 2.4252 - val_loss: 2.8805
Epoch 368/5000
26/26 - 1s - loss: 2.4188 - val_loss: 2.8795
Epoch 369/5000
26/26 - 1s - loss: 2.4193 - val_loss: 2.8773
Epoch 370/5000
26/26 - 2s - loss: 2.4143 - val_loss: 2.8729
Epoch 00370: val_loss improved from 2.90065 to 2.87290, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 371/5000
26/26 - 1s - loss: 2.4113 - val_loss: 2.8707
Epoch 372/5000
26/26 - 1s - loss: 2.4084 - val_loss: 2.8691
Epoch 373/5000
26/26 - 1s - loss: 2.4075 - val_loss: 2.8646
Epoch 374/5000
26/26 - 1s - loss: 2.4067 - val_loss: 2.8660
Epoch 375/5000
26/26 - 1s - loss: 2.4025 - val_loss: 2.8606
Epoch 376/5000
26/26 - 1s - loss: 2.4010 - val_loss: 2.8579
Epoch 377/5000
26/26 - 1s - loss: 2.3967 - val_loss: 2.8545
Epoch 378/5000
26/26 - 1s - loss: 2.3939 - val_loss: 2.8549
Epoch 379/5000
26/26 - 1s - loss: 2.3917 - val_loss: 2.8522
Epoch 380/5000
26/26 - 1s - loss: 2.3905 - val_loss: 2.8502
Epoch 00380: val_loss improved from 2.87290 to 2.85021, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 381/5000
26/26 - 1s - loss: 2.3874 - val_loss: 2.8501
Epoch 382/5000
26/26 - 1s - loss: 2.3840 - val_loss: 2.8446
Epoch 383/5000
26/26 - 1s - loss: 2.3822 - val_loss: 2.8425
Epoch 384/5000
26/26 - 1s - loss: 2.3807 - val_loss: 2.8410
Epoch 385/5000
26/26 - 1s - loss: 2.3767 - val_loss: 2.8380
Epoch 386/5000
26/26 - 1s - loss: 2.3730 - val_loss: 2.8343
Epoch 387/5000
26/26 - 1s - loss: 2.3720 - val_loss: 2.8332
Epoch 388/5000
26/26 - 1s - loss: 2.3722 - val_loss: 2.8295
Epoch 389/5000
26/26 - 1s - loss: 2.3668 - val_loss: 2.8276
Epoch 390/5000
26/26 - 1s - loss: 2.3677 - val_loss: 2.8256
Epoch 00390: val_loss improved from 2.85021 to 2.82558, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 391/5000
26/26 - 1s - loss: 2.3622 - val_loss: 2.8227
Epoch 392/5000
26/26 - 1s - loss: 2.3611 - val_loss: 2.8199
Epoch 393/5000
26/26 - 1s - loss: 2.3566 - val_loss: 2.8166
Epoch 394/5000
26/26 - 1s - loss: 2.3559 - val_loss: 2.8146
Epoch 395/5000
26/26 - 1s - loss: 2.3520 - val_loss: 2.8123
Epoch 396/5000
26/26 - 1s - loss: 2.3500 - val_loss: 2.8099
Epoch 397/5000
26/26 - 1s - loss: 2.3498 - val_loss: 2.8072
Epoch 398/5000
26/26 - 1s - loss: 2.3470 - val_loss: 2.8055
Epoch 399/5000
26/26 - 1s - loss: 2.3434 - val_loss: 2.8037
Epoch 400/5000
26/26 - 1s - loss: 2.3409 - val_loss: 2.8008
Epoch 00400: val_loss improved from 2.82558 to 2.80082, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 401/5000
26/26 - 1s - loss: 2.3396 - val_loss: 2.8010
Epoch 402/5000
26/26 - 1s - loss: 2.3368 - val_loss: 2.7957
Epoch 403/5000
26/26 - 1s - loss: 2.3330 - val_loss: 2.7947
Epoch 404/5000
26/26 - 1s - loss: 2.3302 - val_loss: 2.7914
Epoch 405/5000
26/26 - 1s - loss: 2.3281 - val_loss: 2.7868
Epoch 406/5000
26/26 - 1s - loss: 2.3309 - val_loss: 2.7871
Epoch 407/5000
26/26 - 1s - loss: 2.3255 - val_loss: 2.7844
Epoch 408/5000
26/26 - 1s - loss: 2.3222 - val_loss: 2.7825
Epoch 409/5000
26/26 - 1s - loss: 2.3206 - val_loss: 2.7810
Epoch 410/5000
26/26 - 1s - loss: 2.3195 - val_loss: 2.7773
Epoch 00410: val_loss improved from 2.80082 to 2.77734, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 411/5000
26/26 - 2s - loss: 2.3160 - val_loss: 2.7763
Epoch 412/5000
26/26 - 1s - loss: 2.3137 - val_loss: 2.7734
Epoch 413/5000
26/26 - 1s - loss: 2.3095 - val_loss: 2.7716
Epoch 414/5000
26/26 - 1s - loss: 2.3084 - val_loss: 2.7680
Epoch 415/5000
26/26 - 1s - loss: 2.3055 - val_loss: 2.7646
Epoch 416/5000
26/26 - 1s - loss: 2.3035 - val_loss: 2.7614
Epoch 417/5000
26/26 - 1s - loss: 2.3014 - val_loss: 2.7611
Epoch 418/5000
26/26 - 1s - loss: 2.2998 - val_loss: 2.7609
Epoch 419/5000
26/26 - 1s - loss: 2.2972 - val_loss: 2.7605
Epoch 420/5000
26/26 - 1s - loss: 2.2965 - val_loss: 2.7552
Epoch 00420: val_loss improved from 2.77734 to 2.75524, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 421/5000
26/26 - 1s - loss: 2.2941 - val_loss: 2.7526
Epoch 422/5000
26/26 - 1s - loss: 2.2893 - val_loss: 2.7477
Epoch 423/5000
26/26 - 1s - loss: 2.2872 - val_loss: 2.7464
Epoch 424/5000
26/26 - 1s - loss: 2.2847 - val_loss: 2.7471
Epoch 425/5000
26/26 - 1s - loss: 2.2823 - val_loss: 2.7436
Epoch 426/5000
26/26 - 1s - loss: 2.2784 - val_loss: 2.7389
Epoch 427/5000
26/26 - 1s - loss: 2.2779 - val_loss: 2.7375
Epoch 428/5000
26/26 - 1s - loss: 2.2757 - val_loss: 2.7362
Epoch 429/5000
26/26 - 1s - loss: 2.2744 - val_loss: 2.7347
Epoch 430/5000
26/26 - 1s - loss: 2.2717 - val_loss: 2.7305
Epoch 00430: val_loss improved from 2.75524 to 2.73054, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 431/5000
26/26 - 1s - loss: 2.2682 - val_loss: 2.7283
Epoch 432/5000
26/26 - 1s - loss: 2.2644 - val_loss: 2.7266
Epoch 433/5000
26/26 - 1s - loss: 2.2637 - val_loss: 2.7258
Epoch 434/5000
26/26 - 1s - loss: 2.2611 - val_loss: 2.7239
Epoch 435/5000
26/26 - 1s - loss: 2.2569 - val_loss: 2.7239
Epoch 436/5000
26/26 - 1s - loss: 2.2573 - val_loss: 2.7200
Epoch 437/5000
26/26 - 1s - loss: 2.2522 - val_loss: 2.7187
Epoch 438/5000
26/26 - 1s - loss: 2.2517 - val_loss: 2.7133
Epoch 439/5000
26/26 - 1s - loss: 2.2497 - val_loss: 2.7110
Epoch 440/5000
26/26 - 1s - loss: 2.2484 - val_loss: 2.7077
Epoch 00440: val_loss improved from 2.73054 to 2.70766, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 441/5000
26/26 - 1s - loss: 2.2465 - val_loss: 2.7059
Epoch 442/5000
26/26 - 1s - loss: 2.2457 - val_loss: 2.7077
Epoch 443/5000
26/26 - 1s - loss: 2.2426 - val_loss: 2.7033
Epoch 444/5000
26/26 - 1s - loss: 2.2381 - val_loss: 2.6998
Epoch 445/5000
26/26 - 1s - loss: 2.2359 - val_loss: 2.6973
Epoch 446/5000
26/26 - 1s - loss: 2.2364 - val_loss: 2.6938
Epoch 447/5000
26/26 - 1s - loss: 2.2345 - val_loss: 2.6960
Epoch 448/5000
26/26 - 1s - loss: 2.2290 - val_loss: 2.6943
Epoch 449/5000
26/26 - 1s - loss: 2.2288 - val_loss: 2.6923
Epoch 450/5000
26/26 - 1s - loss: 2.2261 - val_loss: 2.6885
Epoch 00450: val_loss improved from 2.70766 to 2.68847, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 451/5000
26/26 - 1s - loss: 2.2218 - val_loss: 2.6880
Epoch 452/5000
26/26 - 1s - loss: 2.2210 - val_loss: 2.6828
Epoch 453/5000
26/26 - 1s - loss: 2.2197 - val_loss: 2.6788
Epoch 454/5000
26/26 - 1s - loss: 2.2175 - val_loss: 2.6804
Epoch 455/5000
26/26 - 1s - loss: 2.2159 - val_loss: 2.6775
Epoch 456/5000
26/26 - 1s - loss: 2.2135 - val_loss: 2.6756
Epoch 457/5000
26/26 - 1s - loss: 2.2087 - val_loss: 2.6735
Epoch 458/5000
26/26 - 1s - loss: 2.2104 - val_loss: 2.6731
Epoch 459/5000
26/26 - 1s - loss: 2.2084 - val_loss: 2.6701
Epoch 460/5000
26/26 - 1s - loss: 2.2052 - val_loss: 2.6660
Epoch 00460: val_loss improved from 2.68847 to 2.66605, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 461/5000
26/26 - 1s - loss: 2.2023 - val_loss: 2.6661
Epoch 462/5000
26/26 - 1s - loss: 2.1997 - val_loss: 2.6644
Epoch 463/5000
26/26 - 1s - loss: 2.1970 - val_loss: 2.6605
Epoch 464/5000
26/26 - 1s - loss: 2.1970 - val_loss: 2.6612
Epoch 465/5000
26/26 - 1s - loss: 2.1928 - val_loss: 2.6596
Epoch 466/5000
26/26 - 1s - loss: 2.1910 - val_loss: 2.6563
Epoch 467/5000
26/26 - 1s - loss: 2.1877 - val_loss: 2.6533
Epoch 468/5000
26/26 - 1s - loss: 2.1870 - val_loss: 2.6517
Epoch 469/5000
26/26 - 1s - loss: 2.1844 - val_loss: 2.6478
Epoch 470/5000
26/26 - 1s - loss: 2.1835 - val_loss: 2.6494
Epoch 00470: val_loss improved from 2.66605 to 2.64939, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 471/5000
26/26 - 1s - loss: 2.1811 - val_loss: 2.6427
Epoch 472/5000
26/26 - 1s - loss: 2.1791 - val_loss: 2.6413
Epoch 473/5000
26/26 - 1s - loss: 2.1776 - val_loss: 2.6401
Epoch 474/5000
26/26 - 1s - loss: 2.1741 - val_loss: 2.6395
Epoch 475/5000
26/26 - 1s - loss: 2.1737 - val_loss: 2.6378
Epoch 476/5000
26/26 - 1s - loss: 2.1679 - val_loss: 2.6352
Epoch 477/5000
26/26 - 1s - loss: 2.1677 - val_loss: 2.6313
Epoch 478/5000
26/26 - 1s - loss: 2.1664 - val_loss: 2.6307
Epoch 479/5000
26/26 - 1s - loss: 2.1640 - val_loss: 2.6292
Epoch 480/5000
26/26 - 1s - loss: 2.1621 - val_loss: 2.6259
Epoch 00480: val_loss improved from 2.64939 to 2.62593, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 481/5000
26/26 - 1s - loss: 2.1581 - val_loss: 2.6245
Epoch 482/5000
26/26 - 1s - loss: 2.1585 - val_loss: 2.6226
Epoch 483/5000
26/26 - 1s - loss: 2.1558 - val_loss: 2.6211
Epoch 484/5000
26/26 - 1s - loss: 2.1531 - val_loss: 2.6166
Epoch 485/5000
26/26 - 1s - loss: 2.1515 - val_loss: 2.6151
Epoch 486/5000
26/26 - 1s - loss: 2.1491 - val_loss: 2.6126
Epoch 487/5000
26/26 - 1s - loss: 2.1462 - val_loss: 2.6101
Epoch 488/5000
26/26 - 1s - loss: 2.1440 - val_loss: 2.6125
Epoch 489/5000
26/26 - 1s - loss: 2.1433 - val_loss: 2.6086
Epoch 490/5000
26/26 - 1s - loss: 2.1407 - val_loss: 2.6050
Epoch 00490: val_loss improved from 2.62593 to 2.60502, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 491/5000
26/26 - 1s - loss: 2.1403 - val_loss: 2.6037
Epoch 492/5000
26/26 - 1s - loss: 2.1363 - val_loss: 2.6021
Epoch 493/5000
26/26 - 1s - loss: 2.1338 - val_loss: 2.5996
Epoch 494/5000
26/26 - 1s - loss: 2.1336 - val_loss: 2.5964
Epoch 495/5000
26/26 - 2s - loss: 2.1284 - val_loss: 2.5960
Epoch 496/5000
26/26 - 1s - loss: 2.1298 - val_loss: 2.5964
Epoch 497/5000
26/26 - 1s - loss: 2.1271 - val_loss: 2.5897
Epoch 498/5000
26/26 - 2s - loss: 2.1238 - val_loss: 2.5905
Epoch 499/5000
26/26 - 1s - loss: 2.1221 - val_loss: 2.5888
Epoch 500/5000
26/26 - 1s - loss: 2.1185 - val_loss: 2.5854
Epoch 00500: val_loss improved from 2.60502 to 2.58540, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 501/5000
26/26 - 1s - loss: 2.1187 - val_loss: 2.5843
Epoch 502/5000
26/26 - 1s - loss: 2.1172 - val_loss: 2.5824
Epoch 503/5000
26/26 - 1s - loss: 2.1126 - val_loss: 2.5810
Epoch 504/5000
26/26 - 1s - loss: 2.1103 - val_loss: 2.5770
Epoch 505/5000
26/26 - 1s - loss: 2.1107 - val_loss: 2.5767
Epoch 506/5000
26/26 - 1s - loss: 2.1080 - val_loss: 2.5724
Epoch 507/5000
26/26 - 1s - loss: 2.1052 - val_loss: 2.5720
Epoch 508/5000
26/26 - 1s - loss: 2.1034 - val_loss: 2.5672
Epoch 509/5000
26/26 - 1s - loss: 2.1011 - val_loss: 2.5692
Epoch 510/5000
26/26 - 1s - loss: 2.0989 - val_loss: 2.5692
Epoch 00510: val_loss improved from 2.58540 to 2.56921, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 511/5000
26/26 - 1s - loss: 2.0985 - val_loss: 2.5636
Epoch 512/5000
26/26 - 1s - loss: 2.0953 - val_loss: 2.5613
Epoch 513/5000
26/26 - 1s - loss: 2.0917 - val_loss: 2.5595
Epoch 514/5000
26/26 - 1s - loss: 2.0934 - val_loss: 2.5574
Epoch 515/5000
26/26 - 1s - loss: 2.0903 - val_loss: 2.5556
Epoch 516/5000
26/26 - 1s - loss: 2.0866 - val_loss: 2.5558
Epoch 517/5000
26/26 - 1s - loss: 2.0890 - val_loss: 2.5518
Epoch 518/5000
26/26 - 1s - loss: 2.0850 - val_loss: 2.5521
Epoch 519/5000
26/26 - 1s - loss: 2.0810 - val_loss: 2.5478
Epoch 520/5000
26/26 - 1s - loss: 2.0803 - val_loss: 2.5438
Epoch 00520: val_loss improved from 2.56921 to 2.54380, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 521/5000
26/26 - 1s - loss: 2.0786 - val_loss: 2.5439
Epoch 522/5000
26/26 - 1s - loss: 2.0756 - val_loss: 2.5413
Epoch 523/5000
26/26 - 1s - loss: 2.0728 - val_loss: 2.5390
Epoch 524/5000
26/26 - 1s - loss: 2.0724 - val_loss: 2.5381
Epoch 525/5000
26/26 - 1s - loss: 2.0700 - val_loss: 2.5350
Epoch 526/5000
26/26 - 1s - loss: 2.0706 - val_loss: 2.5343
Epoch 527/5000
26/26 - 1s - loss: 2.0663 - val_loss: 2.5327
Epoch 528/5000
26/26 - 1s - loss: 2.0639 - val_loss: 2.5282
Epoch 529/5000
26/26 - 1s - loss: 2.0638 - val_loss: 2.5288
Epoch 530/5000
26/26 - 1s - loss: 2.0608 - val_loss: 2.5259
Epoch 00530: val_loss improved from 2.54380 to 2.52588, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 531/5000
26/26 - 1s - loss: 2.0579 - val_loss: 2.5220
Epoch 532/5000
26/26 - 1s - loss: 2.0566 - val_loss: 2.5220
Epoch 533/5000
26/26 - 2s - loss: 2.0541 - val_loss: 2.5237
Epoch 534/5000
26/26 - 1s - loss: 2.0544 - val_loss: 2.5188
Epoch 535/5000
26/26 - 2s - loss: 2.0523 - val_loss: 2.5153
Epoch 536/5000
26/26 - 1s - loss: 2.0475 - val_loss: 2.5141
Epoch 537/5000
26/26 - 2s - loss: 2.0466 - val_loss: 2.5109
Epoch 538/5000
26/26 - 1s - loss: 2.0456 - val_loss: 2.5111
Epoch 539/5000
26/26 - 1s - loss: 2.0424 - val_loss: 2.5096
Epoch 540/5000
26/26 - 1s - loss: 2.0417 - val_loss: 2.5058
Epoch 00540: val_loss improved from 2.52588 to 2.50580, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 541/5000
26/26 - 1s - loss: 2.0363 - val_loss: 2.5066
Epoch 542/5000
26/26 - 1s - loss: 2.0381 - val_loss: 2.5042
Epoch 543/5000
26/26 - 1s - loss: 2.0354 - val_loss: 2.5009
Epoch 544/5000
26/26 - 1s - loss: 2.0323 - val_loss: 2.4979
Epoch 545/5000
26/26 - 1s - loss: 2.0296 - val_loss: 2.5023
Epoch 546/5000
26/26 - 1s - loss: 2.0302 - val_loss: 2.4955
Epoch 547/5000
26/26 - 1s - loss: 2.0278 - val_loss: 2.4943
Epoch 548/5000
26/26 - 1s - loss: 2.0266 - val_loss: 2.4922
Epoch 549/5000
26/26 - 1s - loss: 2.0234 - val_loss: 2.4902
Epoch 550/5000
26/26 - 1s - loss: 2.0192 - val_loss: 2.4890
Epoch 00550: val_loss improved from 2.50580 to 2.48899, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 551/5000
26/26 - 1s - loss: 2.0192 - val_loss: 2.4880
Epoch 552/5000
26/26 - 1s - loss: 2.0199 - val_loss: 2.4846
Epoch 553/5000
26/26 - 1s - loss: 2.0131 - val_loss: 2.4822
Epoch 554/5000
26/26 - 1s - loss: 2.0133 - val_loss: 2.4833
Epoch 555/5000
26/26 - 1s - loss: 2.0111 - val_loss: 2.4789
Epoch 556/5000
26/26 - 1s - loss: 2.0125 - val_loss: 2.4778
Epoch 557/5000
26/26 - 1s - loss: 2.0079 - val_loss: 2.4748
Epoch 558/5000
26/26 - 1s - loss: 2.0065 - val_loss: 2.4717
Epoch 559/5000
26/26 - 1s - loss: 2.0050 - val_loss: 2.4729
Epoch 560/5000
26/26 - 1s - loss: 2.0021 - val_loss: 2.4690
Epoch 00560: val_loss improved from 2.48899 to 2.46900, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 561/5000
26/26 - 1s - loss: 2.0002 - val_loss: 2.4692
Epoch 562/5000
26/26 - 1s - loss: 1.9991 - val_loss: 2.4682
Epoch 563/5000
26/26 - 1s - loss: 1.9980 - val_loss: 2.4658
Epoch 564/5000
26/26 - 1s - loss: 1.9951 - val_loss: 2.4633
Epoch 565/5000
26/26 - 1s - loss: 1.9945 - val_loss: 2.4630
Epoch 566/5000
26/26 - 1s - loss: 1.9902 - val_loss: 2.4604
Epoch 567/5000
26/26 - 1s - loss: 1.9898 - val_loss: 2.4585
Epoch 568/5000
26/26 - 1s - loss: 1.9871 - val_loss: 2.4574
Epoch 569/5000
26/26 - 1s - loss: 1.9867 - val_loss: 2.4543
Epoch 570/5000
26/26 - 1s - loss: 1.9832 - val_loss: 2.4518
Epoch 00570: val_loss improved from 2.46900 to 2.45184, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 571/5000
26/26 - 1s - loss: 1.9823 - val_loss: 2.4520
Epoch 572/5000
26/26 - 1s - loss: 1.9811 - val_loss: 2.4512
Epoch 573/5000
26/26 - 1s - loss: 1.9795 - val_loss: 2.4470
Epoch 574/5000
26/26 - 1s - loss: 1.9774 - val_loss: 2.4426
Epoch 575/5000
26/26 - 1s - loss: 1.9756 - val_loss: 2.4428
Epoch 576/5000
26/26 - 1s - loss: 1.9740 - val_loss: 2.4426
Epoch 577/5000
26/26 - 1s - loss: 1.9712 - val_loss: 2.4393
Epoch 578/5000
26/26 - 1s - loss: 1.9685 - val_loss: 2.4373
Epoch 579/5000
26/26 - 1s - loss: 1.9666 - val_loss: 2.4348
Epoch 580/5000
26/26 - 2s - loss: 1.9648 - val_loss: 2.4322
Epoch 00580: val_loss improved from 2.45184 to 2.43225, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 581/5000
26/26 - 1s - loss: 1.9656 - val_loss: 2.4314
Epoch 582/5000
26/26 - 1s - loss: 1.9616 - val_loss: 2.4300
Epoch 583/5000
26/26 - 1s - loss: 1.9615 - val_loss: 2.4288
Epoch 584/5000
26/26 - 1s - loss: 1.9574 - val_loss: 2.4260
Epoch 585/5000
26/26 - 1s - loss: 1.9536 - val_loss: 2.4257
Epoch 586/5000
26/26 - 1s - loss: 1.9540 - val_loss: 2.4242
Epoch 587/5000
26/26 - 1s - loss: 1.9546 - val_loss: 2.4214
Epoch 588/5000
26/26 - 1s - loss: 1.9508 - val_loss: 2.4202
Epoch 589/5000
26/26 - 1s - loss: 1.9504 - val_loss: 2.4180
Epoch 590/5000
26/26 - 1s - loss: 1.9476 - val_loss: 2.4164
Epoch 00590: val_loss improved from 2.43225 to 2.41637, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 591/5000
26/26 - 2s - loss: 1.9476 - val_loss: 2.4150
Epoch 592/5000
26/26 - 1s - loss: 1.9433 - val_loss: 2.4129
Epoch 593/5000
26/26 - 1s - loss: 1.9426 - val_loss: 2.4119
Epoch 594/5000
26/26 - 1s - loss: 1.9396 - val_loss: 2.4120
Epoch 595/5000
26/26 - 1s - loss: 1.9391 - val_loss: 2.4082
Epoch 596/5000
26/26 - 1s - loss: 1.9378 - val_loss: 2.4087
Epoch 597/5000
26/26 - 1s - loss: 1.9375 - val_loss: 2.4039
Epoch 598/5000
26/26 - 1s - loss: 1.9339 - val_loss: 2.4042
Epoch 599/5000
26/26 - 1s - loss: 1.9320 - val_loss: 2.4021
Epoch 600/5000
26/26 - 1s - loss: 1.9276 - val_loss: 2.4000
Epoch 00600: val_loss improved from 2.41637 to 2.40002, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 601/5000
26/26 - 1s - loss: 1.9279 - val_loss: 2.3978
Epoch 602/5000
26/26 - 1s - loss: 1.9281 - val_loss: 2.3948
Epoch 603/5000
26/26 - 1s - loss: 1.9245 - val_loss: 2.3928
Epoch 604/5000
26/26 - 1s - loss: 1.9246 - val_loss: 2.3931
Epoch 605/5000
26/26 - 1s - loss: 1.9206 - val_loss: 2.3912
Epoch 606/5000
26/26 - 1s - loss: 1.9192 - val_loss: 2.3883
Epoch 607/5000
26/26 - 1s - loss: 1.9181 - val_loss: 2.3877
Epoch 608/5000
26/26 - 1s - loss: 1.9152 - val_loss: 2.3860
Epoch 609/5000
26/26 - 1s - loss: 1.9124 - val_loss: 2.3843
Epoch 610/5000
26/26 - 1s - loss: 1.9136 - val_loss: 2.3821
Epoch 00610: val_loss improved from 2.40002 to 2.38205, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 611/5000
26/26 - 1s - loss: 1.9098 - val_loss: 2.3800
Epoch 612/5000
26/26 - 1s - loss: 1.9105 - val_loss: 2.3799
Epoch 613/5000
26/26 - 1s - loss: 1.9069 - val_loss: 2.3779
Epoch 614/5000
26/26 - 1s - loss: 1.9075 - val_loss: 2.3733
Epoch 615/5000
26/26 - 1s - loss: 1.9045 - val_loss: 2.3752
Epoch 616/5000
26/26 - 1s - loss: 1.9025 - val_loss: 2.3723
Epoch 617/5000
26/26 - 1s - loss: 1.9014 - val_loss: 2.3694
Epoch 618/5000
26/26 - 1s - loss: 1.8998 - val_loss: 2.3672
Epoch 619/5000
26/26 - 1s - loss: 1.8972 - val_loss: 2.3674
Epoch 620/5000
26/26 - 1s - loss: 1.8930 - val_loss: 2.3635
Epoch 00620: val_loss improved from 2.38205 to 2.36348, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 621/5000
26/26 - 1s - loss: 1.8922 - val_loss: 2.3620
Epoch 622/5000
26/26 - 1s - loss: 1.8905 - val_loss: 2.3630
Epoch 623/5000
26/26 - 1s - loss: 1.8896 - val_loss: 2.3635
Epoch 624/5000
26/26 - 1s - loss: 1.8884 - val_loss: 2.3616
Epoch 625/5000
26/26 - 1s - loss: 1.8862 - val_loss: 2.3581
Epoch 626/5000
26/26 - 1s - loss: 1.8868 - val_loss: 2.3564
Epoch 627/5000
26/26 - 1s - loss: 1.8850 - val_loss: 2.3585
Epoch 628/5000
26/26 - 1s - loss: 1.8821 - val_loss: 2.3555
Epoch 629/5000
26/26 - 1s - loss: 1.8807 - val_loss: 2.3542
Epoch 630/5000
26/26 - 1s - loss: 1.8788 - val_loss: 2.3521
Epoch 00630: val_loss improved from 2.36348 to 2.35209, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 631/5000
26/26 - 1s - loss: 1.8773 - val_loss: 2.3500
Epoch 632/5000
26/26 - 1s - loss: 1.8751 - val_loss: 2.3479
Epoch 633/5000
26/26 - 1s - loss: 1.8724 - val_loss: 2.3457
Epoch 634/5000
26/26 - 1s - loss: 1.8701 - val_loss: 2.3431
Epoch 635/5000
26/26 - 1s - loss: 1.8696 - val_loss: 2.3419
Epoch 636/5000
26/26 - 1s - loss: 1.8682 - val_loss: 2.3399
Epoch 637/5000
26/26 - 1s - loss: 1.8679 - val_loss: 2.3380
Epoch 638/5000
26/26 - 1s - loss: 1.8652 - val_loss: 2.3361
Epoch 639/5000
26/26 - 1s - loss: 1.8621 - val_loss: 2.3375
Epoch 640/5000
26/26 - 1s - loss: 1.8623 - val_loss: 2.3311
Epoch 00640: val_loss improved from 2.35209 to 2.33113, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 641/5000
26/26 - 1s - loss: 1.8607 - val_loss: 2.3295
Epoch 642/5000
26/26 - 1s - loss: 1.8593 - val_loss: 2.3307
Epoch 643/5000
26/26 - 1s - loss: 1.8553 - val_loss: 2.3285
Epoch 644/5000
26/26 - 1s - loss: 1.8539 - val_loss: 2.3279
Epoch 645/5000
26/26 - 1s - loss: 1.8522 - val_loss: 2.3227
Epoch 646/5000
26/26 - 1s - loss: 1.8527 - val_loss: 2.3234
Epoch 647/5000
26/26 - 1s - loss: 1.8503 - val_loss: 2.3208
Epoch 648/5000
26/26 - 1s - loss: 1.8491 - val_loss: 2.3187
Epoch 649/5000
26/26 - 1s - loss: 1.8461 - val_loss: 2.3153
Epoch 650/5000
26/26 - 1s - loss: 1.8459 - val_loss: 2.3167
Epoch 00650: val_loss improved from 2.33113 to 2.31666, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 651/5000
26/26 - 1s - loss: 1.8421 - val_loss: 2.3152
Epoch 652/5000
26/26 - 1s - loss: 1.8416 - val_loss: 2.3151
Epoch 653/5000
26/26 - 1s - loss: 1.8427 - val_loss: 2.3110
Epoch 654/5000
26/26 - 1s - loss: 1.8370 - val_loss: 2.3083
Epoch 655/5000
26/26 - 1s - loss: 1.8367 - val_loss: 2.3072
Epoch 656/5000
26/26 - 1s - loss: 1.8351 - val_loss: 2.3057
Epoch 657/5000
26/26 - 1s - loss: 1.8353 - val_loss: 2.3044
Epoch 658/5000
26/26 - 1s - loss: 1.8326 - val_loss: 2.3044
Epoch 659/5000
26/26 - 2s - loss: 1.8300 - val_loss: 2.3013
Epoch 660/5000
26/26 - 1s - loss: 1.8280 - val_loss: 2.3009
Epoch 00660: val_loss improved from 2.31666 to 2.30091, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 661/5000
26/26 - 1s - loss: 1.8271 - val_loss: 2.2966
Epoch 662/5000
26/26 - 1s - loss: 1.8269 - val_loss: 2.2988
Epoch 663/5000
26/26 - 2s - loss: 1.8239 - val_loss: 2.2965
Epoch 664/5000
26/26 - 1s - loss: 1.8204 - val_loss: 2.2927
Epoch 665/5000
26/26 - 1s - loss: 1.8196 - val_loss: 2.2908
Epoch 666/5000
26/26 - 1s - loss: 1.8212 - val_loss: 2.2921
Epoch 667/5000
26/26 - 1s - loss: 1.8184 - val_loss: 2.2920
Epoch 668/5000
26/26 - 1s - loss: 1.8170 - val_loss: 2.2890
Epoch 669/5000
26/26 - 1s - loss: 1.8147 - val_loss: 2.2874
Epoch 670/5000
26/26 - 1s - loss: 1.8149 - val_loss: 2.2850
Epoch 00670: val_loss improved from 2.30091 to 2.28499, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 671/5000
26/26 - 1s - loss: 1.8133 - val_loss: 2.2834
Epoch 672/5000
26/26 - 1s - loss: 1.8096 - val_loss: 2.2823
Epoch 673/5000
26/26 - 1s - loss: 1.8073 - val_loss: 2.2810
Epoch 674/5000
26/26 - 1s - loss: 1.8059 - val_loss: 2.2793
Epoch 675/5000
26/26 - 1s - loss: 1.8045 - val_loss: 2.2795
Epoch 676/5000
26/26 - 1s - loss: 1.8018 - val_loss: 2.2773
Epoch 677/5000
26/26 - 1s - loss: 1.8027 - val_loss: 2.2761
Epoch 678/5000
26/26 - 1s - loss: 1.7996 - val_loss: 2.2756
Epoch 679/5000
26/26 - 2s - loss: 1.8006 - val_loss: 2.2724
Epoch 680/5000
26/26 - 1s - loss: 1.7976 - val_loss: 2.2694
Epoch 00680: val_loss improved from 2.28499 to 2.26935, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 681/5000
26/26 - 1s - loss: 1.7953 - val_loss: 2.2672
Epoch 682/5000
26/26 - 1s - loss: 1.7943 - val_loss: 2.2665
Epoch 683/5000
26/26 - 1s - loss: 1.7925 - val_loss: 2.2651
Epoch 684/5000
26/26 - 1s - loss: 1.7898 - val_loss: 2.2620
Epoch 685/5000
26/26 - 1s - loss: 1.7896 - val_loss: 2.2612
Epoch 686/5000
26/26 - 1s - loss: 1.7901 - val_loss: 2.2607
Epoch 687/5000
26/26 - 1s - loss: 1.7868 - val_loss: 2.2580
Epoch 688/5000
26/26 - 1s - loss: 1.7840 - val_loss: 2.2561
Epoch 689/5000
26/26 - 1s - loss: 1.7831 - val_loss: 2.2545
Epoch 690/5000
26/26 - 1s - loss: 1.7802 - val_loss: 2.2535
Epoch 00690: val_loss improved from 2.26935 to 2.25351, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 691/5000
26/26 - 1s - loss: 1.7787 - val_loss: 2.2535
Epoch 692/5000
26/26 - 2s - loss: 1.7789 - val_loss: 2.2517
Epoch 693/5000
26/26 - 1s - loss: 1.7750 - val_loss: 2.2484
Epoch 694/5000
26/26 - 1s - loss: 1.7749 - val_loss: 2.2465
Epoch 695/5000
26/26 - 1s - loss: 1.7739 - val_loss: 2.2447
Epoch 696/5000
26/26 - 1s - loss: 1.7714 - val_loss: 2.2450
Epoch 697/5000
26/26 - 1s - loss: 1.7709 - val_loss: 2.2407
Epoch 698/5000
26/26 - 1s - loss: 1.7697 - val_loss: 2.2404
Epoch 699/5000
26/26 - 1s - loss: 1.7673 - val_loss: 2.2393
Epoch 700/5000
26/26 - 1s - loss: 1.7647 - val_loss: 2.2372
Epoch 00700: val_loss improved from 2.25351 to 2.23722, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 701/5000
26/26 - 1s - loss: 1.7625 - val_loss: 2.2368
Epoch 702/5000
26/26 - 1s - loss: 1.7642 - val_loss: 2.2363
Epoch 703/5000
26/26 - 1s - loss: 1.7606 - val_loss: 2.2346
Epoch 704/5000
26/26 - 1s - loss: 1.7609 - val_loss: 2.2337
Epoch 705/5000
26/26 - 2s - loss: 1.7580 - val_loss: 2.2303
Epoch 706/5000
26/26 - 1s - loss: 1.7574 - val_loss: 2.2281
Epoch 707/5000
26/26 - 1s - loss: 1.7560 - val_loss: 2.2271
Epoch 708/5000
26/26 - 1s - loss: 1.7524 - val_loss: 2.2264
Epoch 709/5000
26/26 - 1s - loss: 1.7530 - val_loss: 2.2250
Epoch 710/5000
26/26 - 1s - loss: 1.7513 - val_loss: 2.2238
Epoch 00710: val_loss improved from 2.23722 to 2.22381, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 711/5000
26/26 - 1s - loss: 1.7480 - val_loss: 2.2234
Epoch 712/5000
26/26 - 1s - loss: 1.7465 - val_loss: 2.2206
Epoch 713/5000
26/26 - 1s - loss: 1.7451 - val_loss: 2.2185
Epoch 714/5000
26/26 - 1s - loss: 1.7431 - val_loss: 2.2177
Epoch 715/5000
26/26 - 1s - loss: 1.7427 - val_loss: 2.2152
Epoch 716/5000
26/26 - 1s - loss: 1.7437 - val_loss: 2.2119
Epoch 717/5000
26/26 - 1s - loss: 1.7374 - val_loss: 2.2114
Epoch 718/5000
26/26 - 1s - loss: 1.7385 - val_loss: 2.2091
Epoch 719/5000
26/26 - 1s - loss: 1.7359 - val_loss: 2.2093
Epoch 720/5000
26/26 - 1s - loss: 1.7354 - val_loss: 2.2090
Epoch 00720: val_loss improved from 2.22381 to 2.20902, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 721/5000
26/26 - 1s - loss: 1.7333 - val_loss: 2.2066
Epoch 722/5000
26/26 - 1s - loss: 1.7319 - val_loss: 2.2065
Epoch 723/5000
26/26 - 1s - loss: 1.7311 - val_loss: 2.2050
Epoch 724/5000
26/26 - 1s - loss: 1.7310 - val_loss: 2.2011
Epoch 725/5000
26/26 - 1s - loss: 1.7290 - val_loss: 2.1995
Epoch 726/5000
26/26 - 1s - loss: 1.7264 - val_loss: 2.1995
Epoch 727/5000
26/26 - 1s - loss: 1.7246 - val_loss: 2.2015
Epoch 728/5000
26/26 - 1s - loss: 1.7243 - val_loss: 2.1986
Epoch 729/5000
26/26 - 1s - loss: 1.7211 - val_loss: 2.1952
Epoch 730/5000
26/26 - 1s - loss: 1.7217 - val_loss: 2.1917
Epoch 00730: val_loss improved from 2.20902 to 2.19172, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 731/5000
26/26 - 1s - loss: 1.7189 - val_loss: 2.1900
Epoch 732/5000
26/26 - 1s - loss: 1.7177 - val_loss: 2.1897
Epoch 733/5000
26/26 - 1s - loss: 1.7166 - val_loss: 2.1892
Epoch 734/5000
26/26 - 1s - loss: 1.7147 - val_loss: 2.1885
Epoch 735/5000
26/26 - 1s - loss: 1.7130 - val_loss: 2.1856
Epoch 736/5000
26/26 - 1s - loss: 1.7123 - val_loss: 2.1837
Epoch 737/5000
26/26 - 1s - loss: 1.7117 - val_loss: 2.1784
Epoch 738/5000
26/26 - 1s - loss: 1.7099 - val_loss: 2.1826
Epoch 739/5000
26/26 - 1s - loss: 1.7079 - val_loss: 2.1801
Epoch 740/5000
26/26 - 1s - loss: 1.7058 - val_loss: 2.1782
Epoch 00740: val_loss improved from 2.19172 to 2.17816, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 741/5000
26/26 - 1s - loss: 1.7031 - val_loss: 2.1797
Epoch 742/5000
26/26 - 1s - loss: 1.7035 - val_loss: 2.1755
Epoch 743/5000
26/26 - 1s - loss: 1.7025 - val_loss: 2.1752
Epoch 744/5000
26/26 - 1s - loss: 1.7020 - val_loss: 2.1733
Epoch 745/5000
26/26 - 1s - loss: 1.6983 - val_loss: 2.1726
Epoch 746/5000
26/26 - 1s - loss: 1.6983 - val_loss: 2.1679
Epoch 747/5000
26/26 - 1s - loss: 1.6958 - val_loss: 2.1696
Epoch 748/5000
26/26 - 1s - loss: 1.6963 - val_loss: 2.1676
Epoch 749/5000
26/26 - 1s - loss: 1.6920 - val_loss: 2.1635
Epoch 750/5000
26/26 - 1s - loss: 1.6923 - val_loss: 2.1635
Epoch 00750: val_loss improved from 2.17816 to 2.16353, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 751/5000
26/26 - 1s - loss: 1.6883 - val_loss: 2.1594
Epoch 752/5000
26/26 - 1s - loss: 1.6884 - val_loss: 2.1599
Epoch 753/5000
26/26 - 1s - loss: 1.6872 - val_loss: 2.1597
Epoch 754/5000
26/26 - 1s - loss: 1.6843 - val_loss: 2.1592
Epoch 755/5000
26/26 - 1s - loss: 1.6853 - val_loss: 2.1570
Epoch 756/5000
26/26 - 1s - loss: 1.6820 - val_loss: 2.1554
Epoch 757/5000
26/26 - 1s - loss: 1.6820 - val_loss: 2.1539
Epoch 758/5000
26/26 - 1s - loss: 1.6801 - val_loss: 2.1539
Epoch 759/5000
26/26 - 1s - loss: 1.6789 - val_loss: 2.1527
Epoch 760/5000
26/26 - 1s - loss: 1.6765 - val_loss: 2.1509
Epoch 00760: val_loss improved from 2.16353 to 2.15088, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 761/5000
26/26 - 1s - loss: 1.6748 - val_loss: 2.1494
Epoch 762/5000
26/26 - 1s - loss: 1.6735 - val_loss: 2.1494
Epoch 763/5000
26/26 - 1s - loss: 1.6715 - val_loss: 2.1464
Epoch 764/5000
26/26 - 1s - loss: 1.6723 - val_loss: 2.1466
Epoch 765/5000
26/26 - 1s - loss: 1.6693 - val_loss: 2.1432
Epoch 766/5000
26/26 - 1s - loss: 1.6706 - val_loss: 2.1457
Epoch 767/5000
26/26 - 1s - loss: 1.6669 - val_loss: 2.1431
Epoch 768/5000
26/26 - 1s - loss: 1.6661 - val_loss: 2.1418
Epoch 769/5000
26/26 - 1s - loss: 1.6656 - val_loss: 2.1390
Epoch 770/5000
26/26 - 1s - loss: 1.6631 - val_loss: 2.1385
Epoch 00770: val_loss improved from 2.15088 to 2.13853, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 771/5000
26/26 - 1s - loss: 1.6628 - val_loss: 2.1350
Epoch 772/5000
26/26 - 1s - loss: 1.6634 - val_loss: 2.1330
Epoch 773/5000
26/26 - 1s - loss: 1.6600 - val_loss: 2.1318
Epoch 774/5000
26/26 - 1s - loss: 1.6586 - val_loss: 2.1311
Epoch 775/5000
26/26 - 1s - loss: 1.6563 - val_loss: 2.1295
Epoch 776/5000
26/26 - 1s - loss: 1.6547 - val_loss: 2.1298
Epoch 777/5000
26/26 - 1s - loss: 1.6532 - val_loss: 2.1265
Epoch 778/5000
26/26 - 1s - loss: 1.6528 - val_loss: 2.1251
Epoch 779/5000
26/26 - 1s - loss: 1.6507 - val_loss: 2.1250
Epoch 780/5000
26/26 - 1s - loss: 1.6484 - val_loss: 2.1237
Epoch 00780: val_loss improved from 2.13853 to 2.12369, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 781/5000
26/26 - 1s - loss: 1.6497 - val_loss: 2.1224
Epoch 782/5000
26/26 - 1s - loss: 1.6461 - val_loss: 2.1192
Epoch 783/5000
26/26 - 1s - loss: 1.6457 - val_loss: 2.1204
Epoch 784/5000
26/26 - 1s - loss: 1.6441 - val_loss: 2.1188
Epoch 785/5000
26/26 - 1s - loss: 1.6442 - val_loss: 2.1146
Epoch 786/5000
26/26 - 1s - loss: 1.6412 - val_loss: 2.1154
Epoch 787/5000
26/26 - 1s - loss: 1.6389 - val_loss: 2.1127
Epoch 788/5000
26/26 - 1s - loss: 1.6393 - val_loss: 2.1122
Epoch 789/5000
26/26 - 1s - loss: 1.6389 - val_loss: 2.1109
Epoch 790/5000
26/26 - 1s - loss: 1.6351 - val_loss: 2.1146
Epoch 00790: val_loss improved from 2.12369 to 2.11458, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 791/5000
26/26 - 1s - loss: 1.6363 - val_loss: 2.1111
Epoch 792/5000
26/26 - 1s - loss: 1.6343 - val_loss: 2.1072
Epoch 793/5000
26/26 - 1s - loss: 1.6297 - val_loss: 2.1085
Epoch 794/5000
26/26 - 1s - loss: 1.6309 - val_loss: 2.1048
Epoch 795/5000
26/26 - 1s - loss: 1.6301 - val_loss: 2.1032
Epoch 796/5000
26/26 - 1s - loss: 1.6279 - val_loss: 2.1043
Epoch 797/5000
26/26 - 1s - loss: 1.6268 - val_loss: 2.1013
Epoch 798/5000
26/26 - 1s - loss: 1.6279 - val_loss: 2.1001
Epoch 799/5000
26/26 - 1s - loss: 1.6237 - val_loss: 2.0976
Epoch 800/5000
26/26 - 1s - loss: 1.6224 - val_loss: 2.0988
Epoch 00800: val_loss improved from 2.11458 to 2.09882, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 801/5000
26/26 - 1s - loss: 1.6224 - val_loss: 2.0942
Epoch 802/5000
26/26 - 1s - loss: 1.6190 - val_loss: 2.0926
Epoch 803/5000
26/26 - 1s - loss: 1.6187 - val_loss: 2.0926
Epoch 804/5000
26/26 - 1s - loss: 1.6177 - val_loss: 2.0905
Epoch 805/5000
26/26 - 1s - loss: 1.6150 - val_loss: 2.0910
Epoch 806/5000
26/26 - 1s - loss: 1.6137 - val_loss: 2.0890
Epoch 807/5000
26/26 - 1s - loss: 1.6123 - val_loss: 2.0869
Epoch 808/5000
26/26 - 1s - loss: 1.6124 - val_loss: 2.0860
Epoch 809/5000
26/26 - 1s - loss: 1.6123 - val_loss: 2.0850
Epoch 810/5000
26/26 - 1s - loss: 1.6095 - val_loss: 2.0821
Epoch 00810: val_loss improved from 2.09882 to 2.08210, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 811/5000
26/26 - 1s - loss: 1.6087 - val_loss: 2.0846
Epoch 812/5000
26/26 - 1s - loss: 1.6075 - val_loss: 2.0828
Epoch 813/5000
26/26 - 1s - loss: 1.6052 - val_loss: 2.0798
Epoch 814/5000
26/26 - 1s - loss: 1.6031 - val_loss: 2.0789
Epoch 815/5000
26/26 - 1s - loss: 1.6023 - val_loss: 2.0783
Epoch 816/5000
26/26 - 2s - loss: 1.6015 - val_loss: 2.0774
Epoch 817/5000
26/26 - 1s - loss: 1.5994 - val_loss: 2.0764
Epoch 818/5000
26/26 - 1s - loss: 1.5978 - val_loss: 2.0749
Epoch 819/5000
26/26 - 1s - loss: 1.5973 - val_loss: 2.0738
Epoch 820/5000
26/26 - 1s - loss: 1.5960 - val_loss: 2.0715
Epoch 00820: val_loss improved from 2.08210 to 2.07146, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 821/5000
26/26 - 1s - loss: 1.5963 - val_loss: 2.0681
Epoch 822/5000
26/26 - 1s - loss: 1.5937 - val_loss: 2.0689
Epoch 823/5000
26/26 - 1s - loss: 1.5927 - val_loss: 2.0676
Epoch 824/5000
26/26 - 1s - loss: 1.5897 - val_loss: 2.0658
Epoch 825/5000
26/26 - 1s - loss: 1.5890 - val_loss: 2.0651
Epoch 826/5000
26/26 - 1s - loss: 1.5884 - val_loss: 2.0628
Epoch 827/5000
26/26 - 1s - loss: 1.5863 - val_loss: 2.0621
Epoch 828/5000
26/26 - 1s - loss: 1.5861 - val_loss: 2.0591
Epoch 829/5000
26/26 - 2s - loss: 1.5864 - val_loss: 2.0589
Epoch 830/5000
26/26 - 1s - loss: 1.5845 - val_loss: 2.0594
Epoch 00830: val_loss improved from 2.07146 to 2.05940, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 831/5000
26/26 - 1s - loss: 1.5803 - val_loss: 2.0562
Epoch 832/5000
26/26 - 1s - loss: 1.5803 - val_loss: 2.0551
Epoch 833/5000
26/26 - 1s - loss: 1.5802 - val_loss: 2.0535
Epoch 834/5000
26/26 - 1s - loss: 1.5788 - val_loss: 2.0535
Epoch 835/5000
26/26 - 1s - loss: 1.5782 - val_loss: 2.0522
Epoch 836/5000
26/26 - 1s - loss: 1.5752 - val_loss: 2.0481
Epoch 837/5000
26/26 - 1s - loss: 1.5731 - val_loss: 2.0488
Epoch 838/5000
26/26 - 1s - loss: 1.5729 - val_loss: 2.0478
Epoch 839/5000
26/26 - 1s - loss: 1.5719 - val_loss: 2.0483
Epoch 840/5000
26/26 - 1s - loss: 1.5735 - val_loss: 2.0439
Epoch 00840: val_loss improved from 2.05940 to 2.04390, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 841/5000
26/26 - 1s - loss: 1.5686 - val_loss: 2.0436
Epoch 842/5000
26/26 - 1s - loss: 1.5685 - val_loss: 2.0418
Epoch 843/5000
26/26 - 1s - loss: 1.5666 - val_loss: 2.0396
Epoch 844/5000
26/26 - 1s - loss: 1.5665 - val_loss: 2.0416
Epoch 845/5000
26/26 - 1s - loss: 1.5638 - val_loss: 2.0388
Epoch 846/5000
26/26 - 1s - loss: 1.5619 - val_loss: 2.0390
Epoch 847/5000
26/26 - 1s - loss: 1.5592 - val_loss: 2.0378
Epoch 848/5000
26/26 - 1s - loss: 1.5597 - val_loss: 2.0354
Epoch 849/5000
26/26 - 1s - loss: 1.5587 - val_loss: 2.0339
Epoch 850/5000
26/26 - 1s - loss: 1.5581 - val_loss: 2.0325
Epoch 00850: val_loss improved from 2.04390 to 2.03248, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 851/5000
26/26 - 1s - loss: 1.5562 - val_loss: 2.0329
Epoch 852/5000
26/26 - 1s - loss: 1.5534 - val_loss: 2.0308
Epoch 853/5000
26/26 - 1s - loss: 1.5528 - val_loss: 2.0311
Epoch 854/5000
26/26 - 2s - loss: 1.5537 - val_loss: 2.0285
Epoch 855/5000
26/26 - 1s - loss: 1.5510 - val_loss: 2.0261
Epoch 856/5000
26/26 - 1s - loss: 1.5503 - val_loss: 2.0264
Epoch 857/5000
26/26 - 1s - loss: 1.5477 - val_loss: 2.0226
Epoch 858/5000
26/26 - 1s - loss: 1.5481 - val_loss: 2.0238
Epoch 859/5000
26/26 - 1s - loss: 1.5471 - val_loss: 2.0209
Epoch 860/5000
26/26 - 1s - loss: 1.5430 - val_loss: 2.0218
Epoch 00860: val_loss improved from 2.03248 to 2.02176, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 861/5000
26/26 - 1s - loss: 1.5422 - val_loss: 2.0197
Epoch 862/5000
26/26 - 1s - loss: 1.5423 - val_loss: 2.0173
Epoch 863/5000
26/26 - 1s - loss: 1.5439 - val_loss: 2.0173
Epoch 864/5000
26/26 - 1s - loss: 1.5405 - val_loss: 2.0170
Epoch 865/5000
26/26 - 1s - loss: 1.5388 - val_loss: 2.0147
Epoch 866/5000
26/26 - 1s - loss: 1.5372 - val_loss: 2.0128
Epoch 867/5000
26/26 - 1s - loss: 1.5361 - val_loss: 2.0143
Epoch 868/5000
26/26 - 1s - loss: 1.5359 - val_loss: 2.0115
Epoch 869/5000
26/26 - 1s - loss: 1.5347 - val_loss: 2.0074
Epoch 870/5000
26/26 - 1s - loss: 1.5322 - val_loss: 2.0098
Epoch 00870: val_loss improved from 2.02176 to 2.00984, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 871/5000
26/26 - 2s - loss: 1.5310 - val_loss: 2.0075
Epoch 872/5000
26/26 - 1s - loss: 1.5294 - val_loss: 2.0056
Epoch 873/5000
26/26 - 1s - loss: 1.5267 - val_loss: 2.0068
Epoch 874/5000
26/26 - 1s - loss: 1.5287 - val_loss: 2.0017
Epoch 875/5000
26/26 - 1s - loss: 1.5261 - val_loss: 1.9997
Epoch 876/5000
26/26 - 1s - loss: 1.5254 - val_loss: 1.9989
Epoch 877/5000
26/26 - 1s - loss: 1.5239 - val_loss: 1.9983
Epoch 878/5000
26/26 - 1s - loss: 1.5241 - val_loss: 1.9982
Epoch 879/5000
26/26 - 1s - loss: 1.5200 - val_loss: 1.9970
Epoch 880/5000
26/26 - 1s - loss: 1.5190 - val_loss: 1.9947
Epoch 00880: val_loss improved from 2.00984 to 1.99469, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 881/5000
26/26 - 1s - loss: 1.5199 - val_loss: 1.9943
Epoch 882/5000
26/26 - 1s - loss: 1.5172 - val_loss: 1.9931
Epoch 883/5000
26/26 - 1s - loss: 1.5159 - val_loss: 1.9931
Epoch 884/5000
26/26 - 1s - loss: 1.5168 - val_loss: 1.9898
Epoch 885/5000
26/26 - 1s - loss: 1.5162 - val_loss: 1.9890
Epoch 886/5000
26/26 - 1s - loss: 1.5137 - val_loss: 1.9897
Epoch 887/5000
26/26 - 1s - loss: 1.5135 - val_loss: 1.9872
Epoch 888/5000
26/26 - 1s - loss: 1.5112 - val_loss: 1.9842
Epoch 889/5000
26/26 - 1s - loss: 1.5086 - val_loss: 1.9850
Epoch 890/5000
26/26 - 2s - loss: 1.5071 - val_loss: 1.9850
Epoch 00890: val_loss improved from 1.99469 to 1.98499, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 891/5000
26/26 - 1s - loss: 1.5055 - val_loss: 1.9843
Epoch 892/5000
26/26 - 1s - loss: 1.5075 - val_loss: 1.9837
Epoch 893/5000
26/26 - 1s - loss: 1.5043 - val_loss: 1.9810
Epoch 894/5000
26/26 - 1s - loss: 1.5035 - val_loss: 1.9793
Epoch 895/5000
26/26 - 1s - loss: 1.5028 - val_loss: 1.9789
Epoch 896/5000
26/26 - 1s - loss: 1.5016 - val_loss: 1.9774
Epoch 897/5000
26/26 - 1s - loss: 1.5003 - val_loss: 1.9744
Epoch 898/5000
26/26 - 1s - loss: 1.4996 - val_loss: 1.9743
Epoch 899/5000
26/26 - 1s - loss: 1.4968 - val_loss: 1.9713
Epoch 900/5000
26/26 - 1s - loss: 1.4965 - val_loss: 1.9700
Epoch 00900: val_loss improved from 1.98499 to 1.96999, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 901/5000
26/26 - 1s - loss: 1.4936 - val_loss: 1.9690
Epoch 902/5000
26/26 - 1s - loss: 1.4935 - val_loss: 1.9684
Epoch 903/5000
26/26 - 1s - loss: 1.4941 - val_loss: 1.9673
Epoch 904/5000
26/26 - 1s - loss: 1.4923 - val_loss: 1.9660
Epoch 905/5000
26/26 - 1s - loss: 1.4895 - val_loss: 1.9641
Epoch 906/5000
26/26 - 1s - loss: 1.4892 - val_loss: 1.9635
Epoch 907/5000
26/26 - 1s - loss: 1.4884 - val_loss: 1.9630
Epoch 908/5000
26/26 - 1s - loss: 1.4851 - val_loss: 1.9608
Epoch 909/5000
26/26 - 1s - loss: 1.4850 - val_loss: 1.9609
Epoch 910/5000
26/26 - 1s - loss: 1.4853 - val_loss: 1.9603
Epoch 00910: val_loss improved from 1.96999 to 1.96035, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 911/5000
26/26 - 1s - loss: 1.4823 - val_loss: 1.9584
Epoch 912/5000
26/26 - 1s - loss: 1.4805 - val_loss: 1.9581
Epoch 913/5000
26/26 - 2s - loss: 1.4822 - val_loss: 1.9554
Epoch 914/5000
26/26 - 1s - loss: 1.4783 - val_loss: 1.9556
Epoch 915/5000
26/26 - 1s - loss: 1.4773 - val_loss: 1.9549
Epoch 916/5000
26/26 - 1s - loss: 1.4765 - val_loss: 1.9534
Epoch 917/5000
26/26 - 1s - loss: 1.4752 - val_loss: 1.9496
Epoch 918/5000
26/26 - 1s - loss: 1.4769 - val_loss: 1.9506
Epoch 919/5000
26/26 - 1s - loss: 1.4741 - val_loss: 1.9484
Epoch 920/5000
26/26 - 1s - loss: 1.4726 - val_loss: 1.9512
Epoch 00920: val_loss improved from 1.96035 to 1.95121, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 921/5000
26/26 - 1s - loss: 1.4726 - val_loss: 1.9464
Epoch 922/5000
26/26 - 1s - loss: 1.4692 - val_loss: 1.9461
Epoch 923/5000
26/26 - 1s - loss: 1.4704 - val_loss: 1.9438
Epoch 924/5000
26/26 - 1s - loss: 1.4673 - val_loss: 1.9451
Epoch 925/5000
26/26 - 1s - loss: 1.4669 - val_loss: 1.9401
Epoch 926/5000
26/26 - 1s - loss: 1.4661 - val_loss: 1.9403
Epoch 927/5000
26/26 - 1s - loss: 1.4647 - val_loss: 1.9398
Epoch 928/5000
26/26 - 1s - loss: 1.4646 - val_loss: 1.9425
Epoch 929/5000
26/26 - 2s - loss: 1.4619 - val_loss: 1.9391
Epoch 930/5000
26/26 - 1s - loss: 1.4604 - val_loss: 1.9366
Epoch 00930: val_loss improved from 1.95121 to 1.93662, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 931/5000
26/26 - 1s - loss: 1.4608 - val_loss: 1.9362
Epoch 932/5000
26/26 - 1s - loss: 1.4576 - val_loss: 1.9350
Epoch 933/5000
26/26 - 1s - loss: 1.4581 - val_loss: 1.9337
Epoch 934/5000
26/26 - 1s - loss: 1.4581 - val_loss: 1.9333
Epoch 935/5000
26/26 - 1s - loss: 1.4551 - val_loss: 1.9338
Epoch 936/5000
26/26 - 1s - loss: 1.4549 - val_loss: 1.9301
Epoch 937/5000
26/26 - 1s - loss: 1.4522 - val_loss: 1.9291
Epoch 938/5000
26/26 - 1s - loss: 1.4521 - val_loss: 1.9258
Epoch 939/5000
26/26 - 1s - loss: 1.4505 - val_loss: 1.9254
Epoch 940/5000
26/26 - 1s - loss: 1.4499 - val_loss: 1.9260
Epoch 00940: val_loss improved from 1.93662 to 1.92600, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 941/5000
26/26 - 1s - loss: 1.4498 - val_loss: 1.9229
Epoch 942/5000
26/26 - 1s - loss: 1.4460 - val_loss: 1.9237
Epoch 943/5000
26/26 - 1s - loss: 1.4453 - val_loss: 1.9243
Epoch 944/5000
26/26 - 1s - loss: 1.4447 - val_loss: 1.9217
Epoch 945/5000
26/26 - 1s - loss: 1.4419 - val_loss: 1.9204
Epoch 946/5000
26/26 - 1s - loss: 1.4422 - val_loss: 1.9200
Epoch 947/5000
26/26 - 1s - loss: 1.4412 - val_loss: 1.9190
Epoch 948/5000
26/26 - 1s - loss: 1.4407 - val_loss: 1.9170
Epoch 949/5000
26/26 - 1s - loss: 1.4392 - val_loss: 1.9156
Epoch 950/5000
26/26 - 2s - loss: 1.4377 - val_loss: 1.9136
Epoch 00950: val_loss improved from 1.92600 to 1.91362, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 951/5000
26/26 - 1s - loss: 1.4383 - val_loss: 1.9148
Epoch 952/5000
26/26 - 1s - loss: 1.4357 - val_loss: 1.9132
Epoch 953/5000
26/26 - 1s - loss: 1.4333 - val_loss: 1.9120
Epoch 954/5000
26/26 - 1s - loss: 1.4342 - val_loss: 1.9137
Epoch 955/5000
26/26 - 1s - loss: 1.4333 - val_loss: 1.9118
Epoch 956/5000
26/26 - 1s - loss: 1.4323 - val_loss: 1.9092
Epoch 957/5000
26/26 - 1s - loss: 1.4295 - val_loss: 1.9063
Epoch 958/5000
26/26 - 1s - loss: 1.4295 - val_loss: 1.9054
Epoch 959/5000
26/26 - 1s - loss: 1.4291 - val_loss: 1.9065
Epoch 960/5000
26/26 - 1s - loss: 1.4262 - val_loss: 1.9033
Epoch 00960: val_loss improved from 1.91362 to 1.90331, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 961/5000
26/26 - 1s - loss: 1.4277 - val_loss: 1.9027
Epoch 962/5000
26/26 - 1s - loss: 1.4247 - val_loss: 1.9009
Epoch 963/5000
26/26 - 1s - loss: 1.4253 - val_loss: 1.9027
Epoch 964/5000
26/26 - 1s - loss: 1.4215 - val_loss: 1.9007
Epoch 965/5000
26/26 - 1s - loss: 1.4203 - val_loss: 1.8989
Epoch 966/5000
26/26 - 1s - loss: 1.4222 - val_loss: 1.8963
Epoch 967/5000
26/26 - 1s - loss: 1.4215 - val_loss: 1.8960
Epoch 968/5000
26/26 - 1s - loss: 1.4187 - val_loss: 1.8975
Epoch 969/5000
26/26 - 1s - loss: 1.4184 - val_loss: 1.8963
Epoch 970/5000
26/26 - 1s - loss: 1.4156 - val_loss: 1.8957
Epoch 00970: val_loss improved from 1.90331 to 1.89565, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 971/5000
26/26 - 1s - loss: 1.4161 - val_loss: 1.8941
Epoch 972/5000
26/26 - 1s - loss: 1.4140 - val_loss: 1.8916
Epoch 973/5000
26/26 - 1s - loss: 1.4130 - val_loss: 1.8915
Epoch 974/5000
26/26 - 1s - loss: 1.4112 - val_loss: 1.8916
Epoch 975/5000
26/26 - 1s - loss: 1.4102 - val_loss: 1.8896
Epoch 976/5000
26/26 - 1s - loss: 1.4109 - val_loss: 1.8875
Epoch 977/5000
26/26 - 1s - loss: 1.4095 - val_loss: 1.8884
Epoch 978/5000
26/26 - 1s - loss: 1.4079 - val_loss: 1.8856
Epoch 979/5000
26/26 - 1s - loss: 1.4066 - val_loss: 1.8841
Epoch 980/5000
26/26 - 1s - loss: 1.4035 - val_loss: 1.8822
Epoch 00980: val_loss improved from 1.89565 to 1.88218, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 981/5000
26/26 - 1s - loss: 1.4070 - val_loss: 1.8813
Epoch 982/5000
26/26 - 1s - loss: 1.4047 - val_loss: 1.8804
Epoch 983/5000
26/26 - 1s - loss: 1.4011 - val_loss: 1.8813
Epoch 984/5000
26/26 - 1s - loss: 1.4004 - val_loss: 1.8777
Epoch 985/5000
26/26 - 1s - loss: 1.4004 - val_loss: 1.8777
Epoch 986/5000
26/26 - 2s - loss: 1.3988 - val_loss: 1.8769
Epoch 987/5000
26/26 - 1s - loss: 1.3981 - val_loss: 1.8777
Epoch 988/5000
26/26 - 2s - loss: 1.3974 - val_loss: 1.8750
Epoch 989/5000
26/26 - 1s - loss: 1.3959 - val_loss: 1.8730
Epoch 990/5000
26/26 - 1s - loss: 1.3952 - val_loss: 1.8736
Epoch 00990: val_loss improved from 1.88218 to 1.87358, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 991/5000
26/26 - 1s - loss: 1.3953 - val_loss: 1.8716
Epoch 992/5000
26/26 - 1s - loss: 1.3931 - val_loss: 1.8703
Epoch 993/5000
26/26 - 1s - loss: 1.3921 - val_loss: 1.8697
Epoch 994/5000
26/26 - 1s - loss: 1.3896 - val_loss: 1.8706
Epoch 995/5000
26/26 - 1s - loss: 1.3915 - val_loss: 1.8677
Epoch 996/5000
26/26 - 2s - loss: 1.3894 - val_loss: 1.8642
Epoch 997/5000
26/26 - 1s - loss: 1.3866 - val_loss: 1.8663
Epoch 998/5000
26/26 - 1s - loss: 1.3859 - val_loss: 1.8661
Epoch 999/5000
26/26 - 1s - loss: 1.3853 - val_loss: 1.8647
Epoch 1000/5000
26/26 - 1s - loss: 1.3850 - val_loss: 1.8631
Epoch 01000: val_loss improved from 1.87358 to 1.86306, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1001/5000
26/26 - 1s - loss: 1.3851 - val_loss: 1.8632
Epoch 1002/5000
26/26 - 1s - loss: 1.3831 - val_loss: 1.8617
Epoch 1003/5000
26/26 - 1s - loss: 1.3801 - val_loss: 1.8587
Epoch 1004/5000
26/26 - 1s - loss: 1.3790 - val_loss: 1.8562
Epoch 1005/5000
26/26 - 1s - loss: 1.3786 - val_loss: 1.8575
Epoch 1006/5000
26/26 - 1s - loss: 1.3793 - val_loss: 1.8562
Epoch 1007/5000
26/26 - 1s - loss: 1.3762 - val_loss: 1.8570
Epoch 1008/5000
26/26 - 1s - loss: 1.3769 - val_loss: 1.8562
Epoch 1009/5000
26/26 - 1s - loss: 1.3755 - val_loss: 1.8524
Epoch 1010/5000
26/26 - 1s - loss: 1.3753 - val_loss: 1.8501
Epoch 01010: val_loss improved from 1.86306 to 1.85008, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1011/5000
26/26 - 1s - loss: 1.3734 - val_loss: 1.8509
Epoch 1012/5000
26/26 - 1s - loss: 1.3724 - val_loss: 1.8488
Epoch 1013/5000
26/26 - 1s - loss: 1.3705 - val_loss: 1.8485
Epoch 1014/5000
26/26 - 1s - loss: 1.3702 - val_loss: 1.8463
Epoch 1015/5000
26/26 - 1s - loss: 1.3698 - val_loss: 1.8470
Epoch 1016/5000
26/26 - 1s - loss: 1.3677 - val_loss: 1.8442
Epoch 1017/5000
26/26 - 1s - loss: 1.3680 - val_loss: 1.8451
Epoch 1018/5000
26/26 - 1s - loss: 1.3658 - val_loss: 1.8438
Epoch 1019/5000
26/26 - 1s - loss: 1.3640 - val_loss: 1.8420
Epoch 1020/5000
26/26 - 1s - loss: 1.3630 - val_loss: 1.8413
Epoch 01020: val_loss improved from 1.85008 to 1.84133, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1021/5000
26/26 - 1s - loss: 1.3617 - val_loss: 1.8399
Epoch 1022/5000
26/26 - 1s - loss: 1.3637 - val_loss: 1.8382
Epoch 1023/5000
26/26 - 1s - loss: 1.3613 - val_loss: 1.8350
Epoch 1024/5000
26/26 - 1s - loss: 1.3605 - val_loss: 1.8360
Epoch 1025/5000
26/26 - 1s - loss: 1.3593 - val_loss: 1.8354
Epoch 1026/5000
26/26 - 1s - loss: 1.3589 - val_loss: 1.8372
Epoch 1027/5000
26/26 - 1s - loss: 1.3573 - val_loss: 1.8350
Epoch 1028/5000
26/26 - 1s - loss: 1.3564 - val_loss: 1.8334
Epoch 1029/5000
26/26 - 1s - loss: 1.3555 - val_loss: 1.8322
Epoch 1030/5000
26/26 - 1s - loss: 1.3538 - val_loss: 1.8340
Epoch 01030: val_loss improved from 1.84133 to 1.83397, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1031/5000
26/26 - 1s - loss: 1.3552 - val_loss: 1.8310
Epoch 1032/5000
26/26 - 2s - loss: 1.3509 - val_loss: 1.8283
Epoch 1033/5000
26/26 - 1s - loss: 1.3498 - val_loss: 1.8287
Epoch 1034/5000
26/26 - 1s - loss: 1.3498 - val_loss: 1.8285
Epoch 1035/5000
26/26 - 2s - loss: 1.3487 - val_loss: 1.8290
Epoch 1036/5000
26/26 - 1s - loss: 1.3470 - val_loss: 1.8275
Epoch 1037/5000
26/26 - 1s - loss: 1.3468 - val_loss: 1.8274
Epoch 1038/5000
26/26 - 1s - loss: 1.3467 - val_loss: 1.8251
Epoch 1039/5000
26/26 - 1s - loss: 1.3446 - val_loss: 1.8241
Epoch 1040/5000
26/26 - 1s - loss: 1.3420 - val_loss: 1.8219
Epoch 01040: val_loss improved from 1.83397 to 1.82185, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1041/5000
26/26 - 1s - loss: 1.3443 - val_loss: 1.8200
Epoch 1042/5000
26/26 - 1s - loss: 1.3395 - val_loss: 1.8210
Epoch 1043/5000
26/26 - 1s - loss: 1.3404 - val_loss: 1.8205
Epoch 1044/5000
26/26 - 1s - loss: 1.3395 - val_loss: 1.8213
Epoch 1045/5000
26/26 - 1s - loss: 1.3397 - val_loss: 1.8181
Epoch 1046/5000
26/26 - 1s - loss: 1.3375 - val_loss: 1.8173
Epoch 1047/5000
26/26 - 1s - loss: 1.3359 - val_loss: 1.8154
Epoch 1048/5000
26/26 - 1s - loss: 1.3349 - val_loss: 1.8167
Epoch 1049/5000
26/26 - 1s - loss: 1.3351 - val_loss: 1.8153
Epoch 1050/5000
26/26 - 1s - loss: 1.3334 - val_loss: 1.8142
Epoch 01050: val_loss improved from 1.82185 to 1.81422, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1051/5000
26/26 - 1s - loss: 1.3344 - val_loss: 1.8106
Epoch 1052/5000
26/26 - 1s - loss: 1.3326 - val_loss: 1.8100
Epoch 1053/5000
26/26 - 1s - loss: 1.3315 - val_loss: 1.8093
Epoch 1054/5000
26/26 - 1s - loss: 1.3319 - val_loss: 1.8100
Epoch 1055/5000
26/26 - 1s - loss: 1.3290 - val_loss: 1.8058
Epoch 1056/5000
26/26 - 1s - loss: 1.3272 - val_loss: 1.8069
Epoch 1057/5000
26/26 - 1s - loss: 1.3278 - val_loss: 1.8067
Epoch 1058/5000
26/26 - 1s - loss: 1.3258 - val_loss: 1.8042
Epoch 1059/5000
26/26 - 1s - loss: 1.3244 - val_loss: 1.8033
Epoch 1060/5000
26/26 - 1s - loss: 1.3233 - val_loss: 1.8024
Epoch 01060: val_loss improved from 1.81422 to 1.80236, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1061/5000
26/26 - 1s - loss: 1.3221 - val_loss: 1.7999
Epoch 1062/5000
26/26 - 1s - loss: 1.3220 - val_loss: 1.8015
Epoch 1063/5000
26/26 - 2s - loss: 1.3215 - val_loss: 1.7987
Epoch 1064/5000
26/26 - 2s - loss: 1.3210 - val_loss: 1.8005
Epoch 1065/5000
26/26 - 1s - loss: 1.3176 - val_loss: 1.7983
Epoch 1066/5000
26/26 - 1s - loss: 1.3198 - val_loss: 1.7957
Epoch 1067/5000
26/26 - 1s - loss: 1.3173 - val_loss: 1.7956
Epoch 1068/5000
26/26 - 1s - loss: 1.3169 - val_loss: 1.7962
Epoch 1069/5000
26/26 - 1s - loss: 1.3163 - val_loss: 1.7942
Epoch 1070/5000
26/26 - 1s - loss: 1.3152 - val_loss: 1.7927
Epoch 01070: val_loss improved from 1.80236 to 1.79270, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1071/5000
26/26 - 1s - loss: 1.3113 - val_loss: 1.7924
Epoch 1072/5000
26/26 - 1s - loss: 1.3125 - val_loss: 1.7896
Epoch 1073/5000
26/26 - 1s - loss: 1.3113 - val_loss: 1.7880
Epoch 1074/5000
26/26 - 1s - loss: 1.3114 - val_loss: 1.7856
Epoch 1075/5000
26/26 - 1s - loss: 1.3083 - val_loss: 1.7864
Epoch 1076/5000
26/26 - 1s - loss: 1.3091 - val_loss: 1.7852
Epoch 1077/5000
26/26 - 1s - loss: 1.3081 - val_loss: 1.7838
Epoch 1078/5000
26/26 - 1s - loss: 1.3046 - val_loss: 1.7857
Epoch 1079/5000
26/26 - 2s - loss: 1.3046 - val_loss: 1.7848
Epoch 1080/5000
26/26 - 1s - loss: 1.3039 - val_loss: 1.7813
Epoch 01080: val_loss improved from 1.79270 to 1.78129, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1081/5000
26/26 - 1s - loss: 1.3046 - val_loss: 1.7831
Epoch 1082/5000
26/26 - 1s - loss: 1.3023 - val_loss: 1.7805
Epoch 1083/5000
26/26 - 1s - loss: 1.3025 - val_loss: 1.7793
Epoch 1084/5000
26/26 - 1s - loss: 1.3020 - val_loss: 1.7771
Epoch 1085/5000
26/26 - 1s - loss: 1.3001 - val_loss: 1.7777
Epoch 1086/5000
26/26 - 1s - loss: 1.2995 - val_loss: 1.7744
Epoch 1087/5000
26/26 - 1s - loss: 1.2985 - val_loss: 1.7765
Epoch 1088/5000
26/26 - 2s - loss: 1.2974 - val_loss: 1.7750
Epoch 1089/5000
26/26 - 2s - loss: 1.2968 - val_loss: 1.7744
Epoch 1090/5000
26/26 - 1s - loss: 1.2950 - val_loss: 1.7764
Epoch 01090: val_loss improved from 1.78129 to 1.77644, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1091/5000
26/26 - 1s - loss: 1.2951 - val_loss: 1.7725
Epoch 1092/5000
26/26 - 1s - loss: 1.2944 - val_loss: 1.7715
Epoch 1093/5000
26/26 - 1s - loss: 1.2925 - val_loss: 1.7696
Epoch 1094/5000
26/26 - 1s - loss: 1.2905 - val_loss: 1.7709
Epoch 1095/5000
26/26 - 1s - loss: 1.2893 - val_loss: 1.7705
Epoch 1096/5000
26/26 - 1s - loss: 1.2885 - val_loss: 1.7669
Epoch 1097/5000
26/26 - 1s - loss: 1.2880 - val_loss: 1.7683
Epoch 1098/5000
26/26 - 1s - loss: 1.2877 - val_loss: 1.7674
Epoch 1099/5000
26/26 - 1s - loss: 1.2872 - val_loss: 1.7653
Epoch 1100/5000
26/26 - 1s - loss: 1.2854 - val_loss: 1.7631
Epoch 01100: val_loss improved from 1.77644 to 1.76307, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1101/5000
26/26 - 1s - loss: 1.2858 - val_loss: 1.7628
Epoch 1102/5000
26/26 - 1s - loss: 1.2850 - val_loss: 1.7618
Epoch 1103/5000
26/26 - 1s - loss: 1.2821 - val_loss: 1.7618
Epoch 1104/5000
26/26 - 1s - loss: 1.2813 - val_loss: 1.7620
Epoch 1105/5000
26/26 - 2s - loss: 1.2819 - val_loss: 1.7602
Epoch 1106/5000
26/26 - 1s - loss: 1.2802 - val_loss: 1.7592
Epoch 1107/5000
26/26 - 1s - loss: 1.2778 - val_loss: 1.7590
Epoch 1108/5000
26/26 - 1s - loss: 1.2786 - val_loss: 1.7587
Epoch 1109/5000
26/26 - 1s - loss: 1.2782 - val_loss: 1.7561
Epoch 1110/5000
26/26 - 1s - loss: 1.2773 - val_loss: 1.7543
Epoch 01110: val_loss improved from 1.76307 to 1.75432, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1111/5000
26/26 - 1s - loss: 1.2765 - val_loss: 1.7536
Epoch 1112/5000
26/26 - 1s - loss: 1.2755 - val_loss: 1.7525
Epoch 1113/5000
26/26 - 1s - loss: 1.2745 - val_loss: 1.7540
Epoch 1114/5000
26/26 - 1s - loss: 1.2744 - val_loss: 1.7528
Epoch 1115/5000
26/26 - 1s - loss: 1.2730 - val_loss: 1.7524
Epoch 1116/5000
26/26 - 1s - loss: 1.2721 - val_loss: 1.7517
Epoch 1117/5000
26/26 - 1s - loss: 1.2695 - val_loss: 1.7474
Epoch 1118/5000
26/26 - 1s - loss: 1.2697 - val_loss: 1.7482
Epoch 1119/5000
26/26 - 1s - loss: 1.2691 - val_loss: 1.7486
Epoch 1120/5000
26/26 - 1s - loss: 1.2668 - val_loss: 1.7465
Epoch 01120: val_loss improved from 1.75432 to 1.74652, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1121/5000
26/26 - 1s - loss: 1.2665 - val_loss: 1.7472
Epoch 1122/5000
26/26 - 1s - loss: 1.2653 - val_loss: 1.7425
Epoch 1123/5000
26/26 - 1s - loss: 1.2646 - val_loss: 1.7437
Epoch 1124/5000
26/26 - 1s - loss: 1.2650 - val_loss: 1.7449
Epoch 1125/5000
26/26 - 1s - loss: 1.2631 - val_loss: 1.7423
Epoch 1126/5000
26/26 - 1s - loss: 1.2624 - val_loss: 1.7420
Epoch 1127/5000
26/26 - 1s - loss: 1.2611 - val_loss: 1.7388
Epoch 1128/5000
26/26 - 1s - loss: 1.2610 - val_loss: 1.7419
Epoch 1129/5000
26/26 - 1s - loss: 1.2587 - val_loss: 1.7406
Epoch 1130/5000
26/26 - 1s - loss: 1.2587 - val_loss: 1.7399
Epoch 01130: val_loss improved from 1.74652 to 1.73994, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1131/5000
26/26 - 1s - loss: 1.2571 - val_loss: 1.7359
Epoch 1132/5000
26/26 - 1s - loss: 1.2558 - val_loss: 1.7354
Epoch 1133/5000
26/26 - 1s - loss: 1.2565 - val_loss: 1.7336
Epoch 1134/5000
26/26 - 1s - loss: 1.2542 - val_loss: 1.7356
Epoch 1135/5000
26/26 - 1s - loss: 1.2548 - val_loss: 1.7324
Epoch 1136/5000
26/26 - 1s - loss: 1.2526 - val_loss: 1.7309
Epoch 1137/5000
26/26 - 1s - loss: 1.2524 - val_loss: 1.7292
Epoch 1138/5000
26/26 - 1s - loss: 1.2520 - val_loss: 1.7287
Epoch 1139/5000
26/26 - 1s - loss: 1.2503 - val_loss: 1.7286
Epoch 1140/5000
26/26 - 1s - loss: 1.2504 - val_loss: 1.7297
Epoch 01140: val_loss improved from 1.73994 to 1.72974, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1141/5000
26/26 - 1s - loss: 1.2470 - val_loss: 1.7288
Epoch 1142/5000
26/26 - 1s - loss: 1.2479 - val_loss: 1.7267
Epoch 1143/5000
26/26 - 1s - loss: 1.2469 - val_loss: 1.7251
Epoch 1144/5000
26/26 - 1s - loss: 1.2462 - val_loss: 1.7269
Epoch 1145/5000
26/26 - 1s - loss: 1.2437 - val_loss: 1.7246
Epoch 1146/5000
26/26 - 1s - loss: 1.2432 - val_loss: 1.7244
Epoch 1147/5000
26/26 - 1s - loss: 1.2429 - val_loss: 1.7221
Epoch 1148/5000
26/26 - 1s - loss: 1.2432 - val_loss: 1.7229
Epoch 1149/5000
26/26 - 1s - loss: 1.2420 - val_loss: 1.7235
Epoch 1150/5000
26/26 - 1s - loss: 1.2400 - val_loss: 1.7226
Epoch 01150: val_loss improved from 1.72974 to 1.72264, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1151/5000
26/26 - 1s - loss: 1.2393 - val_loss: 1.7221
Epoch 1152/5000
26/26 - 1s - loss: 1.2377 - val_loss: 1.7183
Epoch 1153/5000
26/26 - 1s - loss: 1.2391 - val_loss: 1.7177
Epoch 1154/5000
26/26 - 1s - loss: 1.2372 - val_loss: 1.7187
Epoch 1155/5000
26/26 - 1s - loss: 1.2359 - val_loss: 1.7190
Epoch 1156/5000
26/26 - 1s - loss: 1.2347 - val_loss: 1.7143
Epoch 1157/5000
26/26 - 1s - loss: 1.2361 - val_loss: 1.7139
Epoch 1158/5000
26/26 - 1s - loss: 1.2329 - val_loss: 1.7146
Epoch 1159/5000
26/26 - 1s - loss: 1.2329 - val_loss: 1.7140
Epoch 1160/5000
26/26 - 1s - loss: 1.2312 - val_loss: 1.7109
Epoch 01160: val_loss improved from 1.72264 to 1.71094, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1161/5000
26/26 - 1s - loss: 1.2312 - val_loss: 1.7110
Epoch 1162/5000
26/26 - 2s - loss: 1.2311 - val_loss: 1.7087
Epoch 1163/5000
26/26 - 1s - loss: 1.2288 - val_loss: 1.7086
Epoch 1164/5000
26/26 - 1s - loss: 1.2290 - val_loss: 1.7061
Epoch 1165/5000
26/26 - 1s - loss: 1.2284 - val_loss: 1.7082
Epoch 1166/5000
26/26 - 1s - loss: 1.2268 - val_loss: 1.7052
Epoch 1167/5000
26/26 - 1s - loss: 1.2269 - val_loss: 1.7059
Epoch 1168/5000
26/26 - 1s - loss: 1.2255 - val_loss: 1.7060
Epoch 1169/5000
26/26 - 1s - loss: 1.2236 - val_loss: 1.7036
Epoch 1170/5000
26/26 - 1s - loss: 1.2235 - val_loss: 1.7019
Epoch 01170: val_loss improved from 1.71094 to 1.70185, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1171/5000
26/26 - 1s - loss: 1.2228 - val_loss: 1.7007
Epoch 1172/5000
26/26 - 1s - loss: 1.2215 - val_loss: 1.6999
Epoch 1173/5000
26/26 - 1s - loss: 1.2205 - val_loss: 1.6988
Epoch 1174/5000
26/26 - 1s - loss: 1.2198 - val_loss: 1.6977
Epoch 1175/5000
26/26 - 1s - loss: 1.2190 - val_loss: 1.6986
Epoch 1176/5000
26/26 - 1s - loss: 1.2172 - val_loss: 1.6964
Epoch 1177/5000
26/26 - 1s - loss: 1.2164 - val_loss: 1.6942
Epoch 1178/5000
26/26 - 1s - loss: 1.2161 - val_loss: 1.6957
Epoch 1179/5000
26/26 - 1s - loss: 1.2154 - val_loss: 1.6944
Epoch 1180/5000
26/26 - 1s - loss: 1.2145 - val_loss: 1.6940
Epoch 01180: val_loss improved from 1.70185 to 1.69401, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1181/5000
26/26 - 1s - loss: 1.2136 - val_loss: 1.6923
Epoch 1182/5000
26/26 - 1s - loss: 1.2131 - val_loss: 1.6907
Epoch 1183/5000
26/26 - 1s - loss: 1.2105 - val_loss: 1.6912
Epoch 1184/5000
26/26 - 1s - loss: 1.2100 - val_loss: 1.6913
Epoch 1185/5000
26/26 - 1s - loss: 1.2115 - val_loss: 1.6908
Epoch 1186/5000
26/26 - 1s - loss: 1.2085 - val_loss: 1.6876
Epoch 1187/5000
26/26 - 1s - loss: 1.2103 - val_loss: 1.6881
Epoch 1188/5000
26/26 - 1s - loss: 1.2082 - val_loss: 1.6888
Epoch 1189/5000
26/26 - 1s - loss: 1.2050 - val_loss: 1.6851
Epoch 1190/5000
26/26 - 1s - loss: 1.2077 - val_loss: 1.6856
Epoch 01190: val_loss improved from 1.69401 to 1.68563, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1191/5000
26/26 - 1s - loss: 1.2056 - val_loss: 1.6871
Epoch 1192/5000
26/26 - 1s - loss: 1.2057 - val_loss: 1.6847
Epoch 1193/5000
26/26 - 1s - loss: 1.2046 - val_loss: 1.6834
Epoch 1194/5000
26/26 - 1s - loss: 1.2022 - val_loss: 1.6827
Epoch 1195/5000
26/26 - 1s - loss: 1.2012 - val_loss: 1.6814
Epoch 1196/5000
26/26 - 1s - loss: 1.2020 - val_loss: 1.6815
Epoch 1197/5000
26/26 - 1s - loss: 1.2007 - val_loss: 1.6807
Epoch 1198/5000
26/26 - 1s - loss: 1.1991 - val_loss: 1.6802
Epoch 1199/5000
26/26 - 1s - loss: 1.1986 - val_loss: 1.6809
Epoch 1200/5000
26/26 - 1s - loss: 1.1970 - val_loss: 1.6780
Epoch 01200: val_loss improved from 1.68563 to 1.67800, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1201/5000
26/26 - 1s - loss: 1.1979 - val_loss: 1.6735
Epoch 1202/5000
26/26 - 1s - loss: 1.1973 - val_loss: 1.6767
Epoch 1203/5000
26/26 - 1s - loss: 1.1948 - val_loss: 1.6750
Epoch 1204/5000
26/26 - 1s - loss: 1.1937 - val_loss: 1.6726
Epoch 1205/5000
26/26 - 1s - loss: 1.1956 - val_loss: 1.6743
Epoch 1206/5000
26/26 - 1s - loss: 1.1926 - val_loss: 1.6723
Epoch 1207/5000
26/26 - 1s - loss: 1.1915 - val_loss: 1.6730
Epoch 1208/5000
26/26 - 1s - loss: 1.1902 - val_loss: 1.6704
Epoch 1209/5000
26/26 - 1s - loss: 1.1898 - val_loss: 1.6687
Epoch 1210/5000
26/26 - 1s - loss: 1.1899 - val_loss: 1.6658
Epoch 01210: val_loss improved from 1.67800 to 1.66580, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1211/5000
26/26 - 1s - loss: 1.1883 - val_loss: 1.6698
Epoch 1212/5000
26/26 - 1s - loss: 1.1882 - val_loss: 1.6674
Epoch 1213/5000
26/26 - 1s - loss: 1.1873 - val_loss: 1.6667
Epoch 1214/5000
26/26 - 1s - loss: 1.1872 - val_loss: 1.6680
Epoch 1215/5000
26/26 - 1s - loss: 1.1863 - val_loss: 1.6679
Epoch 1216/5000
26/26 - 1s - loss: 1.1845 - val_loss: 1.6650
Epoch 1217/5000
26/26 - 1s - loss: 1.1843 - val_loss: 1.6632
Epoch 1218/5000
26/26 - 1s - loss: 1.1824 - val_loss: 1.6628
Epoch 1219/5000
26/26 - 1s - loss: 1.1831 - val_loss: 1.6629
Epoch 1220/5000
26/26 - 1s - loss: 1.1824 - val_loss: 1.6630
Epoch 01220: val_loss improved from 1.66580 to 1.66297, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1221/5000
26/26 - 1s - loss: 1.1794 - val_loss: 1.6594
Epoch 1222/5000
26/26 - 1s - loss: 1.1796 - val_loss: 1.6608
Epoch 1223/5000
26/26 - 1s - loss: 1.1792 - val_loss: 1.6580
Epoch 1224/5000
26/26 - 1s - loss: 1.1778 - val_loss: 1.6573
Epoch 1225/5000
26/26 - 1s - loss: 1.1766 - val_loss: 1.6570
Epoch 1226/5000
26/26 - 1s - loss: 1.1769 - val_loss: 1.6558
Epoch 1227/5000
26/26 - 1s - loss: 1.1748 - val_loss: 1.6557
Epoch 1228/5000
26/26 - 1s - loss: 1.1736 - val_loss: 1.6563
Epoch 1229/5000
26/26 - 1s - loss: 1.1738 - val_loss: 1.6550
Epoch 1230/5000
26/26 - 1s - loss: 1.1742 - val_loss: 1.6539
Epoch 01230: val_loss improved from 1.66297 to 1.65386, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1231/5000
26/26 - 1s - loss: 1.1721 - val_loss: 1.6534
Epoch 1232/5000
26/26 - 1s - loss: 1.1714 - val_loss: 1.6513
Epoch 1233/5000
26/26 - 1s - loss: 1.1714 - val_loss: 1.6513
Epoch 1234/5000
26/26 - 1s - loss: 1.1703 - val_loss: 1.6507
Epoch 1235/5000
26/26 - 1s - loss: 1.1702 - val_loss: 1.6520
Epoch 1236/5000
26/26 - 1s - loss: 1.1696 - val_loss: 1.6504
Epoch 1237/5000
26/26 - 1s - loss: 1.1675 - val_loss: 1.6485
Epoch 1238/5000
26/26 - 1s - loss: 1.1678 - val_loss: 1.6479
Epoch 1239/5000
26/26 - 1s - loss: 1.1647 - val_loss: 1.6471
Epoch 1240/5000
26/26 - 1s - loss: 1.1644 - val_loss: 1.6450
Epoch 01240: val_loss improved from 1.65386 to 1.64496, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1241/5000
26/26 - 1s - loss: 1.1660 - val_loss: 1.6432
Epoch 1242/5000
26/26 - 1s - loss: 1.1623 - val_loss: 1.6425
Epoch 1243/5000
26/26 - 1s - loss: 1.1637 - val_loss: 1.6432
Epoch 1244/5000
26/26 - 1s - loss: 1.1628 - val_loss: 1.6428
Epoch 1245/5000
26/26 - 1s - loss: 1.1616 - val_loss: 1.6430
Epoch 1246/5000
26/26 - 1s - loss: 1.1604 - val_loss: 1.6422
Epoch 1247/5000
26/26 - 2s - loss: 1.1599 - val_loss: 1.6395
Epoch 1248/5000
26/26 - 1s - loss: 1.1592 - val_loss: 1.6399
Epoch 1249/5000
26/26 - 1s - loss: 1.1578 - val_loss: 1.6383
Epoch 1250/5000
26/26 - 1s - loss: 1.1581 - val_loss: 1.6367
Epoch 01250: val_loss improved from 1.64496 to 1.63667, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1251/5000
26/26 - 1s - loss: 1.1562 - val_loss: 1.6376
Epoch 1252/5000
26/26 - 1s - loss: 1.1567 - val_loss: 1.6368
Epoch 1253/5000
26/26 - 1s - loss: 1.1564 - val_loss: 1.6368
Epoch 1254/5000
26/26 - 1s - loss: 1.1531 - val_loss: 1.6356
Epoch 1255/5000
26/26 - 1s - loss: 1.1535 - val_loss: 1.6351
Epoch 1256/5000
26/26 - 1s - loss: 1.1513 - val_loss: 1.6341
Epoch 1257/5000
26/26 - 1s - loss: 1.1529 - val_loss: 1.6349
Epoch 1258/5000
26/26 - 1s - loss: 1.1517 - val_loss: 1.6336
Epoch 1259/5000
26/26 - 1s - loss: 1.1505 - val_loss: 1.6309
Epoch 1260/5000
26/26 - 1s - loss: 1.1503 - val_loss: 1.6306
Epoch 01260: val_loss improved from 1.63667 to 1.63058, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1261/5000
26/26 - 1s - loss: 1.1490 - val_loss: 1.6301
Epoch 1262/5000
26/26 - 1s - loss: 1.1475 - val_loss: 1.6279
Epoch 1263/5000
26/26 - 1s - loss: 1.1468 - val_loss: 1.6270
Epoch 1264/5000
26/26 - 1s - loss: 1.1458 - val_loss: 1.6267
Epoch 1265/5000
26/26 - 1s - loss: 1.1446 - val_loss: 1.6268
Epoch 1266/5000
26/26 - 1s - loss: 1.1453 - val_loss: 1.6275
Epoch 1267/5000
26/26 - 1s - loss: 1.1426 - val_loss: 1.6247
Epoch 1268/5000
26/26 - 1s - loss: 1.1441 - val_loss: 1.6250
Epoch 1269/5000
26/26 - 1s - loss: 1.1423 - val_loss: 1.6235
Epoch 1270/5000
26/26 - 1s - loss: 1.1425 - val_loss: 1.6253
Epoch 01270: val_loss improved from 1.63058 to 1.62534, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1271/5000
26/26 - 1s - loss: 1.1413 - val_loss: 1.6229
Epoch 1272/5000
26/26 - 1s - loss: 1.1392 - val_loss: 1.6230
Epoch 1273/5000
26/26 - 1s - loss: 1.1407 - val_loss: 1.6202
Epoch 1274/5000
26/26 - 1s - loss: 1.1389 - val_loss: 1.6194
Epoch 1275/5000
26/26 - 1s - loss: 1.1391 - val_loss: 1.6206
Epoch 1276/5000
26/26 - 1s - loss: 1.1370 - val_loss: 1.6192
Epoch 1277/5000
26/26 - 1s - loss: 1.1370 - val_loss: 1.6183
Epoch 1278/5000
26/26 - 1s - loss: 1.1343 - val_loss: 1.6157
Epoch 1279/5000
26/26 - 1s - loss: 1.1343 - val_loss: 1.6141
Epoch 1280/5000
26/26 - 1s - loss: 1.1359 - val_loss: 1.6160
Epoch 01280: val_loss improved from 1.62534 to 1.61596, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1281/5000
26/26 - 1s - loss: 1.1343 - val_loss: 1.6152
Epoch 1282/5000
26/26 - 1s - loss: 1.1333 - val_loss: 1.6131
Epoch 1283/5000
26/26 - 1s - loss: 1.1329 - val_loss: 1.6143
Epoch 1284/5000
26/26 - 1s - loss: 1.1332 - val_loss: 1.6127
Epoch 1285/5000
26/26 - 1s - loss: 1.1299 - val_loss: 1.6082
Epoch 1286/5000
26/26 - 1s - loss: 1.1305 - val_loss: 1.6122
Epoch 1287/5000
26/26 - 1s - loss: 1.1283 - val_loss: 1.6077
Epoch 1288/5000
26/26 - 1s - loss: 1.1274 - val_loss: 1.6098
Epoch 1289/5000
26/26 - 1s - loss: 1.1286 - val_loss: 1.6090
Epoch 1290/5000
26/26 - 1s - loss: 1.1283 - val_loss: 1.6077
Epoch 01290: val_loss improved from 1.61596 to 1.60769, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1291/5000
26/26 - 1s - loss: 1.1263 - val_loss: 1.6071
Epoch 1292/5000
26/26 - 1s - loss: 1.1249 - val_loss: 1.6077
Epoch 1293/5000
26/26 - 1s - loss: 1.1234 - val_loss: 1.6045
Epoch 1294/5000
26/26 - 1s - loss: 1.1247 - val_loss: 1.6050
Epoch 1295/5000
26/26 - 1s - loss: 1.1225 - val_loss: 1.6049
Epoch 1296/5000
26/26 - 1s - loss: 1.1217 - val_loss: 1.6031
Epoch 1297/5000
26/26 - 1s - loss: 1.1238 - val_loss: 1.6017
Epoch 1298/5000
26/26 - 1s - loss: 1.1197 - val_loss: 1.6022
Epoch 1299/5000
26/26 - 1s - loss: 1.1193 - val_loss: 1.6008
Epoch 1300/5000
26/26 - 1s - loss: 1.1198 - val_loss: 1.6007
Epoch 01300: val_loss improved from 1.60769 to 1.60068, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1301/5000
26/26 - 1s - loss: 1.1183 - val_loss: 1.6002
Epoch 1302/5000
26/26 - 1s - loss: 1.1184 - val_loss: 1.5993
Epoch 1303/5000
26/26 - 1s - loss: 1.1184 - val_loss: 1.5984
Epoch 1304/5000
26/26 - 1s - loss: 1.1169 - val_loss: 1.5965
Epoch 1305/5000
26/26 - 1s - loss: 1.1172 - val_loss: 1.5978
Epoch 1306/5000
26/26 - 1s - loss: 1.1155 - val_loss: 1.5952
Epoch 1307/5000
26/26 - 1s - loss: 1.1127 - val_loss: 1.5952
Epoch 1308/5000
26/26 - 1s - loss: 1.1144 - val_loss: 1.5923
Epoch 1309/5000
26/26 - 1s - loss: 1.1134 - val_loss: 1.5920
Epoch 1310/5000
26/26 - 1s - loss: 1.1119 - val_loss: 1.5931
Epoch 01310: val_loss improved from 1.60068 to 1.59312, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1311/5000
26/26 - 1s - loss: 1.1138 - val_loss: 1.5934
Epoch 1312/5000
26/26 - 1s - loss: 1.1115 - val_loss: 1.5900
Epoch 1313/5000
26/26 - 1s - loss: 1.1106 - val_loss: 1.5893
Epoch 1314/5000
26/26 - 1s - loss: 1.1095 - val_loss: 1.5899
Epoch 1315/5000
26/26 - 1s - loss: 1.1076 - val_loss: 1.5897
Epoch 1316/5000
26/26 - 1s - loss: 1.1090 - val_loss: 1.5885
Epoch 1317/5000
26/26 - 1s - loss: 1.1070 - val_loss: 1.5866
Epoch 1318/5000
26/26 - 1s - loss: 1.1064 - val_loss: 1.5877
Epoch 1319/5000
26/26 - 1s - loss: 1.1049 - val_loss: 1.5831
Epoch 1320/5000
26/26 - 1s - loss: 1.1047 - val_loss: 1.5864
Epoch 01320: val_loss improved from 1.59312 to 1.58644, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1321/5000
26/26 - 2s - loss: 1.1035 - val_loss: 1.5880
Epoch 1322/5000
26/26 - 1s - loss: 1.1027 - val_loss: 1.5877
Epoch 1323/5000
26/26 - 1s - loss: 1.1035 - val_loss: 1.5862
Epoch 1324/5000
26/26 - 1s - loss: 1.1025 - val_loss: 1.5838
Epoch 1325/5000
26/26 - 1s - loss: 1.1012 - val_loss: 1.5823
Epoch 1326/5000
26/26 - 1s - loss: 1.1000 - val_loss: 1.5819
Epoch 1327/5000
26/26 - 1s - loss: 1.0986 - val_loss: 1.5822
Epoch 1328/5000
26/26 - 1s - loss: 1.0984 - val_loss: 1.5805
Epoch 1329/5000
26/26 - 1s - loss: 1.0993 - val_loss: 1.5794
Epoch 1330/5000
26/26 - 1s - loss: 1.0988 - val_loss: 1.5784
Epoch 01330: val_loss improved from 1.58644 to 1.57839, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1331/5000
26/26 - 1s - loss: 1.0967 - val_loss: 1.5781
Epoch 1332/5000
26/26 - 1s - loss: 1.0972 - val_loss: 1.5789
Epoch 1333/5000
26/26 - 1s - loss: 1.0962 - val_loss: 1.5761
Epoch 1334/5000
26/26 - 2s - loss: 1.0952 - val_loss: 1.5751
Epoch 1335/5000
26/26 - 1s - loss: 1.0927 - val_loss: 1.5751
Epoch 1336/5000
26/26 - 1s - loss: 1.0940 - val_loss: 1.5745
Epoch 1337/5000
26/26 - 1s - loss: 1.0912 - val_loss: 1.5731
Epoch 1338/5000
26/26 - 1s - loss: 1.0921 - val_loss: 1.5722
Epoch 1339/5000
26/26 - 1s - loss: 1.0918 - val_loss: 1.5724
Epoch 1340/5000
26/26 - 1s - loss: 1.0894 - val_loss: 1.5722
Epoch 01340: val_loss improved from 1.57839 to 1.57223, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1341/5000
26/26 - 1s - loss: 1.0902 - val_loss: 1.5717
Epoch 1342/5000
26/26 - 1s - loss: 1.0884 - val_loss: 1.5702
Epoch 1343/5000
26/26 - 1s - loss: 1.0895 - val_loss: 1.5696
Epoch 1344/5000
26/26 - 1s - loss: 1.0888 - val_loss: 1.5652
Epoch 1345/5000
26/26 - 1s - loss: 1.0886 - val_loss: 1.5666
Epoch 1346/5000
26/26 - 1s - loss: 1.0867 - val_loss: 1.5665
Epoch 1347/5000
26/26 - 2s - loss: 1.0853 - val_loss: 1.5676
Epoch 1348/5000
26/26 - 1s - loss: 1.0847 - val_loss: 1.5657
Epoch 1349/5000
26/26 - 1s - loss: 1.0862 - val_loss: 1.5658
Epoch 1350/5000
26/26 - 1s - loss: 1.0832 - val_loss: 1.5648
Epoch 01350: val_loss improved from 1.57223 to 1.56482, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1351/5000
26/26 - 1s - loss: 1.0823 - val_loss: 1.5650
Epoch 1352/5000
26/26 - 1s - loss: 1.0822 - val_loss: 1.5639
Epoch 1353/5000
26/26 - 1s - loss: 1.0821 - val_loss: 1.5642
Epoch 1354/5000
26/26 - 2s - loss: 1.0802 - val_loss: 1.5622
Epoch 1355/5000
26/26 - 1s - loss: 1.0811 - val_loss: 1.5610
Epoch 1356/5000
26/26 - 1s - loss: 1.0792 - val_loss: 1.5603
Epoch 1357/5000
26/26 - 1s - loss: 1.0803 - val_loss: 1.5578
Epoch 1358/5000
26/26 - 1s - loss: 1.0785 - val_loss: 1.5612
Epoch 1359/5000
26/26 - 1s - loss: 1.0769 - val_loss: 1.5575
Epoch 1360/5000
26/26 - 1s - loss: 1.0780 - val_loss: 1.5576
Epoch 01360: val_loss improved from 1.56482 to 1.55760, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1361/5000
26/26 - 1s - loss: 1.0751 - val_loss: 1.5579
Epoch 1362/5000
26/26 - 1s - loss: 1.0747 - val_loss: 1.5572
Epoch 1363/5000
26/26 - 1s - loss: 1.0761 - val_loss: 1.5568
Epoch 1364/5000
26/26 - 1s - loss: 1.0745 - val_loss: 1.5566
Epoch 1365/5000
26/26 - 1s - loss: 1.0735 - val_loss: 1.5540
Epoch 1366/5000
26/26 - 1s - loss: 1.0729 - val_loss: 1.5540
Epoch 1367/5000
26/26 - 1s - loss: 1.0719 - val_loss: 1.5541
Epoch 1368/5000
26/26 - 1s - loss: 1.0715 - val_loss: 1.5553
Epoch 1369/5000
26/26 - 1s - loss: 1.0714 - val_loss: 1.5544
Epoch 1370/5000
26/26 - 1s - loss: 1.0711 - val_loss: 1.5518
Epoch 01370: val_loss improved from 1.55760 to 1.55183, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1371/5000
26/26 - 2s - loss: 1.0696 - val_loss: 1.5519
Epoch 1372/5000
26/26 - 2s - loss: 1.0706 - val_loss: 1.5488
Epoch 1373/5000
26/26 - 2s - loss: 1.0682 - val_loss: 1.5473
Epoch 1374/5000
26/26 - 1s - loss: 1.0678 - val_loss: 1.5481
Epoch 1375/5000
26/26 - 1s - loss: 1.0662 - val_loss: 1.5475
Epoch 1376/5000
26/26 - 1s - loss: 1.0659 - val_loss: 1.5469
Epoch 1377/5000
26/26 - 1s - loss: 1.0651 - val_loss: 1.5457
Epoch 1378/5000
26/26 - 1s - loss: 1.0654 - val_loss: 1.5454
Epoch 1379/5000
26/26 - 1s - loss: 1.0642 - val_loss: 1.5470
Epoch 1380/5000
26/26 - 1s - loss: 1.0637 - val_loss: 1.5457
Epoch 01380: val_loss improved from 1.55183 to 1.54568, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1381/5000
26/26 - 1s - loss: 1.0617 - val_loss: 1.5428
Epoch 1382/5000
26/26 - 1s - loss: 1.0623 - val_loss: 1.5428
Epoch 1383/5000
26/26 - 1s - loss: 1.0625 - val_loss: 1.5428
Epoch 1384/5000
26/26 - 1s - loss: 1.0604 - val_loss: 1.5435
Epoch 1385/5000
26/26 - 1s - loss: 1.0613 - val_loss: 1.5433
Epoch 1386/5000
26/26 - 1s - loss: 1.0582 - val_loss: 1.5378
Epoch 1387/5000
26/26 - 1s - loss: 1.0582 - val_loss: 1.5392
Epoch 1388/5000
26/26 - 1s - loss: 1.0572 - val_loss: 1.5390
Epoch 1389/5000
26/26 - 1s - loss: 1.0573 - val_loss: 1.5387
Epoch 1390/5000
26/26 - 1s - loss: 1.0568 - val_loss: 1.5376
Epoch 01390: val_loss improved from 1.54568 to 1.53762, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1391/5000
26/26 - 1s - loss: 1.0548 - val_loss: 1.5372
Epoch 1392/5000
26/26 - 1s - loss: 1.0563 - val_loss: 1.5348
Epoch 1393/5000
26/26 - 1s - loss: 1.0548 - val_loss: 1.5352
Epoch 1394/5000
26/26 - 1s - loss: 1.0538 - val_loss: 1.5363
Epoch 1395/5000
26/26 - 1s - loss: 1.0527 - val_loss: 1.5348
Epoch 1396/5000
26/26 - 1s - loss: 1.0519 - val_loss: 1.5337
Epoch 1397/5000
26/26 - 1s - loss: 1.0529 - val_loss: 1.5341
Epoch 1398/5000
26/26 - 1s - loss: 1.0518 - val_loss: 1.5313
Epoch 1399/5000
26/26 - 1s - loss: 1.0497 - val_loss: 1.5315
Epoch 1400/5000
26/26 - 1s - loss: 1.0510 - val_loss: 1.5291
Epoch 01400: val_loss improved from 1.53762 to 1.52906, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1401/5000
26/26 - 1s - loss: 1.0502 - val_loss: 1.5289
Epoch 1402/5000
26/26 - 1s - loss: 1.0492 - val_loss: 1.5275
Epoch 1403/5000
26/26 - 1s - loss: 1.0473 - val_loss: 1.5259
Epoch 1404/5000
26/26 - 1s - loss: 1.0473 - val_loss: 1.5266
Epoch 1405/5000
26/26 - 1s - loss: 1.0474 - val_loss: 1.5259
Epoch 1406/5000
26/26 - 1s - loss: 1.0455 - val_loss: 1.5251
Epoch 1407/5000
26/26 - 1s - loss: 1.0443 - val_loss: 1.5252
Epoch 1408/5000
26/26 - 1s - loss: 1.0458 - val_loss: 1.5231
Epoch 1409/5000
26/26 - 1s - loss: 1.0449 - val_loss: 1.5241
Epoch 1410/5000
26/26 - 1s - loss: 1.0426 - val_loss: 1.5254
Epoch 01410: val_loss improved from 1.52906 to 1.52540, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1411/5000
26/26 - 1s - loss: 1.0415 - val_loss: 1.5237
Epoch 1412/5000
26/26 - 1s - loss: 1.0420 - val_loss: 1.5232
Epoch 1413/5000
26/26 - 1s - loss: 1.0419 - val_loss: 1.5191
Epoch 1414/5000
26/26 - 1s - loss: 1.0412 - val_loss: 1.5211
Epoch 1415/5000
26/26 - 1s - loss: 1.0400 - val_loss: 1.5229
Epoch 1416/5000
26/26 - 1s - loss: 1.0399 - val_loss: 1.5212
Epoch 1417/5000
26/26 - 1s - loss: 1.0384 - val_loss: 1.5195
Epoch 1418/5000
26/26 - 1s - loss: 1.0374 - val_loss: 1.5195
Epoch 1419/5000
26/26 - 1s - loss: 1.0389 - val_loss: 1.5178
Epoch 1420/5000
26/26 - 1s - loss: 1.0370 - val_loss: 1.5149
Epoch 01420: val_loss improved from 1.52540 to 1.51492, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1421/5000
26/26 - 1s - loss: 1.0372 - val_loss: 1.5160
Epoch 1422/5000
26/26 - 1s - loss: 1.0346 - val_loss: 1.5171
Epoch 1423/5000
26/26 - 1s - loss: 1.0344 - val_loss: 1.5178
Epoch 1424/5000
26/26 - 1s - loss: 1.0346 - val_loss: 1.5156
Epoch 1425/5000
26/26 - 1s - loss: 1.0338 - val_loss: 1.5157
Epoch 1426/5000
26/26 - 1s - loss: 1.0335 - val_loss: 1.5131
Epoch 1427/5000
26/26 - 1s - loss: 1.0331 - val_loss: 1.5145
Epoch 1428/5000
26/26 - 1s - loss: 1.0314 - val_loss: 1.5095
Epoch 1429/5000
26/26 - 1s - loss: 1.0313 - val_loss: 1.5110
Epoch 1430/5000
26/26 - 1s - loss: 1.0304 - val_loss: 1.5099
Epoch 01430: val_loss improved from 1.51492 to 1.50991, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1431/5000
26/26 - 1s - loss: 1.0301 - val_loss: 1.5121
Epoch 1432/5000
26/26 - 1s - loss: 1.0273 - val_loss: 1.5098
Epoch 1433/5000
26/26 - 1s - loss: 1.0281 - val_loss: 1.5102
Epoch 1434/5000
26/26 - 1s - loss: 1.0267 - val_loss: 1.5102
Epoch 1435/5000
26/26 - 1s - loss: 1.0274 - val_loss: 1.5081
Epoch 1436/5000
26/26 - 1s - loss: 1.0262 - val_loss: 1.5076
Epoch 1437/5000
26/26 - 1s - loss: 1.0261 - val_loss: 1.5056
Epoch 1438/5000
26/26 - 1s - loss: 1.0249 - val_loss: 1.5058
Epoch 1439/5000
26/26 - 1s - loss: 1.0235 - val_loss: 1.5057
Epoch 1440/5000
26/26 - 1s - loss: 1.0243 - val_loss: 1.5058
Epoch 01440: val_loss improved from 1.50991 to 1.50582, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1441/5000
26/26 - 1s - loss: 1.0225 - val_loss: 1.5031
Epoch 1442/5000
26/26 - 1s - loss: 1.0217 - val_loss: 1.5015
Epoch 1443/5000
26/26 - 1s - loss: 1.0223 - val_loss: 1.5039
Epoch 1444/5000
26/26 - 1s - loss: 1.0216 - val_loss: 1.5024
Epoch 1445/5000
26/26 - 1s - loss: 1.0230 - val_loss: 1.5036
Epoch 1446/5000
26/26 - 1s - loss: 1.0216 - val_loss: 1.5019
Epoch 1447/5000
26/26 - 1s - loss: 1.0223 - val_loss: 1.5013
Epoch 1448/5000
26/26 - 1s - loss: 1.0209 - val_loss: 1.5009
Epoch 1449/5000
26/26 - 1s - loss: 1.0190 - val_loss: 1.4998
Epoch 1450/5000
26/26 - 1s - loss: 1.0173 - val_loss: 1.5019
Epoch 01450: val_loss improved from 1.50582 to 1.50190, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1451/5000
26/26 - 1s - loss: 1.0177 - val_loss: 1.4981
Epoch 1452/5000
26/26 - 1s - loss: 1.0163 - val_loss: 1.4995
Epoch 1453/5000
26/26 - 1s - loss: 1.0152 - val_loss: 1.4988
Epoch 1454/5000
26/26 - 1s - loss: 1.0148 - val_loss: 1.4951
Epoch 1455/5000
26/26 - 1s - loss: 1.0134 - val_loss: 1.4953
Epoch 1456/5000
26/26 - 2s - loss: 1.0128 - val_loss: 1.4958
Epoch 1457/5000
26/26 - 1s - loss: 1.0143 - val_loss: 1.4951
Epoch 1458/5000
26/26 - 1s - loss: 1.0132 - val_loss: 1.4933
Epoch 1459/5000
26/26 - 1s - loss: 1.0119 - val_loss: 1.4937
Epoch 1460/5000
26/26 - 1s - loss: 1.0102 - val_loss: 1.4920
Epoch 01460: val_loss improved from 1.50190 to 1.49197, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1461/5000
26/26 - 1s - loss: 1.0109 - val_loss: 1.4919
Epoch 1462/5000
26/26 - 1s - loss: 1.0092 - val_loss: 1.4924
Epoch 1463/5000
26/26 - 1s - loss: 1.0085 - val_loss: 1.4936
Epoch 1464/5000
26/26 - 1s - loss: 1.0090 - val_loss: 1.4923
Epoch 1465/5000
26/26 - 1s - loss: 1.0078 - val_loss: 1.4902
Epoch 1466/5000
26/26 - 1s - loss: 1.0081 - val_loss: 1.4889
Epoch 1467/5000
26/26 - 1s - loss: 1.0071 - val_loss: 1.4898
Epoch 1468/5000
26/26 - 1s - loss: 1.0062 - val_loss: 1.4864
Epoch 1469/5000
26/26 - 1s - loss: 1.0073 - val_loss: 1.4886
Epoch 1470/5000
26/26 - 1s - loss: 1.0054 - val_loss: 1.4873
Epoch 01470: val_loss improved from 1.49197 to 1.48732, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1471/5000
26/26 - 1s - loss: 1.0068 - val_loss: 1.4874
Epoch 1472/5000
26/26 - 1s - loss: 1.0037 - val_loss: 1.4841
Epoch 1473/5000
26/26 - 1s - loss: 1.0030 - val_loss: 1.4850
Epoch 1474/5000
26/26 - 1s - loss: 1.0019 - val_loss: 1.4840
Epoch 1475/5000
26/26 - 1s - loss: 1.0015 - val_loss: 1.4845
Epoch 1476/5000
26/26 - 1s - loss: 1.0024 - val_loss: 1.4827
Epoch 1477/5000
26/26 - 2s - loss: 0.9997 - val_loss: 1.4813
Epoch 1478/5000
26/26 - 1s - loss: 1.0012 - val_loss: 1.4801
Epoch 1479/5000
26/26 - 1s - loss: 0.9991 - val_loss: 1.4792
Epoch 1480/5000
26/26 - 1s - loss: 0.9986 - val_loss: 1.4830
Epoch 01480: val_loss improved from 1.48732 to 1.48295, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1481/5000
26/26 - 1s - loss: 0.9984 - val_loss: 1.4778
Epoch 1482/5000
26/26 - 1s - loss: 0.9976 - val_loss: 1.4798
Epoch 1483/5000
26/26 - 1s - loss: 0.9963 - val_loss: 1.4792
Epoch 1484/5000
26/26 - 1s - loss: 0.9975 - val_loss: 1.4774
Epoch 1485/5000
26/26 - 1s - loss: 0.9953 - val_loss: 1.4764
Epoch 1486/5000
26/26 - 1s - loss: 0.9968 - val_loss: 1.4746
Epoch 1487/5000
26/26 - 1s - loss: 0.9951 - val_loss: 1.4766
Epoch 1488/5000
26/26 - 1s - loss: 0.9937 - val_loss: 1.4749
Epoch 1489/5000
26/26 - 1s - loss: 0.9944 - val_loss: 1.4738
Epoch 1490/5000
26/26 - 1s - loss: 0.9925 - val_loss: 1.4733
Epoch 01490: val_loss improved from 1.48295 to 1.47334, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1491/5000
26/26 - 1s - loss: 0.9927 - val_loss: 1.4739
Epoch 1492/5000
26/26 - 1s - loss: 0.9918 - val_loss: 1.4712
Epoch 1493/5000
26/26 - 1s - loss: 0.9919 - val_loss: 1.4722
Epoch 1494/5000
26/26 - 1s - loss: 0.9909 - val_loss: 1.4725
Epoch 1495/5000
26/26 - 1s - loss: 0.9909 - val_loss: 1.4717
Epoch 1496/5000
26/26 - 1s - loss: 0.9891 - val_loss: 1.4713
Epoch 1497/5000
26/26 - 2s - loss: 0.9881 - val_loss: 1.4692
Epoch 1498/5000
26/26 - 1s - loss: 0.9894 - val_loss: 1.4713
Epoch 1499/5000
26/26 - 1s - loss: 0.9879 - val_loss: 1.4709
Epoch 1500/5000
26/26 - 1s - loss: 0.9872 - val_loss: 1.4687
Epoch 01500: val_loss improved from 1.47334 to 1.46868, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1501/5000
26/26 - 1s - loss: 0.9879 - val_loss: 1.4679
Epoch 1502/5000
26/26 - 1s - loss: 0.9841 - val_loss: 1.4677
Epoch 1503/5000
26/26 - 1s - loss: 0.9855 - val_loss: 1.4674
Epoch 1504/5000
26/26 - 1s - loss: 0.9830 - val_loss: 1.4663
Epoch 1505/5000
26/26 - 1s - loss: 0.9848 - val_loss: 1.4679
Epoch 1506/5000
26/26 - 1s - loss: 0.9829 - val_loss: 1.4665
Epoch 1507/5000
26/26 - 1s - loss: 0.9840 - val_loss: 1.4656
Epoch 1508/5000
26/26 - 2s - loss: 0.9826 - val_loss: 1.4656
Epoch 1509/5000
26/26 - 1s - loss: 0.9829 - val_loss: 1.4628
Epoch 1510/5000
26/26 - 1s - loss: 0.9810 - val_loss: 1.4620
Epoch 01510: val_loss improved from 1.46868 to 1.46198, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1511/5000
26/26 - 1s - loss: 0.9810 - val_loss: 1.4641
Epoch 1512/5000
26/26 - 1s - loss: 0.9814 - val_loss: 1.4657
Epoch 1513/5000
26/26 - 1s - loss: 0.9784 - val_loss: 1.4635
Epoch 1514/5000
26/26 - 2s - loss: 0.9808 - val_loss: 1.4622
Epoch 1515/5000
26/26 - 1s - loss: 0.9769 - val_loss: 1.4630
Epoch 1516/5000
26/26 - 1s - loss: 0.9781 - val_loss: 1.4608
Epoch 1517/5000
26/26 - 1s - loss: 0.9789 - val_loss: 1.4637
Epoch 1518/5000
26/26 - 1s - loss: 0.9772 - val_loss: 1.4607
Epoch 1519/5000
26/26 - 1s - loss: 0.9763 - val_loss: 1.4606
Epoch 1520/5000
26/26 - 1s - loss: 0.9752 - val_loss: 1.4578
Epoch 01520: val_loss improved from 1.46198 to 1.45785, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1521/5000
26/26 - 1s - loss: 0.9746 - val_loss: 1.4574
Epoch 1522/5000
26/26 - 1s - loss: 0.9762 - val_loss: 1.4600
Epoch 1523/5000
26/26 - 1s - loss: 0.9747 - val_loss: 1.4583
Epoch 1524/5000
26/26 - 1s - loss: 0.9722 - val_loss: 1.4530
Epoch 1525/5000
26/26 - 1s - loss: 0.9733 - val_loss: 1.4561
Epoch 1526/5000
26/26 - 1s - loss: 0.9738 - val_loss: 1.4531
Epoch 1527/5000
26/26 - 1s - loss: 0.9724 - val_loss: 1.4513
Epoch 1528/5000
26/26 - 1s - loss: 0.9708 - val_loss: 1.4512
Epoch 1529/5000
26/26 - 1s - loss: 0.9708 - val_loss: 1.4544
Epoch 1530/5000
26/26 - 1s - loss: 0.9723 - val_loss: 1.4516
Epoch 01530: val_loss improved from 1.45785 to 1.45161, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1531/5000
26/26 - 1s - loss: 0.9696 - val_loss: 1.4523
Epoch 1532/5000
26/26 - 1s - loss: 0.9677 - val_loss: 1.4497
Epoch 1533/5000
26/26 - 1s - loss: 0.9704 - val_loss: 1.4512
Epoch 1534/5000
26/26 - 1s - loss: 0.9665 - val_loss: 1.4508
Epoch 1535/5000
26/26 - 1s - loss: 0.9673 - val_loss: 1.4467
Epoch 1536/5000
26/26 - 1s - loss: 0.9658 - val_loss: 1.4500
Epoch 1537/5000
26/26 - 1s - loss: 0.9654 - val_loss: 1.4491
Epoch 1538/5000
26/26 - 2s - loss: 0.9648 - val_loss: 1.4477
Epoch 1539/5000
26/26 - 2s - loss: 0.9647 - val_loss: 1.4477
Epoch 1540/5000
26/26 - 1s - loss: 0.9651 - val_loss: 1.4467
Epoch 01540: val_loss improved from 1.45161 to 1.44666, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1541/5000
26/26 - 1s - loss: 0.9621 - val_loss: 1.4451
Epoch 1542/5000
26/26 - 1s - loss: 0.9637 - val_loss: 1.4446
Epoch 1543/5000
26/26 - 1s - loss: 0.9630 - val_loss: 1.4434
Epoch 1544/5000
26/26 - 1s - loss: 0.9622 - val_loss: 1.4440
Epoch 1545/5000
26/26 - 1s - loss: 0.9610 - val_loss: 1.4443
Epoch 1546/5000
26/26 - 1s - loss: 0.9612 - val_loss: 1.4407
Epoch 1547/5000
26/26 - 1s - loss: 0.9605 - val_loss: 1.4426
Epoch 1548/5000
26/26 - 1s - loss: 0.9587 - val_loss: 1.4418
Epoch 1549/5000
26/26 - 1s - loss: 0.9593 - val_loss: 1.4400
Epoch 1550/5000
26/26 - 1s - loss: 0.9588 - val_loss: 1.4397
Epoch 01550: val_loss improved from 1.44666 to 1.43965, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1551/5000
26/26 - 1s - loss: 0.9578 - val_loss: 1.4407
Epoch 1552/5000
26/26 - 1s - loss: 0.9573 - val_loss: 1.4388
Epoch 1553/5000
26/26 - 1s - loss: 0.9561 - val_loss: 1.4388
Epoch 1554/5000
26/26 - 1s - loss: 0.9550 - val_loss: 1.4384
Epoch 1555/5000
26/26 - 1s - loss: 0.9565 - val_loss: 1.4358
Epoch 1556/5000
26/26 - 1s - loss: 0.9557 - val_loss: 1.4371
Epoch 1557/5000
26/26 - 1s - loss: 0.9552 - val_loss: 1.4383
Epoch 1558/5000
26/26 - 1s - loss: 0.9534 - val_loss: 1.4360
Epoch 1559/5000
26/26 - 1s - loss: 0.9532 - val_loss: 1.4367
Epoch 1560/5000
26/26 - 1s - loss: 0.9542 - val_loss: 1.4338
Epoch 01560: val_loss improved from 1.43965 to 1.43376, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1561/5000
26/26 - 1s - loss: 0.9532 - val_loss: 1.4350
Epoch 1562/5000
26/26 - 1s - loss: 0.9525 - val_loss: 1.4336
Epoch 1563/5000
26/26 - 1s - loss: 0.9521 - val_loss: 1.4306
Epoch 1564/5000
26/26 - 1s - loss: 0.9508 - val_loss: 1.4299
Epoch 1565/5000
26/26 - 1s - loss: 0.9501 - val_loss: 1.4296
Epoch 1566/5000
26/26 - 1s - loss: 0.9499 - val_loss: 1.4314
Epoch 1567/5000
26/26 - 1s - loss: 0.9494 - val_loss: 1.4317
Epoch 1568/5000
26/26 - 1s - loss: 0.9485 - val_loss: 1.4291
Epoch 1569/5000
26/26 - 1s - loss: 0.9477 - val_loss: 1.4298
Epoch 1570/5000
26/26 - 1s - loss: 0.9484 - val_loss: 1.4311
Epoch 01570: val_loss improved from 1.43376 to 1.43111, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1571/5000
26/26 - 1s - loss: 0.9479 - val_loss: 1.4300
Epoch 1572/5000
26/26 - 1s - loss: 0.9463 - val_loss: 1.4276
Epoch 1573/5000
26/26 - 1s - loss: 0.9468 - val_loss: 1.4268
Epoch 1574/5000
26/26 - 1s - loss: 0.9460 - val_loss: 1.4272
Epoch 1575/5000
26/26 - 1s - loss: 0.9451 - val_loss: 1.4273
Epoch 1576/5000
26/26 - 1s - loss: 0.9440 - val_loss: 1.4250
Epoch 1577/5000
26/26 - 1s - loss: 0.9447 - val_loss: 1.4262
Epoch 1578/5000
26/26 - 1s - loss: 0.9423 - val_loss: 1.4266
Epoch 1579/5000
26/26 - 1s - loss: 0.9428 - val_loss: 1.4240
Epoch 1580/5000
26/26 - 2s - loss: 0.9417 - val_loss: 1.4235
Epoch 01580: val_loss improved from 1.43111 to 1.42349, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1581/5000
26/26 - 1s - loss: 0.9409 - val_loss: 1.4236
Epoch 1582/5000
26/26 - 1s - loss: 0.9421 - val_loss: 1.4230
Epoch 1583/5000
26/26 - 1s - loss: 0.9432 - val_loss: 1.4215
Epoch 1584/5000
26/26 - 1s - loss: 0.9397 - val_loss: 1.4206
Epoch 1585/5000
26/26 - 1s - loss: 0.9389 - val_loss: 1.4211
Epoch 1586/5000
26/26 - 1s - loss: 0.9388 - val_loss: 1.4253
Epoch 1587/5000
26/26 - 1s - loss: 0.9388 - val_loss: 1.4214
Epoch 1588/5000
26/26 - 1s - loss: 0.9370 - val_loss: 1.4207
Epoch 1589/5000
26/26 - 1s - loss: 0.9370 - val_loss: 1.4209
Epoch 1590/5000
26/26 - 1s - loss: 0.9374 - val_loss: 1.4202
Epoch 01590: val_loss improved from 1.42349 to 1.42019, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1591/5000
26/26 - 1s - loss: 0.9366 - val_loss: 1.4186
Epoch 1592/5000
26/26 - 1s - loss: 0.9357 - val_loss: 1.4169
Epoch 1593/5000
26/26 - 1s - loss: 0.9366 - val_loss: 1.4174
Epoch 1594/5000
26/26 - 1s - loss: 0.9359 - val_loss: 1.4180
Epoch 1595/5000
26/26 - 1s - loss: 0.9353 - val_loss: 1.4147
Epoch 1596/5000
26/26 - 1s - loss: 0.9347 - val_loss: 1.4152
Epoch 1597/5000
26/26 - 1s - loss: 0.9334 - val_loss: 1.4152
Epoch 1598/5000
26/26 - 1s - loss: 0.9330 - val_loss: 1.4129
Epoch 1599/5000
26/26 - 1s - loss: 0.9327 - val_loss: 1.4130
Epoch 1600/5000
26/26 - 1s - loss: 0.9323 - val_loss: 1.4164
Epoch 01600: val_loss improved from 1.42019 to 1.41642, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1601/5000
26/26 - 1s - loss: 0.9309 - val_loss: 1.4132
Epoch 1602/5000
26/26 - 1s - loss: 0.9317 - val_loss: 1.4135
Epoch 1603/5000
26/26 - 1s - loss: 0.9303 - val_loss: 1.4121
Epoch 1604/5000
26/26 - 1s - loss: 0.9293 - val_loss: 1.4115
Epoch 1605/5000
26/26 - 1s - loss: 0.9289 - val_loss: 1.4089
Epoch 1606/5000
26/26 - 1s - loss: 0.9277 - val_loss: 1.4089
Epoch 1607/5000
26/26 - 1s - loss: 0.9273 - val_loss: 1.4116
Epoch 1608/5000
26/26 - 1s - loss: 0.9274 - val_loss: 1.4073
Epoch 1609/5000
26/26 - 1s - loss: 0.9281 - val_loss: 1.4101
Epoch 1610/5000
26/26 - 1s - loss: 0.9261 - val_loss: 1.4094
Epoch 01610: val_loss improved from 1.41642 to 1.40937, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1611/5000
26/26 - 1s - loss: 0.9263 - val_loss: 1.4092
Epoch 1612/5000
26/26 - 1s - loss: 0.9259 - val_loss: 1.4047
Epoch 1613/5000
26/26 - 1s - loss: 0.9254 - val_loss: 1.4078
Epoch 1614/5000
26/26 - 1s - loss: 0.9250 - val_loss: 1.4057
Epoch 1615/5000
26/26 - 1s - loss: 0.9242 - val_loss: 1.4056
Epoch 1616/5000
26/26 - 1s - loss: 0.9217 - val_loss: 1.4052
Epoch 1617/5000
26/26 - 1s - loss: 0.9228 - val_loss: 1.4068
Epoch 1618/5000
26/26 - 1s - loss: 0.9228 - val_loss: 1.4034
Epoch 1619/5000
26/26 - 1s - loss: 0.9216 - val_loss: 1.4063
Epoch 1620/5000
26/26 - 1s - loss: 0.9216 - val_loss: 1.4041
Epoch 01620: val_loss improved from 1.40937 to 1.40410, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1621/5000
26/26 - 1s - loss: 0.9197 - val_loss: 1.4042
Epoch 1622/5000
26/26 - 2s - loss: 0.9201 - val_loss: 1.4011
Epoch 1623/5000
26/26 - 1s - loss: 0.9191 - val_loss: 1.4030
Epoch 1624/5000
26/26 - 1s - loss: 0.9206 - val_loss: 1.4039
Epoch 1625/5000
26/26 - 1s - loss: 0.9176 - val_loss: 1.4026
Epoch 1626/5000
26/26 - 1s - loss: 0.9184 - val_loss: 1.4020
Epoch 1627/5000
26/26 - 1s - loss: 0.9161 - val_loss: 1.4017
Epoch 1628/5000
26/26 - 1s - loss: 0.9171 - val_loss: 1.3995
Epoch 1629/5000
26/26 - 1s - loss: 0.9157 - val_loss: 1.3993
Epoch 1630/5000
26/26 - 1s - loss: 0.9161 - val_loss: 1.3986
Epoch 01630: val_loss improved from 1.40410 to 1.39857, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1631/5000
26/26 - 1s - loss: 0.9150 - val_loss: 1.3981
Epoch 1632/5000
26/26 - 1s - loss: 0.9144 - val_loss: 1.3968
Epoch 1633/5000
26/26 - 1s - loss: 0.9153 - val_loss: 1.3959
Epoch 1634/5000
26/26 - 1s - loss: 0.9138 - val_loss: 1.3976
Epoch 1635/5000
26/26 - 1s - loss: 0.9145 - val_loss: 1.3962
Epoch 1636/5000
26/26 - 1s - loss: 0.9126 - val_loss: 1.3935
Epoch 1637/5000
26/26 - 1s - loss: 0.9135 - val_loss: 1.3946
Epoch 1638/5000
26/26 - 1s - loss: 0.9131 - val_loss: 1.3952
Epoch 1639/5000
26/26 - 1s - loss: 0.9113 - val_loss: 1.3946
Epoch 1640/5000
26/26 - 1s - loss: 0.9105 - val_loss: 1.3934
Epoch 01640: val_loss improved from 1.39857 to 1.39338, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1641/5000
26/26 - 1s - loss: 0.9093 - val_loss: 1.3916
Epoch 1642/5000
26/26 - 1s - loss: 0.9108 - val_loss: 1.3942
Epoch 1643/5000
26/26 - 1s - loss: 0.9107 - val_loss: 1.3912
Epoch 1644/5000
26/26 - 1s - loss: 0.9086 - val_loss: 1.3908
Epoch 1645/5000
26/26 - 1s - loss: 0.9090 - val_loss: 1.3899
Epoch 1646/5000
26/26 - 1s - loss: 0.9082 - val_loss: 1.3888
Epoch 1647/5000
26/26 - 1s - loss: 0.9065 - val_loss: 1.3908
Epoch 1648/5000
26/26 - 1s - loss: 0.9071 - val_loss: 1.3913
Epoch 1649/5000
26/26 - 1s - loss: 0.9081 - val_loss: 1.3886
Epoch 1650/5000
26/26 - 1s - loss: 0.9080 - val_loss: 1.3879
Epoch 01650: val_loss improved from 1.39338 to 1.38785, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1651/5000
26/26 - 1s - loss: 0.9050 - val_loss: 1.3865
Epoch 1652/5000
26/26 - 1s - loss: 0.9037 - val_loss: 1.3861
Epoch 1653/5000
26/26 - 1s - loss: 0.9047 - val_loss: 1.3869
Epoch 1654/5000
26/26 - 1s - loss: 0.9040 - val_loss: 1.3843
Epoch 1655/5000
26/26 - 1s - loss: 0.9041 - val_loss: 1.3850
Epoch 1656/5000
26/26 - 1s - loss: 0.9040 - val_loss: 1.3853
Epoch 1657/5000
26/26 - 1s - loss: 0.9020 - val_loss: 1.3844
Epoch 1658/5000
26/26 - 1s - loss: 0.9021 - val_loss: 1.3853
Epoch 1659/5000
26/26 - 1s - loss: 0.9022 - val_loss: 1.3849
Epoch 1660/5000
26/26 - 1s - loss: 0.9008 - val_loss: 1.3858
Epoch 01660: val_loss improved from 1.38785 to 1.38585, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1661/5000
26/26 - 1s - loss: 0.9003 - val_loss: 1.3819
Epoch 1662/5000
26/26 - 1s - loss: 0.9005 - val_loss: 1.3848
Epoch 1663/5000
26/26 - 1s - loss: 0.9000 - val_loss: 1.3808
Epoch 1664/5000
26/26 - 1s - loss: 0.8991 - val_loss: 1.3815
Epoch 1665/5000
26/26 - 1s - loss: 0.8970 - val_loss: 1.3811
Epoch 1666/5000
26/26 - 1s - loss: 0.8980 - val_loss: 1.3798
Epoch 1667/5000
26/26 - 1s - loss: 0.8969 - val_loss: 1.3790
Epoch 1668/5000
26/26 - 1s - loss: 0.8973 - val_loss: 1.3799
Epoch 1669/5000
26/26 - 1s - loss: 0.8979 - val_loss: 1.3778
Epoch 1670/5000
26/26 - 1s - loss: 0.8963 - val_loss: 1.3792
Epoch 01670: val_loss improved from 1.38585 to 1.37923, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1671/5000
26/26 - 1s - loss: 0.8943 - val_loss: 1.3758
Epoch 1672/5000
26/26 - 1s - loss: 0.8938 - val_loss: 1.3772
Epoch 1673/5000
26/26 - 1s - loss: 0.8937 - val_loss: 1.3774
Epoch 1674/5000
26/26 - 1s - loss: 0.8939 - val_loss: 1.3771
Epoch 1675/5000
26/26 - 1s - loss: 0.8931 - val_loss: 1.3779
Epoch 1676/5000
26/26 - 1s - loss: 0.8925 - val_loss: 1.3770
Epoch 1677/5000
26/26 - 1s - loss: 0.8920 - val_loss: 1.3776
Epoch 1678/5000
26/26 - 1s - loss: 0.8925 - val_loss: 1.3775
Epoch 1679/5000
26/26 - 1s - loss: 0.8901 - val_loss: 1.3732
Epoch 1680/5000
26/26 - 1s - loss: 0.8899 - val_loss: 1.3736
Epoch 01680: val_loss improved from 1.37923 to 1.37361, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1681/5000
26/26 - 1s - loss: 0.8897 - val_loss: 1.3741
Epoch 1682/5000
26/26 - 1s - loss: 0.8884 - val_loss: 1.3740
Epoch 1683/5000
26/26 - 1s - loss: 0.8900 - val_loss: 1.3735
Epoch 1684/5000
26/26 - 1s - loss: 0.8880 - val_loss: 1.3710
Epoch 1685/5000
26/26 - 1s - loss: 0.8892 - val_loss: 1.3707
Epoch 1686/5000
26/26 - 1s - loss: 0.8888 - val_loss: 1.3697
Epoch 1687/5000
26/26 - 1s - loss: 0.8863 - val_loss: 1.3723
Epoch 1688/5000
26/26 - 1s - loss: 0.8880 - val_loss: 1.3682
Epoch 1689/5000
26/26 - 1s - loss: 0.8864 - val_loss: 1.3681
Epoch 1690/5000
26/26 - 1s - loss: 0.8852 - val_loss: 1.3663
Epoch 01690: val_loss improved from 1.37361 to 1.36630, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1691/5000
26/26 - 1s - loss: 0.8857 - val_loss: 1.3658
Epoch 1692/5000
26/26 - 1s - loss: 0.8853 - val_loss: 1.3668
Epoch 1693/5000
26/26 - 1s - loss: 0.8855 - val_loss: 1.3669
Epoch 1694/5000
26/26 - 1s - loss: 0.8840 - val_loss: 1.3651
Epoch 1695/5000
26/26 - 1s - loss: 0.8847 - val_loss: 1.3665
Epoch 1696/5000
26/26 - 1s - loss: 0.8838 - val_loss: 1.3647
Epoch 1697/5000
26/26 - 1s - loss: 0.8818 - val_loss: 1.3641
Epoch 1698/5000
26/26 - 1s - loss: 0.8813 - val_loss: 1.3655
Epoch 1699/5000
26/26 - 1s - loss: 0.8822 - val_loss: 1.3636
Epoch 1700/5000
26/26 - 1s - loss: 0.8800 - val_loss: 1.3622
Epoch 01700: val_loss improved from 1.36630 to 1.36221, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1701/5000
26/26 - 1s - loss: 0.8810 - val_loss: 1.3653
Epoch 1702/5000
26/26 - 1s - loss: 0.8809 - val_loss: 1.3627
Epoch 1703/5000
26/26 - 1s - loss: 0.8803 - val_loss: 1.3601
Epoch 1704/5000
26/26 - 1s - loss: 0.8790 - val_loss: 1.3614
Epoch 1705/5000
26/26 - 1s - loss: 0.8807 - val_loss: 1.3619
Epoch 1706/5000
26/26 - 1s - loss: 0.8789 - val_loss: 1.3603
Epoch 1707/5000
26/26 - 2s - loss: 0.8776 - val_loss: 1.3614
Epoch 1708/5000
26/26 - 1s - loss: 0.8784 - val_loss: 1.3601
Epoch 1709/5000
26/26 - 1s - loss: 0.8762 - val_loss: 1.3587
Epoch 1710/5000
26/26 - 2s - loss: 0.8753 - val_loss: 1.3577
Epoch 01710: val_loss improved from 1.36221 to 1.35767, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1711/5000
26/26 - 1s - loss: 0.8760 - val_loss: 1.3590
Epoch 1712/5000
26/26 - 1s - loss: 0.8756 - val_loss: 1.3598
Epoch 1713/5000
26/26 - 2s - loss: 0.8756 - val_loss: 1.3566
Epoch 1714/5000
26/26 - 1s - loss: 0.8740 - val_loss: 1.3552
Epoch 1715/5000
26/26 - 1s - loss: 0.8749 - val_loss: 1.3558
Epoch 1716/5000
26/26 - 1s - loss: 0.8744 - val_loss: 1.3531
Epoch 1717/5000
26/26 - 1s - loss: 0.8743 - val_loss: 1.3552
Epoch 1718/5000
26/26 - 2s - loss: 0.8738 - val_loss: 1.3556
Epoch 1719/5000
26/26 - 1s - loss: 0.8719 - val_loss: 1.3557
Epoch 1720/5000
26/26 - 1s - loss: 0.8719 - val_loss: 1.3564
Epoch 01720: val_loss improved from 1.35767 to 1.35635, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1721/5000
26/26 - 1s - loss: 0.8708 - val_loss: 1.3555
Epoch 1722/5000
26/26 - 1s - loss: 0.8716 - val_loss: 1.3532
Epoch 1723/5000
26/26 - 1s - loss: 0.8688 - val_loss: 1.3542
Epoch 1724/5000
26/26 - 1s - loss: 0.8709 - val_loss: 1.3510
Epoch 1725/5000
26/26 - 1s - loss: 0.8697 - val_loss: 1.3531
Epoch 1726/5000
26/26 - 1s - loss: 0.8682 - val_loss: 1.3528
Epoch 1727/5000
26/26 - 1s - loss: 0.8691 - val_loss: 1.3521
Epoch 1728/5000
26/26 - 1s - loss: 0.8681 - val_loss: 1.3508
Epoch 1729/5000
26/26 - 1s - loss: 0.8690 - val_loss: 1.3493
Epoch 1730/5000
26/26 - 1s - loss: 0.8681 - val_loss: 1.3486
Epoch 01730: val_loss improved from 1.35635 to 1.34858, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1731/5000
26/26 - 1s - loss: 0.8675 - val_loss: 1.3502
Epoch 1732/5000
26/26 - 1s - loss: 0.8671 - val_loss: 1.3460
Epoch 1733/5000
26/26 - 1s - loss: 0.8665 - val_loss: 1.3449
Epoch 1734/5000
26/26 - 1s - loss: 0.8664 - val_loss: 1.3453
Epoch 1735/5000
26/26 - 1s - loss: 0.8648 - val_loss: 1.3466
Epoch 1736/5000
26/26 - 1s - loss: 0.8630 - val_loss: 1.3482
Epoch 1737/5000
26/26 - 1s - loss: 0.8634 - val_loss: 1.3475
Epoch 1738/5000
26/26 - 1s - loss: 0.8633 - val_loss: 1.3469
Epoch 1739/5000
26/26 - 1s - loss: 0.8631 - val_loss: 1.3458
Epoch 1740/5000
26/26 - 1s - loss: 0.8631 - val_loss: 1.3453
Epoch 01740: val_loss improved from 1.34858 to 1.34530, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1741/5000
26/26 - 1s - loss: 0.8613 - val_loss: 1.3454
Epoch 1742/5000
26/26 - 1s - loss: 0.8623 - val_loss: 1.3454
Epoch 1743/5000
26/26 - 1s - loss: 0.8600 - val_loss: 1.3419
Epoch 1744/5000
26/26 - 1s - loss: 0.8630 - val_loss: 1.3454
Epoch 1745/5000
26/26 - 1s - loss: 0.8614 - val_loss: 1.3423
Epoch 1746/5000
26/26 - 1s - loss: 0.8589 - val_loss: 1.3431
Epoch 1747/5000
26/26 - 2s - loss: 0.8588 - val_loss: 1.3429
Epoch 1748/5000
26/26 - 1s - loss: 0.8595 - val_loss: 1.3412
Epoch 1749/5000
26/26 - 1s - loss: 0.8567 - val_loss: 1.3432
Epoch 1750/5000
26/26 - 1s - loss: 0.8565 - val_loss: 1.3432
Epoch 01750: val_loss improved from 1.34530 to 1.34317, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1751/5000
26/26 - 1s - loss: 0.8598 - val_loss: 1.3433
Epoch 1752/5000
26/26 - 1s - loss: 0.8575 - val_loss: 1.3425
Epoch 1753/5000
26/26 - 1s - loss: 0.8561 - val_loss: 1.3415
Epoch 1754/5000
26/26 - 2s - loss: 0.8575 - val_loss: 1.3422
Epoch 1755/5000
26/26 - 1s - loss: 0.8552 - val_loss: 1.3398
Epoch 1756/5000
26/26 - 2s - loss: 0.8538 - val_loss: 1.3379
Epoch 1757/5000
26/26 - 1s - loss: 0.8557 - val_loss: 1.3382
Epoch 1758/5000
26/26 - 1s - loss: 0.8547 - val_loss: 1.3371
Epoch 1759/5000
26/26 - 1s - loss: 0.8551 - val_loss: 1.3374
Epoch 1760/5000
26/26 - 1s - loss: 0.8532 - val_loss: 1.3363
Epoch 01760: val_loss improved from 1.34317 to 1.33629, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1761/5000
26/26 - 1s - loss: 0.8536 - val_loss: 1.3377
Epoch 1762/5000
26/26 - 1s - loss: 0.8533 - val_loss: 1.3359
Epoch 1763/5000
26/26 - 1s - loss: 0.8538 - val_loss: 1.3354
Epoch 1764/5000
26/26 - 1s - loss: 0.8512 - val_loss: 1.3344
Epoch 1765/5000
26/26 - 1s - loss: 0.8500 - val_loss: 1.3359
Epoch 1766/5000
26/26 - 1s - loss: 0.8513 - val_loss: 1.3360
Epoch 1767/5000
26/26 - 1s - loss: 0.8525 - val_loss: 1.3359
Epoch 1768/5000
26/26 - 1s - loss: 0.8512 - val_loss: 1.3320
Epoch 1769/5000
26/26 - 1s - loss: 0.8495 - val_loss: 1.3318
Epoch 1770/5000
26/26 - 1s - loss: 0.8493 - val_loss: 1.3323
Epoch 01770: val_loss improved from 1.33629 to 1.33227, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1771/5000
26/26 - 1s - loss: 0.8489 - val_loss: 1.3326
Epoch 1772/5000
26/26 - 1s - loss: 0.8490 - val_loss: 1.3331
Epoch 1773/5000
26/26 - 1s - loss: 0.8474 - val_loss: 1.3314
Epoch 1774/5000
26/26 - 1s - loss: 0.8471 - val_loss: 1.3313
Epoch 1775/5000
26/26 - 1s - loss: 0.8468 - val_loss: 1.3323
Epoch 1776/5000
26/26 - 1s - loss: 0.8477 - val_loss: 1.3314
Epoch 1777/5000
26/26 - 1s - loss: 0.8458 - val_loss: 1.3281
Epoch 1778/5000
26/26 - 1s - loss: 0.8465 - val_loss: 1.3287
Epoch 1779/5000
26/26 - 1s - loss: 0.8460 - val_loss: 1.3303
Epoch 1780/5000
26/26 - 1s - loss: 0.8462 - val_loss: 1.3255
Epoch 01780: val_loss improved from 1.33227 to 1.32547, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1781/5000
26/26 - 1s - loss: 0.8457 - val_loss: 1.3276
Epoch 1782/5000
26/26 - 1s - loss: 0.8458 - val_loss: 1.3269
Epoch 1783/5000
26/26 - 1s - loss: 0.8433 - val_loss: 1.3274
Epoch 1784/5000
26/26 - 1s - loss: 0.8433 - val_loss: 1.3251
Epoch 1785/5000
26/26 - 1s - loss: 0.8426 - val_loss: 1.3260
Epoch 1786/5000
26/26 - 1s - loss: 0.8420 - val_loss: 1.3249
Epoch 1787/5000
26/26 - 1s - loss: 0.8407 - val_loss: 1.3230
Epoch 1788/5000
26/26 - 1s - loss: 0.8422 - val_loss: 1.3247
Epoch 1789/5000
26/26 - 1s - loss: 0.8408 - val_loss: 1.3262
Epoch 1790/5000
26/26 - 1s - loss: 0.8409 - val_loss: 1.3237
Epoch 01790: val_loss improved from 1.32547 to 1.32365, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1791/5000
26/26 - 1s - loss: 0.8382 - val_loss: 1.3229
Epoch 1792/5000
26/26 - 1s - loss: 0.8382 - val_loss: 1.3228
Epoch 1793/5000
26/26 - 1s - loss: 0.8399 - val_loss: 1.3216
Epoch 1794/5000
26/26 - 1s - loss: 0.8379 - val_loss: 1.3230
Epoch 1795/5000
26/26 - 1s - loss: 0.8392 - val_loss: 1.3212
Epoch 1796/5000
26/26 - 1s - loss: 0.8384 - val_loss: 1.3193
Epoch 1797/5000
26/26 - 1s - loss: 0.8385 - val_loss: 1.3196
Epoch 1798/5000
26/26 - 1s - loss: 0.8381 - val_loss: 1.3225
Epoch 1799/5000
26/26 - 1s - loss: 0.8383 - val_loss: 1.3214
Epoch 1800/5000
26/26 - 1s - loss: 0.8371 - val_loss: 1.3208
Epoch 01800: val_loss improved from 1.32365 to 1.32080, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1801/5000
26/26 - 1s - loss: 0.8364 - val_loss: 1.3183
Epoch 1802/5000
26/26 - 1s - loss: 0.8347 - val_loss: 1.3163
Epoch 1803/5000
26/26 - 1s - loss: 0.8376 - val_loss: 1.3179
Epoch 1804/5000
26/26 - 1s - loss: 0.8338 - val_loss: 1.3201
Epoch 1805/5000
26/26 - 1s - loss: 0.8343 - val_loss: 1.3160
Epoch 1806/5000
26/26 - 1s - loss: 0.8339 - val_loss: 1.3184
Epoch 1807/5000
26/26 - 1s - loss: 0.8338 - val_loss: 1.3173
Epoch 1808/5000
26/26 - 1s - loss: 0.8321 - val_loss: 1.3172
Epoch 1809/5000
26/26 - 1s - loss: 0.8326 - val_loss: 1.3169
Epoch 1810/5000
26/26 - 1s - loss: 0.8315 - val_loss: 1.3143
Epoch 01810: val_loss improved from 1.32080 to 1.31433, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1811/5000
26/26 - 1s - loss: 0.8306 - val_loss: 1.3138
Epoch 1812/5000
26/26 - 1s - loss: 0.8312 - val_loss: 1.3155
Epoch 1813/5000
26/26 - 1s - loss: 0.8291 - val_loss: 1.3168
Epoch 1814/5000
26/26 - 1s - loss: 0.8314 - val_loss: 1.3138
Epoch 1815/5000
26/26 - 1s - loss: 0.8298 - val_loss: 1.3131
Epoch 1816/5000
26/26 - 1s - loss: 0.8308 - val_loss: 1.3119
Epoch 1817/5000
26/26 - 1s - loss: 0.8296 - val_loss: 1.3133
Epoch 1818/5000
26/26 - 1s - loss: 0.8283 - val_loss: 1.3125
Epoch 1819/5000
26/26 - 1s - loss: 0.8264 - val_loss: 1.3124
Epoch 1820/5000
26/26 - 1s - loss: 0.8269 - val_loss: 1.3110
Epoch 01820: val_loss improved from 1.31433 to 1.31098, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1821/5000
26/26 - 1s - loss: 0.8276 - val_loss: 1.3132
Epoch 1822/5000
26/26 - 1s - loss: 0.8267 - val_loss: 1.3115
Epoch 1823/5000
26/26 - 1s - loss: 0.8264 - val_loss: 1.3093
Epoch 1824/5000
26/26 - 1s - loss: 0.8258 - val_loss: 1.3108
Epoch 1825/5000
26/26 - 1s - loss: 0.8266 - val_loss: 1.3078
Epoch 1826/5000
26/26 - 1s - loss: 0.8254 - val_loss: 1.3086
Epoch 1827/5000
26/26 - 1s - loss: 0.8256 - val_loss: 1.3085
Epoch 1828/5000
26/26 - 1s - loss: 0.8242 - val_loss: 1.3091
Epoch 1829/5000
26/26 - 1s - loss: 0.8224 - val_loss: 1.3102
Epoch 1830/5000
26/26 - 2s - loss: 0.8227 - val_loss: 1.3064
Epoch 01830: val_loss improved from 1.31098 to 1.30645, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1831/5000
26/26 - 1s - loss: 0.8245 - val_loss: 1.3100
Epoch 1832/5000
26/26 - 1s - loss: 0.8227 - val_loss: 1.3072
Epoch 1833/5000
26/26 - 1s - loss: 0.8215 - val_loss: 1.3070
Epoch 1834/5000
26/26 - 1s - loss: 0.8238 - val_loss: 1.3050
Epoch 1835/5000
26/26 - 1s - loss: 0.8213 - val_loss: 1.3044
Epoch 1836/5000
26/26 - 1s - loss: 0.8218 - val_loss: 1.3049
Epoch 1837/5000
26/26 - 1s - loss: 0.8195 - val_loss: 1.3036
Epoch 1838/5000
26/26 - 1s - loss: 0.8206 - val_loss: 1.3026
Epoch 1839/5000
26/26 - 1s - loss: 0.8180 - val_loss: 1.3026
Epoch 1840/5000
26/26 - 1s - loss: 0.8204 - val_loss: 1.3024
Epoch 01840: val_loss improved from 1.30645 to 1.30236, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1841/5000
26/26 - 1s - loss: 0.8188 - val_loss: 1.3022
Epoch 1842/5000
26/26 - 1s - loss: 0.8191 - val_loss: 1.3014
Epoch 1843/5000
26/26 - 1s - loss: 0.8176 - val_loss: 1.3003
Epoch 1844/5000
26/26 - 1s - loss: 0.8186 - val_loss: 1.3031
Epoch 1845/5000
26/26 - 1s - loss: 0.8169 - val_loss: 1.3015
Epoch 1846/5000
26/26 - 1s - loss: 0.8175 - val_loss: 1.2995
Epoch 1847/5000
26/26 - 1s - loss: 0.8174 - val_loss: 1.3012
Epoch 1848/5000
26/26 - 1s - loss: 0.8166 - val_loss: 1.3013
Epoch 1849/5000
26/26 - 1s - loss: 0.8158 - val_loss: 1.2992
Epoch 1850/5000
26/26 - 1s - loss: 0.8148 - val_loss: 1.2999
Epoch 01850: val_loss improved from 1.30236 to 1.29988, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1851/5000
26/26 - 1s - loss: 0.8154 - val_loss: 1.2966
Epoch 1852/5000
26/26 - 1s - loss: 0.8161 - val_loss: 1.2993
Epoch 1853/5000
26/26 - 1s - loss: 0.8144 - val_loss: 1.2980
Epoch 1854/5000
26/26 - 1s - loss: 0.8132 - val_loss: 1.2972
Epoch 1855/5000
26/26 - 1s - loss: 0.8139 - val_loss: 1.2976
Epoch 1856/5000
26/26 - 1s - loss: 0.8134 - val_loss: 1.2985
Epoch 1857/5000
26/26 - 1s - loss: 0.8133 - val_loss: 1.2973
Epoch 1858/5000
26/26 - 1s - loss: 0.8124 - val_loss: 1.2933
Epoch 1859/5000
26/26 - 1s - loss: 0.8125 - val_loss: 1.2979
Epoch 1860/5000
26/26 - 1s - loss: 0.8123 - val_loss: 1.2932
Epoch 01860: val_loss improved from 1.29988 to 1.29321, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1861/5000
26/26 - 1s - loss: 0.8111 - val_loss: 1.2947
Epoch 1862/5000
26/26 - 1s - loss: 0.8106 - val_loss: 1.2950
Epoch 1863/5000
26/26 - 1s - loss: 0.8113 - val_loss: 1.2935
Epoch 1864/5000
26/26 - 1s - loss: 0.8095 - val_loss: 1.2937
Epoch 1865/5000
26/26 - 1s - loss: 0.8084 - val_loss: 1.2935
Epoch 1866/5000
26/26 - 1s - loss: 0.8087 - val_loss: 1.2928
Epoch 1867/5000
26/26 - 1s - loss: 0.8081 - val_loss: 1.2913
Epoch 1868/5000
26/26 - 1s - loss: 0.8086 - val_loss: 1.2923
Epoch 1869/5000
26/26 - 1s - loss: 0.8082 - val_loss: 1.2902
Epoch 1870/5000
26/26 - 1s - loss: 0.8084 - val_loss: 1.2936
Epoch 01870: val_loss did not improve from 1.29321
Epoch 1871/5000
26/26 - 1s - loss: 0.8070 - val_loss: 1.2904
Epoch 1872/5000
26/26 - 2s - loss: 0.8050 - val_loss: 1.2910
Epoch 1873/5000
26/26 - 1s - loss: 0.8058 - val_loss: 1.2890
Epoch 1874/5000
26/26 - 1s - loss: 0.8054 - val_loss: 1.2896
Epoch 1875/5000
26/26 - 1s - loss: 0.8041 - val_loss: 1.2881
Epoch 1876/5000
26/26 - 1s - loss: 0.8048 - val_loss: 1.2898
Epoch 1877/5000
26/26 - 1s - loss: 0.8047 - val_loss: 1.2892
Epoch 1878/5000
26/26 - 1s - loss: 0.8045 - val_loss: 1.2881
Epoch 1879/5000
26/26 - 1s - loss: 0.8032 - val_loss: 1.2875
Epoch 1880/5000
26/26 - 1s - loss: 0.8019 - val_loss: 1.2886
Epoch 01880: val_loss improved from 1.29321 to 1.28858, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1881/5000
26/26 - 1s - loss: 0.8029 - val_loss: 1.2888
Epoch 1882/5000
26/26 - 1s - loss: 0.8015 - val_loss: 1.2869
Epoch 1883/5000
26/26 - 1s - loss: 0.8019 - val_loss: 1.2856
Epoch 1884/5000
26/26 - 1s - loss: 0.8009 - val_loss: 1.2865
Epoch 1885/5000
26/26 - 1s - loss: 0.8025 - val_loss: 1.2877
Epoch 1886/5000
26/26 - 1s - loss: 0.8009 - val_loss: 1.2872
Epoch 1887/5000
26/26 - 1s - loss: 0.8013 - val_loss: 1.2845
Epoch 1888/5000
26/26 - 1s - loss: 0.7993 - val_loss: 1.2851
Epoch 1889/5000
26/26 - 1s - loss: 0.8002 - val_loss: 1.2850
Epoch 1890/5000
26/26 - 1s - loss: 0.7980 - val_loss: 1.2853
Epoch 01890: val_loss improved from 1.28858 to 1.28530, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1891/5000
26/26 - 1s - loss: 0.7994 - val_loss: 1.2833
Epoch 1892/5000
26/26 - 1s - loss: 0.8003 - val_loss: 1.2812
Epoch 1893/5000
26/26 - 1s - loss: 0.8000 - val_loss: 1.2828
Epoch 1894/5000
26/26 - 1s - loss: 0.7984 - val_loss: 1.2822
Epoch 1895/5000
26/26 - 1s - loss: 0.7990 - val_loss: 1.2829
Epoch 1896/5000
26/26 - 1s - loss: 0.7973 - val_loss: 1.2808
Epoch 1897/5000
26/26 - 1s - loss: 0.7965 - val_loss: 1.2790
Epoch 1898/5000
26/26 - 1s - loss: 0.7962 - val_loss: 1.2797
Epoch 1899/5000
26/26 - 1s - loss: 0.7964 - val_loss: 1.2794
Epoch 1900/5000
26/26 - 1s - loss: 0.7957 - val_loss: 1.2811
Epoch 01900: val_loss improved from 1.28530 to 1.28115, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1901/5000
26/26 - 1s - loss: 0.7960 - val_loss: 1.2803
Epoch 1902/5000
26/26 - 1s - loss: 0.7943 - val_loss: 1.2824
Epoch 1903/5000
26/26 - 1s - loss: 0.7950 - val_loss: 1.2791
Epoch 1904/5000
26/26 - 1s - loss: 0.7954 - val_loss: 1.2772
Epoch 1905/5000
26/26 - 1s - loss: 0.7943 - val_loss: 1.2776
Epoch 1906/5000
26/26 - 1s - loss: 0.7929 - val_loss: 1.2770
Epoch 1907/5000
26/26 - 1s - loss: 0.7940 - val_loss: 1.2767
Epoch 1908/5000
26/26 - 1s - loss: 0.7928 - val_loss: 1.2764
Epoch 1909/5000
26/26 - 1s - loss: 0.7918 - val_loss: 1.2756
Epoch 1910/5000
26/26 - 1s - loss: 0.7930 - val_loss: 1.2748
Epoch 01910: val_loss improved from 1.28115 to 1.27481, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1911/5000
26/26 - 1s - loss: 0.7926 - val_loss: 1.2750
Epoch 1912/5000
26/26 - 1s - loss: 0.7894 - val_loss: 1.2737
Epoch 1913/5000
26/26 - 1s - loss: 0.7915 - val_loss: 1.2759
Epoch 1914/5000
26/26 - 2s - loss: 0.7910 - val_loss: 1.2758
Epoch 1915/5000
26/26 - 1s - loss: 0.7902 - val_loss: 1.2710
Epoch 1916/5000
26/26 - 1s - loss: 0.7898 - val_loss: 1.2735
Epoch 1917/5000
26/26 - 1s - loss: 0.7887 - val_loss: 1.2741
Epoch 1918/5000
26/26 - 1s - loss: 0.7894 - val_loss: 1.2731
Epoch 1919/5000
26/26 - 1s - loss: 0.7884 - val_loss: 1.2729
Epoch 1920/5000
26/26 - 1s - loss: 0.7879 - val_loss: 1.2729
Epoch 01920: val_loss improved from 1.27481 to 1.27287, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1921/5000
26/26 - 1s - loss: 0.7881 - val_loss: 1.2748
Epoch 1922/5000
26/26 - 1s - loss: 0.7888 - val_loss: 1.2707
Epoch 1923/5000
26/26 - 1s - loss: 0.7889 - val_loss: 1.2696
Epoch 1924/5000
26/26 - 1s - loss: 0.7877 - val_loss: 1.2699
Epoch 1925/5000
26/26 - 1s - loss: 0.7858 - val_loss: 1.2702
Epoch 1926/5000
26/26 - 1s - loss: 0.7862 - val_loss: 1.2698
Epoch 1927/5000
26/26 - 1s - loss: 0.7857 - val_loss: 1.2706
Epoch 1928/5000
26/26 - 1s - loss: 0.7856 - val_loss: 1.2681
Epoch 1929/5000
26/26 - 1s - loss: 0.7847 - val_loss: 1.2697
Epoch 1930/5000
26/26 - 1s - loss: 0.7830 - val_loss: 1.2702
Epoch 01930: val_loss improved from 1.27287 to 1.27021, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1931/5000
26/26 - 1s - loss: 0.7846 - val_loss: 1.2714
Epoch 1932/5000
26/26 - 1s - loss: 0.7831 - val_loss: 1.2682
Epoch 1933/5000
26/26 - 1s - loss: 0.7830 - val_loss: 1.2688
Epoch 1934/5000
26/26 - 1s - loss: 0.7815 - val_loss: 1.2679
Epoch 1935/5000
26/26 - 1s - loss: 0.7838 - val_loss: 1.2685
Epoch 1936/5000
26/26 - 1s - loss: 0.7830 - val_loss: 1.2654
Epoch 1937/5000
26/26 - 1s - loss: 0.7828 - val_loss: 1.2638
Epoch 1938/5000
26/26 - 1s - loss: 0.7812 - val_loss: 1.2653
Epoch 1939/5000
26/26 - 1s - loss: 0.7817 - val_loss: 1.2668
Epoch 1940/5000
26/26 - 1s - loss: 0.7806 - val_loss: 1.2670
Epoch 01940: val_loss improved from 1.27021 to 1.26702, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1941/5000
26/26 - 1s - loss: 0.7821 - val_loss: 1.2660
Epoch 1942/5000
26/26 - 1s - loss: 0.7789 - val_loss: 1.2630
Epoch 1943/5000
26/26 - 1s - loss: 0.7782 - val_loss: 1.2643
Epoch 1944/5000
26/26 - 1s - loss: 0.7779 - val_loss: 1.2636
Epoch 1945/5000
26/26 - 1s - loss: 0.7784 - val_loss: 1.2635
Epoch 1946/5000
26/26 - 1s - loss: 0.7780 - val_loss: 1.2615
Epoch 1947/5000
26/26 - 1s - loss: 0.7768 - val_loss: 1.2642
Epoch 1948/5000
26/26 - 1s - loss: 0.7768 - val_loss: 1.2625
Epoch 1949/5000
26/26 - 1s - loss: 0.7769 - val_loss: 1.2612
Epoch 1950/5000
26/26 - 1s - loss: 0.7760 - val_loss: 1.2624
Epoch 01950: val_loss improved from 1.26702 to 1.26241, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1951/5000
26/26 - 1s - loss: 0.7752 - val_loss: 1.2603
Epoch 1952/5000
26/26 - 1s - loss: 0.7764 - val_loss: 1.2616
Epoch 1953/5000
26/26 - 1s - loss: 0.7756 - val_loss: 1.2604
Epoch 1954/5000
26/26 - 1s - loss: 0.7763 - val_loss: 1.2636
Epoch 1955/5000
26/26 - 1s - loss: 0.7767 - val_loss: 1.2590
Epoch 1956/5000
26/26 - 1s - loss: 0.7753 - val_loss: 1.2589
Epoch 1957/5000
26/26 - 1s - loss: 0.7746 - val_loss: 1.2577
Epoch 1958/5000
26/26 - 1s - loss: 0.7740 - val_loss: 1.2585
Epoch 1959/5000
26/26 - 1s - loss: 0.7747 - val_loss: 1.2596
Epoch 1960/5000
26/26 - 1s - loss: 0.7735 - val_loss: 1.2587
Epoch 01960: val_loss improved from 1.26241 to 1.25867, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1961/5000
26/26 - 1s - loss: 0.7734 - val_loss: 1.2564
Epoch 1962/5000
26/26 - 1s - loss: 0.7719 - val_loss: 1.2582
Epoch 1963/5000
26/26 - 1s - loss: 0.7735 - val_loss: 1.2578
Epoch 1964/5000
26/26 - 1s - loss: 0.7716 - val_loss: 1.2553
Epoch 1965/5000
26/26 - 1s - loss: 0.7724 - val_loss: 1.2575
Epoch 1966/5000
26/26 - 1s - loss: 0.7694 - val_loss: 1.2565
Epoch 1967/5000
26/26 - 1s - loss: 0.7705 - val_loss: 1.2535
Epoch 1968/5000
26/26 - 1s - loss: 0.7708 - val_loss: 1.2531
Epoch 1969/5000
26/26 - 1s - loss: 0.7708 - val_loss: 1.2545
Epoch 1970/5000
26/26 - 1s - loss: 0.7687 - val_loss: 1.2547
Epoch 01970: val_loss improved from 1.25867 to 1.25470, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1971/5000
26/26 - 1s - loss: 0.7690 - val_loss: 1.2540
Epoch 1972/5000
26/26 - 1s - loss: 0.7674 - val_loss: 1.2548
Epoch 1973/5000
26/26 - 2s - loss: 0.7683 - val_loss: 1.2539
Epoch 1974/5000
26/26 - 2s - loss: 0.7684 - val_loss: 1.2506
Epoch 1975/5000
26/26 - 1s - loss: 0.7672 - val_loss: 1.2551
Epoch 1976/5000
26/26 - 1s - loss: 0.7662 - val_loss: 1.2536
Epoch 1977/5000
26/26 - 1s - loss: 0.7665 - val_loss: 1.2533
Epoch 1978/5000
26/26 - 1s - loss: 0.7662 - val_loss: 1.2521
Epoch 1979/5000
26/26 - 1s - loss: 0.7662 - val_loss: 1.2498
Epoch 1980/5000
26/26 - 1s - loss: 0.7662 - val_loss: 1.2520
Epoch 01980: val_loss improved from 1.25470 to 1.25198, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1981/5000
26/26 - 1s - loss: 0.7670 - val_loss: 1.2478
Epoch 1982/5000
26/26 - 1s - loss: 0.7652 - val_loss: 1.2484
Epoch 1983/5000
26/26 - 1s - loss: 0.7654 - val_loss: 1.2494
Epoch 1984/5000
26/26 - 1s - loss: 0.7658 - val_loss: 1.2512
Epoch 1985/5000
26/26 - 1s - loss: 0.7647 - val_loss: 1.2471
Epoch 1986/5000
26/26 - 1s - loss: 0.7638 - val_loss: 1.2479
Epoch 1987/5000
26/26 - 1s - loss: 0.7629 - val_loss: 1.2474
Epoch 1988/5000
26/26 - 1s - loss: 0.7634 - val_loss: 1.2491
Epoch 1989/5000
26/26 - 1s - loss: 0.7634 - val_loss: 1.2480
Epoch 1990/5000
26/26 - 2s - loss: 0.7638 - val_loss: 1.2490
Epoch 01990: val_loss improved from 1.25198 to 1.24897, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 1991/5000
26/26 - 1s - loss: 0.7625 - val_loss: 1.2460
Epoch 1992/5000
26/26 - 1s - loss: 0.7629 - val_loss: 1.2455
Epoch 1993/5000
26/26 - 1s - loss: 0.7610 - val_loss: 1.2453
Epoch 1994/5000
26/26 - 1s - loss: 0.7608 - val_loss: 1.2474
Epoch 1995/5000
26/26 - 1s - loss: 0.7613 - val_loss: 1.2455
Epoch 1996/5000
26/26 - 1s - loss: 0.7606 - val_loss: 1.2442
Epoch 1997/5000
26/26 - 1s - loss: 0.7589 - val_loss: 1.2453
Epoch 1998/5000
26/26 - 2s - loss: 0.7587 - val_loss: 1.2443
Epoch 1999/5000
26/26 - 1s - loss: 0.7600 - val_loss: 1.2430
Epoch 2000/5000
26/26 - 1s - loss: 0.7593 - val_loss: 1.2434
Epoch 02000: val_loss improved from 1.24897 to 1.24343, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 2001/5000
26/26 - 1s - loss: 0.7591 - val_loss: 1.2430
Epoch 2002/5000
26/26 - 1s - loss: 0.7574 - val_loss: 1.2458
Epoch 2003/5000
26/26 - 1s - loss: 0.7588 - val_loss: 1.2429
Epoch 2004/5000
26/26 - 1s - loss: 0.7572 - val_loss: 1.2414
Epoch 2005/5000
26/26 - 1s - loss: 0.7588 - val_loss: 1.2433
Epoch 2006/5000
26/26 - 1s - loss: 0.7581 - val_loss: 1.2428
Epoch 2007/5000
26/26 - 1s - loss: 0.7563 - val_loss: 1.2399
Epoch 2008/5000
26/26 - 1s - loss: 0.7569 - val_loss: 1.2406
Epoch 2009/5000
26/26 - 1s - loss: 0.7553 - val_loss: 1.2406
Epoch 2010/5000
26/26 - 1s - loss: 0.7566 - val_loss: 1.2396
Epoch 02010: val_loss improved from 1.24343 to 1.23960, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 2011/5000
26/26 - 1s - loss: 0.7557 - val_loss: 1.2412
Epoch 2012/5000
26/26 - 1s - loss: 0.7558 - val_loss: 1.2413
Epoch 2013/5000
26/26 - 1s - loss: 0.7542 - val_loss: 1.2407
Epoch 2014/5000
26/26 - 1s - loss: 0.7556 - val_loss: 1.2409
Epoch 2015/5000
26/26 - 1s - loss: 0.7547 - val_loss: 1.2415
Epoch 2016/5000
26/26 - 1s - loss: 0.7551 - val_loss: 1.2407
Epoch 2017/5000
26/26 - 1s - loss: 0.7539 - val_loss: 1.2375
Epoch 2018/5000
26/26 - 1s - loss: 0.7526 - val_loss: 1.2390
Epoch 2019/5000
26/26 - 1s - loss: 0.7528 - val_loss: 1.2384
Epoch 2020/5000
26/26 - 1s - loss: 0.7518 - val_loss: 1.2372
Epoch 02020: val_loss improved from 1.23960 to 1.23725, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 2021/5000
26/26 - 1s - loss: 0.7512 - val_loss: 1.2361
Epoch 2022/5000
26/26 - 1s - loss: 0.7521 - val_loss: 1.2375
Epoch 2023/5000
26/26 - 1s - loss: 0.7510 - val_loss: 1.2367
Epoch 2024/5000
26/26 - 1s - loss: 0.7510 - val_loss: 1.2382
Epoch 2025/5000
26/26 - 1s - loss: 0.7510 - val_loss: 1.2357
Epoch 2026/5000
26/26 - 1s - loss: 0.7505 - val_loss: 1.2350
Epoch 2027/5000
26/26 - 1s - loss: 0.7477 - val_loss: 1.2348
Epoch 2028/5000
26/26 - 1s - loss: 0.7497 - val_loss: 1.2329
Epoch 2029/5000
26/26 - 1s - loss: 0.7487 - val_loss: 1.2341
Epoch 2030/5000
26/26 - 1s - loss: 0.7487 - val_loss: 1.2324
Epoch 02030: val_loss improved from 1.23725 to 1.23244, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 2031/5000
26/26 - 1s - loss: 0.7483 - val_loss: 1.2309
Epoch 2032/5000
26/26 - 1s - loss: 0.7482 - val_loss: 1.2339
Epoch 2033/5000
26/26 - 1s - loss: 0.7485 - val_loss: 1.2300
Epoch 2034/5000
26/26 - 1s - loss: 0.7470 - val_loss: 1.2298
Epoch 2035/5000
26/26 - 1s - loss: 0.7470 - val_loss: 1.2310
Epoch 2036/5000
26/26 - 1s - loss: 0.7466 - val_loss: 1.2290
Epoch 2037/5000
26/26 - 1s - loss: 0.7477 - val_loss: 1.2312
Epoch 2038/5000
26/26 - 1s - loss: 0.7452 - val_loss: 1.2286
Epoch 2039/5000
26/26 - 1s - loss: 0.7460 - val_loss: 1.2324
Epoch 2040/5000
26/26 - 2s - loss: 0.7449 - val_loss: 1.2284
Epoch 02040: val_loss improved from 1.23244 to 1.22838, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 2041/5000
26/26 - 1s - loss: 0.7456 - val_loss: 1.2312
Epoch 2042/5000
26/26 - 1s - loss: 0.7438 - val_loss: 1.2288
Epoch 2043/5000
26/26 - 1s - loss: 0.7444 - val_loss: 1.2327
Epoch 2044/5000
26/26 - 1s - loss: 0.7431 - val_loss: 1.2287
Epoch 2045/5000
26/26 - 2s - loss: 0.7434 - val_loss: 1.2315
Epoch 2046/5000
26/26 - 1s - loss: 0.7440 - val_loss: 1.2294
Epoch 2047/5000
26/26 - 1s - loss: 0.7422 - val_loss: 1.2284
Epoch 2048/5000
26/26 - 1s - loss: 0.7425 - val_loss: 1.2264
Epoch 2049/5000
26/26 - 1s - loss: 0.7430 - val_loss: 1.2273
Epoch 2050/5000
26/26 - 1s - loss: 0.7411 - val_loss: 1.2283
Epoch 02050: val_loss improved from 1.22838 to 1.22831, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 2051/5000
26/26 - 1s - loss: 0.7417 - val_loss: 1.2269
Epoch 2052/5000
26/26 - 1s - loss: 0.7412 - val_loss: 1.2277
Epoch 2053/5000
26/26 - 1s - loss: 0.7396 - val_loss: 1.2237
Epoch 2054/5000
26/26 - 1s - loss: 0.7409 - val_loss: 1.2254
Epoch 2055/5000
26/26 - 1s - loss: 0.7401 - val_loss: 1.2241
Epoch 2056/5000
26/26 - 1s - loss: 0.7398 - val_loss: 1.2251
Epoch 2057/5000
26/26 - 1s - loss: 0.7400 - val_loss: 1.2239
Epoch 2058/5000
26/26 - 1s - loss: 0.7394 - val_loss: 1.2234
Epoch 2059/5000
26/26 - 1s - loss: 0.7380 - val_loss: 1.2249
Epoch 2060/5000
26/26 - 1s - loss: 0.7389 - val_loss: 1.2223
Epoch 02060: val_loss improved from 1.22831 to 1.22231, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 2061/5000
26/26 - 1s - loss: 0.7379 - val_loss: 1.2248
Epoch 2062/5000
26/26 - 1s - loss: 0.7388 - val_loss: 1.2230
Epoch 2063/5000
26/26 - 1s - loss: 0.7375 - val_loss: 1.2214
Epoch 2064/5000
26/26 - 1s - loss: 0.7372 - val_loss: 1.2231
Epoch 2065/5000
26/26 - 1s - loss: 0.7368 - val_loss: 1.2241
Epoch 2066/5000
26/26 - 1s - loss: 0.7365 - val_loss: 1.2233
Epoch 2067/5000
26/26 - 1s - loss: 0.7356 - val_loss: 1.2233
Epoch 2068/5000
26/26 - 1s - loss: 0.7352 - val_loss: 1.2214
Epoch 2069/5000
26/26 - 1s - loss: 0.7360 - val_loss: 1.2180
Epoch 2070/5000
26/26 - 1s - loss: 0.7345 - val_loss: 1.2196
Epoch 02070: val_loss improved from 1.22231 to 1.21964, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 2071/5000
26/26 - 1s - loss: 0.7355 - val_loss: 1.2211
Epoch 2072/5000
26/26 - 1s - loss: 0.7347 - val_loss: 1.2186
Epoch 2073/5000
26/26 - 1s - loss: 0.7349 - val_loss: 1.2189
Epoch 2074/5000
26/26 - 1s - loss: 0.7344 - val_loss: 1.2187
Epoch 2075/5000
26/26 - 1s - loss: 0.7340 - val_loss: 1.2178
Epoch 2076/5000
26/26 - 1s - loss: 0.7350 - val_loss: 1.2172
Epoch 2077/5000
26/26 - 2s - loss: 0.7341 - val_loss: 1.2190
Epoch 2078/5000
26/26 - 1s - loss: 0.7319 - val_loss: 1.2159
Epoch 2079/5000
26/26 - 1s - loss: 0.7312 - val_loss: 1.2182
Epoch 2080/5000
26/26 - 1s - loss: 0.7321 - val_loss: 1.2160
Epoch 02080: val_loss improved from 1.21964 to 1.21599, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 2081/5000
26/26 - 1s - loss: 0.7323 - val_loss: 1.2137
Epoch 2082/5000
26/26 - 2s - loss: 0.7320 - val_loss: 1.2155
Epoch 2083/5000
26/26 - 1s - loss: 0.7296 - val_loss: 1.2151
Epoch 2084/5000
26/26 - 1s - loss: 0.7325 - val_loss: 1.2157
Epoch 2085/5000
26/26 - 1s - loss: 0.7299 - val_loss: 1.2130
Epoch 2086/5000
26/26 - 1s - loss: 0.7306 - val_loss: 1.2151
Epoch 2087/5000
26/26 - 1s - loss: 0.7290 - val_loss: 1.2124
Epoch 2088/5000
26/26 - 1s - loss: 0.7281 - val_loss: 1.2145
Epoch 2089/5000
26/26 - 1s - loss: 0.7285 - val_loss: 1.2123
Epoch 2090/5000
26/26 - 1s - loss: 0.7301 - val_loss: 1.2134
Epoch 02090: val_loss improved from 1.21599 to 1.21335, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 2091/5000
26/26 - 1s - loss: 0.7283 - val_loss: 1.2150
Epoch 2092/5000
26/26 - 1s - loss: 0.7297 - val_loss: 1.2157
Epoch 2093/5000
26/26 - 1s - loss: 0.7280 - val_loss: 1.2114
Epoch 2094/5000
26/26 - 1s - loss: 0.7279 - val_loss: 1.2131
Epoch 2095/5000
26/26 - 1s - loss: 0.7273 - val_loss: 1.2122
Epoch 2096/5000
26/26 - 1s - loss: 0.7277 - val_loss: 1.2114
Epoch 2097/5000
26/26 - 1s - loss: 0.7276 - val_loss: 1.2125
Epoch 2098/5000
26/26 - 1s - loss: 0.7270 - val_loss: 1.2133
Epoch 2099/5000
26/26 - 1s - loss: 0.7263 - val_loss: 1.2100
Epoch 2100/5000
26/26 - 1s - loss: 0.7254 - val_loss: 1.2113
Epoch 02100: val_loss improved from 1.21335 to 1.21128, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 2101/5000
26/26 - 1s - loss: 0.7256 - val_loss: 1.2096
Epoch 2102/5000
26/26 - 1s - loss: 0.7251 - val_loss: 1.2110
Epoch 2103/5000
26/26 - 1s - loss: 0.7252 - val_loss: 1.2106
Epoch 2104/5000
26/26 - 1s - loss: 0.7234 - val_loss: 1.2095
Epoch 2105/5000
26/26 - 1s - loss: 0.7245 - val_loss: 1.2087
Epoch 2106/5000
26/26 - 1s - loss: 0.7237 - val_loss: 1.2077
Epoch 2107/5000
26/26 - 1s - loss: 0.7246 - val_loss: 1.2095
Epoch 2108/5000
26/26 - 1s - loss: 0.7240 - val_loss: 1.2091
Epoch 2109/5000
26/26 - 1s - loss: 0.7228 - val_loss: 1.2092
Epoch 2110/5000
26/26 - 1s - loss: 0.7234 - val_loss: 1.2084
Epoch 02110: val_loss improved from 1.21128 to 1.20835, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 2111/5000
26/26 - 1s - loss: 0.7229 - val_loss: 1.2080
Epoch 2112/5000
26/26 - 1s - loss: 0.7220 - val_loss: 1.2069
Epoch 2113/5000
26/26 - 1s - loss: 0.7223 - val_loss: 1.2084
Epoch 2114/5000
26/26 - 1s - loss: 0.7223 - val_loss: 1.2052
Epoch 2115/5000
26/26 - 1s - loss: 0.7223 - val_loss: 1.2056
Epoch 2116/5000
26/26 - 1s - loss: 0.7205 - val_loss: 1.2075
Epoch 2117/5000
26/26 - 1s - loss: 0.7214 - val_loss: 1.2050
Epoch 2118/5000
26/26 - 1s - loss: 0.7189 - val_loss: 1.2040
Epoch 2119/5000
26/26 - 1s - loss: 0.7188 - val_loss: 1.2053
Epoch 2120/5000
26/26 - 1s - loss: 0.7177 - val_loss: 1.2046
Epoch 02120: val_loss improved from 1.20835 to 1.20457, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 2121/5000
26/26 - 1s - loss: 0.7206 - val_loss: 1.2014
Epoch 2122/5000
26/26 - 1s - loss: 0.7196 - val_loss: 1.2044
Epoch 2123/5000
26/26 - 2s - loss: 0.7186 - val_loss: 1.2019
Epoch 2124/5000
26/26 - 1s - loss: 0.7178 - val_loss: 1.2029
Epoch 2125/5000
26/26 - 1s - loss: 0.7177 - val_loss: 1.2002
Epoch 2126/5000
26/26 - 1s - loss: 0.7176 - val_loss: 1.2016
Epoch 2127/5000
26/26 - 1s - loss: 0.7161 - val_loss: 1.2030
Epoch 2128/5000
26/26 - 1s - loss: 0.7156 - val_loss: 1.2011
Epoch 2129/5000
26/26 - 1s - loss: 0.7175 - val_loss: 1.1990
Epoch 2130/5000
26/26 - 1s - loss: 0.7166 - val_loss: 1.2012
Epoch 02130: val_loss improved from 1.20457 to 1.20123, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 2131/5000
26/26 - 1s - loss: 0.7170 - val_loss: 1.1998
Epoch 2132/5000
26/26 - 1s - loss: 0.7157 - val_loss: 1.1993
Epoch 2133/5000
26/26 - 1s - loss: 0.7165 - val_loss: 1.2013
Epoch 2134/5000
26/26 - 1s - loss: 0.7152 - val_loss: 1.2016
Epoch 2135/5000
26/26 - 1s - loss: 0.7145 - val_loss: 1.1985
Epoch 2136/5000
26/26 - 1s - loss: 0.7150 - val_loss: 1.1989
Epoch 2137/5000
26/26 - 1s - loss: 0.7140 - val_loss: 1.1985
Epoch 2138/5000
26/26 - 1s - loss: 0.7138 - val_loss: 1.1983
Epoch 2139/5000
26/26 - 1s - loss: 0.7137 - val_loss: 1.1987
Epoch 2140/5000
26/26 - 1s - loss: 0.7127 - val_loss: 1.1983
Epoch 02140: val_loss improved from 1.20123 to 1.19834, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 2141/5000
26/26 - 1s - loss: 0.7124 - val_loss: 1.1994
Epoch 2142/5000
26/26 - 1s - loss: 0.7138 - val_loss: 1.1949
Epoch 2143/5000
26/26 - 1s - loss: 0.7125 - val_loss: 1.1947
Epoch 2144/5000
26/26 - 1s - loss: 0.7129 - val_loss: 1.1996
Epoch 2145/5000
26/26 - 1s - loss: 0.7123 - val_loss: 1.1964
Epoch 2146/5000
26/26 - 1s - loss: 0.7116 - val_loss: 1.1952
Epoch 2147/5000
26/26 - 1s - loss: 0.7110 - val_loss: 1.1942
Epoch 2148/5000
26/26 - 1s - loss: 0.7096 - val_loss: 1.1954
Epoch 2149/5000
26/26 - 1s - loss: 0.7103 - val_loss: 1.1944
Epoch 2150/5000
26/26 - 1s - loss: 0.7115 - val_loss: 1.1939
Epoch 02150: val_loss improved from 1.19834 to 1.19387, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 2151/5000
26/26 - 1s - loss: 0.7100 - val_loss: 1.1929
Epoch 2152/5000
26/26 - 1s - loss: 0.7111 - val_loss: 1.1948
Epoch 2153/5000
26/26 - 1s - loss: 0.7099 - val_loss: 1.1919
Epoch 2154/5000
26/26 - 1s - loss: 0.7092 - val_loss: 1.1918
Epoch 2155/5000
26/26 - 1s - loss: 0.7088 - val_loss: 1.1944
Epoch 2156/5000
26/26 - 1s - loss: 0.7087 - val_loss: 1.1939
Epoch 2157/5000
26/26 - 1s - loss: 0.7082 - val_loss: 1.1912
Epoch 2158/5000
26/26 - 1s - loss: 0.7086 - val_loss: 1.1903
Epoch 2159/5000
26/26 - 1s - loss: 0.7073 - val_loss: 1.1911
Epoch 2160/5000
26/26 - 1s - loss: 0.7080 - val_loss: 1.1897
Epoch 02160: val_loss improved from 1.19387 to 1.18968, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 2161/5000
26/26 - 1s - loss: 0.7068 - val_loss: 1.1938
Epoch 2162/5000
26/26 - 1s - loss: 0.7073 - val_loss: 1.1903
Epoch 2163/5000
26/26 - 1s - loss: 0.7061 - val_loss: 1.1918
Epoch 2164/5000
26/26 - 1s - loss: 0.7071 - val_loss: 1.1900
Epoch 2165/5000
26/26 - 2s - loss: 0.7050 - val_loss: 1.1902
Epoch 2166/5000
26/26 - 1s - loss: 0.7051 - val_loss: 1.1903
Epoch 2167/5000
26/26 - 1s - loss: 0.7053 - val_loss: 1.1893
Epoch 2168/5000
26/26 - 1s - loss: 0.7064 - val_loss: 1.1909
Epoch 2169/5000
26/26 - 2s - loss: 0.7049 - val_loss: 1.1905
Epoch 2170/5000
26/26 - 1s - loss: 0.7041 - val_loss: 1.1894
Epoch 02170: val_loss improved from 1.18968 to 1.18944, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 2171/5000
26/26 - 1s - loss: 0.7043 - val_loss: 1.1857
Epoch 2172/5000
26/26 - 1s - loss: 0.7035 - val_loss: 1.1872
Epoch 2173/5000
26/26 - 1s - loss: 0.7038 - val_loss: 1.1872
Epoch 2174/5000
26/26 - 1s - loss: 0.7030 - val_loss: 1.1871
Epoch 2175/5000
26/26 - 1s - loss: 0.7030 - val_loss: 1.1880
Epoch 2176/5000
26/26 - 1s - loss: 0.7015 - val_loss: 1.1891
Epoch 2177/5000
26/26 - 1s - loss: 0.7017 - val_loss: 1.1859
Epoch 2178/5000
26/26 - 1s - loss: 0.7019 - val_loss: 1.1854
Epoch 2179/5000
26/26 - 1s - loss: 0.7021 - val_loss: 1.1864
Epoch 2180/5000
26/26 - 1s - loss: 0.7004 - val_loss: 1.1845
Epoch 02180: val_loss improved from 1.18944 to 1.18452, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 2181/5000
26/26 - 1s - loss: 0.6996 - val_loss: 1.1866
Epoch 2182/5000
26/26 - 1s - loss: 0.7012 - val_loss: 1.1852
Epoch 2183/5000
26/26 - 1s - loss: 0.7016 - val_loss: 1.1844
Epoch 2184/5000
26/26 - 1s - loss: 0.7005 - val_loss: 1.1835
Epoch 2185/5000
26/26 - 1s - loss: 0.6997 - val_loss: 1.1852
Epoch 2186/5000
26/26 - 1s - loss: 0.6997 - val_loss: 1.1857
Epoch 2187/5000
26/26 - 1s - loss: 0.6990 - val_loss: 1.1843
Epoch 2188/5000
26/26 - 1s - loss: 0.6969 - val_loss: 1.1852
Epoch 2189/5000
26/26 - 1s - loss: 0.6984 - val_loss: 1.1826
Epoch 2190/5000
26/26 - 1s - loss: 0.6981 - val_loss: 1.1821
Epoch 02190: val_loss improved from 1.18452 to 1.18215, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 2191/5000
26/26 - 1s - loss: 0.6975 - val_loss: 1.1799
Epoch 2192/5000
26/26 - 1s - loss: 0.6975 - val_loss: 1.1827
Epoch 2193/5000
26/26 - 1s - loss: 0.6977 - val_loss: 1.1817
Epoch 2194/5000
26/26 - 2s - loss: 0.6962 - val_loss: 1.1814
Epoch 2195/5000
26/26 - 1s - loss: 0.6960 - val_loss: 1.1812
Epoch 2196/5000
26/26 - 1s - loss: 0.6947 - val_loss: 1.1814
Epoch 2197/5000
26/26 - 1s - loss: 0.6954 - val_loss: 1.1809
Epoch 2198/5000
26/26 - 1s - loss: 0.6975 - val_loss: 1.1816
Epoch 2199/5000
26/26 - 1s - loss: 0.6954 - val_loss: 1.1824
Epoch 2200/5000
26/26 - 1s - loss: 0.6958 - val_loss: 1.1816
Epoch 02200: val_loss improved from 1.18215 to 1.18160, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 2201/5000
26/26 - 1s - loss: 0.6953 - val_loss: 1.1788
Epoch 2202/5000
26/26 - 2s - loss: 0.6941 - val_loss: 1.1807
Epoch 2203/5000
26/26 - 1s - loss: 0.6944 - val_loss: 1.1772
Epoch 2204/5000
26/26 - 1s - loss: 0.6934 - val_loss: 1.1780
Epoch 2205/5000
26/26 - 1s - loss: 0.6917 - val_loss: 1.1768
Epoch 2206/5000
26/26 - 1s - loss: 0.6933 - val_loss: 1.1797
Epoch 2207/5000
26/26 - 2s - loss: 0.6936 - val_loss: 1.1789
Epoch 2208/5000
26/26 - 1s - loss: 0.6935 - val_loss: 1.1770
Epoch 2209/5000
26/26 - 1s - loss: 0.6926 - val_loss: 1.1767
Epoch 2210/5000
26/26 - 1s - loss: 0.6932 - val_loss: 1.1782
Epoch 02210: val_loss improved from 1.18160 to 1.17816, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 2211/5000
26/26 - 1s - loss: 0.6939 - val_loss: 1.1785
Epoch 2212/5000
26/26 - 1s - loss: 0.6918 - val_loss: 1.1784
Epoch 2213/5000
26/26 - 1s - loss: 0.6932 - val_loss: 1.1777
Epoch 2214/5000
26/26 - 1s - loss: 0.6917 - val_loss: 1.1774
Epoch 2215/5000
26/26 - 1s - loss: 0.6903 - val_loss: 1.1763
Epoch 2216/5000
26/26 - 1s - loss: 0.6905 - val_loss: 1.1765
Epoch 2217/5000
26/26 - 1s - loss: 0.6905 - val_loss: 1.1765
Epoch 2218/5000
26/26 - 1s - loss: 0.6900 - val_loss: 1.1774
Epoch 2219/5000
26/26 - 1s - loss: 0.6890 - val_loss: 1.1747
Epoch 2220/5000
26/26 - 1s - loss: 0.6891 - val_loss: 1.1736
Epoch 02220: val_loss improved from 1.17816 to 1.17363, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 2221/5000
26/26 - 1s - loss: 0.6905 - val_loss: 1.1747
Epoch 2222/5000
26/26 - 1s - loss: 0.6895 - val_loss: 1.1755
Epoch 2223/5000
26/26 - 1s - loss: 0.6889 - val_loss: 1.1722
Epoch 2224/5000
26/26 - 2s - loss: 0.6890 - val_loss: 1.1736
Epoch 2225/5000
26/26 - 1s - loss: 0.6890 - val_loss: 1.1728
Epoch 2226/5000
26/26 - 1s - loss: 0.6880 - val_loss: 1.1717
Epoch 2227/5000
26/26 - 1s - loss: 0.6871 - val_loss: 1.1722
Epoch 2228/5000
26/26 - 1s - loss: 0.6885 - val_loss: 1.1723
Epoch 2229/5000
26/26 - 1s - loss: 0.6863 - val_loss: 1.1720
Epoch 2230/5000
26/26 - 1s - loss: 0.6870 - val_loss: 1.1712
Epoch 02230: val_loss improved from 1.17363 to 1.17124, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 2231/5000
26/26 - 1s - loss: 0.6863 - val_loss: 1.1716
Epoch 2232/5000
26/26 - 1s - loss: 0.6872 - val_loss: 1.1705
Epoch 2233/5000
26/26 - 1s - loss: 0.6868 - val_loss: 1.1719
Epoch 2234/5000
26/26 - 1s - loss: 0.6867 - val_loss: 1.1690
Epoch 2235/5000
26/26 - 1s - loss: 0.6861 - val_loss: 1.1700
Epoch 2236/5000
26/26 - 1s - loss: 0.6861 - val_loss: 1.1687
Epoch 2237/5000
26/26 - 1s - loss: 0.6851 - val_loss: 1.1679
Epoch 2238/5000
26/26 - 1s - loss: 0.6849 - val_loss: 1.1701
Epoch 2239/5000
26/26 - 1s - loss: 0.6856 - val_loss: 1.1700
Epoch 2240/5000
26/26 - 1s - loss: 0.6846 - val_loss: 1.1680
Epoch 02240: val_loss improved from 1.17124 to 1.16802, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 2241/5000
26/26 - 1s - loss: 0.6838 - val_loss: 1.1658
Epoch 2242/5000
26/26 - 1s - loss: 0.6842 - val_loss: 1.1685
Epoch 2243/5000
26/26 - 1s - loss: 0.6838 - val_loss: 1.1677
Epoch 2244/5000
26/26 - 1s - loss: 0.6838 - val_loss: 1.1682
Epoch 2245/5000
26/26 - 1s - loss: 0.6832 - val_loss: 1.1673
Epoch 2246/5000
26/26 - 1s - loss: 0.6826 - val_loss: 1.1662
Epoch 2247/5000
26/26 - 1s - loss: 0.6829 - val_loss: 1.1683
Epoch 2248/5000
26/26 - 1s - loss: 0.6833 - val_loss: 1.1654
Epoch 2249/5000
26/26 - 1s - loss: 0.6816 - val_loss: 1.1667
Epoch 2250/5000
26/26 - 1s - loss: 0.6815 - val_loss: 1.1674
Epoch 02250: val_loss improved from 1.16802 to 1.16743, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 2251/5000
26/26 - 1s - loss: 0.6813 - val_loss: 1.1658
Epoch 2252/5000
26/26 - 1s - loss: 0.6817 - val_loss: 1.1666
Epoch 2253/5000
26/26 - 1s - loss: 0.6803 - val_loss: 1.1654
Epoch 2254/5000
26/26 - 1s - loss: 0.6804 - val_loss: 1.1678
Epoch 2255/5000
26/26 - 1s - loss: 0.6807 - val_loss: 1.1671
Epoch 2256/5000
26/26 - 1s - loss: 0.6803 - val_loss: 1.1660
Epoch 2257/5000
26/26 - 1s - loss: 0.6812 - val_loss: 1.1647
Epoch 2258/5000
26/26 - 1s - loss: 0.6795 - val_loss: 1.1633
Epoch 2259/5000
26/26 - 1s - loss: 0.6794 - val_loss: 1.1644
Epoch 2260/5000
26/26 - 1s - loss: 0.6779 - val_loss: 1.1637
Epoch 02260: val_loss improved from 1.16743 to 1.16370, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 2261/5000
26/26 - 1s - loss: 0.6777 - val_loss: 1.1651
Epoch 2262/5000
26/26 - 1s - loss: 0.6779 - val_loss: 1.1640
Epoch 2263/5000
26/26 - 1s - loss: 0.6774 - val_loss: 1.1646
Epoch 2264/5000
26/26 - 1s - loss: 0.6764 - val_loss: 1.1641
Epoch 2265/5000
26/26 - 1s - loss: 0.6771 - val_loss: 1.1628
Epoch 2266/5000
26/26 - 1s - loss: 0.6763 - val_loss: 1.1620
Epoch 2267/5000
26/26 - 1s - loss: 0.6765 - val_loss: 1.1619
Epoch 2268/5000
26/26 - 1s - loss: 0.6763 - val_loss: 1.1603
Epoch 2269/5000
26/26 - 1s - loss: 0.6756 - val_loss: 1.1601
Epoch 2270/5000
26/26 - 1s - loss: 0.6766 - val_loss: 1.1611
Epoch 02270: val_loss improved from 1.16370 to 1.16114, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 2271/5000
26/26 - 1s - loss: 0.6754 - val_loss: 1.1591
Epoch 2272/5000
26/26 - 1s - loss: 0.6756 - val_loss: 1.1611
Epoch 2273/5000
26/26 - 1s - loss: 0.6743 - val_loss: 1.1593
Epoch 2274/5000
26/26 - 1s - loss: 0.6747 - val_loss: 1.1577
Epoch 2275/5000
26/26 - 1s - loss: 0.6749 - val_loss: 1.1604
Epoch 2276/5000
26/26 - 1s - loss: 0.6751 - val_loss: 1.1579
Epoch 2277/5000
26/26 - 1s - loss: 0.6729 - val_loss: 1.1602
Epoch 2278/5000
26/26 - 1s - loss: 0.6753 - val_loss: 1.1567
Epoch 2279/5000
26/26 - 1s - loss: 0.6742 - val_loss: 1.1576
Epoch 2280/5000
26/26 - 1s - loss: 0.6737 - val_loss: 1.1577
Epoch 02280: val_loss improved from 1.16114 to 1.15770, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 2281/5000
26/26 - 1s - loss: 0.6733 - val_loss: 1.1550
Epoch 2282/5000
26/26 - 1s - loss: 0.6740 - val_loss: 1.1562
Epoch 2283/5000
26/26 - 1s - loss: 0.6724 - val_loss: 1.1549
Epoch 2284/5000
26/26 - 1s - loss: 0.6713 - val_loss: 1.1559
Epoch 2285/5000
26/26 - 1s - loss: 0.6723 - val_loss: 1.1568
Epoch 2286/5000
26/26 - 1s - loss: 0.6723 - val_loss: 1.1566
Epoch 2287/5000
26/26 - 1s - loss: 0.6718 - val_loss: 1.1557
Epoch 2288/5000
26/26 - 1s - loss: 0.6714 - val_loss: 1.1554
Epoch 2289/5000
26/26 - 1s - loss: 0.6709 - val_loss: 1.1563
Epoch 2290/5000
26/26 - 2s - loss: 0.6718 - val_loss: 1.1568
Epoch 02290: val_loss improved from 1.15770 to 1.15680, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 2291/5000
26/26 - 1s - loss: 0.6699 - val_loss: 1.1539
Epoch 2292/5000
26/26 - 1s - loss: 0.6699 - val_loss: 1.1561
Epoch 2293/5000
26/26 - 1s - loss: 0.6708 - val_loss: 1.1566
Epoch 2294/5000
26/26 - 1s - loss: 0.6705 - val_loss: 1.1541
Epoch 2295/5000
26/26 - 1s - loss: 0.6687 - val_loss: 1.1563
Epoch 2296/5000
26/26 - 1s - loss: 0.6701 - val_loss: 1.1544
Epoch 2297/5000
26/26 - 1s - loss: 0.6677 - val_loss: 1.1549
Epoch 2298/5000
26/26 - 1s - loss: 0.6681 - val_loss: 1.1562
Epoch 2299/5000
26/26 - 1s - loss: 0.6686 - val_loss: 1.1542
Epoch 2300/5000
26/26 - 1s - loss: 0.6686 - val_loss: 1.1523
Epoch 02300: val_loss improved from 1.15680 to 1.15233, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 2301/5000
26/26 - 1s - loss: 0.6678 - val_loss: 1.1529
Epoch 2302/5000
26/26 - 1s - loss: 0.6664 - val_loss: 1.1516
Epoch 2303/5000
26/26 - 1s - loss: 0.6663 - val_loss: 1.1519
Epoch 2304/5000
26/26 - 1s - loss: 0.6665 - val_loss: 1.1508
Epoch 2305/5000
26/26 - 1s - loss: 0.6667 - val_loss: 1.1528
Epoch 2306/5000
26/26 - 1s - loss: 0.6668 - val_loss: 1.1508
Epoch 2307/5000
26/26 - 1s - loss: 0.6671 - val_loss: 1.1514
Epoch 2308/5000
26/26 - 1s - loss: 0.6661 - val_loss: 1.1516
Epoch 2309/5000
26/26 - 1s - loss: 0.6648 - val_loss: 1.1514
Epoch 2310/5000
26/26 - 1s - loss: 0.6656 - val_loss: 1.1504
Epoch 02310: val_loss improved from 1.15233 to 1.15035, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 2311/5000
26/26 - 1s - loss: 0.6657 - val_loss: 1.1523
Epoch 2312/5000
26/26 - 1s - loss: 0.6650 - val_loss: 1.1498
Epoch 2313/5000
26/26 - 1s - loss: 0.6654 - val_loss: 1.1501
Epoch 2314/5000
26/26 - 1s - loss: 0.6635 - val_loss: 1.1494
Epoch 2315/5000
26/26 - 1s - loss: 0.6647 - val_loss: 1.1498
Epoch 2316/5000
26/26 - 1s - loss: 0.6635 - val_loss: 1.1481
Epoch 2317/5000
26/26 - 1s - loss: 0.6633 - val_loss: 1.1484
Epoch 2318/5000
26/26 - 1s - loss: 0.6635 - val_loss: 1.1471
Epoch 2319/5000
26/26 - 1s - loss: 0.6641 - val_loss: 1.1485
Epoch 2320/5000
26/26 - 1s - loss: 0.6638 - val_loss: 1.1464
Epoch 02320: val_loss improved from 1.15035 to 1.14639, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 2321/5000
26/26 - 1s - loss: 0.6615 - val_loss: 1.1472
Epoch 2322/5000
26/26 - 1s - loss: 0.6627 - val_loss: 1.1461
Epoch 2323/5000
26/26 - 1s - loss: 0.6621 - val_loss: 1.1464
Epoch 2324/5000
26/26 - 1s - loss: 0.6618 - val_loss: 1.1465
Epoch 2325/5000
26/26 - 1s - loss: 0.6620 - val_loss: 1.1450
Epoch 2326/5000
26/26 - 1s - loss: 0.6615 - val_loss: 1.1468
Epoch 2327/5000
26/26 - 1s - loss: 0.6625 - val_loss: 1.1472
Epoch 2328/5000
26/26 - 1s - loss: 0.6606 - val_loss: 1.1454
Epoch 2329/5000
26/26 - 1s - loss: 0.6617 - val_loss: 1.1463
Epoch 2330/5000
26/26 - 1s - loss: 0.6598 - val_loss: 1.1466
Epoch 02330: val_loss did not improve from 1.14639
Epoch 2331/5000
26/26 - 1s - loss: 0.6599 - val_loss: 1.1464
Epoch 2332/5000
26/26 - 1s - loss: 0.6605 - val_loss: 1.1447
Epoch 2333/5000
26/26 - 1s - loss: 0.6604 - val_loss: 1.1438
Epoch 2334/5000
26/26 - 1s - loss: 0.6594 - val_loss: 1.1422
Epoch 2335/5000
26/26 - 1s - loss: 0.6587 - val_loss: 1.1426
Epoch 2336/5000
26/26 - 1s - loss: 0.6583 - val_loss: 1.1446
Epoch 2337/5000
26/26 - 1s - loss: 0.6579 - val_loss: 1.1442
Epoch 2338/5000
26/26 - 1s - loss: 0.6584 - val_loss: 1.1413
Epoch 2339/5000
26/26 - 1s - loss: 0.6583 - val_loss: 1.1412
Epoch 2340/5000
26/26 - 1s - loss: 0.6583 - val_loss: 1.1414
Epoch 02340: val_loss improved from 1.14639 to 1.14138, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 2341/5000
26/26 - 1s - loss: 0.6566 - val_loss: 1.1425
Epoch 2342/5000
26/26 - 1s - loss: 0.6571 - val_loss: 1.1424
Epoch 2343/5000
26/26 - 1s - loss: 0.6567 - val_loss: 1.1418
Epoch 2344/5000
26/26 - 1s - loss: 0.6579 - val_loss: 1.1406
Epoch 2345/5000
26/26 - 1s - loss: 0.6566 - val_loss: 1.1388
Epoch 2346/5000
26/26 - 1s - loss: 0.6569 - val_loss: 1.1412
Epoch 2347/5000
26/26 - 1s - loss: 0.6556 - val_loss: 1.1413
Epoch 2348/5000
26/26 - 1s - loss: 0.6562 - val_loss: 1.1409
Epoch 2349/5000
26/26 - 1s - loss: 0.6558 - val_loss: 1.1403
Epoch 2350/5000
26/26 - 1s - loss: 0.6562 - val_loss: 1.1402
Epoch 02350: val_loss improved from 1.14138 to 1.14023, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 2351/5000
26/26 - 1s - loss: 0.6556 - val_loss: 1.1370
Epoch 2352/5000
26/26 - 1s - loss: 0.6562 - val_loss: 1.1387
Epoch 2353/5000
26/26 - 1s - loss: 0.6522 - val_loss: 1.1393
Epoch 2354/5000
26/26 - 1s - loss: 0.6536 - val_loss: 1.1400
Epoch 2355/5000
26/26 - 1s - loss: 0.6538 - val_loss: 1.1388
Epoch 2356/5000
26/26 - 1s - loss: 0.6543 - val_loss: 1.1361
Epoch 2357/5000
26/26 - 1s - loss: 0.6532 - val_loss: 1.1392
Epoch 2358/5000
26/26 - 1s - loss: 0.6531 - val_loss: 1.1370
Epoch 2359/5000
26/26 - 1s - loss: 0.6528 - val_loss: 1.1370
Epoch 2360/5000
26/26 - 1s - loss: 0.6536 - val_loss: 1.1387
Epoch 02360: val_loss improved from 1.14023 to 1.13866, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 2361/5000
26/26 - 1s - loss: 0.6528 - val_loss: 1.1381
Epoch 2362/5000
26/26 - 1s - loss: 0.6532 - val_loss: 1.1372
Epoch 2363/5000
26/26 - 1s - loss: 0.6529 - val_loss: 1.1364
Epoch 2364/5000
26/26 - 1s - loss: 0.6527 - val_loss: 1.1382
Epoch 2365/5000
26/26 - 1s - loss: 0.6504 - val_loss: 1.1354
Epoch 2366/5000
26/26 - 1s - loss: 0.6501 - val_loss: 1.1374
Epoch 2367/5000
26/26 - 1s - loss: 0.6498 - val_loss: 1.1348
Epoch 2368/5000
26/26 - 1s - loss: 0.6523 - val_loss: 1.1367
Epoch 2369/5000
26/26 - 1s - loss: 0.6519 - val_loss: 1.1361
Epoch 2370/5000
26/26 - 1s - loss: 0.6491 - val_loss: 1.1342
Epoch 02370: val_loss improved from 1.13866 to 1.13419, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 2371/5000
26/26 - 1s - loss: 0.6505 - val_loss: 1.1356
Epoch 2372/5000
26/26 - 1s - loss: 0.6498 - val_loss: 1.1364
Epoch 2373/5000
26/26 - 1s - loss: 0.6490 - val_loss: 1.1354
Epoch 2374/5000
26/26 - 2s - loss: 0.6506 - val_loss: 1.1358
Epoch 2375/5000
26/26 - 1s - loss: 0.6488 - val_loss: 1.1353
Epoch 2376/5000
26/26 - 1s - loss: 0.6495 - val_loss: 1.1343
Epoch 2377/5000
26/26 - 1s - loss: 0.6483 - val_loss: 1.1337
Epoch 2378/5000
26/26 - 1s - loss: 0.6483 - val_loss: 1.1353
Epoch 2379/5000
26/26 - 1s - loss: 0.6490 - val_loss: 1.1331
Epoch 2380/5000
26/26 - 1s - loss: 0.6480 - val_loss: 1.1318
Epoch 02380: val_loss improved from 1.13419 to 1.13179, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 2381/5000
26/26 - 1s - loss: 0.6489 - val_loss: 1.1315
Epoch 2382/5000
26/26 - 1s - loss: 0.6472 - val_loss: 1.1337
Epoch 2383/5000
26/26 - 1s - loss: 0.6486 - val_loss: 1.1344
Epoch 2384/5000
26/26 - 1s - loss: 0.6472 - val_loss: 1.1339
Epoch 2385/5000
26/26 - 1s - loss: 0.6470 - val_loss: 1.1349
Epoch 2386/5000
26/26 - 1s - loss: 0.6472 - val_loss: 1.1305
Epoch 2387/5000
26/26 - 1s - loss: 0.6463 - val_loss: 1.1294
Epoch 2388/5000
26/26 - 1s - loss: 0.6470 - val_loss: 1.1309
Epoch 2389/5000
26/26 - 1s - loss: 0.6458 - val_loss: 1.1304
Epoch 2390/5000
26/26 - 1s - loss: 0.6450 - val_loss: 1.1319
Epoch 02390: val_loss did not improve from 1.13179
Epoch 2391/5000
26/26 - 1s - loss: 0.6458 - val_loss: 1.1329
Epoch 2392/5000
26/26 - 1s - loss: 0.6455 - val_loss: 1.1320
Epoch 2393/5000
26/26 - 1s - loss: 0.6441 - val_loss: 1.1306
Epoch 2394/5000
26/26 - 1s - loss: 0.6464 - val_loss: 1.1303
Epoch 2395/5000
26/26 - 1s - loss: 0.6437 - val_loss: 1.1278
Epoch 2396/5000
26/26 - 1s - loss: 0.6440 - val_loss: 1.1280
Epoch 2397/5000
26/26 - 2s - loss: 0.6442 - val_loss: 1.1310
Epoch 2398/5000
26/26 - 1s - loss: 0.6438 - val_loss: 1.1275
Epoch 2399/5000
26/26 - 2s - loss: 0.6438 - val_loss: 1.1297
Epoch 2400/5000
26/26 - 1s - loss: 0.6430 - val_loss: 1.1291
Epoch 02400: val_loss improved from 1.13179 to 1.12910, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 2401/5000
26/26 - 1s - loss: 0.6424 - val_loss: 1.1273
Epoch 2402/5000
26/26 - 1s - loss: 0.6422 - val_loss: 1.1276
Epoch 2403/5000
26/26 - 1s - loss: 0.6434 - val_loss: 1.1263
Epoch 2404/5000
26/26 - 1s - loss: 0.6418 - val_loss: 1.1277
Epoch 2405/5000
26/26 - 1s - loss: 0.6430 - val_loss: 1.1289
Epoch 2406/5000
26/26 - 1s - loss: 0.6417 - val_loss: 1.1243
Epoch 2407/5000
26/26 - 1s - loss: 0.6408 - val_loss: 1.1267
Epoch 2408/5000
26/26 - 1s - loss: 0.6412 - val_loss: 1.1266
Epoch 2409/5000
26/26 - 1s - loss: 0.6415 - val_loss: 1.1239
Epoch 2410/5000
26/26 - 1s - loss: 0.6411 - val_loss: 1.1260
Epoch 02410: val_loss improved from 1.12910 to 1.12600, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 2411/5000
26/26 - 1s - loss: 0.6412 - val_loss: 1.1278
Epoch 2412/5000
26/26 - 1s - loss: 0.6417 - val_loss: 1.1283
Epoch 2413/5000
26/26 - 1s - loss: 0.6397 - val_loss: 1.1233
Epoch 2414/5000
26/26 - 1s - loss: 0.6404 - val_loss: 1.1255
Epoch 2415/5000
26/26 - 1s - loss: 0.6395 - val_loss: 1.1227
Epoch 2416/5000
26/26 - 1s - loss: 0.6403 - val_loss: 1.1242
Epoch 2417/5000
26/26 - 1s - loss: 0.6405 - val_loss: 1.1242
Epoch 2418/5000
26/26 - 1s - loss: 0.6385 - val_loss: 1.1240
Epoch 2419/5000
26/26 - 1s - loss: 0.6379 - val_loss: 1.1238
Epoch 2420/5000
26/26 - 1s - loss: 0.6385 - val_loss: 1.1255
Epoch 02420: val_loss improved from 1.12600 to 1.12552, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 2421/5000
26/26 - 1s - loss: 0.6389 - val_loss: 1.1248
Epoch 2422/5000
26/26 - 1s - loss: 0.6366 - val_loss: 1.1257
Epoch 2423/5000
26/26 - 1s - loss: 0.6370 - val_loss: 1.1235
Epoch 2424/5000
26/26 - 1s - loss: 0.6375 - val_loss: 1.1234
Epoch 2425/5000
26/26 - 1s - loss: 0.6385 - val_loss: 1.1217
Epoch 2426/5000
26/26 - 1s - loss: 0.6366 - val_loss: 1.1204
Epoch 2427/5000
26/26 - 1s - loss: 0.6360 - val_loss: 1.1234
Epoch 2428/5000
26/26 - 1s - loss: 0.6381 - val_loss: 1.1218
Epoch 2429/5000
26/26 - 1s - loss: 0.6362 - val_loss: 1.1205
Epoch 2430/5000
26/26 - 1s - loss: 0.6353 - val_loss: 1.1195
Epoch 02430: val_loss improved from 1.12552 to 1.11954, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 2431/5000
26/26 - 1s - loss: 0.6373 - val_loss: 1.1201
Epoch 2432/5000
26/26 - 1s - loss: 0.6359 - val_loss: 1.1211
Epoch 2433/5000
26/26 - 1s - loss: 0.6356 - val_loss: 1.1206
Epoch 2434/5000
26/26 - 1s - loss: 0.6348 - val_loss: 1.1206
Epoch 2435/5000
26/26 - 1s - loss: 0.6355 - val_loss: 1.1192
Epoch 2436/5000
26/26 - 1s - loss: 0.6341 - val_loss: 1.1194
Epoch 2437/5000
26/26 - 1s - loss: 0.6355 - val_loss: 1.1205
Epoch 2438/5000
26/26 - 1s - loss: 0.6338 - val_loss: 1.1197
Epoch 2439/5000
26/26 - 1s - loss: 0.6347 - val_loss: 1.1218
Epoch 2440/5000
26/26 - 1s - loss: 0.6331 - val_loss: 1.1193
Epoch 02440: val_loss improved from 1.11954 to 1.11932, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 2441/5000
26/26 - 1s - loss: 0.6334 - val_loss: 1.1188
Epoch 2442/5000
26/26 - 1s - loss: 0.6330 - val_loss: 1.1181
Epoch 2443/5000
26/26 - 1s - loss: 0.6325 - val_loss: 1.1195
Epoch 2444/5000
26/26 - 1s - loss: 0.6333 - val_loss: 1.1203
Epoch 2445/5000
26/26 - 1s - loss: 0.6326 - val_loss: 1.1170
Epoch 2446/5000
26/26 - 1s - loss: 0.6316 - val_loss: 1.1190
Epoch 2447/5000
26/26 - 1s - loss: 0.6339 - val_loss: 1.1177
Epoch 2448/5000
26/26 - 1s - loss: 0.6323 - val_loss: 1.1181
Epoch 2449/5000
26/26 - 2s - loss: 0.6315 - val_loss: 1.1153
Epoch 2450/5000
26/26 - 1s - loss: 0.6316 - val_loss: 1.1191
Epoch 02450: val_loss improved from 1.11932 to 1.11911, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 2451/5000
26/26 - 1s - loss: 0.6318 - val_loss: 1.1175
Epoch 2452/5000
26/26 - 1s - loss: 0.6298 - val_loss: 1.1160
Epoch 2453/5000
26/26 - 1s - loss: 0.6303 - val_loss: 1.1190
Epoch 2454/5000
26/26 - 1s - loss: 0.6303 - val_loss: 1.1181
Epoch 2455/5000
26/26 - 1s - loss: 0.6303 - val_loss: 1.1197
Epoch 2456/5000
26/26 - 1s - loss: 0.6308 - val_loss: 1.1163
Epoch 2457/5000
26/26 - 1s - loss: 0.6295 - val_loss: 1.1171
Epoch 2458/5000
26/26 - 2s - loss: 0.6305 - val_loss: 1.1159
Epoch 2459/5000
26/26 - 1s - loss: 0.6298 - val_loss: 1.1166
Epoch 2460/5000
26/26 - 1s - loss: 0.6279 - val_loss: 1.1146
Epoch 02460: val_loss improved from 1.11911 to 1.11458, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 2461/5000
26/26 - 1s - loss: 0.6287 - val_loss: 1.1143
Epoch 2462/5000
26/26 - 1s - loss: 0.6292 - val_loss: 1.1149
Epoch 2463/5000
26/26 - 1s - loss: 0.6288 - val_loss: 1.1153
Epoch 2464/5000
26/26 - 1s - loss: 0.6283 - val_loss: 1.1135
Epoch 2465/5000
26/26 - 1s - loss: 0.6277 - val_loss: 1.1142
Epoch 2466/5000
26/26 - 1s - loss: 0.6289 - val_loss: 1.1138
Epoch 2467/5000
26/26 - 1s - loss: 0.6271 - val_loss: 1.1136
Epoch 2468/5000
26/26 - 1s - loss: 0.6268 - val_loss: 1.1143
Epoch 2469/5000
26/26 - 1s - loss: 0.6257 - val_loss: 1.1131
Epoch 2470/5000
26/26 - 1s - loss: 0.6269 - val_loss: 1.1120
Epoch 02470: val_loss improved from 1.11458 to 1.11201, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 2471/5000
26/26 - 1s - loss: 0.6259 - val_loss: 1.1120
Epoch 2472/5000
26/26 - 1s - loss: 0.6274 - val_loss: 1.1129
Epoch 2473/5000
26/26 - 1s - loss: 0.6264 - val_loss: 1.1125
Epoch 2474/5000
26/26 - 1s - loss: 0.6272 - val_loss: 1.1095
Epoch 2475/5000
26/26 - 1s - loss: 0.6264 - val_loss: 1.1120
Epoch 2476/5000
26/26 - 1s - loss: 0.6261 - val_loss: 1.1097
Epoch 2477/5000
26/26 - 1s - loss: 0.6258 - val_loss: 1.1125
Epoch 2478/5000
26/26 - 1s - loss: 0.6265 - val_loss: 1.1105
Epoch 2479/5000
26/26 - 1s - loss: 0.6250 - val_loss: 1.1077
Epoch 2480/5000
26/26 - 1s - loss: 0.6244 - val_loss: 1.1112
Epoch 02480: val_loss improved from 1.11201 to 1.11124, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 2481/5000
26/26 - 1s - loss: 0.6248 - val_loss: 1.1136
Epoch 2482/5000
26/26 - 1s - loss: 0.6248 - val_loss: 1.1079
Epoch 2483/5000
26/26 - 1s - loss: 0.6250 - val_loss: 1.1091
Epoch 2484/5000
26/26 - 1s - loss: 0.6250 - val_loss: 1.1102
Epoch 2485/5000
26/26 - 1s - loss: 0.6231 - val_loss: 1.1120
Epoch 2486/5000
26/26 - 2s - loss: 0.6228 - val_loss: 1.1105
Epoch 2487/5000
26/26 - 1s - loss: 0.6232 - val_loss: 1.1100
Epoch 2488/5000
26/26 - 1s - loss: 0.6229 - val_loss: 1.1088
Epoch 2489/5000
26/26 - 1s - loss: 0.6227 - val_loss: 1.1088
Epoch 2490/5000
26/26 - 1s - loss: 0.6237 - val_loss: 1.1088
Epoch 02490: val_loss improved from 1.11124 to 1.10885, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 2491/5000
26/26 - 1s - loss: 0.6222 - val_loss: 1.1096
Epoch 2492/5000
26/26 - 1s - loss: 0.6220 - val_loss: 1.1064
Epoch 2493/5000
26/26 - 1s - loss: 0.6224 - val_loss: 1.1077
Epoch 2494/5000
26/26 - 1s - loss: 0.6221 - val_loss: 1.1084
Epoch 2495/5000
26/26 - 1s - loss: 0.6222 - val_loss: 1.1080
Epoch 2496/5000
26/26 - 1s - loss: 0.6218 - val_loss: 1.1075
Epoch 2497/5000
26/26 - 1s - loss: 0.6214 - val_loss: 1.1073
Epoch 2498/5000
26/26 - 1s - loss: 0.6214 - val_loss: 1.1071
Epoch 2499/5000
26/26 - 1s - loss: 0.6196 - val_loss: 1.1062
Epoch 2500/5000
26/26 - 1s - loss: 0.6202 - val_loss: 1.1072
Epoch 02500: val_loss improved from 1.10885 to 1.10715, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 2501/5000
26/26 - 1s - loss: 0.6192 - val_loss: 1.1063
Epoch 2502/5000
26/26 - 1s - loss: 0.6191 - val_loss: 1.1050
Epoch 2503/5000
26/26 - 1s - loss: 0.6209 - val_loss: 1.1053
Epoch 2504/5000
26/26 - 1s - loss: 0.6207 - val_loss: 1.1076
Epoch 2505/5000
26/26 - 1s - loss: 0.6192 - val_loss: 1.1065
Epoch 2506/5000
26/26 - 1s - loss: 0.6195 - val_loss: 1.1078
Epoch 2507/5000
26/26 - 1s - loss: 0.6189 - val_loss: 1.1063
Epoch 2508/5000
26/26 - 1s - loss: 0.6184 - val_loss: 1.1051
Epoch 2509/5000
26/26 - 1s - loss: 0.6185 - val_loss: 1.1043
Epoch 2510/5000
26/26 - 1s - loss: 0.6181 - val_loss: 1.1084
Epoch 02510: val_loss did not improve from 1.10715
Epoch 2511/5000
26/26 - 1s - loss: 0.6184 - val_loss: 1.1050
Epoch 2512/5000
26/26 - 1s - loss: 0.6169 - val_loss: 1.1031
Epoch 2513/5000
26/26 - 1s - loss: 0.6175 - val_loss: 1.1047
Epoch 2514/5000
26/26 - 1s - loss: 0.6162 - val_loss: 1.1020
Epoch 2515/5000
26/26 - 1s - loss: 0.6172 - val_loss: 1.1029
Epoch 2516/5000
26/26 - 1s - loss: 0.6168 - val_loss: 1.1024
Epoch 2517/5000
26/26 - 1s - loss: 0.6159 - val_loss: 1.1037
Epoch 2518/5000
26/26 - 1s - loss: 0.6169 - val_loss: 1.1036
Epoch 2519/5000
26/26 - 1s - loss: 0.6175 - val_loss: 1.1028
Epoch 2520/5000
26/26 - 1s - loss: 0.6162 - val_loss: 1.0997
Epoch 02520: val_loss improved from 1.10715 to 1.09965, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 2521/5000
26/26 - 1s - loss: 0.6158 - val_loss: 1.1002
Epoch 2522/5000
26/26 - 1s - loss: 0.6165 - val_loss: 1.1010
Epoch 2523/5000
26/26 - 1s - loss: 0.6143 - val_loss: 1.1015
Epoch 2524/5000
26/26 - 1s - loss: 0.6162 - val_loss: 1.1004
Epoch 2525/5000
26/26 - 1s - loss: 0.6156 - val_loss: 1.1023
Epoch 2526/5000
26/26 - 1s - loss: 0.6142 - val_loss: 1.1003
Epoch 2527/5000
26/26 - 1s - loss: 0.6152 - val_loss: 1.0994
Epoch 2528/5000
26/26 - 1s - loss: 0.6145 - val_loss: 1.1018
Epoch 2529/5000
26/26 - 1s - loss: 0.6139 - val_loss: 1.0997
Epoch 2530/5000
26/26 - 2s - loss: 0.6133 - val_loss: 1.0994
Epoch 02530: val_loss improved from 1.09965 to 1.09939, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 2531/5000
26/26 - 1s - loss: 0.6133 - val_loss: 1.0988
Epoch 2532/5000
26/26 - 1s - loss: 0.6130 - val_loss: 1.1015
Epoch 2533/5000
26/26 - 1s - loss: 0.6137 - val_loss: 1.0996
Epoch 2534/5000
26/26 - 1s - loss: 0.6134 - val_loss: 1.1007
Epoch 2535/5000
26/26 - 1s - loss: 0.6133 - val_loss: 1.0973
Epoch 2536/5000
26/26 - 1s - loss: 0.6119 - val_loss: 1.0990
Epoch 2537/5000
26/26 - 1s - loss: 0.6122 - val_loss: 1.1010
Epoch 2538/5000
26/26 - 1s - loss: 0.6121 - val_loss: 1.1002
Epoch 2539/5000
26/26 - 1s - loss: 0.6126 - val_loss: 1.1010
Epoch 2540/5000
26/26 - 1s - loss: 0.6119 - val_loss: 1.1000
Epoch 02540: val_loss did not improve from 1.09939
Epoch 2541/5000
26/26 - 2s - loss: 0.6116 - val_loss: 1.0998
Epoch 2542/5000
26/26 - 1s - loss: 0.6117 - val_loss: 1.0963
Epoch 2543/5000
26/26 - 1s - loss: 0.6119 - val_loss: 1.0939
Epoch 2544/5000
26/26 - 1s - loss: 0.6127 - val_loss: 1.0986
Epoch 2545/5000
26/26 - 1s - loss: 0.6118 - val_loss: 1.0970
Epoch 2546/5000
26/26 - 1s - loss: 0.6108 - val_loss: 1.0955
Epoch 2547/5000
26/26 - 1s - loss: 0.6101 - val_loss: 1.0963
Epoch 2548/5000
26/26 - 1s - loss: 0.6120 - val_loss: 1.0961
Epoch 2549/5000
26/26 - 1s - loss: 0.6100 - val_loss: 1.0945
Epoch 2550/5000
26/26 - 1s - loss: 0.6103 - val_loss: 1.0945
Epoch 02550: val_loss improved from 1.09939 to 1.09449, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 2551/5000
26/26 - 1s - loss: 0.6090 - val_loss: 1.0941
Epoch 2552/5000
26/26 - 1s - loss: 0.6096 - val_loss: 1.0954
Epoch 2553/5000
26/26 - 1s - loss: 0.6084 - val_loss: 1.0938
Epoch 2554/5000
26/26 - 1s - loss: 0.6088 - val_loss: 1.0946
Epoch 2555/5000
26/26 - 1s - loss: 0.6087 - val_loss: 1.0947
Epoch 2556/5000
26/26 - 1s - loss: 0.6089 - val_loss: 1.0938
Epoch 2557/5000
26/26 - 1s - loss: 0.6084 - val_loss: 1.0945
Epoch 2558/5000
26/26 - 1s - loss: 0.6089 - val_loss: 1.0935
Epoch 2559/5000
26/26 - 1s - loss: 0.6076 - val_loss: 1.0930
Epoch 2560/5000
26/26 - 1s - loss: 0.6066 - val_loss: 1.0923
Epoch 02560: val_loss improved from 1.09449 to 1.09228, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 2561/5000
26/26 - 1s - loss: 0.6082 - val_loss: 1.0944
Epoch 2562/5000
26/26 - 1s - loss: 0.6080 - val_loss: 1.0928
Epoch 2563/5000
26/26 - 1s - loss: 0.6073 - val_loss: 1.0919
Epoch 2564/5000
26/26 - 1s - loss: 0.6071 - val_loss: 1.0931
Epoch 2565/5000
26/26 - 1s - loss: 0.6067 - val_loss: 1.0934
Epoch 2566/5000
26/26 - 1s - loss: 0.6058 - val_loss: 1.0922
Epoch 2567/5000
26/26 - 1s - loss: 0.6059 - val_loss: 1.0933
Epoch 2568/5000
26/26 - 1s - loss: 0.6066 - val_loss: 1.0906
Epoch 2569/5000
26/26 - 1s - loss: 0.6060 - val_loss: 1.0932
Epoch 2570/5000
26/26 - 1s - loss: 0.6060 - val_loss: 1.0909
Epoch 02570: val_loss improved from 1.09228 to 1.09094, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 2571/5000
26/26 - 1s - loss: 0.6044 - val_loss: 1.0913
Epoch 2572/5000
26/26 - 1s - loss: 0.6055 - val_loss: 1.0893
Epoch 2573/5000
26/26 - 1s - loss: 0.6053 - val_loss: 1.0901
Epoch 2574/5000
26/26 - 1s - loss: 0.6056 - val_loss: 1.0913
Epoch 2575/5000
26/26 - 1s - loss: 0.6049 - val_loss: 1.0911
Epoch 2576/5000
26/26 - 1s - loss: 0.6054 - val_loss: 1.0916
Epoch 2577/5000
26/26 - 1s - loss: 0.6046 - val_loss: 1.0916
Epoch 2578/5000
26/26 - 1s - loss: 0.6046 - val_loss: 1.0902
Epoch 2579/5000
26/26 - 1s - loss: 0.6044 - val_loss: 1.0898
Epoch 2580/5000
26/26 - 1s - loss: 0.6041 - val_loss: 1.0881
Epoch 02580: val_loss improved from 1.09094 to 1.08813, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 2581/5000
26/26 - 1s - loss: 0.6042 - val_loss: 1.0898
Epoch 2582/5000
26/26 - 1s - loss: 0.6035 - val_loss: 1.0890
Epoch 2583/5000
26/26 - 2s - loss: 0.6042 - val_loss: 1.0869
Epoch 2584/5000
26/26 - 1s - loss: 0.6037 - val_loss: 1.0893
Epoch 2585/5000
26/26 - 1s - loss: 0.6023 - val_loss: 1.0889
Epoch 2586/5000
26/26 - 1s - loss: 0.6036 - val_loss: 1.0869
Epoch 2587/5000
26/26 - 1s - loss: 0.6020 - val_loss: 1.0897
Epoch 2588/5000
26/26 - 1s - loss: 0.6025 - val_loss: 1.0870
Epoch 2589/5000
26/26 - 1s - loss: 0.6015 - val_loss: 1.0879
Epoch 2590/5000
26/26 - 1s - loss: 0.6018 - val_loss: 1.0891
Epoch 02590: val_loss did not improve from 1.08813
Epoch 2591/5000
26/26 - 1s - loss: 0.6026 - val_loss: 1.0881
Epoch 2592/5000
26/26 - 1s - loss: 0.5998 - val_loss: 1.0896
Epoch 2593/5000
26/26 - 1s - loss: 0.6004 - val_loss: 1.0864
Epoch 2594/5000
26/26 - 1s - loss: 0.6009 - val_loss: 1.0856
Epoch 2595/5000
26/26 - 1s - loss: 0.6009 - val_loss: 1.0863
Epoch 2596/5000
26/26 - 1s - loss: 0.6001 - val_loss: 1.0880
Epoch 2597/5000
26/26 - 1s - loss: 0.5996 - val_loss: 1.0869
Epoch 2598/5000
26/26 - 1s - loss: 0.6002 - val_loss: 1.0881
Epoch 2599/5000
26/26 - 1s - loss: 0.5997 - val_loss: 1.0861
Epoch 2600/5000
26/26 - 1s - loss: 0.5998 - val_loss: 1.0875
Epoch 02600: val_loss improved from 1.08813 to 1.08755, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 2601/5000
26/26 - 1s - loss: 0.5997 - val_loss: 1.0844
Epoch 2602/5000
26/26 - 1s - loss: 0.5986 - val_loss: 1.0874
Epoch 2603/5000
26/26 - 1s - loss: 0.5991 - val_loss: 1.0869
Epoch 2604/5000
26/26 - 1s - loss: 0.5992 - val_loss: 1.0863
Epoch 2605/5000
26/26 - 1s - loss: 0.5989 - val_loss: 1.0859
Epoch 2606/5000
26/26 - 1s - loss: 0.5982 - val_loss: 1.0843
Epoch 2607/5000
26/26 - 1s - loss: 0.5980 - val_loss: 1.0836
Epoch 2608/5000
26/26 - 1s - loss: 0.5973 - val_loss: 1.0843
Epoch 2609/5000
26/26 - 1s - loss: 0.5979 - val_loss: 1.0837
Epoch 2610/5000
26/26 - 1s - loss: 0.5970 - val_loss: 1.0845
Epoch 02610: val_loss improved from 1.08755 to 1.08448, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 2611/5000
26/26 - 1s - loss: 0.5971 - val_loss: 1.0813
Epoch 2612/5000
26/26 - 1s - loss: 0.5974 - val_loss: 1.0844
Epoch 2613/5000
26/26 - 1s - loss: 0.5970 - val_loss: 1.0829
Epoch 2614/5000
26/26 - 1s - loss: 0.5975 - val_loss: 1.0831
Epoch 2615/5000
26/26 - 1s - loss: 0.5964 - val_loss: 1.0831
Epoch 2616/5000
26/26 - 1s - loss: 0.5968 - val_loss: 1.0825
Epoch 2617/5000
26/26 - 1s - loss: 0.5961 - val_loss: 1.0848
Epoch 2618/5000
26/26 - 1s - loss: 0.5965 - val_loss: 1.0833
Epoch 2619/5000
26/26 - 1s - loss: 0.5965 - val_loss: 1.0833
Epoch 2620/5000
26/26 - 1s - loss: 0.5959 - val_loss: 1.0828
Epoch 02620: val_loss improved from 1.08448 to 1.08275, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 2621/5000
26/26 - 1s - loss: 0.5966 - val_loss: 1.0811
Epoch 2622/5000
26/26 - 1s - loss: 0.5958 - val_loss: 1.0812
Epoch 2623/5000
26/26 - 1s - loss: 0.5965 - val_loss: 1.0816
Epoch 2624/5000
26/26 - 1s - loss: 0.5949 - val_loss: 1.0817
Epoch 2625/5000
26/26 - 2s - loss: 0.5942 - val_loss: 1.0807
Epoch 2626/5000
26/26 - 1s - loss: 0.5934 - val_loss: 1.0811
Epoch 2627/5000
26/26 - 1s - loss: 0.5950 - val_loss: 1.0801
Epoch 2628/5000
26/26 - 1s - loss: 0.5943 - val_loss: 1.0810
Epoch 2629/5000
26/26 - 1s - loss: 0.5932 - val_loss: 1.0789
Epoch 2630/5000
26/26 - 1s - loss: 0.5944 - val_loss: 1.0782
Epoch 02630: val_loss improved from 1.08275 to 1.07824, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 2631/5000
26/26 - 1s - loss: 0.5939 - val_loss: 1.0785
Epoch 2632/5000
26/26 - 1s - loss: 0.5932 - val_loss: 1.0769
Epoch 2633/5000
26/26 - 1s - loss: 0.5932 - val_loss: 1.0765
Epoch 2634/5000
26/26 - 1s - loss: 0.5947 - val_loss: 1.0788
Epoch 2635/5000
26/26 - 1s - loss: 0.5906 - val_loss: 1.0776
Epoch 2636/5000
26/26 - 1s - loss: 0.5922 - val_loss: 1.0788
Epoch 2637/5000
26/26 - 1s - loss: 0.5932 - val_loss: 1.0797
Epoch 2638/5000
26/26 - 1s - loss: 0.5909 - val_loss: 1.0772
Epoch 2639/5000
26/26 - 1s - loss: 0.5926 - val_loss: 1.0801
Epoch 2640/5000
26/26 - 1s - loss: 0.5924 - val_loss: 1.0773
Epoch 02640: val_loss improved from 1.07824 to 1.07732, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 2641/5000
26/26 - 1s - loss: 0.5915 - val_loss: 1.0787
Epoch 2642/5000
26/26 - 1s - loss: 0.5912 - val_loss: 1.0768
Epoch 2643/5000
26/26 - 1s - loss: 0.5920 - val_loss: 1.0765
Epoch 2644/5000
26/26 - 1s - loss: 0.5920 - val_loss: 1.0774
Epoch 2645/5000
26/26 - 1s - loss: 0.5917 - val_loss: 1.0751
Epoch 2646/5000
26/26 - 1s - loss: 0.5912 - val_loss: 1.0772
Epoch 2647/5000
26/26 - 1s - loss: 0.5909 - val_loss: 1.0755
Epoch 2648/5000
26/26 - 1s - loss: 0.5903 - val_loss: 1.0754
Epoch 2649/5000
26/26 - 1s - loss: 0.5905 - val_loss: 1.0762
Epoch 2650/5000
26/26 - 1s - loss: 0.5904 - val_loss: 1.0757
Epoch 02650: val_loss improved from 1.07732 to 1.07573, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 2651/5000
26/26 - 1s - loss: 0.5890 - val_loss: 1.0751
Epoch 2652/5000
26/26 - 1s - loss: 0.5898 - val_loss: 1.0753
Epoch 2653/5000
26/26 - 2s - loss: 0.5877 - val_loss: 1.0743
Epoch 2654/5000
26/26 - 1s - loss: 0.5890 - val_loss: 1.0767
Epoch 2655/5000
26/26 - 1s - loss: 0.5898 - val_loss: 1.0767
Epoch 2656/5000
26/26 - 1s - loss: 0.5879 - val_loss: 1.0755
Epoch 2657/5000
26/26 - 1s - loss: 0.5889 - val_loss: 1.0748
Epoch 2658/5000
26/26 - 1s - loss: 0.5882 - val_loss: 1.0749
Epoch 2659/5000
26/26 - 1s - loss: 0.5885 - val_loss: 1.0743
Epoch 2660/5000
26/26 - 2s - loss: 0.5890 - val_loss: 1.0750
Epoch 02660: val_loss improved from 1.07573 to 1.07500, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 2661/5000
26/26 - 1s - loss: 0.5884 - val_loss: 1.0734
Epoch 2662/5000
26/26 - 1s - loss: 0.5882 - val_loss: 1.0751
Epoch 2663/5000
26/26 - 1s - loss: 0.5886 - val_loss: 1.0725
Epoch 2664/5000
26/26 - 1s - loss: 0.5864 - val_loss: 1.0716
Epoch 2665/5000
26/26 - 1s - loss: 0.5866 - val_loss: 1.0751
Epoch 2666/5000
26/26 - 2s - loss: 0.5869 - val_loss: 1.0761
Epoch 2667/5000
26/26 - 2s - loss: 0.5875 - val_loss: 1.0741
Epoch 2668/5000
26/26 - 1s - loss: 0.5864 - val_loss: 1.0715
Epoch 2669/5000
26/26 - 1s - loss: 0.5860 - val_loss: 1.0723
Epoch 2670/5000
26/26 - 1s - loss: 0.5871 - val_loss: 1.0712
Epoch 02670: val_loss improved from 1.07500 to 1.07120, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 2671/5000
26/26 - 1s - loss: 0.5866 - val_loss: 1.0709
Epoch 2672/5000
26/26 - 1s - loss: 0.5860 - val_loss: 1.0703
Epoch 2673/5000
26/26 - 1s - loss: 0.5865 - val_loss: 1.0714
Epoch 2674/5000
26/26 - 1s - loss: 0.5862 - val_loss: 1.0720
Epoch 2675/5000
26/26 - 1s - loss: 0.5857 - val_loss: 1.0725
Epoch 2676/5000
26/26 - 1s - loss: 0.5841 - val_loss: 1.0715
Epoch 2677/5000
26/26 - 1s - loss: 0.5847 - val_loss: 1.0714
Epoch 2678/5000
26/26 - 1s - loss: 0.5844 - val_loss: 1.0711
Epoch 2679/5000
26/26 - 1s - loss: 0.5839 - val_loss: 1.0723
Epoch 2680/5000
26/26 - 2s - loss: 0.5840 - val_loss: 1.0699
Epoch 02680: val_loss improved from 1.07120 to 1.06995, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 2681/5000
26/26 - 1s - loss: 0.5841 - val_loss: 1.0682
Epoch 2682/5000
26/26 - 1s - loss: 0.5837 - val_loss: 1.0704
Epoch 2683/5000
26/26 - 1s - loss: 0.5848 - val_loss: 1.0701
Epoch 2684/5000
26/26 - 1s - loss: 0.5851 - val_loss: 1.0703
Epoch 2685/5000
26/26 - 1s - loss: 0.5837 - val_loss: 1.0719
Epoch 2686/5000
26/26 - 1s - loss: 0.5830 - val_loss: 1.0679
Epoch 2687/5000
26/26 - 1s - loss: 0.5826 - val_loss: 1.0699
Epoch 2688/5000
26/26 - 1s - loss: 0.5823 - val_loss: 1.0716
Epoch 2689/5000
26/26 - 1s - loss: 0.5818 - val_loss: 1.0714
Epoch 2690/5000
26/26 - 1s - loss: 0.5840 - val_loss: 1.0696
Epoch 02690: val_loss improved from 1.06995 to 1.06961, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 2691/5000
26/26 - 1s - loss: 0.5823 - val_loss: 1.0690
Epoch 2692/5000
26/26 - 1s - loss: 0.5817 - val_loss: 1.0659
Epoch 2693/5000
26/26 - 1s - loss: 0.5814 - val_loss: 1.0657
Epoch 2694/5000
26/26 - 1s - loss: 0.5836 - val_loss: 1.0668
Epoch 2695/5000
26/26 - 1s - loss: 0.5819 - val_loss: 1.0684
Epoch 2696/5000
26/26 - 1s - loss: 0.5801 - val_loss: 1.0645
Epoch 2697/5000
26/26 - 1s - loss: 0.5821 - val_loss: 1.0689
Epoch 2698/5000
26/26 - 1s - loss: 0.5807 - val_loss: 1.0666
Epoch 2699/5000
26/26 - 1s - loss: 0.5805 - val_loss: 1.0674
Epoch 2700/5000
26/26 - 1s - loss: 0.5800 - val_loss: 1.0658
Epoch 02700: val_loss improved from 1.06961 to 1.06579, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 2701/5000
26/26 - 1s - loss: 0.5798 - val_loss: 1.0656
Epoch 2702/5000
26/26 - 1s - loss: 0.5815 - val_loss: 1.0661
Epoch 2703/5000
26/26 - 1s - loss: 0.5805 - val_loss: 1.0673
Epoch 2704/5000
26/26 - 2s - loss: 0.5802 - val_loss: 1.0647
Epoch 2705/5000
26/26 - 1s - loss: 0.5799 - val_loss: 1.0650
Epoch 2706/5000
26/26 - 1s - loss: 0.5802 - val_loss: 1.0659
Epoch 2707/5000
26/26 - 2s - loss: 0.5794 - val_loss: 1.0642
Epoch 2708/5000
26/26 - 2s - loss: 0.5796 - val_loss: 1.0667
Epoch 2709/5000
26/26 - 1s - loss: 0.5788 - val_loss: 1.0670
Epoch 2710/5000
26/26 - 1s - loss: 0.5777 - val_loss: 1.0657
Epoch 02710: val_loss improved from 1.06579 to 1.06566, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 2711/5000
26/26 - 1s - loss: 0.5791 - val_loss: 1.0641
Epoch 2712/5000
26/26 - 1s - loss: 0.5782 - val_loss: 1.0651
Epoch 2713/5000
26/26 - 1s - loss: 0.5783 - val_loss: 1.0637
Epoch 2714/5000
26/26 - 1s - loss: 0.5785 - val_loss: 1.0647
Epoch 2715/5000
26/26 - 1s - loss: 0.5782 - val_loss: 1.0616
Epoch 2716/5000
26/26 - 1s - loss: 0.5784 - val_loss: 1.0632
Epoch 2717/5000
26/26 - 1s - loss: 0.5776 - val_loss: 1.0629
Epoch 2718/5000
26/26 - 1s - loss: 0.5777 - val_loss: 1.0655
Epoch 2719/5000
26/26 - 1s - loss: 0.5783 - val_loss: 1.0651
Epoch 2720/5000
26/26 - 1s - loss: 0.5764 - val_loss: 1.0655
Epoch 02720: val_loss improved from 1.06566 to 1.06553, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 2721/5000
26/26 - 1s - loss: 0.5779 - val_loss: 1.0635
Epoch 2722/5000
26/26 - 1s - loss: 0.5759 - val_loss: 1.0623
Epoch 2723/5000
26/26 - 1s - loss: 0.5763 - val_loss: 1.0626
Epoch 2724/5000
26/26 - 1s - loss: 0.5758 - val_loss: 1.0628
Epoch 2725/5000
26/26 - 1s - loss: 0.5766 - val_loss: 1.0617
Epoch 2726/5000
26/26 - 1s - loss: 0.5766 - val_loss: 1.0591
Epoch 2727/5000
26/26 - 1s - loss: 0.5775 - val_loss: 1.0615
Epoch 2728/5000
26/26 - 1s - loss: 0.5757 - val_loss: 1.0612
Epoch 2729/5000
26/26 - 1s - loss: 0.5774 - val_loss: 1.0604
Epoch 2730/5000
26/26 - 1s - loss: 0.5748 - val_loss: 1.0620
Epoch 02730: val_loss improved from 1.06553 to 1.06200, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 2731/5000
26/26 - 1s - loss: 0.5761 - val_loss: 1.0626
Epoch 2732/5000
26/26 - 1s - loss: 0.5763 - val_loss: 1.0610
Epoch 2733/5000
26/26 - 1s - loss: 0.5738 - val_loss: 1.0606
Epoch 2734/5000
26/26 - 1s - loss: 0.5741 - val_loss: 1.0614
Epoch 2735/5000
26/26 - 1s - loss: 0.5738 - val_loss: 1.0597
Epoch 2736/5000
26/26 - 1s - loss: 0.5758 - val_loss: 1.0597
Epoch 2737/5000
26/26 - 1s - loss: 0.5748 - val_loss: 1.0613
Epoch 2738/5000
26/26 - 1s - loss: 0.5740 - val_loss: 1.0604
Epoch 2739/5000
26/26 - 1s - loss: 0.5734 - val_loss: 1.0597
Epoch 2740/5000
26/26 - 1s - loss: 0.5746 - val_loss: 1.0585
Epoch 02740: val_loss improved from 1.06200 to 1.05851, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 2741/5000
26/26 - 1s - loss: 0.5735 - val_loss: 1.0596
Epoch 2742/5000
26/26 - 1s - loss: 0.5738 - val_loss: 1.0602
Epoch 2743/5000
26/26 - 1s - loss: 0.5744 - val_loss: 1.0589
Epoch 2744/5000
26/26 - 1s - loss: 0.5738 - val_loss: 1.0580
Epoch 2745/5000
26/26 - 1s - loss: 0.5722 - val_loss: 1.0605
Epoch 2746/5000
26/26 - 1s - loss: 0.5722 - val_loss: 1.0577
Epoch 2747/5000
26/26 - 1s - loss: 0.5727 - val_loss: 1.0563
Epoch 2748/5000
26/26 - 1s - loss: 0.5725 - val_loss: 1.0582
Epoch 2749/5000
26/26 - 1s - loss: 0.5723 - val_loss: 1.0582
Epoch 2750/5000
26/26 - 1s - loss: 0.5723 - val_loss: 1.0582
Epoch 02750: val_loss improved from 1.05851 to 1.05817, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 2751/5000
26/26 - 1s - loss: 0.5721 - val_loss: 1.0570
Epoch 2752/5000
26/26 - 1s - loss: 0.5718 - val_loss: 1.0602
Epoch 2753/5000
26/26 - 1s - loss: 0.5724 - val_loss: 1.0594
Epoch 2754/5000
26/26 - 1s - loss: 0.5714 - val_loss: 1.0573
Epoch 2755/5000
26/26 - 1s - loss: 0.5715 - val_loss: 1.0568
Epoch 2756/5000
26/26 - 1s - loss: 0.5709 - val_loss: 1.0567
Epoch 2757/5000
26/26 - 1s - loss: 0.5705 - val_loss: 1.0583
Epoch 2758/5000
26/26 - 1s - loss: 0.5704 - val_loss: 1.0564
Epoch 2759/5000
26/26 - 1s - loss: 0.5700 - val_loss: 1.0584
Epoch 2760/5000
26/26 - 1s - loss: 0.5706 - val_loss: 1.0556
Epoch 02760: val_loss improved from 1.05817 to 1.05562, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 2761/5000
26/26 - 1s - loss: 0.5689 - val_loss: 1.0559
Epoch 2762/5000
26/26 - 1s - loss: 0.5692 - val_loss: 1.0570
Epoch 2763/5000
26/26 - 1s - loss: 0.5695 - val_loss: 1.0548
Epoch 2764/5000
26/26 - 1s - loss: 0.5699 - val_loss: 1.0545
Epoch 2765/5000
26/26 - 1s - loss: 0.5698 - val_loss: 1.0549
Epoch 2766/5000
26/26 - 1s - loss: 0.5700 - val_loss: 1.0540
Epoch 2767/5000
26/26 - 1s - loss: 0.5687 - val_loss: 1.0572
Epoch 2768/5000
26/26 - 1s - loss: 0.5692 - val_loss: 1.0552
Epoch 2769/5000
26/26 - 1s - loss: 0.5685 - val_loss: 1.0511
Epoch 2770/5000
26/26 - 1s - loss: 0.5694 - val_loss: 1.0549
Epoch 02770: val_loss improved from 1.05562 to 1.05494, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 2771/5000
26/26 - 1s - loss: 0.5690 - val_loss: 1.0527
Epoch 2772/5000
26/26 - 1s - loss: 0.5683 - val_loss: 1.0543
Epoch 2773/5000
26/26 - 1s - loss: 0.5685 - val_loss: 1.0537
Epoch 2774/5000
26/26 - 1s - loss: 0.5678 - val_loss: 1.0550
Epoch 2775/5000
26/26 - 1s - loss: 0.5689 - val_loss: 1.0521
Epoch 2776/5000
26/26 - 1s - loss: 0.5664 - val_loss: 1.0553
Epoch 2777/5000
26/26 - 1s - loss: 0.5663 - val_loss: 1.0522
Epoch 2778/5000
26/26 - 1s - loss: 0.5679 - val_loss: 1.0542
Epoch 2779/5000
26/26 - 1s - loss: 0.5665 - val_loss: 1.0531
Epoch 2780/5000
26/26 - 1s - loss: 0.5663 - val_loss: 1.0522
Epoch 02780: val_loss improved from 1.05494 to 1.05222, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 2781/5000
26/26 - 1s - loss: 0.5669 - val_loss: 1.0529
Epoch 2782/5000
26/26 - 1s - loss: 0.5660 - val_loss: 1.0540
Epoch 2783/5000
26/26 - 1s - loss: 0.5666 - val_loss: 1.0534
Epoch 2784/5000
26/26 - 1s - loss: 0.5657 - val_loss: 1.0534
Epoch 2785/5000
26/26 - 1s - loss: 0.5663 - val_loss: 1.0504
Epoch 2786/5000
26/26 - 1s - loss: 0.5661 - val_loss: 1.0503
Epoch 2787/5000
26/26 - 1s - loss: 0.5656 - val_loss: 1.0532
Epoch 2788/5000
26/26 - 1s - loss: 0.5663 - val_loss: 1.0513
Epoch 2789/5000
26/26 - 1s - loss: 0.5651 - val_loss: 1.0533
Epoch 2790/5000
26/26 - 1s - loss: 0.5650 - val_loss: 1.0488
Epoch 02790: val_loss improved from 1.05222 to 1.04882, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 2791/5000
26/26 - 1s - loss: 0.5650 - val_loss: 1.0525
Epoch 2792/5000
26/26 - 1s - loss: 0.5642 - val_loss: 1.0520
Epoch 2793/5000
26/26 - 2s - loss: 0.5659 - val_loss: 1.0510
Epoch 2794/5000
26/26 - 1s - loss: 0.5647 - val_loss: 1.0494
Epoch 2795/5000
26/26 - 1s - loss: 0.5640 - val_loss: 1.0508
Epoch 2796/5000
26/26 - 1s - loss: 0.5644 - val_loss: 1.0501
Epoch 2797/5000
26/26 - 1s - loss: 0.5638 - val_loss: 1.0480
Epoch 2798/5000
26/26 - 1s - loss: 0.5650 - val_loss: 1.0502
Epoch 2799/5000
26/26 - 1s - loss: 0.5645 - val_loss: 1.0503
Epoch 2800/5000
26/26 - 1s - loss: 0.5639 - val_loss: 1.0507
Epoch 02800: val_loss did not improve from 1.04882
Epoch 2801/5000
26/26 - 1s - loss: 0.5639 - val_loss: 1.0512
Epoch 2802/5000
26/26 - 2s - loss: 0.5638 - val_loss: 1.0509
Epoch 2803/5000
26/26 - 1s - loss: 0.5629 - val_loss: 1.0480
Epoch 2804/5000
26/26 - 1s - loss: 0.5632 - val_loss: 1.0484
Epoch 2805/5000
26/26 - 1s - loss: 0.5632 - val_loss: 1.0512
Epoch 2806/5000
26/26 - 1s - loss: 0.5636 - val_loss: 1.0484
Epoch 2807/5000
26/26 - 2s - loss: 0.5615 - val_loss: 1.0480
Epoch 2808/5000
26/26 - 1s - loss: 0.5631 - val_loss: 1.0459
Epoch 2809/5000
26/26 - 1s - loss: 0.5625 - val_loss: 1.0489
Epoch 2810/5000
26/26 - 1s - loss: 0.5631 - val_loss: 1.0495
Epoch 02810: val_loss did not improve from 1.04882
Epoch 2811/5000
26/26 - 1s - loss: 0.5626 - val_loss: 1.0452
Epoch 2812/5000
26/26 - 1s - loss: 0.5614 - val_loss: 1.0477
Epoch 2813/5000
26/26 - 1s - loss: 0.5615 - val_loss: 1.0463
Epoch 2814/5000
26/26 - 1s - loss: 0.5611 - val_loss: 1.0487
Epoch 2815/5000
26/26 - 1s - loss: 0.5611 - val_loss: 1.0460
Epoch 2816/5000
26/26 - 1s - loss: 0.5604 - val_loss: 1.0468
Epoch 2817/5000
26/26 - 1s - loss: 0.5614 - val_loss: 1.0466
Epoch 2818/5000
26/26 - 1s - loss: 0.5605 - val_loss: 1.0482
Epoch 2819/5000
26/26 - 1s - loss: 0.5610 - val_loss: 1.0450
Epoch 2820/5000
26/26 - 1s - loss: 0.5605 - val_loss: 1.0463
Epoch 02820: val_loss improved from 1.04882 to 1.04633, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 2821/5000
26/26 - 1s - loss: 0.5590 - val_loss: 1.0470
Epoch 2822/5000
26/26 - 1s - loss: 0.5608 - val_loss: 1.0468
Epoch 2823/5000
26/26 - 1s - loss: 0.5593 - val_loss: 1.0472
Epoch 2824/5000
26/26 - 1s - loss: 0.5599 - val_loss: 1.0446
Epoch 2825/5000
26/26 - 1s - loss: 0.5587 - val_loss: 1.0439
Epoch 2826/5000
26/26 - 1s - loss: 0.5589 - val_loss: 1.0453
Epoch 2827/5000
26/26 - 1s - loss: 0.5588 - val_loss: 1.0439
Epoch 2828/5000
26/26 - 1s - loss: 0.5590 - val_loss: 1.0468
Epoch 2829/5000
26/26 - 1s - loss: 0.5596 - val_loss: 1.0444
Epoch 2830/5000
26/26 - 1s - loss: 0.5587 - val_loss: 1.0439
Epoch 02830: val_loss improved from 1.04633 to 1.04388, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 2831/5000
26/26 - 1s - loss: 0.5588 - val_loss: 1.0436
Epoch 2832/5000
26/26 - 1s - loss: 0.5580 - val_loss: 1.0476
Epoch 2833/5000
26/26 - 1s - loss: 0.5583 - val_loss: 1.0425
Epoch 2834/5000
26/26 - 1s - loss: 0.5579 - val_loss: 1.0428
Epoch 2835/5000
26/26 - 1s - loss: 0.5582 - val_loss: 1.0419
Epoch 2836/5000
26/26 - 1s - loss: 0.5586 - val_loss: 1.0424
Epoch 2837/5000
26/26 - 1s - loss: 0.5584 - val_loss: 1.0437
Epoch 2838/5000
26/26 - 1s - loss: 0.5582 - val_loss: 1.0441
Epoch 2839/5000
26/26 - 1s - loss: 0.5572 - val_loss: 1.0440
Epoch 2840/5000
26/26 - 1s - loss: 0.5568 - val_loss: 1.0431
Epoch 02840: val_loss improved from 1.04388 to 1.04315, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 2841/5000
26/26 - 1s - loss: 0.5570 - val_loss: 1.0414
Epoch 2842/5000
26/26 - 1s - loss: 0.5565 - val_loss: 1.0467
Epoch 2843/5000
26/26 - 1s - loss: 0.5570 - val_loss: 1.0431
Epoch 2844/5000
26/26 - 1s - loss: 0.5581 - val_loss: 1.0421
Epoch 2845/5000
26/26 - 1s - loss: 0.5561 - val_loss: 1.0428
Epoch 2846/5000
26/26 - 1s - loss: 0.5563 - val_loss: 1.0422
Epoch 2847/5000
26/26 - 1s - loss: 0.5558 - val_loss: 1.0431
Epoch 2848/5000
26/26 - 1s - loss: 0.5548 - val_loss: 1.0394
Epoch 2849/5000
26/26 - 1s - loss: 0.5577 - val_loss: 1.0401
Epoch 2850/5000
26/26 - 1s - loss: 0.5557 - val_loss: 1.0434
Epoch 02850: val_loss did not improve from 1.04315
Epoch 2851/5000
26/26 - 1s - loss: 0.5551 - val_loss: 1.0437
Epoch 2852/5000
26/26 - 1s - loss: 0.5547 - val_loss: 1.0416
Epoch 2853/5000
26/26 - 1s - loss: 0.5553 - val_loss: 1.0397
Epoch 2854/5000
26/26 - 1s - loss: 0.5549 - val_loss: 1.0419
Epoch 2855/5000
26/26 - 1s - loss: 0.5541 - val_loss: 1.0423
Epoch 2856/5000
26/26 - 1s - loss: 0.5541 - val_loss: 1.0420
Epoch 2857/5000
26/26 - 1s - loss: 0.5538 - val_loss: 1.0410
Epoch 2858/5000
26/26 - 1s - loss: 0.5541 - val_loss: 1.0402
Epoch 2859/5000
26/26 - 1s - loss: 0.5539 - val_loss: 1.0396
Epoch 2860/5000
26/26 - 1s - loss: 0.5522 - val_loss: 1.0409
Epoch 02860: val_loss improved from 1.04315 to 1.04088, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 2861/5000
26/26 - 1s - loss: 0.5527 - val_loss: 1.0394
Epoch 2862/5000
26/26 - 1s - loss: 0.5540 - val_loss: 1.0406
Epoch 2863/5000
26/26 - 1s - loss: 0.5523 - val_loss: 1.0380
Epoch 2864/5000
26/26 - 1s - loss: 0.5528 - val_loss: 1.0384
Epoch 2865/5000
26/26 - 1s - loss: 0.5536 - val_loss: 1.0384
Epoch 2866/5000
26/26 - 1s - loss: 0.5537 - val_loss: 1.0386
Epoch 2867/5000
26/26 - 1s - loss: 0.5516 - val_loss: 1.0376
Epoch 2868/5000
26/26 - 1s - loss: 0.5518 - val_loss: 1.0382
Epoch 2869/5000
26/26 - 1s - loss: 0.5534 - val_loss: 1.0387
Epoch 2870/5000
26/26 - 1s - loss: 0.5524 - val_loss: 1.0390
Epoch 02870: val_loss improved from 1.04088 to 1.03904, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 2871/5000
26/26 - 1s - loss: 0.5531 - val_loss: 1.0384
Epoch 2872/5000
26/26 - 1s - loss: 0.5510 - val_loss: 1.0381
Epoch 2873/5000
26/26 - 1s - loss: 0.5510 - val_loss: 1.0407
Epoch 2874/5000
26/26 - 1s - loss: 0.5522 - val_loss: 1.0394
Epoch 2875/5000
26/26 - 1s - loss: 0.5526 - val_loss: 1.0387
Epoch 2876/5000
26/26 - 2s - loss: 0.5508 - val_loss: 1.0401
Epoch 2877/5000
26/26 - 1s - loss: 0.5520 - val_loss: 1.0400
Epoch 2878/5000
26/26 - 1s - loss: 0.5505 - val_loss: 1.0378
Epoch 2879/5000
26/26 - 1s - loss: 0.5498 - val_loss: 1.0384
Epoch 2880/5000
26/26 - 1s - loss: 0.5502 - val_loss: 1.0378
Epoch 02880: val_loss improved from 1.03904 to 1.03775, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 2881/5000
26/26 - 1s - loss: 0.5497 - val_loss: 1.0374
Epoch 2882/5000
26/26 - 1s - loss: 0.5499 - val_loss: 1.0368
Epoch 2883/5000
26/26 - 1s - loss: 0.5497 - val_loss: 1.0382
Epoch 2884/5000
26/26 - 1s - loss: 0.5503 - val_loss: 1.0349
Epoch 2885/5000
26/26 - 1s - loss: 0.5501 - val_loss: 1.0375
Epoch 2886/5000
26/26 - 1s - loss: 0.5488 - val_loss: 1.0358
Epoch 2887/5000
26/26 - 1s - loss: 0.5491 - val_loss: 1.0346
Epoch 2888/5000
26/26 - 1s - loss: 0.5502 - val_loss: 1.0371
Epoch 2889/5000
26/26 - 1s - loss: 0.5503 - val_loss: 1.0360
Epoch 2890/5000
26/26 - 1s - loss: 0.5489 - val_loss: 1.0364
Epoch 02890: val_loss improved from 1.03775 to 1.03635, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 2891/5000
26/26 - 1s - loss: 0.5478 - val_loss: 1.0351
Epoch 2892/5000
26/26 - 1s - loss: 0.5489 - val_loss: 1.0349
Epoch 2893/5000
26/26 - 1s - loss: 0.5485 - val_loss: 1.0340
Epoch 2894/5000
26/26 - 1s - loss: 0.5489 - val_loss: 1.0355
Epoch 2895/5000
26/26 - 1s - loss: 0.5494 - val_loss: 1.0370
Epoch 2896/5000
26/26 - 1s - loss: 0.5479 - val_loss: 1.0350
Epoch 2897/5000
26/26 - 1s - loss: 0.5484 - val_loss: 1.0334
Epoch 2898/5000
26/26 - 1s - loss: 0.5474 - val_loss: 1.0356
Epoch 2899/5000
26/26 - 1s - loss: 0.5470 - val_loss: 1.0334
Epoch 2900/5000
26/26 - 1s - loss: 0.5468 - val_loss: 1.0351
Epoch 02900: val_loss improved from 1.03635 to 1.03512, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 2901/5000
26/26 - 1s - loss: 0.5479 - val_loss: 1.0342
Epoch 2902/5000
26/26 - 1s - loss: 0.5473 - val_loss: 1.0334
Epoch 2903/5000
26/26 - 1s - loss: 0.5475 - val_loss: 1.0334
Epoch 2904/5000
26/26 - 1s - loss: 0.5472 - val_loss: 1.0347
Epoch 2905/5000
26/26 - 1s - loss: 0.5459 - val_loss: 1.0310
Epoch 2906/5000
26/26 - 1s - loss: 0.5481 - val_loss: 1.0330
Epoch 2907/5000
26/26 - 1s - loss: 0.5466 - val_loss: 1.0320
Epoch 2908/5000
26/26 - 1s - loss: 0.5459 - val_loss: 1.0318
Epoch 2909/5000
26/26 - 1s - loss: 0.5468 - val_loss: 1.0329
Epoch 2910/5000
26/26 - 1s - loss: 0.5454 - val_loss: 1.0315
Epoch 02910: val_loss improved from 1.03512 to 1.03149, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 2911/5000
26/26 - 1s - loss: 0.5460 - val_loss: 1.0319
Epoch 2912/5000
26/26 - 1s - loss: 0.5460 - val_loss: 1.0342
Epoch 2913/5000
26/26 - 1s - loss: 0.5466 - val_loss: 1.0314
Epoch 2914/5000
26/26 - 1s - loss: 0.5456 - val_loss: 1.0306
Epoch 2915/5000
26/26 - 1s - loss: 0.5470 - val_loss: 1.0321
Epoch 2916/5000
26/26 - 1s - loss: 0.5453 - val_loss: 1.0297
Epoch 2917/5000
26/26 - 2s - loss: 0.5443 - val_loss: 1.0321
Epoch 2918/5000
26/26 - 2s - loss: 0.5447 - val_loss: 1.0323
Epoch 2919/5000
26/26 - 1s - loss: 0.5454 - val_loss: 1.0310
Epoch 2920/5000
26/26 - 1s - loss: 0.5438 - val_loss: 1.0312
Epoch 02920: val_loss improved from 1.03149 to 1.03123, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 2921/5000
26/26 - 1s - loss: 0.5430 - val_loss: 1.0287
Epoch 2922/5000
26/26 - 1s - loss: 0.5446 - val_loss: 1.0312
Epoch 2923/5000
26/26 - 1s - loss: 0.5440 - val_loss: 1.0327
Epoch 2924/5000
26/26 - 1s - loss: 0.5458 - val_loss: 1.0291
Epoch 2925/5000
26/26 - 1s - loss: 0.5442 - val_loss: 1.0304
Epoch 2926/5000
26/26 - 1s - loss: 0.5435 - val_loss: 1.0323
Epoch 2927/5000
26/26 - 1s - loss: 0.5435 - val_loss: 1.0286
Epoch 2928/5000
26/26 - 1s - loss: 0.5438 - val_loss: 1.0293
Epoch 2929/5000
26/26 - 1s - loss: 0.5435 - val_loss: 1.0311
Epoch 2930/5000
26/26 - 1s - loss: 0.5435 - val_loss: 1.0315
Epoch 02930: val_loss did not improve from 1.03123
Epoch 2931/5000
26/26 - 1s - loss: 0.5425 - val_loss: 1.0294
Epoch 2932/5000
26/26 - 1s - loss: 0.5418 - val_loss: 1.0297
Epoch 2933/5000
26/26 - 1s - loss: 0.5436 - val_loss: 1.0322
Epoch 2934/5000
26/26 - 1s - loss: 0.5424 - val_loss: 1.0285
Epoch 2935/5000
26/26 - 1s - loss: 0.5417 - val_loss: 1.0298
Epoch 2936/5000
26/26 - 1s - loss: 0.5416 - val_loss: 1.0305
Epoch 2937/5000
26/26 - 1s - loss: 0.5420 - val_loss: 1.0294
Epoch 2938/5000
26/26 - 1s - loss: 0.5417 - val_loss: 1.0286
Epoch 2939/5000
26/26 - 1s - loss: 0.5410 - val_loss: 1.0283
Epoch 2940/5000
26/26 - 1s - loss: 0.5412 - val_loss: 1.0286
Epoch 02940: val_loss improved from 1.03123 to 1.02859, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 2941/5000
26/26 - 1s - loss: 0.5406 - val_loss: 1.0282
Epoch 2942/5000
26/26 - 1s - loss: 0.5409 - val_loss: 1.0270
Epoch 2943/5000
26/26 - 1s - loss: 0.5407 - val_loss: 1.0300
Epoch 2944/5000
26/26 - 1s - loss: 0.5418 - val_loss: 1.0262
Epoch 2945/5000
26/26 - 1s - loss: 0.5410 - val_loss: 1.0245
Epoch 2946/5000
26/26 - 1s - loss: 0.5413 - val_loss: 1.0265
Epoch 2947/5000
26/26 - 1s - loss: 0.5400 - val_loss: 1.0258
Epoch 2948/5000
26/26 - 1s - loss: 0.5403 - val_loss: 1.0270
Epoch 2949/5000
26/26 - 1s - loss: 0.5409 - val_loss: 1.0245
Epoch 2950/5000
26/26 - 1s - loss: 0.5400 - val_loss: 1.0287
Epoch 02950: val_loss did not improve from 1.02859
Epoch 2951/5000
26/26 - 1s - loss: 0.5410 - val_loss: 1.0255
Epoch 2952/5000
26/26 - 1s - loss: 0.5402 - val_loss: 1.0251
Epoch 2953/5000
26/26 - 1s - loss: 0.5404 - val_loss: 1.0260
Epoch 2954/5000
26/26 - 1s - loss: 0.5395 - val_loss: 1.0263
Epoch 2955/5000
26/26 - 1s - loss: 0.5399 - val_loss: 1.0230
Epoch 2956/5000
26/26 - 1s - loss: 0.5391 - val_loss: 1.0262
Epoch 2957/5000
26/26 - 1s - loss: 0.5397 - val_loss: 1.0243
Epoch 2958/5000
26/26 - 1s - loss: 0.5390 - val_loss: 1.0266
Epoch 2959/5000
26/26 - 1s - loss: 0.5386 - val_loss: 1.0258
Epoch 2960/5000
26/26 - 1s - loss: 0.5375 - val_loss: 1.0272
Epoch 02960: val_loss improved from 1.02859 to 1.02716, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 2961/5000
26/26 - 1s - loss: 0.5383 - val_loss: 1.0243
Epoch 2962/5000
26/26 - 1s - loss: 0.5388 - val_loss: 1.0239
Epoch 2963/5000
26/26 - 1s - loss: 0.5378 - val_loss: 1.0246
Epoch 2964/5000
26/26 - 1s - loss: 0.5375 - val_loss: 1.0254
Epoch 2965/5000
26/26 - 1s - loss: 0.5368 - val_loss: 1.0243
Epoch 2966/5000
26/26 - 1s - loss: 0.5390 - val_loss: 1.0252
Epoch 2967/5000
26/26 - 1s - loss: 0.5377 - val_loss: 1.0247
Epoch 2968/5000
26/26 - 1s - loss: 0.5368 - val_loss: 1.0253
Epoch 2969/5000
26/26 - 1s - loss: 0.5375 - val_loss: 1.0235
Epoch 2970/5000
26/26 - 1s - loss: 0.5367 - val_loss: 1.0240
Epoch 02970: val_loss improved from 1.02716 to 1.02398, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 2971/5000
26/26 - 1s - loss: 0.5374 - val_loss: 1.0234
Epoch 2972/5000
26/26 - 2s - loss: 0.5373 - val_loss: 1.0241
Epoch 2973/5000
26/26 - 1s - loss: 0.5367 - val_loss: 1.0247
Epoch 2974/5000
26/26 - 1s - loss: 0.5363 - val_loss: 1.0216
Epoch 2975/5000
26/26 - 1s - loss: 0.5365 - val_loss: 1.0219
Epoch 2976/5000
26/26 - 1s - loss: 0.5360 - val_loss: 1.0225
Epoch 2977/5000
26/26 - 1s - loss: 0.5354 - val_loss: 1.0237
Epoch 2978/5000
26/26 - 1s - loss: 0.5364 - val_loss: 1.0226
Epoch 2979/5000
26/26 - 1s - loss: 0.5368 - val_loss: 1.0226
Epoch 2980/5000
26/26 - 1s - loss: 0.5365 - val_loss: 1.0217
Epoch 02980: val_loss improved from 1.02398 to 1.02169, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 2981/5000
26/26 - 1s - loss: 0.5353 - val_loss: 1.0224
Epoch 2982/5000
26/26 - 1s - loss: 0.5355 - val_loss: 1.0204
Epoch 2983/5000
26/26 - 1s - loss: 0.5351 - val_loss: 1.0229
Epoch 2984/5000
26/26 - 1s - loss: 0.5340 - val_loss: 1.0210
Epoch 2985/5000
26/26 - 1s - loss: 0.5350 - val_loss: 1.0213
Epoch 2986/5000
26/26 - 1s - loss: 0.5345 - val_loss: 1.0214
Epoch 2987/5000
26/26 - 1s - loss: 0.5341 - val_loss: 1.0202
Epoch 2988/5000
26/26 - 1s - loss: 0.5335 - val_loss: 1.0223
Epoch 2989/5000
26/26 - 1s - loss: 0.5354 - val_loss: 1.0194
Epoch 2990/5000
26/26 - 1s - loss: 0.5359 - val_loss: 1.0218
Epoch 02990: val_loss did not improve from 1.02169
Epoch 2991/5000
26/26 - 1s - loss: 0.5337 - val_loss: 1.0205
Epoch 2992/5000
26/26 - 1s - loss: 0.5340 - val_loss: 1.0198
Epoch 2993/5000
26/26 - 1s - loss: 0.5338 - val_loss: 1.0200
Epoch 2994/5000
26/26 - 1s - loss: 0.5337 - val_loss: 1.0203
Epoch 2995/5000
26/26 - 1s - loss: 0.5329 - val_loss: 1.0219
Epoch 2996/5000
26/26 - 1s - loss: 0.5337 - val_loss: 1.0180
Epoch 2997/5000
26/26 - 1s - loss: 0.5332 - val_loss: 1.0204
Epoch 2998/5000
26/26 - 1s - loss: 0.5341 - val_loss: 1.0180
Epoch 2999/5000
26/26 - 1s - loss: 0.5328 - val_loss: 1.0211
Epoch 3000/5000
26/26 - 1s - loss: 0.5329 - val_loss: 1.0174
Epoch 03000: val_loss improved from 1.02169 to 1.01735, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 3001/5000
26/26 - 2s - loss: 0.5328 - val_loss: 1.0176
Epoch 3002/5000
26/26 - 1s - loss: 0.5329 - val_loss: 1.0195
Epoch 3003/5000
26/26 - 1s - loss: 0.5315 - val_loss: 1.0203
Epoch 3004/5000
26/26 - 1s - loss: 0.5325 - val_loss: 1.0189
Epoch 3005/5000
26/26 - 1s - loss: 0.5323 - val_loss: 1.0202
Epoch 3006/5000
26/26 - 1s - loss: 0.5326 - val_loss: 1.0187
Epoch 3007/5000
26/26 - 1s - loss: 0.5307 - val_loss: 1.0196
Epoch 3008/5000
26/26 - 1s - loss: 0.5317 - val_loss: 1.0181
Epoch 3009/5000
26/26 - 1s - loss: 0.5313 - val_loss: 1.0179
Epoch 3010/5000
26/26 - 1s - loss: 0.5323 - val_loss: 1.0191
Epoch 03010: val_loss did not improve from 1.01735
Epoch 3011/5000
26/26 - 1s - loss: 0.5309 - val_loss: 1.0184
Epoch 3012/5000
26/26 - 1s - loss: 0.5320 - val_loss: 1.0195
Epoch 3013/5000
26/26 - 1s - loss: 0.5312 - val_loss: 1.0166
Epoch 3014/5000
26/26 - 1s - loss: 0.5305 - val_loss: 1.0182
Epoch 3015/5000
26/26 - 1s - loss: 0.5306 - val_loss: 1.0171
Epoch 3016/5000
26/26 - 1s - loss: 0.5314 - val_loss: 1.0172
Epoch 3017/5000
26/26 - 1s - loss: 0.5310 - val_loss: 1.0173
Epoch 3018/5000
26/26 - 1s - loss: 0.5293 - val_loss: 1.0159
Epoch 3019/5000
26/26 - 1s - loss: 0.5309 - val_loss: 1.0162
Epoch 3020/5000
26/26 - 1s - loss: 0.5305 - val_loss: 1.0154
Epoch 03020: val_loss improved from 1.01735 to 1.01536, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 3021/5000
26/26 - 1s - loss: 0.5299 - val_loss: 1.0187
Epoch 3022/5000
26/26 - 1s - loss: 0.5313 - val_loss: 1.0167
Epoch 3023/5000
26/26 - 1s - loss: 0.5301 - val_loss: 1.0158
Epoch 3024/5000
26/26 - 1s - loss: 0.5294 - val_loss: 1.0176
Epoch 3025/5000
26/26 - 1s - loss: 0.5291 - val_loss: 1.0168
Epoch 3026/5000
26/26 - 1s - loss: 0.5299 - val_loss: 1.0147
Epoch 3027/5000
26/26 - 1s - loss: 0.5295 - val_loss: 1.0164
Epoch 3028/5000
26/26 - 1s - loss: 0.5289 - val_loss: 1.0173
Epoch 3029/5000
26/26 - 1s - loss: 0.5286 - val_loss: 1.0144
Epoch 3030/5000
26/26 - 1s - loss: 0.5300 - val_loss: 1.0159
Epoch 03030: val_loss did not improve from 1.01536
Epoch 3031/5000
26/26 - 1s - loss: 0.5284 - val_loss: 1.0158
Epoch 3032/5000
26/26 - 1s - loss: 0.5292 - val_loss: 1.0176
Epoch 3033/5000
26/26 - 1s - loss: 0.5297 - val_loss: 1.0151
Epoch 3034/5000
26/26 - 1s - loss: 0.5290 - val_loss: 1.0146
Epoch 3035/5000
26/26 - 1s - loss: 0.5284 - val_loss: 1.0164
Epoch 3036/5000
26/26 - 1s - loss: 0.5281 - val_loss: 1.0131
Epoch 3037/5000
26/26 - 1s - loss: 0.5281 - val_loss: 1.0155
Epoch 3038/5000
26/26 - 1s - loss: 0.5270 - val_loss: 1.0141
Epoch 3039/5000
26/26 - 1s - loss: 0.5277 - val_loss: 1.0140
Epoch 3040/5000
26/26 - 1s - loss: 0.5267 - val_loss: 1.0146
Epoch 03040: val_loss improved from 1.01536 to 1.01464, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 3041/5000
26/26 - 1s - loss: 0.5270 - val_loss: 1.0157
Epoch 3042/5000
26/26 - 1s - loss: 0.5270 - val_loss: 1.0129
Epoch 3043/5000
26/26 - 2s - loss: 0.5260 - val_loss: 1.0130
Epoch 3044/5000
26/26 - 1s - loss: 0.5280 - val_loss: 1.0140
Epoch 3045/5000
26/26 - 1s - loss: 0.5263 - val_loss: 1.0126
Epoch 3046/5000
26/26 - 1s - loss: 0.5269 - val_loss: 1.0131
Epoch 3047/5000
26/26 - 1s - loss: 0.5273 - val_loss: 1.0119
Epoch 3048/5000
26/26 - 1s - loss: 0.5262 - val_loss: 1.0112
Epoch 3049/5000
26/26 - 1s - loss: 0.5275 - val_loss: 1.0156
Epoch 3050/5000
26/26 - 1s - loss: 0.5276 - val_loss: 1.0106
Epoch 03050: val_loss improved from 1.01464 to 1.01063, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 3051/5000
26/26 - 1s - loss: 0.5274 - val_loss: 1.0118
Epoch 3052/5000
26/26 - 1s - loss: 0.5260 - val_loss: 1.0141
Epoch 3053/5000
26/26 - 1s - loss: 0.5248 - val_loss: 1.0137
Epoch 3054/5000
26/26 - 1s - loss: 0.5247 - val_loss: 1.0121
Epoch 3055/5000
26/26 - 1s - loss: 0.5251 - val_loss: 1.0125
Epoch 3056/5000
26/26 - 1s - loss: 0.5263 - val_loss: 1.0109
Epoch 3057/5000
26/26 - 1s - loss: 0.5255 - val_loss: 1.0123
Epoch 3058/5000
26/26 - 1s - loss: 0.5255 - val_loss: 1.0108
Epoch 3059/5000
26/26 - 1s - loss: 0.5239 - val_loss: 1.0124
Epoch 3060/5000
26/26 - 1s - loss: 0.5250 - val_loss: 1.0138
Epoch 03060: val_loss did not improve from 1.01063
Epoch 3061/5000
26/26 - 1s - loss: 0.5254 - val_loss: 1.0123
Epoch 3062/5000
26/26 - 1s - loss: 0.5237 - val_loss: 1.0129
Epoch 3063/5000
26/26 - 1s - loss: 0.5242 - val_loss: 1.0101
Epoch 3064/5000
26/26 - 1s - loss: 0.5242 - val_loss: 1.0105
Epoch 3065/5000
26/26 - 1s - loss: 0.5259 - val_loss: 1.0119
Epoch 3066/5000
26/26 - 1s - loss: 0.5248 - val_loss: 1.0113
Epoch 3067/5000
26/26 - 1s - loss: 0.5236 - val_loss: 1.0108
Epoch 3068/5000
26/26 - 1s - loss: 0.5233 - val_loss: 1.0113
Epoch 3069/5000
26/26 - 1s - loss: 0.5249 - val_loss: 1.0130
Epoch 3070/5000
26/26 - 1s - loss: 0.5226 - val_loss: 1.0107
Epoch 03070: val_loss did not improve from 1.01063
Epoch 3071/5000
26/26 - 1s - loss: 0.5233 - val_loss: 1.0098
Epoch 3072/5000
26/26 - 1s - loss: 0.5232 - val_loss: 1.0110
Epoch 3073/5000
26/26 - 1s - loss: 0.5234 - val_loss: 1.0098
Epoch 3074/5000
26/26 - 1s - loss: 0.5243 - val_loss: 1.0090
Epoch 3075/5000
26/26 - 1s - loss: 0.5228 - val_loss: 1.0109
Epoch 3076/5000
26/26 - 1s - loss: 0.5239 - val_loss: 1.0081
Epoch 3077/5000
26/26 - 1s - loss: 0.5231 - val_loss: 1.0077
Epoch 3078/5000
26/26 - 1s - loss: 0.5222 - val_loss: 1.0102
Epoch 3079/5000
26/26 - 1s - loss: 0.5220 - val_loss: 1.0107
Epoch 3080/5000
26/26 - 1s - loss: 0.5222 - val_loss: 1.0095
Epoch 03080: val_loss improved from 1.01063 to 1.00953, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 3081/5000
26/26 - 1s - loss: 0.5234 - val_loss: 1.0092
Epoch 3082/5000
26/26 - 1s - loss: 0.5228 - val_loss: 1.0097
Epoch 3083/5000
26/26 - 1s - loss: 0.5225 - val_loss: 1.0089
Epoch 3084/5000
26/26 - 1s - loss: 0.5211 - val_loss: 1.0073
Epoch 3085/5000
26/26 - 1s - loss: 0.5202 - val_loss: 1.0084
Epoch 3086/5000
26/26 - 1s - loss: 0.5217 - val_loss: 1.0095
Epoch 3087/5000
26/26 - 1s - loss: 0.5214 - val_loss: 1.0088
Epoch 3088/5000
26/26 - 1s - loss: 0.5218 - val_loss: 1.0080
Epoch 3089/5000
26/26 - 1s - loss: 0.5214 - val_loss: 1.0066
Epoch 3090/5000
26/26 - 1s - loss: 0.5201 - val_loss: 1.0096
Epoch 03090: val_loss did not improve from 1.00953
Epoch 3091/5000
26/26 - 1s - loss: 0.5215 - val_loss: 1.0098
Epoch 3092/5000
26/26 - 1s - loss: 0.5226 - val_loss: 1.0062
Epoch 3093/5000
26/26 - 1s - loss: 0.5209 - val_loss: 1.0062
Epoch 3094/5000
26/26 - 1s - loss: 0.5211 - val_loss: 1.0071
Epoch 3095/5000
26/26 - 1s - loss: 0.5198 - val_loss: 1.0064
Epoch 3096/5000
26/26 - 1s - loss: 0.5206 - val_loss: 1.0047
Epoch 3097/5000
26/26 - 1s - loss: 0.5208 - val_loss: 1.0070
Epoch 3098/5000
26/26 - 1s - loss: 0.5202 - val_loss: 1.0062
Epoch 3099/5000
26/26 - 1s - loss: 0.5194 - val_loss: 1.0067
Epoch 3100/5000
26/26 - 1s - loss: 0.5195 - val_loss: 1.0060
Epoch 03100: val_loss improved from 1.00953 to 1.00595, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 3101/5000
26/26 - 1s - loss: 0.5191 - val_loss: 1.0087
Epoch 3102/5000
26/26 - 1s - loss: 0.5199 - val_loss: 1.0056
Epoch 3103/5000
26/26 - 1s - loss: 0.5179 - val_loss: 1.0042
Epoch 3104/5000
26/26 - 1s - loss: 0.5186 - val_loss: 1.0043
Epoch 3105/5000
26/26 - 1s - loss: 0.5187 - val_loss: 1.0048
Epoch 3106/5000
26/26 - 1s - loss: 0.5191 - val_loss: 1.0055
Epoch 3107/5000
26/26 - 2s - loss: 0.5175 - val_loss: 1.0041
Epoch 3108/5000
26/26 - 1s - loss: 0.5184 - val_loss: 1.0073
Epoch 3109/5000
26/26 - 1s - loss: 0.5179 - val_loss: 1.0044
Epoch 3110/5000
26/26 - 1s - loss: 0.5180 - val_loss: 1.0063
Epoch 03110: val_loss did not improve from 1.00595
Epoch 3111/5000
26/26 - 1s - loss: 0.5180 - val_loss: 1.0063
Epoch 3112/5000
26/26 - 1s - loss: 0.5191 - val_loss: 1.0043
Epoch 3113/5000
26/26 - 1s - loss: 0.5171 - val_loss: 1.0031
Epoch 3114/5000
26/26 - 1s - loss: 0.5181 - val_loss: 1.0028
Epoch 3115/5000
26/26 - 1s - loss: 0.5182 - val_loss: 1.0045
Epoch 3116/5000
26/26 - 1s - loss: 0.5171 - val_loss: 1.0075
Epoch 3117/5000
26/26 - 1s - loss: 0.5176 - val_loss: 1.0042
Epoch 3118/5000
26/26 - 1s - loss: 0.5166 - val_loss: 1.0022
Epoch 3119/5000
26/26 - 1s - loss: 0.5162 - val_loss: 1.0015
Epoch 3120/5000
26/26 - 1s - loss: 0.5179 - val_loss: 1.0046
Epoch 03120: val_loss improved from 1.00595 to 1.00464, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 3121/5000
26/26 - 1s - loss: 0.5178 - val_loss: 1.0052
Epoch 3122/5000
26/26 - 1s - loss: 0.5172 - val_loss: 1.0035
Epoch 3123/5000
26/26 - 1s - loss: 0.5175 - val_loss: 1.0031
Epoch 3124/5000
26/26 - 1s - loss: 0.5176 - val_loss: 1.0033
Epoch 3125/5000
26/26 - 1s - loss: 0.5169 - val_loss: 1.0037
Epoch 3126/5000
26/26 - 1s - loss: 0.5163 - val_loss: 1.0030
Epoch 3127/5000
26/26 - 1s - loss: 0.5160 - val_loss: 1.0019
Epoch 3128/5000
26/26 - 1s - loss: 0.5175 - val_loss: 1.0041
Epoch 3129/5000
26/26 - 1s - loss: 0.5166 - val_loss: 1.0031
Epoch 3130/5000
26/26 - 1s - loss: 0.5161 - val_loss: 1.0018
Epoch 03130: val_loss improved from 1.00464 to 1.00182, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-1.model.weights.hdf5
Epoch 3131/5000
26/26 - 1s - loss: 0.5152 - val_loss: 1.0025
Epoch 3132/5000
26/26 - 1s - loss: 0.5145 - val_loss: 1.0029
Epoch 3133/5000
26/26 - 1s - loss: 0.5149 - val_loss: 1.0019
Epoch 3134/5000
26/26 - 1s - loss: 0.5162 - val_loss: 1.0024
Epoch 3135/5000
26/26 - 1s - loss: 0.5153 - val_loss: 1.0021
Epoch 3136/5000
26/26 - 2s - loss: 0.5154 - val_loss: 1.0031
Epoch 3137/5000
26/26 - 1s - loss: 0.5143 - val_loss: 1.0032
Epoch 3138/5000
INFO     Computation time for training the single-label model for AR: 73.88 min
INFO     Network type: REG
INFO     Model: "sequential"
INFO     _________________________________________________________________
INFO     Layer (type)                 Output Shape              Param #
INFO     =================================================================
INFO     dense (Dense)                (None, 1024)              2098176
INFO     _________________________________________________________________
INFO     dropout (Dropout)            (None, 1024)              0
INFO     _________________________________________________________________
INFO     dense_1 (Dense)              (None, 512)               524800
INFO     _________________________________________________________________
INFO     dropout_1 (Dropout)          (None, 512)               0
INFO     _________________________________________________________________
INFO     dense_2 (Dense)              (None, 256)               131328
INFO     _________________________________________________________________
INFO     dropout_2 (Dropout)          (None, 256)               0
INFO     _________________________________________________________________
INFO     dense_3 (Dense)              (None, 128)               32896
INFO     _________________________________________________________________
INFO     dropout_3 (Dropout)          (None, 128)               0
INFO     _________________________________________________________________
INFO     dense_4 (Dense)              (None, 1)                 129
INFO     =================================================================
INFO     Total params: 2,787,329
INFO     Trainable params: 2,787,329
INFO     Non-trainable params: 0
INFO     _________________________________________________________________
26/26 - 1s - loss: 0.5152 - val_loss: 1.0024
Epoch 3139/5000
26/26 - 1s - loss: 0.5140 - val_loss: 1.0027
Restoring model weights from the end of the best epoch.
Epoch 03139: early stopping
Epoch 1/5000
INFO     Evaluating trained model 'AR single-labeled Fold-1' on test data
INFO     Training of fold number: 2
INFO     Training sample distribution: train data: {-1.2016366720199585: 7, -1.2016383409500122: 5, -1.201635479927063: 4, -1.2016377449035645: 4, -1.20163094997406: 4, -1.2016351222991943: 3, -1.2016384601593018: 3, -1.2016355991363525: 3, -1.2016353607177734: 3, -1.201636552810669: 3, -1.2016369104385376: 3, -1.2016324996948242: 3, -1.2016363143920898: 3, -1.201635718345642: 2, -1.2016254663467407: 2, -1.2016315460205078: 2, -1.2016302347183228: 2, -1.2016310691833496: 2, -1.2015819549560547: 2, -1.201622486114502: 2, -1.201629877090454: 2, -1.2016339302062988: 2, -1.2016342878341675: 2, -1.2016345262527466: 2, -1.201621651649475: 2, -1.2016304731369019: 2, -1.2016303539276123: 2, -1.2016288042068481: 2, -1.2016159296035767: 2, -1.201596975326538: 2, -1.2016290426254272: 2, -1.201637625694275: 2, -1.2016295194625854: 2, -1.2016326189041138: 2, -1.2016253471374512: 2, 0.31096193194389343: 1, -0.6966411471366882: 1, 1.8254425525665283: 1, 1.4535586833953857: 1, 0.002237366745248437: 1, 0.31109386682510376: 1, 1.4237861633300781: 1, 0.5247108340263367: 1, 1.2952117919921875: 1, -0.2558962106704712: 1, 0.7778733968734741: 1, 0.8396581411361694: 1, 0.6829988956451416: 1, 0.831580638885498: 1, 1.5720155239105225: 1, 0.5732383131980896: 1, 0.25592291355133057: 1, 0.6158161163330078: 1, 0.8825770616531372: 1, 1.3986883163452148: 1, -0.9010018706321716: 1, -0.5348793864250183: 1, 0.2900018095970154: 1, 1.3083291053771973: 1, 0.5601941347122192: 1, -0.37317758798599243: 1, -0.16027171909809113: 1, 1.168498158454895: 1, -1.0873279571533203: 1, 0.10525540262460709: 1, 0.3664189279079437: 1, -0.4459679424762726: 1, 1.6950387954711914: 1, 1.1870161294937134: 1, 0.5256680846214294: 1, 1.4836735725402832: 1, 0.20332399010658264: 1, -1.2015975713729858: 1, 1.5455868244171143: 1, 0.02149348333477974: 1, 0.2669691741466522: 1, -0.008235386572778225: 1, 0.2470504194498062: 1, -0.9223102331161499: 1, -0.297276109457016: 1, 1.5978947877883911: 1, -1.201623558998108: 1, 1.1970174312591553: 1, 1.5667779445648193: 1, -0.09881063550710678: 1, -0.29657527804374695: 1, -0.425843745470047: 1, 1.5410983562469482: 1, 0.21768306195735931: 1, 1.3818247318267822: 1, 1.1950836181640625: 1, 1.3501269817352295: 1, 0.11128426343202591: 1, -0.5620038509368896: 1, -0.5027830004692078: 1, -0.24468590319156647: 1, 0.25783771276474: 1, -0.5330178141593933: 1, -0.6442912220954895: 1, 0.3488617241382599: 1, 0.5299685597419739: 1, 0.2713952362537384: 1, 1.0667099952697754: 1, -0.4970245659351349: 1, 0.21748410165309906: 1, -0.3774075210094452: 1, -0.16336557269096375: 1, -0.16630633175373077: 1, -0.31709083914756775: 1, 0.08119866997003555: 1, -0.35442060232162476: 1, 0.22109845280647278: 1, 0.8313724994659424: 1, -0.7750424146652222: 1, 0.7218993902206421: 1, 0.0860099047422409: 1, -1.201627492904663: 1, 0.28807583451271057: 1, 0.6573249697685242: 1, -0.06838630884885788: 1, 0.22961212694644928: 1, 0.19487899541854858: 1, 1.166581153869629: 1, -0.2393776774406433: 1, 0.6820655465126038: 1, 0.8028292059898376: 1, 0.6465133428573608: 1, 0.7402693033218384: 1, -1.0873202085494995: 1, 0.540539026260376: 1, 1.4755654335021973: 1, -1.160044550895691: 1, 1.2994223833084106: 1, 0.31334978342056274: 1, 1.4923255443572998: 1, -0.17338967323303223: 1, 0.9650787115097046: 1, -0.9669303297996521: 1, 1.1683435440063477: 1, 0.31522658467292786: 1, 0.695344090461731: 1, 0.2743425965309143: 1, -0.19042591750621796: 1, -0.20045150816440582: 1, 1.4424492120742798: 1, 1.9054079055786133: 1, 0.9285130500793457: 1, 0.17100460827350616: 1, 0.3443826735019684: 1, 1.286658525466919: 1, 1.5370275974273682: 1, 0.7171676754951477: 1, 0.6386443972587585: 1, 1.4500402212142944: 1, 1.088638186454773: 1, 1.5124294757843018: 1, 0.5170465111732483: 1, -1.189130187034607: 1, 0.8274267315864563: 1, -0.2711203992366791: 1, 0.3245508372783661: 1, -0.13643299043178558: 1, 0.16810350120067596: 1, 0.49536043405532837: 1, 1.2379846572875977: 1, 0.23716896772384644: 1, 0.0010552277090027928: 1, 0.2500914931297302: 1, -0.31671836972236633: 1, 0.9998847246170044: 1, 0.06883639097213745: 1, 0.17204155027866364: 1, 0.04832748696208: 1, 0.2610865831375122: 1, 0.1385408341884613: 1, -0.17784874141216278: 1, 1.595760464668274: 1, 0.8440163731575012: 1, -0.5706648826599121: 1, 0.26908448338508606: 1, 1.6063342094421387: 1, 1.5088152885437012: 1, 1.605971336364746: 1, -0.399366557598114: 1, 0.2954026460647583: 1, -0.18418414890766144: 1, 0.8664619326591492: 1, 0.32629087567329407: 1, 1.415509581565857: 1, 1.6089627742767334: 1, 1.5149681568145752: 1, 0.36003923416137695: 1, 0.3422311544418335: 1, 0.29451489448547363: 1, -0.10739652067422867: 1, -1.201636791229248: 1, 0.15692748129367828: 1, -0.026365874335169792: 1, -0.4993174076080322: 1, 0.7279888391494751: 1, 0.015656888484954834: 1, -0.3344474732875824: 1, 1.5035943984985352: 1, 0.34191834926605225: 1, 0.643775999546051: 1, -0.30124133825302124: 1, 1.4653488397598267: 1, 1.4158756732940674: 1, 1.3042255640029907: 1, 1.571059226989746: 1, 0.663663923740387: 1, -1.0201867818832397: 1, 0.15129715204238892: 1, 0.2609155476093292: 1, -1.194837212562561: 1, 1.478103518486023: 1, 1.1153340339660645: 1, -1.1840753555297852: 1, -1.1962217092514038: 1, -1.2009947299957275: 1, -1.2008297443389893: 1, -1.1980621814727783: 1, -1.198028802871704: 1, -1.1962871551513672: 1, -1.201625108718872: 1, -1.1962995529174805: 1, -1.1956098079681396: 1, -1.1269394159317017: 1, -0.7543148398399353: 1, -1.1961579322814941: 1, -0.15980762243270874: 1, -0.8842195272445679: 1, -0.7196366786956787: 1, -0.2302703857421875: 1, -0.5718616843223572: 1, 1.7539664506912231: 1, 1.1079081296920776: 1, 0.8374537825584412: 1, -0.26343750953674316: 1, 1.4342314004898071: 1, -1.1939657926559448: 1, 0.005114047322422266: 1, 0.28549933433532715: 1, -1.1972938776016235: 1, 0.034827083349227905: 1, -0.37911587953567505: 1, -1.1482487916946411: 1, -1.2000701427459717: 1, 0.5436715483665466: 1, -1.2007057666778564: 1, -1.2015410661697388: 1, -1.045175313949585: 1, -1.1976145505905151: 1, -0.7745821475982666: 1, -0.9927355051040649: 1, -1.1987890005111694: 1, -1.196489930152893: 1, -1.1964974403381348: 1, -1.1852269172668457: 1, -1.1929398775100708: 1, -1.1945018768310547: 1, -0.34237241744995117: 1, -1.1871466636657715: 1, -1.1237342357635498: 1, -1.1664562225341797: 1, 0.9686956405639648: 1, -0.5639210939407349: 1, 0.4001854956150055: 1, 0.7675377726554871: 1, -0.883184015750885: 1, -0.19289743900299072: 1, 0.0290671493858099: 1, -0.6085047125816345: 1, 1.0926192998886108: 1, 0.2407364845275879: 1, 1.8768579959869385: 1, 1.8398889303207397: 1, 1.8953936100006104: 1, -1.009895920753479: 1, -0.30134135484695435: 1, 0.7517246007919312: 1, -1.2016119956970215: 1, -0.4007585644721985: 1, 1.5550175905227661: 1, -0.8897131681442261: 1, -0.116549052298069: 1, -0.47023993730545044: 1, 0.7365891337394714: 1, 1.0860453844070435: 1, 1.0165932178497314: 1, -0.4147615134716034: 1, -0.9769929647445679: 1, -0.39423879981040955: 1, -0.3334684669971466: 1, -0.4268537759780884: 1, 0.16395387053489685: 1, -0.29486238956451416: 1, 0.5479292273521423: 1, -0.5222632884979248: 1, -0.27864202857017517: 1, -0.32787415385246277: 1, 0.2115481197834015: 1, 1.4029184579849243: 1, -0.3579169511795044: 1, -1.177890419960022: 1, 1.603068470954895: 1, -0.8235211968421936: 1, 0.2787120044231415: 1, -0.2865978181362152: 1, 1.4387927055358887: 1, 0.18236172199249268: 1, 1.2996296882629395: 1, -0.30772721767425537: 1, 1.3062021732330322: 1, -0.37773171067237854: 1, -0.512914776802063: 1, -1.1987498998641968: 1, 0.12603989243507385: 1, -0.9888867139816284: 1, -1.2013576030731201: 1, -1.092740535736084: 1, -1.2016162872314453: 1, -0.3419588804244995: 1, -1.1730265617370605: 1, -0.8519163131713867: 1, 0.6741006970405579: 1, 0.9611315131187439: 1, 0.031881630420684814: 1, -1.2016046047210693: 1, -0.4260459542274475: 1, -0.506174623966217: 1, -0.3525408208370209: 1, -1.180529236793518: 1, -0.6465518474578857: 1, 0.8243502378463745: 1, -1.1526696681976318: 1, -1.0348321199417114: 1, 0.603252649307251: 1, -0.4781683385372162: 1, -1.2015955448150635: 1, -0.9931707978248596: 1, -1.137963891029358: 1, -1.2016127109527588: 1, -0.916256844997406: 1, 1.621954083442688: 1, 1.5633379220962524: 1, 1.593016266822815: 1, 0.32263097167015076: 1, 0.9703378081321716: 1, -1.1950759887695312: 1, 0.44327667355537415: 1, -0.1421377956867218: 1, 1.6047093868255615: 1, 1.5627902746200562: 1, 0.670230507850647: 1, -1.1937233209609985: 1, -1.2012096643447876: 1, -1.1647279262542725: 1, -1.2015366554260254: 1, 0.37413451075553894: 1, 0.16982176899909973: 1, -0.8657602667808533: 1, -0.6204319000244141: 1, -1.2013626098632812: 1, -1.201349139213562: 1, -0.5828713178634644: 1, -1.201597809791565: 1, -0.33821895718574524: 1, -1.2002341747283936: 1, -1.201564908027649: 1, -1.0070439577102661: 1, 0.1561044156551361: 1, -1.1205617189407349: 1, -0.09781666100025177: 1, -0.06465810537338257: 1, -0.2831217646598816: 1, 0.7523799538612366: 1, -0.8503693342208862: 1, -1.2014657258987427: 1, -1.201615810394287: 1, -1.2014594078063965: 1, -0.27123570442199707: 1, 0.5888639092445374: 1, -1.1972260475158691: 1, -1.1674489974975586: 1, -1.201310396194458: 1, 1.14208984375: 1, -0.9657180905342102: 1, -1.1830931901931763: 1, -0.6864959597587585: 1, -1.2014697790145874: 1, -1.201614260673523: 1, 0.10008653253316879: 1, -0.8027163147926331: 1, -0.9153497219085693: 1, -1.2015149593353271: 1, -0.5025617480278015: 1, -0.6686071157455444: 1, -1.201509714126587: 1, -1.2007629871368408: 1, -1.2015974521636963: 1, -1.2015589475631714: 1, -0.7713357210159302: 1, 0.812082827091217: 1, -1.201629400253296: 1, 0.21984633803367615: 1, -1.1999688148498535: 1, -1.201606273651123: 1, -1.2015992403030396: 1, 0.6647039651870728: 1, -1.201633095741272: 1, -1.2016280889511108: 1, -1.201348900794983: 1, -1.201627254486084: 1, -1.1506352424621582: 1, -0.993471086025238: 1, 1.1508636474609375: 1, -0.7282882928848267: 1, -0.2154475301504135: 1, 1.577986240386963: 1, 1.5516252517700195: 1, -1.1573199033737183: 1, 0.7257186770439148: 1, -1.098894715309143: 1, -0.24214474856853485: 1, -1.1312958002090454: 1, -1.2013144493103027: 1, -0.41735291481018066: 1, 1.5167564153671265: 1, -1.111975073814392: 1, -1.0090559720993042: 1, -1.0634658336639404: 1, -1.1138004064559937: 1, 1.8358889818191528: 1, -0.8241826295852661: 1, -1.0799649953842163: 1, -1.1919200420379639: 1, -0.07864277809858322: 1, -0.04164140671491623: 1, 0.4990178942680359: 1, -0.4610443115234375: 1, 1.3791615962982178: 1, -0.9432769417762756: 1, -0.45158419013023376: 1, -0.6316778063774109: 1, -0.6980454921722412: 1, -0.6311256289482117: 1, -1.188598394393921: 1, -1.1560314893722534: 1, -0.9288858771324158: 1, -0.18136551976203918: 1, -0.5884833335876465: 1, -0.9047161340713501: 1, -1.1412583589553833: 1, -1.0422825813293457: 1, 1.4887964725494385: 1, -0.3497014045715332: 1, 0.7494425773620605: 1, -0.556195080280304: 1, 1.788353681564331: 1, -0.896551251411438: 1, 0.43905025720596313: 1, 1.8447388410568237: 1, -1.1907743215560913: 1, 0.6965684294700623: 1, -1.1941906213760376: 1, -1.0395952463150024: 1, -0.9751180410385132: 1, -1.0622771978378296: 1, -1.1442792415618896: 1, -0.700495719909668: 1, 1.3982725143432617: 1, 1.281778335571289: 1, -1.1798655986785889: 1, -0.4367877244949341: 1, -0.8407037854194641: 1, -1.1983946561813354: 1, -0.5144169330596924: 1, -0.7376867532730103: 1, 1.2280430793762207: 1, 0.16367900371551514: 1, -0.9755853414535522: 1, 0.2268732339143753: 1, 0.5357815027236938: 1, -0.5850008726119995: 1, -1.17500901222229: 1, -0.03840658441185951: 1, -0.9793020486831665: 1, 0.4252239763736725: 1, 0.784543514251709: 1, -0.748826265335083: 1, 0.22341269254684448: 1, -0.5242934823036194: 1, 0.5786248445510864: 1, 0.8691079020500183: 1, -1.1569617986679077: 1, 0.20812031626701355: 1, 1.7476170063018799: 1, 0.19889964163303375: 1, -1.0407896041870117: 1, -1.1999518871307373: 1, 0.06941241025924683: 1, 1.9090871810913086: 1, -1.1800106763839722: 1, 0.006390336435288191: 1, -1.1358855962753296: 1, -0.5620161294937134: 1, -0.3605387806892395: 1, 0.292474627494812: 1, -0.9981153011322021: 1, -0.9736310243606567: 1, 0.9790754318237305: 1, 0.20924636721611023: 1, -0.5120261311531067: 1, 0.05307941138744354: 1, -0.43377333879470825: 1, 0.6492028832435608: 1, 0.5685755014419556: 1, -0.930094301700592: 1, 0.2903974652290344: 1, 1.0995265245437622: 1, -1.1212905645370483: 1, -0.3573164939880371: 1, -1.0539400577545166: 1, -1.1605626344680786: 1, -0.7244925498962402: 1, -1.1898142099380493: 1, -0.8890491724014282: 1, 0.6293842196464539: 1, -0.8917420506477356: 1, 1.1045739650726318: 1, 0.04404761642217636: 1, 0.0663938894867897: 1, -0.5982488989830017: 1, 1.4690353870391846: 1, 0.17207567393779755: 1, 1.4842547178268433: 1, -0.981871485710144: 1, -1.1791499853134155: 1, 0.2662027180194855: 1, -1.078048825263977: 1, 1.8480533361434937: 1, 1.0025136470794678: 1, 1.6904795169830322: 1, -0.3465023636817932: 1, -0.7701697945594788: 1, -0.8680421113967896: 1, 0.6570013761520386: 1, 1.4680920839309692: 1, 1.0112850666046143: 1, -0.9134752750396729: 1, -0.9460977911949158: 1, -1.19014310836792: 1, -0.67027348279953: 1, 0.49092090129852295: 1, 0.638069212436676: 1, 1.4807751178741455: 1, -1.0776159763336182: 1, -0.08619289845228195: 1, 0.6749281883239746: 1, -0.9803085327148438: 1, 0.5575955510139465: 1, 0.39953649044036865: 1, -0.9919153451919556: 1, -0.233070969581604: 1, 1.4876383543014526: 1, 0.5211433172225952: 1, -0.9951556921005249: 1, -1.0858170986175537: 1, -0.90742427110672: 1, -1.0221163034439087: 1, -0.1391475349664688: 1, 0.5474697947502136: 1, -0.923246443271637: 1, -1.2012841701507568: 1, -0.19042661786079407: 1, -0.2147151529788971: 1, -0.9940837025642395: 1, -1.0393953323364258: 1, 1.3862724304199219: 1, 1.6304298639297485: 1, 1.6431117057800293: 1, -0.1989370882511139: 1, -1.0411947965621948: 1, -0.8935588002204895: 1, 1.114488959312439: 1, -1.18907630443573: 1, -1.0768063068389893: 1, -1.0751264095306396: 1, -0.7096079587936401: 1, -0.35407769680023193: 1, -1.197619080543518: 1, -0.2589719295501709: 1, -1.2003082036972046: 1, -0.4400210678577423: 1, -1.1370965242385864: 1, -1.1041064262390137: 1, -0.9642779231071472: 1, -1.194373369216919: 1, -1.1712636947631836: 1, 1.0554637908935547: 1, 0.3200169503688812: 1, 3.259727716445923: 1, -1.1946264505386353: 1, 0.025435535237193108: 1, -0.09378552436828613: 1, -0.8962797522544861: 1, -1.199107050895691: 1, 0.30057549476623535: 1, 0.9532740116119385: 1, 0.8790757060050964: 1, -0.9627416729927063: 1, 1.0116826295852661: 1, -0.9982122778892517: 1, -1.1845873594284058: 1, -1.2009141445159912: 1, -1.1325860023498535: 1, 1.8212419748306274: 1, -0.36594024300575256: 1, -1.1659823656082153: 1, -1.0714704990386963: 1, 0.9346560835838318: 1, -0.8747767210006714: 1, 1.4774726629257202: 1, 1.1008855104446411: 1, -1.1914112567901611: 1, 1.5811641216278076: 1, -0.25767362117767334: 1, -0.027011625468730927: 1, 1.3250715732574463: 1, 0.12876805663108826: 1, 0.465168297290802: 1, 0.2403424084186554: 1, -0.5288078188896179: 1, 0.2308363914489746: 1, 0.8504006266593933: 1, -0.8986467123031616: 1, 1.2588475942611694: 1, 0.4723914861679077: 1, -0.24819114804267883: 1, -0.06996402144432068: 1, 0.36215391755104065: 1, -0.5825971961021423: 1, 1.427193522453308: 1, -0.16857680678367615: 1, -0.39828142523765564: 1, 1.5823100805282593: 1, -0.15746326744556427: 1, 1.4811328649520874: 1, -0.05734042823314667: 1, 0.11351441591978073: 1, 0.8741052150726318: 1, 0.8892757296562195: 1, 0.4746771454811096: 1, 1.0541952848434448: 1, 0.5222756862640381: 1, 0.9129876494407654: 1, 1.2882025241851807: 1, -0.01062939316034317: 1, 1.0303751230239868: 1, 1.5870535373687744: 1, 1.0355088710784912: 1, 1.3002121448516846: 1, 0.9657272696495056: 1, 0.27488669753074646: 1, 0.7110782265663147: 1, 1.210314393043518: 1, 0.5456136465072632: 1, 0.1011820137500763: 1, 1.495969295501709: 1, 1.2984728813171387: 1, 1.3494455814361572: 1, 1.0170906782150269: 1, 1.3024239540100098: 1, 1.2427196502685547: 1, -0.5881722569465637: 1, 1.4308977127075195: 1, -0.20539666712284088: 1, 0.7950616478919983: 1, -0.22176611423492432: 1, 0.13578376173973083: 1, -0.4253336787223816: 1, 0.2726168930530548: 1, -0.02133699133992195: 1, 0.03207547590136528: 1, 1.5805106163024902: 1, 0.4048600494861603: 1, 0.637361466884613: 1, 0.71575528383255: 1, 1.3996703624725342: 1, 1.7721431255340576: 1, 1.0551327466964722: 1, -0.21738800406455994: 1, 1.5277636051177979: 1, 0.20390741527080536: 1, 0.912930965423584: 1, 0.7471779584884644: 1, -1.115770697593689: 1, -0.03796708956360817: 1, 1.6003168821334839: 1, -0.030014334246516228: 1, 0.23328566551208496: 1, -0.004194003064185381: 1, -0.8510425090789795: 1, -0.930620014667511: 1, 1.6178103685379028: 1, -1.2016338109970093: 1, 0.15925046801567078: 1, -0.37734130024909973: 1, 1.0202414989471436: 1, 1.1998642683029175: 1, 0.22981515526771545: 1, 1.3784377574920654: 1, 1.2730098962783813: 1, 0.7611046433448792: 1, -1.113705039024353: 1, 1.2712597846984863: 1, 1.573736548423767: 1, 0.7825468182563782: 1, 0.3604428768157959: 1, 0.4633817672729492: 1, 1.427628755569458: 1, 0.35480692982673645: 1, 1.4536134004592896: 1, 1.5755736827850342: 1, -0.7170498371124268: 1, -0.6611031889915466: 1, 1.1217374801635742: 1, 1.425097107887268: 1, -1.0719594955444336: 1, 1.287703037261963: 1, 1.4413039684295654: 1, -0.8234555125236511: 1, -1.1940946578979492: 1, -1.1507015228271484: 1, -0.9339156150817871: 1, -1.1607003211975098: 1, 1.9238320589065552: 1, -1.1969212293624878: 1, -0.19720852375030518: 1, -0.13197508454322815: 1, 0.1708119511604309: 1, -0.23679472506046295: 1, -0.14438967406749725: 1, -0.4185396134853363: 1, 0.2964378595352173: 1, 0.739153265953064: 1, 0.03332117572426796: 1, 0.002877143444493413: 1, 1.4062000513076782: 1, -1.1895103454589844: 1, -0.9859256744384766: 1, 0.342952162027359: 1, -0.9158876538276672: 1, -1.0084080696105957: 1, -0.9050359129905701: 1, -0.5784834027290344: 1, -0.9131625890731812: 1, 1.1575602293014526: 1, -1.1544640064239502: 1, 0.40835040807724: 1, -0.9035985469818115: 1, -0.8622487187385559: 1, 1.2659807205200195: 1, -0.8956496715545654: 1, -1.0024443864822388: 1, -0.8754668831825256: 1, -0.2101057469844818: 1, -0.38419657945632935: 1, -0.27164560556411743: 1, -0.09434226900339127: 1, -0.0960833728313446: 1, 1.1112544536590576: 1, -0.48030614852905273: 1, 1.6006011962890625: 1, -0.2782626748085022: 1, 0.3313300311565399: 1, 1.0402084589004517: 1, 0.31799715757369995: 1, 1.4644434452056885: 1, 1.053705096244812: 1, 1.001185417175293: 1, 0.6039040088653564: 1, -1.031872034072876: 1, 0.1467430293560028: 1, 0.3286954462528229: 1, 0.46373531222343445: 1, -0.016411839053034782: 1, 0.10751932114362717: 1, 0.003300704760476947: 1, 0.7724436521530151: 1, 0.2640135586261749: 1, 1.3437533378601074: 1, 0.9666041731834412: 1, 0.16298139095306396: 1, 0.9499619007110596: 1, 0.02830575592815876: 1, 1.3615977764129639: 1, -1.2016340494155884: 1, 1.7417840957641602: 1, -0.7180438041687012: 1, -0.32526895403862: 1, -0.589444637298584: 1, -0.021469445899128914: 1, 0.28860634565353394: 1, 1.3262649774551392: 1, -0.11414719372987747: 1, 0.3607413172721863: 1, -0.18094922602176666: 1, -0.10225572437047958: 1, 1.366266131401062: 1, -0.16261765360832214: 1, -0.1799832135438919: 1, 0.1897212415933609: 1, 0.900846004486084: 1, -0.8486149311065674: 1, 1.0683897733688354: 1, -0.514695405960083: 1, 0.9138351678848267: 1, 0.36046868562698364: 1, 0.12815426290035248: 1, -0.16177639365196228: 1, 0.3255096673965454: 1, 0.3698660731315613: 1, -0.539240837097168: 1, 0.2775695323944092: 1, 1.477781891822815: 1, 0.6789939403533936: 1, -0.6538054943084717: 1, -0.41689032316207886: 1, 0.5131713151931763: 1, -1.2006548643112183: 1, -1.0099024772644043: 1, -1.1663979291915894: 1, 1.8360271453857422: 1, 1.5100682973861694: 1, -1.1264077425003052: 1, 1.0540943145751953: 1, 1.5793328285217285: 1, 0.42848649621009827: 1, -0.2686515748500824: 1, 0.4860861599445343: 1, -1.061113953590393: 1, -1.201619267463684: 1, 0.11243647336959839: 1, -1.2014974355697632: 1, 1.2202951908111572: 1, 1.280470848083496: 1, -0.807515025138855: 1, 0.28526046872138977: 1, -1.2013123035430908: 1, -1.2016263008117676: 1, 0.3117649555206299: 1, -0.26848453283309937: 1, -0.2495342493057251: 1, -1.2016352415084839: 1, 1.1086541414260864: 1, 1.7298698425292969: 1, 1.4182209968566895: 1, 1.0101338624954224: 1, 0.9832457304000854: 1, 1.891126036643982: 1, 0.19680047035217285: 1, 0.5497986674308777: 1, 0.08808748424053192: 1, 1.6464146375656128: 1, -1.0590986013412476: 1, 1.9098440408706665: 1, 1.6939563751220703: 1, 0.523788332939148: 1, 0.2675780653953552: 1, 1.6441566944122314: 1, 1.4320223331451416: 1, -0.7435175180435181: 1, 1.7558945417404175: 1, -0.4567924439907074: 1, -0.962668240070343: 1, 1.3301595449447632: 1, -1.1416743993759155: 1, 1.5719680786132812: 1, 1.9020731449127197: 1, 1.8529928922653198: 1, -0.10222127288579941: 1, 1.7350994348526: 1, -0.44384631514549255: 1, 1.5665374994277954: 1, 0.5419895052909851: 1, 1.0374422073364258: 1, 1.5698747634887695: 1, 1.3496158123016357: 1, 1.9196945428848267: 1, -1.2016111612319946: 1, -0.9635469913482666: 1, -1.0791629552841187: 1, -0.5369133949279785: 1, -0.6532332301139832: 1, 0.9807740449905396: 1, 1.796111822128296: 1, -0.94952791929245: 1, -0.7931265234947205: 1, 0.5326334834098816: 1, -0.4422439932823181: 1, -0.9877029061317444: 1, 0.14208731055259705: 1, -0.253825306892395: 1, 0.6003371477127075: 1, -0.5811882019042969: 1, -0.7447215914726257: 1, -0.35984915494918823: 1, 1.1998889446258545: 1, -0.9704957008361816: 1, 1.5992790460586548: 1, -1.2016212940216064: 1, -0.5300003886222839: 1, 1.2604477405548096: 1, -0.864342451095581: 1, 1.6134487390518188: 1, 1.5396013259887695: 1, -1.1658029556274414: 1, -1.2016136646270752: 1, 1.633592128753662: 1, 1.3466922044754028: 1, -1.2002416849136353: 1, 1.5722275972366333: 1, 0.5880253314971924: 1, -0.7490406036376953: 1, -1.042400598526001: 1, -0.2810556888580322: 1, 0.27857378125190735: 1, 0.591416597366333: 1, 1.5315228700637817: 1, 0.2889866232872009: 1, 0.81379234790802: 1, 1.1236602067947388: 1, 0.26557838916778564: 1, 0.43645578622817993: 1, -0.2772659659385681: 1, 1.6143009662628174: 1, 0.7481110692024231: 1, 0.04922454059123993: 1, -0.08275699615478516: 1, 1.4337563514709473: 1, 0.766596257686615: 1, 0.5438616275787354: 1, 0.7045637369155884: 1, -0.03429622948169708: 1, 1.545008897781372: 1, 0.5755087733268738: 1, 1.8893579244613647: 1, -1.0097246170043945: 1, 0.39559683203697205: 1, -0.1754150390625: 1, -0.03317539766430855: 1, 1.7539278268814087: 1, -0.7948459982872009: 1, 0.7833142876625061: 1, -0.03557446971535683: 1, -0.7589280009269714: 1, -0.3100615441799164: 1, -0.22727444767951965: 1, 1.692647099494934: 1, 1.8799982070922852: 1, 1.2042120695114136: 1, -0.15924076735973358: 1, -0.8965012431144714: 1, 1.5299123525619507: 1, 1.9158005714416504: 1, 1.7392624616622925: 1, 0.4917255938053131: 1, 1.6278821229934692: 1, 1.6179100275039673: 1, 0.11675674468278885: 1, 0.16001862287521362: 1, 0.3740043342113495: 1, -0.22299611568450928: 1, 0.056893277913331985: 1, 1.6735926866531372: 1, 0.0023630079813301563: 1, 0.8739101886749268: 1, 1.0263571739196777: 1, -0.4220041036605835: 1, -0.4791189730167389: 1, -0.5665901303291321: 1, 1.6153100728988647: 1, 1.0135881900787354: 1, -0.31705647706985474: 1, 0.8423707485198975: 1, 1.0885628461837769: 1, -0.19816404581069946: 1, -0.2079317569732666: 1, 0.1083393469452858: 1, 0.723430871963501: 1, 0.7741647958755493: 1, 1.6500415802001953: 1, 0.6886505484580994: 1, 0.5059344172477722: 1, 1.8263726234436035: 1, -0.04758370667695999: 1, 0.9454357028007507: 1, 0.8505056500434875: 1, 0.521833598613739: 1, 0.26564425230026245: 1, 0.18436919152736664: 1, 0.9952530264854431: 1, 0.949316143989563: 1, 1.3716888427734375: 1, 0.9761800765991211: 1, -0.0515512116253376: 1, 0.29674577713012695: 1, 0.7860816717147827: 1, 0.48093563318252563: 1, -0.2709258496761322: 1, 1.0254307985305786: 1, -0.12864308059215546: 1, -0.180640310049057: 1, 1.6906383037567139: 1, 1.8846700191497803: 1, 0.6905592679977417: 1, 1.5815212726593018: 1, 1.1628241539001465: 1, 1.7223035097122192: 1, -0.22097758948802948: 1, 0.34220972657203674: 1, 0.6320856809616089: 1, 1.9327584505081177: 1, 0.6905925273895264: 1, 1.4221618175506592: 1, 1.4504951238632202: 1, -1.054260492324829: 1, 0.5422061085700989: 1, 0.23050570487976074: 1, -0.408608615398407: 1, 1.9389169216156006: 1, 1.271135687828064: 1, -0.0861414223909378: 1, 0.562964141368866: 1, 1.6581584215164185: 1, -1.2015928030014038: 1, -1.2015763521194458: 1, 0.6071187853813171: 1, 0.38025134801864624: 1, 1.4854387044906616: 1, 0.9784976243972778: 1, 1.0590068101882935: 1, 0.6801310777664185: 1, 1.2539737224578857: 1, 1.610649824142456: 1, -1.0813920497894287: 1, -0.9450681209564209: 1, 0.8005363941192627: 1, 0.10180643945932388: 1, -1.012891411781311: 1, -0.13542917370796204: 1, -1.2016215324401855: 1, 0.963599681854248: 1, -0.7838892936706543: 1, -0.43114355206489563: 1, 0.261216938495636: 1, 0.5681759119033813: 1, 0.3816104531288147: 1, -1.002498984336853: 1, -0.20698606967926025: 1, 0.20394694805145264: 1, 0.780617356300354: 1, 1.6759966611862183: 1, 1.0316096544265747: 1, -1.2015867233276367: 1, -0.538144052028656: 1, 1.7389863729476929: 1, 1.896134853363037: 1, -1.040601134300232: 1, -0.8136187195777893: 1, 1.519827961921692: 1, -1.085170865058899: 1, 0.8646785616874695: 1, -0.7696746587753296: 1, 1.0987391471862793: 1, 0.2591017186641693: 1, 1.2727433443069458: 1, -1.201594352722168: 1, 0.6376197934150696: 1, -1.2015223503112793: 1, -0.7923315763473511: 1, -1.2007230520248413: 1, -0.9954119920730591: 1, 0.5805153250694275: 1, 0.4620521366596222: 1, -1.0611162185668945: 1, -0.34224218130111694: 1, -0.06900987029075623: 1, -0.3587249517440796: 1, -0.40272125601768494: 1, -1.130736231803894: 1, -0.24564200639724731: 1, -0.20700129866600037: 1, 0.9156394600868225: 1, 0.9158507585525513: 1, 1.3129916191101074: 1, 0.8895251154899597: 1, -1.2016278505325317: 1, -1.1936933994293213: 1, -0.7448302507400513: 1, -1.2016236782073975: 1, 0.8969712257385254: 1, -0.8653848767280579: 1, -1.1320221424102783: 1, 1.2369016408920288: 1, -0.8793452382087708: 1, 1.3847272396087646: 1, 1.3335411548614502: 1, -1.2016057968139648: 1, 0.4253986179828644: 1, -0.2255825698375702: 1, 1.3490053415298462: 1, 1.5920872688293457: 1, 1.7425659894943237: 1, -0.9789384603500366: 1, -1.14544677734375: 1, -0.6306192278862: 1, 0.9750880599021912: 1, 1.3510494232177734: 1, 1.6034055948257446: 1, -0.6586742401123047: 1, 0.1718989461660385: 1, -1.200726866722107: 1, -1.2011404037475586: 1, 1.339892864227295: 1, -0.13886263966560364: 1, -0.24342182278633118: 1, 1.1129239797592163: 1, -1.1302224397659302: 1, 1.8749902248382568: 1, 1.127722144126892: 1, 0.46223318576812744: 1, -0.22918304800987244: 1, 0.9006003737449646: 1, 1.0329307317733765: 1, 1.0343732833862305: 1, -1.2016191482543945: 1, 0.05316608399152756: 1, 1.4475048780441284: 1, 1.9172451496124268: 1, 1.8285820484161377: 1, 0.43482455611228943: 1, 1.8706549406051636: 1, 1.446770191192627: 1, 1.8303353786468506: 1, -0.2059621661901474: 1, 1.7338974475860596: 1, -1.1562846899032593: 1, 1.760259985923767: 1, -0.11584310233592987: 1, -0.9069592952728271: 1, 1.788615107536316: 1, 0.8217967748641968: 1, 0.08071509003639221: 1, 0.8788592219352722: 1, 0.10331395268440247: 1, -0.9175146222114563: 1, 0.9160022139549255: 1, -1.2014168500900269: 1, 0.8521577715873718: 1, -0.009660118259489536: 1, 1.114802360534668: 1, -0.7990305423736572: 1, -1.174880027770996: 1, -1.201361894607544: 1, 1.7429335117340088: 1, 1.92928946018219: 1, 1.7397531270980835: 1, -0.8557604551315308: 1, 1.2302463054656982: 1, -0.8916471600532532: 1, 0.04224063828587532: 1, 1.4848576784133911: 1, 1.8896540403366089: 1, -1.2014458179473877: 1, 0.9216548800468445: 1, 0.8007993698120117: 1, -0.4143541157245636: 1, 0.25442907214164734: 1, 1.636014699935913: 1, 1.5984208583831787: 1, 1.7614209651947021: 1, 0.8861773610115051: 1, 0.32922476530075073: 1, -0.7876200675964355: 1, 0.13984030485153198: 1, 1.9223994016647339: 1, -0.11489463597536087: 1, 1.0331618785858154: 1, 1.5742621421813965: 1, 0.35647323727607727: 1, 1.2849032878875732: 1, 1.5142070055007935: 1, -0.420527845621109: 1, -0.5466570258140564: 1, 1.6787821054458618: 1, 1.0356202125549316: 1, 1.8180814981460571: 1, 1.4667671918869019: 1, 1.455495834350586: 1, -0.21755054593086243: 1, -1.1579643487930298: 1, 0.8198267817497253: 1, 0.12914451956748962: 1, 1.5291143655776978: 1, 1.7499927282333374: 1, -0.1335328370332718: 1, -0.010684509761631489: 1, 1.8225407600402832: 1, 1.887890100479126: 1, 0.09368380159139633: 1, 1.4813575744628906: 1, -0.796614408493042: 1, 1.7114263772964478: 1, -0.8602240085601807: 1, 0.9352988600730896: 1, 1.597861409187317: 1, 0.9059203267097473: 1, 1.703546404838562: 1, 0.9797016382217407: 1, 1.746630072593689: 1, 1.5016316175460815: 1, 1.6772441864013672: 1, 5.270293235778809: 1, 0.7037346363067627: 1, 1.7267848253250122: 1, -0.5458275079727173: 1, -0.7692103385925293: 1, 1.0119178295135498: 1, 0.019096076488494873: 1, 1.231010913848877: 1, -0.4275071322917938: 1, 0.7566526532173157: 1, -1.1957098245620728: 1, -0.4729682505130768: 1, -0.8232591152191162: 1, 1.2115764617919922: 1, -0.558110237121582: 1, 0.9276106357574463: 1, -0.47733497619628906: 1, -0.24040797352790833: 1, -0.6007823944091797: 1, -0.7505127787590027: 1, 0.017721591517329216: 1, 1.3759360313415527: 1, 1.9232004880905151: 1, 0.3244344890117645: 1, 1.1735796928405762: 1, -0.4622204601764679: 1, -0.7031784653663635: 1, -0.37577909231185913: 1, -0.14395083487033844: 1, 0.2143784463405609: 1, 0.5354000329971313: 1, 0.171223446726799: 1, -0.34326422214508057: 1, -0.29665860533714294: 1, -0.9143604040145874: 1, -1.0387167930603027: 1, -0.9877877235412598: 1, 1.2964091300964355: 1, 1.3913298845291138: 1, -0.466978520154953: 1, -0.9003047943115234: 1, -1.2009780406951904: 1, -0.92970210313797: 1, -0.34284013509750366: 1, -0.47152507305145264: 1, -0.23732632398605347: 1, -1.093284010887146: 1, 1.8227369785308838: 1, -0.16766004264354706: 1, 0.7785534858703613: 1, -0.3081098794937134: 1, -0.8751227855682373: 1, 1.456301212310791: 1, -0.46590861678123474: 1, -0.7915438413619995: 1, -0.5345551371574402: 1, 1.2142442464828491: 1, -0.4592617452144623: 1, 1.4956955909729004: 1, -0.33291712403297424: 1, -0.23398016393184662: 1, 1.6555308103561401: 1, 1.7606215476989746: 1, 0.6444322466850281: 1, -0.3169466555118561: 1, -0.9762966632843018: 1, -0.8456059098243713: 1, -0.8443267941474915: 1, -1.0356733798980713: 1, -1.1333893537521362: 1, 0.22667939960956573: 1, 0.43118155002593994: 1, 0.6444940567016602: 1, -0.3439752459526062: 1, -0.2761753499507904: 1, -0.5378462672233582: 1, -0.21501778066158295: 1, -0.0690179392695427: 1, 1.058951735496521: 1, 1.1691573858261108: 1, 1.6570682525634766: 1, 1.4193427562713623: 1, -0.42767956852912903: 1, 1.93668532371521: 1, 1.5919677019119263: 1, -0.46113380789756775: 1, 1.878305196762085: 1, 1.3953920602798462: 1, -0.6786049008369446: 1, -0.07709828019142151: 1, -0.4369107186794281: 1, -0.7822737693786621: 1, -0.28641819953918457: 1, -0.9299617409706116: 1, -1.1991149187088013: 1, 1.8707987070083618: 1, 0.6785101294517517: 1, 0.6026607155799866: 1, -1.185569167137146: 1, 1.6845048666000366: 1, -1.0817426443099976: 1, -1.2014148235321045: 1, -0.31910526752471924: 1, -1.193153738975525: 1, -0.44544142484664917: 1, -0.5857658982276917: 1, -1.1479105949401855: 1, -0.24207068979740143: 1, -0.3858093321323395: 1, 1.052090048789978: 1, 1.4788439273834229: 1, 1.483720302581787: 1, -0.200442373752594: 1, -0.5843047499656677: 1, -0.1967451572418213: 1, 0.9746946096420288: 1, 0.8612715601921082: 1, 0.47734835743904114: 1, 0.16428887844085693: 1, -0.35491982102394104: 1, -0.5582360029220581: 1, -0.5378177762031555: 1, -0.960394024848938: 1, -0.29989245533943176: 1, 0.2551988363265991: 1, -1.1109116077423096: 1, 1.7616217136383057: 1, 0.06790906190872192: 1, -0.2670007050037384: 1, 3.323413610458374: 1, 1.4998488426208496: 1, 1.9010276794433594: 1, -0.007546336855739355: 1, 1.187366247177124: 1, -0.766767144203186: 1, -0.46731990575790405: 1, -0.9987406134605408: 1, -1.1868388652801514: 1, -0.17455770075321198: 1, -1.1897622346878052: 1, -0.46823152899742126: 1, -0.5717604756355286: 1, -0.8472578525543213: 1, 1.5701237916946411: 1, -1.1767117977142334: 1, -1.1579349040985107: 1, 1.3883335590362549: 1, -0.47762712836265564: 1, -0.9396678805351257: 1, 1.0024393796920776: 1, -0.5413272380828857: 1, -0.7269378900527954: 1, 1.6493046283721924: 1, -0.3424704372882843: 1, -0.2937408983707428: 1, 0.15043634176254272: 1, -0.5880576372146606: 1, -0.7417653203010559: 1, -0.3907565474510193: 1, 1.491416096687317: 1, -0.238590806722641: 1, -0.5713714957237244: 1, -1.0160380601882935: 1, -1.0713788270950317: 1, -0.2992294430732727: 1, -0.8521113991737366: 1, -1.179208755493164: 1, 0.17652635276317596: 1, 1.6146701574325562: 1, 0.31867286562919617: 1, 1.7650138139724731: 1, -0.29448574781417847: 1, -0.8708242177963257: 1, 0.025362450629472733: 1, -1.0031712055206299: 1, -0.6234576106071472: 1, -0.014453819021582603: 1, -0.3702247440814972: 1, -0.8769359588623047: 1, -0.0832226425409317: 1, 0.012689988128840923: 1, -0.3753896951675415: 1, 1.5722923278808594: 1, -0.0211151335388422: 1, 0.19413244724273682: 1, -0.5210259556770325: 1, 0.37810415029525757: 1, -0.5338919758796692: 1, 0.6790488958358765: 1, -0.13618627190589905: 1, 0.9851498603820801: 1, -0.5030357241630554: 1, 0.672914445400238: 1, 0.273947536945343: 1, 1.376474142074585: 1, 1.5933094024658203: 1, 1.8017815351486206: 1, -0.4548023045063019: 1, 1.6648616790771484: 1, 0.48826223611831665: 1, -0.5381679534912109: 1, 0.5300026535987854: 1, -0.6655375957489014: 1, 0.3571058511734009: 1, 0.3796524703502655: 1, 0.5128249526023865: 1, -0.8232764601707458: 1, -0.5174428224563599: 1, -0.3349834084510803: 1, 1.6888245344161987: 1, 0.16830426454544067: 1, 0.3824501037597656: 1, 0.1589841991662979: 1, 0.5874748826026917: 1, -1.0319366455078125: 1, -1.1006834506988525: 1, 0.2422785758972168: 1, -0.21730461716651917: 1, -0.03495830297470093: 1, 1.6034691333770752: 1, -0.9720814824104309: 1, -0.18857857584953308: 1, 1.5428107976913452: 1, 0.3213244378566742: 1, -1.0406304597854614: 1, 0.5123543739318848: 1, -0.9834045767784119: 1, -0.1501469612121582: 1, -1.1350090503692627: 1, 1.7555813789367676: 1, -1.1569743156433105: 1, -0.6055639982223511: 1, -0.28438881039619446: 1, -0.4845651388168335: 1, 1.6552691459655762: 1, 1.0491001605987549: 1, 0.2126206010580063: 1, -0.5899453163146973: 1, 1.7796648740768433: 1, -0.39972129464149475: 1, -1.155177116394043: 1, 0.5790113210678101: 1, -0.392406165599823: 1, -0.8350648880004883: 1, -1.103760838508606: 1, -0.8279891610145569: 1, -1.1760764122009277: 1, -0.2970311641693115: 1, -0.7790790796279907: 1, 1.9273109436035156: 1, 0.3162490129470825: 1, 1.552185297012329: 1, -0.13741447031497955: 1, -1.1448076963424683: 1, -0.9971945881843567: 1, 0.2097160518169403: 1, -0.04817575961351395: 1, 1.2531977891921997: 1, -0.46826034784317017: 1, -0.9147067666053772: 1, 0.8109627962112427: 1, -0.25207433104515076: 1, 0.6704513430595398: 1, 0.2092854529619217: 1, -1.2016154527664185: 1, 0.04079057276248932: 1, -0.3426608741283417: 1, 1.7097703218460083: 1, 1.6683679819107056: 1, -0.09393322467803955: 1, 1.905008316040039: 1, -0.14373785257339478: 1, 0.611980676651001: 1, 0.11937177926301956: 1, -0.5307965278625488: 1, -0.27935805916786194: 1, -0.2874172031879425: 1, -0.9310538172721863: 1, 1.4624748229980469: 1, 1.8530430793762207: 1, 0.1769435554742813: 1, 1.2401331663131714: 1, -1.2016234397888184: 1, -0.2558027505874634: 1, -0.4171464443206787: 1, -0.4433523118495941: 1, -1.2016232013702393: 1, -0.22874142229557037: 1, 0.15644948184490204: 1, 0.21555371582508087: 1, 1.8505216836929321: 1, 0.3744315505027771: 1, 1.852098822593689: 1, 0.3915187418460846: 1, 1.5747997760772705: 1, 0.49887171387672424: 1, 0.7721536755561829: 1, -0.01187801081687212: 1, 0.10666077584028244: 1, 1.4984081983566284: 1, 0.956155002117157: 1, -0.9726563692092896: 1, -1.0813374519348145: 1, 1.525660514831543: 1, 1.6697852611541748: 1, 0.7304840087890625: 1, -0.6562255024909973: 1, 0.21643884479999542: 1, -0.3255411982536316: 1, -0.061833277344703674: 1, -0.26246213912963867: 1, -1.1336802244186401: 1, -1.1971925497055054: 1, 0.7185215353965759: 1, -0.539626955986023: 1, 0.18035785853862762: 1, -0.4653182625770569: 1, -1.1008306741714478: 1, -0.8952922821044922: 1, -0.9930511116981506: 1, -0.30786851048469543: 1, -0.27475300431251526: 1, 0.4937743842601776: 1, -0.364163339138031: 1, -0.3189155161380768: 1, -0.7234905958175659: 1, -1.2016146183013916: 1, -0.25122812390327454: 1, -0.9730278849601746: 1, 1.5011951923370361: 1, 1.571593999862671: 1, -1.1689732074737549: 1, -0.44949105381965637: 1, -0.26545509696006775: 1, -1.1831696033477783: 1, -0.6922621726989746: 1, -0.8816695213317871: 1, -1.201271653175354: 1, -0.3123464286327362: 1, 1.896323323249817: 1, -0.563433825969696: 1, -0.608808159828186: 1, 1.8601784706115723: 1, -0.3237372636795044: 1, 0.13262629508972168: 1, 1.629822015762329: 1, -0.8671026229858398: 1, 1.8596769571304321: 1, 1.2151012420654297: 1, -0.43086937069892883: 1, 1.7544053792953491: 1, -0.5209143161773682: 1, -1.1991721391677856: 1, -0.4168057143688202: 1, -0.2183121144771576: 1, -0.906280517578125: 1, -0.7995052933692932: 1, -0.9981749057769775: 1, -1.1617506742477417: 1, -1.061142921447754: 1, 1.5116616487503052: 1, -0.3431845009326935: 1, -0.7939702272415161: 1, -0.5884281992912292: 1, -1.1422733068466187: 1, 0.9462845921516418: 1, -1.2015608549118042: 1, -1.1969398260116577: 1, 0.8056516647338867: 1, -1.201555848121643: 1, -0.9751223921775818: 1, -1.199577808380127: 1, -0.7289202809333801: 1, -1.1694631576538086: 1, -1.1207038164138794: 1, 3.341240167617798: 1, -1.1994960308074951: 1, -1.2015869617462158: 1, 0.7078472375869751: 1, -0.5361114740371704: 1, -1.1236215829849243: 1, -1.2013746500015259: 1, -0.8602992296218872: 1, -1.201583981513977: 1, -0.3992172181606293: 1, -1.1680830717086792: 1, -0.5809049010276794: 1, -1.1829675436019897: 1, -1.1475187540054321: 1, -0.3383774757385254: 1, -1.0460647344589233: 1, 0.4647309482097626: 1, -0.6015652418136597: 1, 0.012895430438220501: 1, -0.36108481884002686: 1, -0.8523722290992737: 1, -1.2016171216964722: 1, -1.1925454139709473: 1, -1.2002716064453125: 1, 0.5143935680389404: 1, -1.201569676399231: 1, -1.1498054265975952: 1, -1.110012173652649: 1, -0.6403390765190125: 1, 4.282702445983887: 1, -1.1945738792419434: 1, -1.1494790315628052: 1, -1.201560139656067: 1, -1.2015953063964844: 1, -1.1953339576721191: 1, -1.2015472650527954: 1, -1.1108695268630981: 1, -1.1999285221099854: 1, 0.2824403941631317: 1, 0.8999701142311096: 1, -0.6715388298034668: 1, 1.2057219743728638: 1, 0.5111210942268372: 1, -0.7557051777839661: 1, 0.7875488996505737: 1, 0.8666924238204956: 1, 0.8550933599472046: 1, -1.2011059522628784: 1, -0.05180063098669052: 1, -0.8982016444206238: 1, -0.35893186926841736: 1, -0.6006320714950562: 1, -0.7642948627471924: 1, -0.12521179020404816: 1, -0.5657321810722351: 1, -1.051085352897644: 1, -0.39080610871315: 1, -0.03033183142542839: 1, -0.10903797298669815: 1, -0.07614605128765106: 1, -1.0079307556152344: 1, -1.1992580890655518: 1, 1.3031054735183716: 1, -1.2016023397445679: 1, 0.7242860198020935: 1, -0.11124473065137863: 1, -1.2008600234985352: 1, 0.9907589554786682: 1, -1.2016347646713257: 1, 1.143233299255371: 1, -1.187011957168579: 1, -0.671938955783844: 1, 0.8696494698524475: 1, -0.7864221334457397: 1, -0.136034294962883: 1, -0.7683218121528625: 1, 0.8018452525138855: 1, 0.1461368203163147: 1, 1.2778337001800537: 1, -1.193078875541687: 1, -1.186597228050232: 1, -1.1420694589614868: 1, -1.1992239952087402: 1, -1.169080138206482: 1, -0.3552163541316986: 1, -0.9202075004577637: 1, 0.264765202999115: 1, -0.8984062075614929: 1, -0.4707425832748413: 1, -1.0164505243301392: 1, -1.150406837463379: 1, -0.6981508731842041: 1, -0.9249427914619446: 1, -0.17132940888404846: 1, -0.0996609777212143: 1, -0.14265145361423492: 1, -1.1139642000198364: 1, 0.944926917552948: 1, 0.5761882066726685: 1, -1.1809542179107666: 1, 0.013616573065519333: 1, 0.00948107335716486: 1, -1.1908975839614868: 1, 0.4138732850551605: 1, -0.8764730095863342: 1, -0.24605818092823029: 1, -0.5374748706817627: 1, -0.171332448720932: 1, -0.8429122567176819: 1, 0.040903303772211075: 1, 1.0700626373291016: 1, 1.257509708404541: 1, -0.7673166990280151: 1, -0.01552529539912939: 1, -1.1882494688034058: 1, -0.1598413586616516: 1, -0.04764068126678467: 1, -1.1756606101989746: 1, 0.032617583870887756: 1, -1.1632294654846191: 1, -1.1747305393218994: 1, -1.1993433237075806: 1, -0.20172715187072754: 1, -0.7597726583480835: 1, -1.0657732486724854: 1, -0.3311309218406677: 1, -0.4824763238430023: 1, -0.4976504147052765: 1, -1.1774768829345703: 1, -1.2016228437423706: 1, 0.3949566185474396: 1, -1.1074978113174438: 1, 1.2888545989990234: 1, -1.1091188192367554: 1, 0.8944936394691467: 1, -1.1906999349594116: 1, -0.012192374095320702: 1, -0.40984249114990234: 1, 0.3299405574798584: 1, 1.25115966796875: 1, 0.7848809361457825: 1, -0.21156159043312073: 1, 0.004596139770001173: 1, -1.0416687726974487: 1, -0.042717207223176956: 1, -0.5342384576797485: 1, -0.39157962799072266: 1, -0.09844925999641418: 1, -0.5935716032981873: 1, 1.0410280227661133: 1, -1.1992988586425781: 1, -1.1941806077957153: 1, -0.3055903911590576: 1, 1.2313082218170166: 1, -0.3122004270553589: 1, 0.8209275603294373: 1, -0.41031748056411743: 1, -0.3642917275428772: 1, -0.005674127489328384: 1, 0.7907249331474304: 1, -0.13443662226200104: 1, 0.03998654708266258: 1, -1.1028809547424316: 1, -1.144519329071045: 1, -1.0224525928497314: 1, -1.1866058111190796: 1, 0.9692907333374023: 1, -0.8211961388587952: 1, 0.842113196849823: 1, 0.5858985185623169: 1, 0.48456287384033203: 1, -0.30494359135627747: 1, -1.2016358375549316: 1, -0.5739816427230835: 1, 0.7670885920524597: 1, -0.4656817615032196: 1, -1.0180860757827759: 1, -0.576421856880188: 1, -1.195853590965271: 1, -0.5976389050483704: 1, 1.2671806812286377: 1, 1.0670119524002075: 1, -0.7073397040367126: 1, 0.6907184720039368: 1, 0.6389100551605225: 1, -1.0666824579238892: 1, 1.282943606376648: 1, -0.2299073189496994: 1, 0.8628989458084106: 1, -0.24963954091072083: 1, 1.1413462162017822: 1, -0.08644621819257736: 1, -0.7184330821037292: 1, -1.201612949371338: 1, -1.0178923606872559: 1, -1.083876609802246: 1, -0.5272284746170044: 1, -1.1375747919082642: 1, -0.19534148275852203: 1, -1.201613426208496: 1, -0.3075839579105377: 1, -1.0825824737548828: 1, -0.29541367292404175: 1, -1.1735186576843262: 1, 0.3790889084339142: 1, 0.7587617635726929: 1, 0.6585915088653564: 1, -0.16215069591999054: 1, -1.2009185552597046: 1, 0.043348729610443115: 1, -0.4258005619049072: 1, 0.19746625423431396: 1, 0.7787987589836121: 1, -0.7811260223388672: 1, -1.192929744720459: 1, -0.5910298824310303: 1, 0.050834186375141144: 1, -0.5807573795318604: 1, 0.9452794194221497: 1, 1.1708778142929077: 1, 0.4334481656551361: 1, -0.13237591087818146: 1, 0.6366953253746033: 1, 1.177069067955017: 1, -1.20154869556427: 1, 1.1094199419021606: 1, 1.0952398777008057: 1, 1.258391261100769: 1, -0.07435453683137894: 1, 1.1807068586349487: 1, -0.7601802349090576: 1, -1.1549781560897827: 1, -0.5920382142066956: 1, 0.669349730014801: 1, 0.5067287683486938: 1, 1.0160198211669922: 1, -0.8395327925682068: 1, -0.5897277593612671: 1, -0.15097910165786743: 1, -0.9743238091468811: 1, 0.605282723903656: 1, -0.13060888648033142: 1, -0.2519146203994751: 1, 0.6152291297912598: 1, -0.3348834216594696: 1, 1.182131290435791: 1, -0.06836508214473724: 1, -0.15388239920139313: 1, 0.05971863865852356: 1, -0.12709279358386993: 1, 1.1462621688842773: 1, -0.7992537021636963: 1, -0.5774872899055481: 1, -0.10418325662612915: 1, 0.2748369872570038: 1, 1.1562855243682861: 1, -0.21517714858055115: 1, 0.9726781249046326: 1, -0.12876513600349426: 1, -1.1697360277175903: 1, -0.2588912844657898: 1, 1.3108209371566772: 1, -0.5649014711380005: 1, -0.1045512706041336: 1, -0.024461327120661736: 1, -0.06410756707191467: 1, -0.048064157366752625: 1, -0.24131079018115997: 1, -0.7010860443115234: 1, -0.01721060648560524: 1, -0.2265346348285675: 1, -0.9494366645812988: 1, -0.18796367943286896: 1, 1.043671727180481: 1, -0.3034926950931549: 1, -0.06601618975400925: 1, -0.21185417473316193: 1, 0.7746310830116272: 1, -0.3917173445224762: 1, -0.02432066947221756: 1, 0.2524677515029907: 1, 0.2549906373023987: 1, 0.7206960916519165: 1, 0.6609118580818176: 1, -0.6903147101402283: 1, 1.00320303440094: 1, -0.6155209541320801: 1, -1.193916916847229: 1, -0.15946216881275177: 1, -0.01294313371181488: 1, -0.21699510514736176: 1, 1.0150052309036255: 1, 1.274375557899475: 1, 1.196900725364685: 1, -0.4809620976448059: 1, 0.2788977324962616: 1, -0.17342792451381683: 1, 1.2882169485092163: 1, -1.1826090812683105: 1, 0.7786849141120911: 1, -1.200330376625061: 1, 1.0692790746688843: 1, -0.257107138633728: 1, 0.606712281703949: 1, -0.5744653940200806: 1, -1.182140827178955: 1, 0.5745441317558289: 1, 0.666642963886261: 1, 0.2699461281299591: 1, -0.024909501895308495: 1, 0.09289468824863434: 1, 1.2464758157730103: 1, 0.8297379016876221: 1, -0.27802959084510803: 1, 1.0399528741836548: 1, -0.09314227104187012: 1, -0.1227283924818039: 1, 0.07662157714366913: 1, 0.8076177835464478: 1, -0.0404001921415329: 1, -0.028692159801721573: 1, -0.11206144839525223: 1, -0.11562295258045197: 1, 1.1690500974655151: 1, -0.7266088128089905: 1, 0.022439440712332726: 1, -0.618209183216095: 1, 1.0350605249404907: 1, -0.3780987560749054: 1, 1.1997345685958862: 1, -0.36745402216911316: 1, -0.26567548513412476: 1, 0.5923774242401123: 1, -0.19212104380130768: 1, 0.9495259523391724: 1, 0.41225412487983704: 1, 0.6296795010566711: 1, -1.1068792343139648: 1, -0.6869497895240784: 1, 0.6174435615539551: 1, 1.1092147827148438: 1, 0.7346874475479126: 1, -0.3076569437980652: 1, -0.9430715441703796: 1, -0.7650251388549805: 1, -1.1914066076278687: 1, -1.0579359531402588: 1, -0.16474246978759766: 1, 1.1363762617111206: 1, -1.2016372680664062: 1, -1.1513574123382568: 1, 1.3037681579589844: 1, -0.03992554917931557: 1, 1.0595874786376953: 1, -0.07969757169485092: 1, -1.0076838731765747: 1, -0.2553582787513733: 1, -0.2878003716468811: 1, -1.155191421508789: 1, -1.1991511583328247: 1, -0.16514527797698975: 1, 1.023429274559021: 1, -0.7420557737350464: 1, -0.3634156584739685: 1, 0.0014178809942677617: 1, -0.27813780307769775: 1, -1.1934514045715332: 1, 1.0866621732711792: 1, 1.1114397048950195: 1, 0.68455970287323: 1, 0.03431294485926628: 1, -0.5050346255302429: 1, -1.1873303651809692: 1, -1.200080394744873: 1, 0.17733901739120483: 1, -0.35624369978904724: 1, -0.03235474228858948: 1, -0.3301823139190674: 1, -1.1322532892227173: 1, -0.8901136517524719: 1, 0.49005118012428284: 1, -0.7893494963645935: 1, -1.2016328573226929: 1, 0.5382747054100037: 1, -0.25587567687034607: 1, 1.0084635019302368: 1, 0.810942530632019: 1, -0.0674244612455368: 1, -1.006115436553955: 1, -1.0054869651794434: 1, -0.002401667181402445: 1, -0.016240552067756653: 1, -0.5908447504043579: 1, -0.29025569558143616: 1, -0.15591250360012054: 1, -0.07907160371541977: 1, -0.0026253368705511093: 1, 0.12906195223331451: 1, -0.08915898948907852: 1, 0.021945519372820854: 1, 0.004652172327041626: 1, -0.32367783784866333: 1, -0.5897085666656494: 1, 0.9278587102890015: 1, -0.19999556243419647: 1, 0.26051849126815796: 1, 0.053155358880758286: 1, 1.0157313346862793: 1, -0.056251220405101776: 1, 1.1558300256729126: 1, -1.1260875463485718: 1, -0.9567206501960754: 1, -1.0963338613510132: 1, -0.07029284536838531: 1, -0.019765598699450493: 1, -1.109034538269043: 1, -0.11618001013994217: 1, 0.9881972074508667: 1, -1.1654343605041504: 1, -0.26975223422050476: 1, 1.2993144989013672: 1, 0.05442709103226662: 1, -1.1338447332382202: 1, -0.6266202926635742: 1, -0.5088394284248352: 1, -0.3548189401626587: 1, 1.2035483121871948: 1, -1.2008949518203735: 1, -0.455705851316452: 1, 0.033837273716926575: 1, -0.5931430459022522: 1, 0.9011586904525757: 1, 0.762050449848175: 1, -0.22990889847278595: 1, 1.2829649448394775: 1, -0.23146884143352509: 1, 0.03461417928338051: 1, 0.7100340127944946: 1, 0.5583640336990356: 1, -1.1689379215240479: 1, -0.35862404108047485: 1, 0.3074534237384796: 1, -1.200749397277832: 1, -1.0309724807739258: 1, -1.1989647150039673: 1, -0.8834558129310608: 1, -0.6588079929351807: 1, -1.1501390933990479: 1, -0.6635865569114685: 1, -0.5772318840026855: 1, -1.194190263748169: 1, -0.19914157688617706: 1, -0.2236318439245224: 1, -1.1722731590270996: 1, -0.6205415725708008: 1, -0.4475323557853699: 1, -1.0135008096694946: 1, -1.1582452058792114: 1, -1.0668615102767944: 1, -0.63347989320755: 1, -1.0160771608352661: 1, -0.5631559491157532: 1, -1.1520825624465942: 1, -1.167817234992981: 1, -0.6395750641822815: 1, -1.0467379093170166: 1, -1.010536789894104: 1, -1.1729844808578491: 1, -1.194840431213379: 1, -1.0399388074874878: 1, -0.8366922736167908: 1, -0.7228063344955444: 1, -0.9656466245651245: 1, -1.0281414985656738: 1, -1.1463063955307007: 1, -0.694696843624115: 1, -1.05448579788208: 1, -0.6411266922950745: 1, -0.4774998426437378: 1, -1.2015254497528076: 1, -1.1344302892684937: 1, -1.026742935180664: 1, -0.4338090121746063: 1, -1.1509727239608765: 1, -0.5115338563919067: 1, -0.9300684928894043: 1, -1.157931923866272: 1, -0.7443411946296692: 1, -0.6325321793556213: 1, -0.8867827653884888: 1, -1.1800310611724854: 1, -1.194927453994751: 1, -1.1923733949661255: 1, -1.1816785335540771: 1, -0.17838822305202484: 1, -1.198327660560608: 1, -1.091170310974121: 1, -1.175873041152954: 1, -1.1691778898239136: 1, -1.0323843955993652: 1, -1.1855055093765259: 1, -1.1832531690597534: 1, -1.2012261152267456: 1, -0.9966712594032288: 1, -1.2013036012649536: 1, -0.6249680519104004: 1, -1.1552096605300903: 1, -1.170555830001831: 1, -0.957834780216217: 1, -1.0740134716033936: 1, -1.1981604099273682: 1, -1.1905823945999146: 1, -1.1300313472747803: 1, -1.2016135454177856: 1, -1.1961623430252075: 1, -1.1568188667297363: 1, -1.1333073377609253: 1, -1.1721937656402588: 1, -1.1859160661697388: 1, -1.0433918237686157: 1, -1.162119746208191: 1, -1.2006478309631348: 1, -1.1339654922485352: 1, -1.1858601570129395: 1, -1.1954984664916992: 1, -0.6137503981590271: 1, -0.788809597492218: 1, -1.0831377506256104: 1, -1.1470420360565186: 1, -1.161582589149475: 1, -0.7897263169288635: 1, -1.1350377798080444: 1, -0.862193763256073: 1, -1.180148720741272: 1, -1.201621413230896: 1, -1.2008585929870605: 1, -0.5804309248924255: 1, -0.9933805465698242: 1, -1.2016000747680664: 1, -0.832706868648529: 1, -0.7736660242080688: 1, -1.0223268270492554: 1, -0.5520062446594238: 1, -0.6635236740112305: 1, -1.1754673719406128: 1, 1.7253838777542114: 1, -0.4444347023963928: 1, -0.7420345544815063: 1, -0.21827441453933716: 1, -0.7001152634620667: 1, -1.0461647510528564: 1, -0.9021695256233215: 1, -0.8267379403114319: 1, -1.0589720010757446: 1, -1.1036909818649292: 1, -0.25539615750312805: 1, -1.1933567523956299: 1, -0.86516273021698: 1, -1.0501618385314941: 1, -1.1981457471847534: 1, -1.200010895729065: 1, -0.9485399723052979: 1, -1.087907314300537: 1, -0.3156169652938843: 1, -1.2006280422210693: 1, 5.012721538543701: 1, -0.4824954569339752: 1, -1.1977088451385498: 1, 5.006033897399902: 1, 0.057195477187633514: 1, 4.863577365875244: 1, -0.9046985507011414: 1, -0.622269868850708: 1, 4.900933742523193: 1, 4.722275257110596: 1, 0.939198911190033: 1, -1.1445417404174805: 1, -0.7939543128013611: 1, 4.302996635437012: 1, 4.58308219909668: 1, -1.1364892721176147: 1, -0.07361169159412384: 1, -1.1480247974395752: 1, 4.875144004821777: 1, 5.066896438598633: 1, 5.072229385375977: 1, 0.4937030076980591: 1, 3.830434560775757: 1, -1.1785725355148315: 1, -1.1136521100997925: 1, -1.1891672611236572: 1, -1.1703253984451294: 1, -0.5385211110115051: 1, 0.216363787651062: 1, -0.9539603590965271: 1, -0.5497206449508667: 1, -1.1790684461593628: 1, -1.167614459991455: 1, -0.6818874478340149: 1, -1.0620733499526978: 1, -1.1940333843231201: 1, -0.8758900761604309: 1, 0.18425099551677704: 1, -0.75212162733078: 1, 0.29684698581695557: 1, -1.145255446434021: 1, -1.1212965250015259: 1, -1.2005667686462402: 1, -0.9272821545600891: 1, -1.02364981174469: 1, -1.0699774026870728: 1, -1.0538722276687622: 1, -0.9141108393669128: 1, -0.712040364742279: 1, -0.8772833347320557: 1, -0.38882899284362793: 1, -0.669135332107544: 1, -1.144382119178772: 1, -0.6867276430130005: 1, -0.9726890325546265: 1, -1.1986464262008667: 1, -1.0186684131622314: 1, -1.1181161403656006: 1, -0.4794164001941681: 1, -1.1468044519424438: 1, -0.967963457107544: 1, -1.0246316194534302: 1, -0.1397843062877655: 1, -0.47614291310310364: 1, -0.8666650652885437: 1, -0.8599510192871094: 1, -0.9453350901603699: 1, -0.8791208267211914: 1, -1.0074002742767334: 1, -1.1503796577453613: 1, -1.189503788948059: 1, -0.5638068318367004: 1, -0.4648326635360718: 1, -1.0434380769729614: 1, -1.00275719165802: 1, -1.1871360540390015: 1, -1.0014375448226929: 1, 0.21256738901138306: 1, 1.5002496242523193: 1, 0.7936035394668579: 1, -0.9092623591423035: 1, 1.593440055847168: 1, 0.6101611256599426: 1, 1.6096405982971191: 1, 0.9798484444618225: 1, -0.6111128330230713: 1, -0.433001846075058: 1, 0.4647965729236603: 1, 0.9053057432174683: 1, -1.1626108884811401: 1, -1.0986745357513428: 1, -0.8635501265525818: 1, 0.2436400204896927: 1, 1.036679744720459: 1, 0.2817704975605011: 1, 1.2748090028762817: 1, -0.750208854675293: 1, 1.7609484195709229: 1, -1.201631784439087: 1, -0.7198786735534668: 1, -0.1808653175830841: 1, -0.1682627946138382: 1, 1.094826102256775: 1, 1.1564749479293823: 1, -0.15451399981975555: 1, -0.39169713854789734: 1, 0.8645955324172974: 1, -1.201418161392212: 1, 1.6113003492355347: 1, 0.29118791222572327: 1, -0.2950673997402191: 1, -0.7970835566520691: 1, 0.5490202307701111: 1, 0.8136993050575256: 1, -0.16204535961151123: 1, 0.8697875142097473: 1, 0.7108749151229858: 1, 0.5599294900894165: 1, -0.6900844573974609: 1, -0.726171612739563: 1, 0.7067909836769104: 1, -1.1882411241531372: 1, 1.5164122581481934: 1, -1.2010202407836914: 1, 0.8852934837341309: 1, -0.23251420259475708: 1, -0.17489729821681976: 1, 0.05797763168811798: 1, 1.0962333679199219: 1, -0.8244988322257996: 1, -0.4005826711654663: 1, -0.6496835947036743: 1, -0.7252834439277649: 1, -0.7019102573394775: 1, -1.1959316730499268: 1, -0.5334532856941223: 1, 1.0577486753463745: 1, -0.5885310769081116: 1, 1.2156920433044434: 1, -0.8736492395401001: 1, 0.9127581715583801: 1, 0.4960012137889862: 1, 1.0147548913955688: 1, -0.35478705167770386: 1, -0.44190311431884766: 1, 1.2256038188934326: 1, 0.9517895579338074: 1, 1.229008674621582: 1, 0.5329916477203369: 1, -0.3316475749015808: 1, 0.880368709564209: 1, -0.5816406607627869: 1, -0.8111313581466675: 1, 0.6412140727043152: 1, 0.6591589450836182: 1, 1.3093386888504028: 1, 1.5706232786178589: 1, -0.3561877906322479: 1, 0.5863446593284607: 1, -0.34684932231903076: 1, 1.585684895515442: 1, 1.1463862657546997: 1, -1.2015637159347534: 1, -1.154268741607666: 1, -1.199397087097168: 1, -1.171905517578125: 1, 0.009732205420732498: 1, -1.1621276140213013: 1, 1.123262643814087: 1, -0.45371508598327637: 1, 0.6509128212928772: 1, 0.8030492067337036: 1, -1.2016229629516602: 1, -0.7776852250099182: 1, -1.1028145551681519: 1, -0.4003660976886749: 1, 0.6052579283714294: 1, 0.6880602836608887: 1, -0.9458178281784058: 1, -0.5026354193687439: 1, -1.1999870538711548: 1, 5.645717620849609: 1, -1.1910632848739624: 1, -0.7359880805015564: 1, -1.195172905921936: 1, -0.459964781999588: 1, -1.1054575443267822: 1, 0.741217315196991: 1, 1.8886953592300415: 1, 0.19575154781341553: 1, -1.1993547677993774: 1, -1.2002415657043457: 1, -1.164048194885254: 1, -1.1929975748062134: 1, 0.3444092869758606: 1, -1.2014538049697876: 1, 0.7184975147247314: 1, -1.157366394996643: 1, -1.194710373878479: 1, 0.6086026430130005: 1, -1.175083041191101: 1, -1.196357011795044: 1, -0.9010990262031555: 1, -0.23024950921535492: 1, -1.1594713926315308: 1, -0.7456731200218201: 1, -1.1595861911773682: 1, -1.1821529865264893: 1, -1.0944702625274658: 1, -1.1784441471099854: 1, -1.1624016761779785: 1, -0.7622874975204468: 1, -1.1929867267608643: 1, -1.18364417552948: 1, -1.1482324600219727: 1, -0.5079992413520813: 1, -1.1857080459594727: 1, -1.0951330661773682: 1, -1.1485586166381836: 1, -1.1643073558807373: 1, -0.40432149171829224: 1, -1.188031554222107: 1, -1.1676386594772339: 1, -0.6259949207305908: 1, -1.186979055404663: 1, -1.0963668823242188: 1, 0.275499552488327: 1, -1.1539435386657715: 1, -0.7167530059814453: 1, -1.0542218685150146: 1, -1.1728081703186035: 1, -0.6460915207862854: 1, -1.1948115825653076: 1, -1.1685289144515991: 1, -1.1994209289550781: 1, -1.173497200012207: 1, -0.6645460724830627: 1, -1.2015430927276611: 1, 0.23683039844036102: 1, 0.05473056063055992: 1, -1.0081939697265625: 1, -0.04312499612569809: 1, -1.1948130130767822: 1, -1.201563835144043: 1, -1.1990872621536255: 1, -1.188680648803711: 1, -0.17054051160812378: 1, -1.1964715719223022: 1, 0.3594037592411041: 1, -0.4477367699146271: 1, -0.31994664669036865: 1, 0.4647902548313141: 1, -0.9958106875419617: 1, -1.1912822723388672: 1, -1.0349972248077393: 1, -1.1865227222442627: 1, -1.1973918676376343: 1, -0.4641222357749939: 1, -1.1811505556106567: 1, 1.6846264600753784: 1, -0.7029581665992737: 1, -0.6273359060287476: 1, -1.0972120761871338: 1, -1.1986582279205322: 1, -1.2010711431503296: 1, -1.1966667175292969: 1, -1.196737289428711: 1, -1.1819733381271362: 1, -1.1612766981124878: 1, -1.1862393617630005: 1, -1.1950762271881104: 1, -0.9986592531204224: 1, -1.1970906257629395: 1, 1.7719837427139282: 1, -1.1995155811309814: 1, -1.2008846998214722: 1, 0.05186900869011879: 1, -0.4923703670501709: 1, 1.2668349742889404: 1, 0.7984791398048401: 1, -0.632728099822998: 1, 0.3140702247619629: 1, -0.11514480412006378: 1, 1.6200270652770996: 1, 0.7718502283096313: 1, 0.20913533866405487: 1, -0.40100592374801636: 1, -0.20442236959934235: 1, -0.10369612276554108: 1, 1.4727312326431274: 1, 1.481839656829834: 1, 0.340713769197464: 1, 0.5713070034980774: 1, 1.5503476858139038: 1, 0.24058645963668823: 1, 0.3425867557525635: 1, 0.35902467370033264: 1, 0.1928955614566803: 1, 0.23876602947711945: 1, 0.23767612874507904: 1, 1.2999117374420166: 1, -0.44012218713760376: 1, 0.18826737999916077: 1, 0.06706182658672333: 1, 1.431997537612915: 1, 1.5148382186889648: 1, 0.2539007067680359: 1, 0.002965901279821992: 1, 1.4364150762557983: 1, 1.4786081314086914: 1, 1.3110779523849487: 1, -0.16230207681655884: 1, 1.4751214981079102: 1, 0.3319496512413025: 1, 0.18870316445827484: 1, 0.858607292175293: 1, 1.282376766204834: 1, 0.7782679200172424: 1, 1.0659617185592651: 1, 0.987504780292511: 1, 1.1887938976287842: 1, 0.9434877038002014: 1, -0.1031346470117569: 1, -0.22873257100582123: 1, 1.379611611366272: 1, -0.0526600144803524: 1, -0.21601253747940063: 1, 1.4899855852127075: 1, -0.1709771454334259: 1, -0.812441349029541: 1, 0.3024345636367798: 1, 0.35236239433288574: 1, 1.4752575159072876: 1, -0.10448039323091507: 1, -0.19414900243282318: 1, -1.1556631326675415: 1, -0.2946179509162903: 1, 0.07517638802528381: 1, -0.48821160197257996: 1, 1.5357881784439087: 1, 0.31122344732284546: 1, -0.05793027952313423: 1, 1.0482161045074463: 1, 0.5573470592498779: 1, 0.81052166223526: 1, 1.233654499053955: 1, 1.215183138847351: 1, 1.3900312185287476: 1, 0.3554361164569855: 1, 1.3589857816696167: 1, 0.9935461282730103: 1, 0.3602719008922577: 1, 0.276736855506897: 1, 0.2222539335489273: 1, 0.3157300651073456: 1, 0.18295887112617493: 1, 0.6854439377784729: 1, -0.6113037467002869: 1, -0.9236016869544983: 1, 1.0590577125549316: 1, 1.4972656965255737: 1, 1.0554699897766113: 1, -0.31073465943336487: 1, 0.21012525260448456: 1, 1.3231751918792725: 1, 1.556430697441101: 1, 0.6647680401802063: 1, -0.9130086302757263: 1, -0.10372047126293182: 1, 1.1509000062942505: 1, 1.1246896982192993: 1, 0.7825908064842224: 1, 1.260263442993164: 1, 1.375723958015442: 1, 1.4145740270614624: 1, 0.7438257932662964: 1, -0.2681446373462677: 1, -0.6454114317893982: 1, 0.8502970337867737: 1, 0.35881760716438293: 1, 0.05546194687485695: 1, -1.0867854356765747: 1, 0.9222752451896667: 1, 1.0225938558578491: 1, 0.8458276391029358: 1, 0.29691436886787415: 1, 1.0407052040100098: 1, -1.1736985445022583: 1, 0.8794386982917786: 1, 0.26258811354637146: 1, -0.20083148777484894: 1, 1.244690179824829: 1, -0.22985504567623138: 1, 1.5059622526168823: 1, 1.059990644454956: 1, -0.4757806956768036: 1, 0.3591007590293884: 1, -0.0920228511095047: 1, -0.06609977036714554: 1, 1.5724915266036987: 1, 0.7346283793449402: 1, 0.11165464669466019: 1, 1.189407229423523: 1, 0.021630888804793358: 1, 1.5909359455108643: 1, 0.4845307767391205: 1, -0.416104793548584: 1, -0.11705746501684189: 1, -1.1903178691864014: 1, -1.171350359916687: 1, -1.150158166885376: 1, -1.1623584032058716: 1, -1.1100589036941528: 1, 0.3900337219238281: 1, -0.6426911950111389: 1, -0.9999420642852783: 1, -0.8209242224693298: 1, -0.06736226379871368: 1, 0.34270647168159485: 1, -0.5888482928276062: 1, -0.5093275308609009: 1, -1.2015849351882935: 1, -0.0922759622335434: 1, -0.8935246467590332: 1, 0.9252344369888306: 1, 0.10832516103982925: 1, -1.140577793121338: 1, 1.111115574836731: 1, 0.017385760322213173: 1, -1.0057076215744019: 1, 0.21266861259937286: 1, 1.466456651687622: 1, 0.5634517669677734: 1, 0.9224774241447449: 1, 1.256608247756958: 1, 1.533963680267334: 1, 1.3359390497207642: 1, 1.4527193307876587: 1, -0.03743843734264374: 1, 1.5988372564315796: 1, 1.3192821741104126: 1, 1.533202886581421: 1, 0.31844547390937805: 1, 0.19674475491046906: 1, -0.3147757053375244: 1, -0.11516252905130386: 1, 0.7623692750930786: 1, -1.1871042251586914: 1, 1.5112932920455933: 1, -0.1960129290819168: 1, -1.1865193843841553: 1, 0.9693878293037415: 1, -0.03138621151447296: 1, -0.5199218392372131: 1, -0.014552570879459381: 1, 0.01178812701255083: 1, 1.3268651962280273: 1, 0.853833794593811: 1, -0.3219013214111328: 1, 0.8655160665512085: 1, 0.3231067657470703: 1, 1.7802213430404663: 1, 0.2615222632884979: 1, -0.06798223406076431: 1, 1.3918044567108154: 1, 1.3780083656311035: 1, 1.1870133876800537: 1, 0.7675086855888367: 1, 0.004675476811826229: 1, 0.7520656585693359: 1, 0.11682117730379105: 1, 1.5651975870132446: 1, 0.7781831622123718: 1, 1.4282907247543335: 1, -0.11230547726154327: 1, 0.19260334968566895: 1, 0.36868804693222046: 1, -0.11455459892749786: 1, 0.2596873939037323: 1, 0.43160757422447205: 1, 0.8805737495422363: 1, 0.09744428098201752: 1, -0.8828660249710083: 1, 1.5512200593948364: 1, -0.16245944797992706: 1, 0.29415011405944824: 1, 1.6108869314193726: 1, 0.05755604803562164: 1, 0.8618729114532471: 1, 0.9195832014083862: 1, -0.7541334629058838: 1, 0.9053120613098145: 1, 0.822748601436615: 1, -0.16789886355400085: 1, -0.17545948922634125: 1, 0.263934850692749: 1, 0.6183062195777893: 1, 1.3844947814941406: 1, 0.7684015035629272: 1, 0.22540217638015747: 1, -0.9639919996261597: 1, -0.025239035487174988: 1, 0.041503969579935074: 1, 0.22126778960227966: 1, 1.1072840690612793: 1, 0.2389289289712906: 1, 1.5033998489379883: 1, 0.19180983304977417: 1, -0.43688738346099854: 1, 0.9983642101287842: 1, 0.6865427494049072: 1, 1.5009726285934448: 1, 0.8533394932746887: 1, 1.133412480354309: 1, 1.3578754663467407: 1, 1.547389030456543: 1, 1.471611738204956: 1, 1.4821670055389404: 1, -0.07051629573106766: 1, 0.11520501971244812: 1, 1.3494865894317627: 1, -0.6971253156661987: 1, 0.24872112274169922: 1, 1.0432312488555908: 1, 1.4069277048110962: 1, -1.1632437705993652: 1, -1.1837621927261353: 1, 1.5597952604293823: 1, 1.4127790927886963: 1, 0.8287729024887085: 1, -0.11932859569787979: 1, 0.7922017574310303: 1, 0.6448225975036621: 1, 1.4735376834869385: 1, -0.870884895324707: 1, 1.5217971801757812: 1, 0.48444247245788574: 1, -0.19083698093891144: 1, -0.091583751142025: 1, -1.193596363067627: 1, 0.3276282548904419: 1, 0.3697844445705414: 1, -1.086830973625183: 1, -0.6757361888885498: 1, 0.6463801264762878: 1, 1.4938876628875732: 1, 0.31239357590675354: 1, 0.7275790572166443: 1, 0.3124358654022217: 1, 1.556864857673645: 1, -0.04237562045454979: 1, 0.9760450720787048: 1, -0.13872799277305603: 1, 1.5027039051055908: 1, 0.7280606031417847: 1, -1.1504056453704834: 1, 0.8865175843238831: 1, 1.5179330110549927: 1, -0.20943275094032288: 1, 0.35639217495918274: 1, 1.405086636543274: 1, 0.7253832817077637: 1, 1.5550216436386108: 1, 2.0702569484710693: 1, -0.2038322389125824: 1, -1.1040070056915283: 1, -0.004799619782716036: 1, 1.1847658157348633: 1, -0.56696617603302: 1, 0.7161386013031006: 1, 0.7092652916908264: 1, 0.0384686179459095: 1, 0.27865079045295715: 1, 0.7332795858383179: 1, -0.5194405913352966: 1, 0.009009350091218948: 1, 1.4103199243545532: 1, 1.3759030103683472: 1, 1.3772739171981812: 1, 1.5916389226913452: 1, 0.3126457929611206: 1, 1.3862555027008057: 1, -0.41700541973114014: 1, 1.5978120565414429: 1, 0.881543755531311: 1, 1.2155990600585938: 1, -1.1900941133499146: 1, 1.5401815176010132: 1, 0.8500742316246033: 1, 1.4411144256591797: 1, 0.02118554152548313: 1, 1.1854524612426758: 1, 0.8342987895011902: 1, 0.968934953212738: 1, 1.007346510887146: 1, -0.6390724778175354: 1, 1.0041277408599854: 1, 0.23464715480804443: 1, -1.1742432117462158: 1, 0.9422100782394409: 1, -0.6223658919334412: 1, 1.4056357145309448: 1, 2.4050354957580566: 1, 1.5425565242767334: 1, 1.4644227027893066: 1, -0.20047886669635773: 1, 0.1911333203315735: 1, 1.1571227312088013: 1, -0.30866673588752747: 1, 1.540098786354065: 1, 1.4051384925842285: 1, 1.0418422222137451: 1, 0.1144905686378479: 1, -0.20360040664672852: 1, 0.2816910445690155: 1, 0.339995801448822: 1, 0.11328306794166565: 1, 0.5472216606140137: 1, 0.2568971812725067: 1, 0.5611000061035156: 1, 0.8098611235618591: 1, 0.4243623912334442: 1, 1.5731940269470215: 1, 0.6104775071144104: 1, 1.069445013999939: 1, 1.5127235651016235: 1, -0.30856987833976746: 1, 1.2513844966888428: 1, 0.06307864934206009: 1, 0.3499395549297333: 1, -0.11868320405483246: 1, 0.30258285999298096: 1, -0.040548212826251984: 1, -0.4737872779369354: 1, -0.2417261004447937: 1, 1.1731380224227905: 1, -0.20986565947532654: 1, 0.6668532490730286: 1, 1.0762102603912354: 1, -1.024586796760559: 1, 0.8943637013435364: 1, 1.4529069662094116: 1, 1.4793431758880615: 1, 0.8744557499885559: 1, 0.5000193119049072: 1, 1.302850365638733: 1, 1.5040947198867798: 1, 0.7159395813941956: 1, 1.0842758417129517: 1, 0.9064841866493225: 1, 1.368375539779663: 1, 0.8933335542678833: 1, 0.0863155722618103: 1, 0.6513680219650269: 1, -0.2168547362089157: 1, -0.1565595418214798: 1, -1.0716415643692017: 1, 0.8222253322601318: 1, -0.28925973176956177: 1, -1.0314160585403442: 1, 1.7404301166534424: 1, -0.08143828064203262: 1, -0.2210041582584381: 1, 0.31707215309143066: 1, 0.9266675710678101: 1, -0.3739321231842041: 1, 0.8433452844619751: 1, 0.7008569240570068: 1, 0.5241292119026184: 1, 0.8512904644012451: 1, 0.5769325494766235: 1, 1.916335940361023: 1, 1.4151798486709595: 1, -0.11862857639789581: 1, -0.11317183822393417: 1, 0.3683989942073822: 1, 0.1563034951686859: 1, -0.1884830892086029: 1, 0.20136801898479462: 1, -1.180970549583435: 1, -1.2015372514724731: 1, -0.674019455909729: 1, -1.0489486455917358: 1, -0.012521528638899326: 1, -0.4453357756137848: 1, -0.12074612081050873: 1, 1.150854468345642: 1, 1.2192449569702148: 1, 1.009254813194275: 1, 0.9730016589164734: 1, 0.889022946357727: 1, -0.24570585787296295: 1, -1.1972460746765137: 1, -0.41403406858444214: 1, 0.055386364459991455: 1, -0.28629931807518005: 1, -0.2671576142311096: 1, 1.256977915763855: 1, 0.23997803032398224: 1, 0.24701066315174103: 1, 1.1922990083694458: 1, -1.1247143745422363: 1, 0.8834699392318726: 1, -0.007039392367005348: 1, 1.3004077672958374: 1, 0.7288377285003662: 1, 1.0863690376281738: 1, 0.8318881988525391: 1, -0.5479841232299805: 1, 0.8476697206497192: 1, -0.22304323315620422: 1, -0.12660875916481018: 1, -0.24156887829303741: 1, -0.18254651129245758: 1, 0.838370680809021: 1, -0.41674312949180603: 1, 0.654328465461731: 1, 1.1694233417510986: 1, 0.7264453172683716: 1, 0.23339834809303284: 1, 1.2736730575561523: 1, -0.16393840312957764: 1, 0.840314507484436: 1, -0.24846623837947845: 1, -1.2016277313232422: 1, -0.0411478653550148: 1, -1.1211071014404297: 1, -0.002975589595735073: 1, 0.5979693531990051: 1, -0.621107280254364: 1, 0.23272329568862915: 1, 1.0945255756378174: 1, 0.8562594652175903: 1, -0.37779542803764343: 1, -0.4684484004974365: 1, -0.5749701261520386: 1, 1.150696039199829: 1, 0.9305616617202759: 1, 0.6777730584144592: 1, 1.0972230434417725: 1, 0.7325264811515808: 1, -0.25068432092666626: 1, 0.8119110465049744: 1, 0.4251300096511841: 1, -0.2664187550544739: 1, 1.2978951930999756: 1, 0.729964017868042: 1, -0.8322789072990417: 1, -0.33581042289733887: 1, -0.2597183287143707: 1, 1.035197138786316: 1, -0.1517976075410843: 1, 1.196738839149475: 1, -0.16826894879341125: 1, -0.9053927063941956: 1, 0.6657525897026062: 1, -0.9168434739112854: 1, 0.9413108229637146: 1, -0.8882886171340942: 1, -0.09275590628385544: 1, -0.06864805519580841: 1, 0.8631362915039062: 1, 0.008864369243383408: 1, -0.20541705191135406: 1, 0.23268936574459076: 1, 0.6013088822364807: 1, -0.35842111706733704: 1, -1.126828908920288: 1, 0.9848727583885193: 1, -0.1916339248418808: 1, -0.39580973982810974: 1, -1.201277732849121: 1, 1.2822530269622803: 1, -0.27776581048965454: 1, -1.1879688501358032: 1, 1.2413678169250488: 1, 1.2350752353668213: 1, -0.2299586683511734: 1, 0.008748067542910576: 1, 0.9235488772392273: 1, 1.2188843488693237: 1, -0.37873539328575134: 1, 0.32448264956474304: 1, -0.7624772787094116: 1, 0.6605957746505737: 1, -0.6921688914299011: 1, -0.019019143655896187: 1, -1.1852235794067383: 1, -0.16533638536930084: 1, 1.104429006576538: 1, 0.525627076625824: 1, 1.2513697147369385: 1, -0.6482374668121338: 1, -0.8494775891304016: 1, -1.1774789094924927: 1, -0.12005668878555298: 1, -0.13838951289653778: 1, -0.11709770560264587: 1, -0.20500345528125763: 1, -0.23531334102153778: 1, -0.15267345309257507: 1, -0.19495585560798645: 1, -0.3641962707042694: 1, -1.2016282081604004: 1, 1.2197721004486084: 1, -0.17582924664020538: 1, 1.0062413215637207: 1, -0.025104349479079247: 1, -0.809281051158905: 1, -1.1104997396469116: 1, 0.34432777762413025: 1, 1.1265980005264282: 1, 0.017293494194746017: 1, -0.32669803500175476: 1, -0.12913857400417328: 1, 0.5908839702606201: 1, 1.0087279081344604: 1, 1.2238743305206299: 1, -1.194933295249939: 1, 0.167218878865242: 1, 1.1819933652877808: 1, -0.5622422695159912: 1, 1.2926610708236694: 1, -1.194725751876831: 1, -0.34817975759506226: 1, -1.1653438806533813: 1, 1.0467203855514526: 1, 0.029728559777140617: 1, 1.0611008405685425: 1, -0.7914682626724243: 1, -0.02250954695045948: 1, 1.2529374361038208: 1, -0.14272816479206085: 1, 1.1865397691726685: 1, 0.02958042360842228: 1, -0.04175446555018425: 1, 1.2327803373336792: 1, -0.29299479722976685: 1, 0.4987215995788574: 1, -0.42109400033950806: 1, -0.6826592683792114: 1, -0.816495954990387: 1, 0.4205377995967865: 1, 0.033690646290779114: 1, 0.8838387131690979: 1, -0.20062661170959473: 1, -1.1656997203826904: 1, 1.0187321901321411: 1, 1.1683013439178467: 1, 1.1942393779754639: 1, 1.2479115724563599: 1, -0.22866979241371155: 1, -1.1798354387283325: 1, 0.6535213589668274: 1, -1.2016297578811646: 1, -0.0449078269302845: 1, 1.2373515367507935: 1, -0.5372467637062073: 1, -0.586733877658844: 1, -0.3953478932380676: 1, 0.7884150743484497: 1, -0.4121406674385071: 1, 1.0134633779525757: 1, 0.44458866119384766: 1, 1.0013794898986816: 1, 0.02054119110107422: 1, -0.35524412989616394: 1, -0.1827705055475235: 1, -1.2004859447479248: 1, 0.41254743933677673: 1, 0.042822714895009995: 1, 0.15707539021968842: 1, 1.1279875040054321: 1, 0.7561059594154358: 1, -0.11954380571842194: 1, -1.1981785297393799: 1, 0.29174771904945374: 1, 1.039072036743164: 1, 1.0578463077545166: 1, -0.9032037854194641: 1, -0.09905305504798889: 1, 1.0347987413406372: 1, 2.26233172416687: 1, -0.848010778427124: 1, -0.3404213488101959: 1, -1.1948002576828003: 1, -1.2016299962997437: 1, -1.179785132408142: 1, -0.11716806888580322: 1, -1.2016327381134033: 1, 0.469992071390152: 1, 0.36895880103111267: 1, -1.2016286849975586: 1, -1.2016292810440063: 1, -1.2015831470489502: 1, -1.2016255855560303: 1, -1.2015341520309448: 1, -1.1318936347961426: 1, -1.1987295150756836: 1, -1.2013877630233765: 1, -0.6771114468574524: 1, -0.07099064439535141: 1, 0.1497194468975067: 1, -1.2014774084091187: 1, -1.2015864849090576: 1, 0.19668835401535034: 1, -1.2015197277069092: 1, -1.201473593711853: 1, 0.9786769151687622: 1, -0.040736664086580276: 1, -0.16829612851142883: 1, -0.1688508540391922: 1, 0.6481048464775085: 1, 1.4819786548614502: 1, 1.018097996711731: 1, -0.3213060796260834: 1, -1.1992485523223877: 1, 0.2079305797815323: 1, -0.10643515735864639: 1, -1.0591267347335815: 1, -1.2015695571899414: 1, -0.16572129726409912: 1, -1.2016041278839111: 1, 0.1331777125597: 1, -1.2016350030899048: 1, -1.2015215158462524: 1, -1.2015316486358643: 1, -1.1484540700912476: 1, -1.201595425605774: 1, -1.2016361951828003: 1, -1.201634407043457: 1, -0.21433545649051666: 1, -0.518195390701294: 1, -0.9676279425621033: 1, -1.2015115022659302: 1, -1.1257261037826538: 1, -1.2014601230621338: 1, -0.006573406048119068: 1, -0.5612114071846008: 1, 0.86496901512146: 1, -0.21048426628112793: 1, -1.201079249382019: 1, -0.9951181411743164: 1, -1.2016206979751587: 1, -1.2003253698349: 1, 0.5647678971290588: 1, -1.1779894828796387: 1, -0.9857455492019653: 1, -1.2016316652297974: 1, -0.5475073456764221: 1, -1.2014092206954956: 1, -1.1383882761001587: 1, -1.2012838125228882: 1, -1.2015061378479004: 1, -0.5834662318229675: 1, -0.28742867708206177: 1, -1.201493740081787: 1, 0.9015107750892639: 1, -1.201407790184021: 1, -0.7989376187324524: 1, -0.8910970091819763: 1, -1.1923046112060547: 1, -1.1979888677597046: 1, -0.053435858339071274: 1, -1.1363192796707153: 1, -1.0203382968902588: 1, -1.2016382217407227: 1, -1.2015154361724854: 1, -1.2011888027191162: 1, -0.4057319462299347: 1, -1.2015478610992432: 1, -0.6676098704338074: 1, -1.1954847574234009: 1, -1.194821834564209: 1, -1.2013384103775024: 1, -0.045158740133047104: 1, -1.1490250825881958: 1, -0.11703558266162872: 1, 0.1581522524356842: 1, 0.5694330334663391: 1, -0.47594985365867615: 1, 0.6233600974082947: 1, -0.5884501934051514: 1, -0.8740671277046204: 1, -0.0332360677421093: 1, -0.34523066878318787: 1, 1.1960792541503906: 1, 0.08744630217552185: 1, 0.01664043217897415: 1, -0.6421114802360535: 1, -0.3469093143939972: 1, -0.029267514124512672: 1, -1.1963841915130615: 1, -0.21881107985973358: 1, 0.33690160512924194: 1, -0.14633353054523468: 1, 0.04192548990249634: 1, -1.2016375064849854: 1, -1.1634924411773682: 1, 0.27843114733695984: 1, 0.6842910647392273: 1, 0.5252094268798828: 1, -0.3879204988479614: 1, -0.14631414413452148: 1, 0.9394524693489075: 1, 0.2588636577129364: 1, -0.0580214224755764: 1, -0.28555259108543396: 1, 1.2749443054199219: 1, 1.113309621810913: 1, -0.7430113554000854: 1, 1.3061707019805908: 1, -1.0537559986114502: 1, -1.1755220890045166: 1, -0.08886344730854034: 1, 0.015011060051620007: 1, 1.310013771057129: 1, 0.09383262693881989: 1, 0.36468705534935: 1, 0.5362502932548523: 1, 0.5997416377067566: 1, 0.6232529878616333: 1, -0.5104274749755859: 1, -0.14243346452713013: 1, 0.5468651056289673: 1, 0.637002170085907: 1, -1.2005233764648438: 1, -1.1812138557434082: 1, -0.05695538595318794: 1, -0.4360010027885437: 1, 0.8548043966293335: 1, 1.0808240175247192: 1, 1.28579580783844: 1, -0.11945565789937973: 1, 1.2526917457580566: 1, 0.3951069414615631: 1, 1.0069524049758911: 1, 0.047311048954725266: 1, 1.1649004220962524: 1, 1.295539140701294: 1, -0.33075013756752014: 1, 1.1717408895492554: 1, 1.2726079225540161: 1, 0.30879271030426025: 1, -0.24580752849578857: 1, -0.10037212073802948: 1, -1.0237786769866943: 1, 0.9712859988212585: 1, 1.176633596420288: 1, -0.04390183836221695: 1, 0.005373222753405571: 1, 1.0674824714660645: 1, 0.09893729537725449: 1, 0.8222996592521667: 1, 0.29349300265312195: 1, 0.8610304594039917: 1, -0.84548020362854: 1, 0.7298784255981445: 1, 1.200035810470581: 1, 0.9529565572738647: 1, 0.01943967677652836: 1, -0.9972583055496216: 1, -0.30726683139801025: 1, 1.2879470586776733: 1, -0.9601404666900635: 1, -0.8751402497291565: 1, 1.2916063070297241: 1, 0.9066123366355896: 1, -0.9734629392623901: 1, -1.193730115890503: 1, -0.19123347103595734: 1, -0.015705665573477745: 1, 0.8673886060714722: 1, 0.3886134922504425: 1, -1.1947468519210815: 1, -1.175829291343689: 1, 3.9672465324401855: 1, -0.463926762342453: 1, -0.37206733226776123: 1, -1.2003904581069946: 1} test data: {-1.2016366720199585: 2, -1.2016327381134033: 2, -1.201636791229248: 2, -0.41311657428741455: 1, 0.2661615014076233: 1, 1.2997812032699585: 1, 0.5403873920440674: 1, 0.10213274508714676: 1, 0.21625953912734985: 1, 0.36155056953430176: 1, 0.7412875294685364: 1, 1.2753889560699463: 1, 1.4251669645309448: 1, -0.4307803809642792: 1, -0.3040960133075714: 1, 0.4933412969112396: 1, 0.21878471970558167: 1, 1.6106586456298828: 1, -0.2516374886035919: 1, 0.6442342400550842: 1, -1.1963087320327759: 1, -1.2016195058822632: 1, 1.4458893537521362: 1, 0.06667295098304749: 1, 0.3489625155925751: 1, 0.10738043487071991: 1, -0.6546655297279358: 1, 0.8569538593292236: 1, -0.059458885341882706: 1, 1.1383615732192993: 1, -0.08030800521373749: 1, 0.7533062696456909: 1, 0.9470359086990356: 1, 1.4694101810455322: 1, 0.699573814868927: 1, 0.3411409258842468: 1, 0.394859254360199: 1, -1.201627254486084: 1, -0.4297850430011749: 1, 1.090896725654602: 1, -0.10035426914691925: 1, 0.34950539469718933: 1, 0.9149655103683472: 1, -0.040320102125406265: 1, -1.1746125221252441: 1, 0.7614589929580688: 1, -0.24653010070323944: 1, -0.22351212799549103: 1, -1.1701372861862183: 1, 1.457643747329712: 1, -0.48075738549232483: 1, -0.3203604817390442: 1, 0.34516385197639465: 1, -0.9218448400497437: 1, -0.005905755329877138: 1, 0.39954620599746704: 1, 1.520749807357788: 1, -1.1323087215423584: 1, -1.1476235389709473: 1, -1.1639471054077148: 1, -1.2007761001586914: 1, -1.197718858718872: 1, -1.197901725769043: 1, -1.1866750717163086: 1, -1.1927686929702759: 1, -1.1402051448822021: 1, 0.007953275926411152: 1, -0.07565759867429733: 1, 1.4295574426651: 1, -0.00951747503131628: 1, -0.09802207350730896: 1, -0.5042855143547058: 1, -0.683555543422699: 1, 0.5139665007591248: 1, 1.6281145811080933: 1, 0.21325090527534485: 1, 0.6560998558998108: 1, -1.1996524333953857: 1, -1.186226725578308: 1, -1.2015609741210938: 1, -0.4154701232910156: 1, 1.3625078201293945: 1, 1.4907145500183105: 1, 0.007752139586955309: 1, -0.46576231718063354: 1, -1.201637625694275: 1, 0.339458167552948: 1, -1.2015936374664307: 1, -1.2014310359954834: 1, 0.1308048814535141: 1, -0.1883729249238968: 1, -1.055851936340332: 1, -1.2004907131195068: 1, -1.1959139108657837: 1, -1.2016221284866333: 1, -0.21383443474769592: 1, -1.2006030082702637: 1, -1.2016041278839111: 1, 0.46835482120513916: 1, -1.1382324695587158: 1, -1.2014739513397217: 1, -0.7784246206283569: 1, 1.4938230514526367: 1, 1.224327802658081: 1, 1.7107861042022705: 1, 1.3899286985397339: 1, -1.1646332740783691: 1, -1.0519613027572632: 1, -1.193503737449646: 1, -1.1848548650741577: 1, 1.192413568496704: 1, -0.5754680633544922: 1, 0.7908697128295898: 1, 0.10187211632728577: 1, -0.8920286297798157: 1, -0.26542410254478455: 1, -0.3594827950000763: 1, -0.9299888610839844: 1, -0.8993450403213501: 1, -1.1072322130203247: 1, -1.1329883337020874: 1, -0.8634552955627441: 1, -0.5990430116653442: 1, 0.15106460452079773: 1, -0.12463472783565521: 1, 1.5657020807266235: 1, -1.18631112575531: 1, 0.46832725405693054: 1, -0.4354245066642761: 1, -1.0507465600967407: 1, -1.1004241704940796: 1, 1.0163378715515137: 1, 0.46070119738578796: 1, -0.49170398712158203: 1, -0.8270519375801086: 1, -0.04211708903312683: 1, -0.974824070930481: 1, -0.7166287899017334: 1, 1.8432141542434692: 1, -0.8671839237213135: 1, -0.8936908841133118: 1, 0.4876076281070709: 1, -1.1343045234680176: 1, -1.2001420259475708: 1, -0.8108161091804504: 1, -1.024053692817688: 1, 1.3890480995178223: 1, -0.8580590486526489: 1, 1.536348581314087: 1, -1.094030499458313: 1, -0.7628646492958069: 1, -0.8923998475074768: 1, -0.608140766620636: 1, -1.0076677799224854: 1, -1.0893759727478027: 1, -0.26597675681114197: 1, 1.9331234693527222: 1, 0.6718612909317017: 1, 1.7614071369171143: 1, 0.9005318284034729: 1, -0.18478137254714966: 1, 0.2634084224700928: 1, -0.5022063255310059: 1, 0.1881372481584549: 1, 0.16384904086589813: 1, 0.2808535397052765: 1, -0.26631277799606323: 1, 1.4656307697296143: 1, -0.9881011247634888: 1, 0.9568199515342712: 1, 0.8268551230430603: 1, 0.6606081128120422: 1, 0.8417104482650757: 1, -1.107023000717163: 1, -0.16252407431602478: 1, 1.2950940132141113: 1, -0.16512484848499298: 1, 0.6780659556388855: 1, 0.6696186065673828: 1, -0.6162902116775513: 1, 0.32160520553588867: 1, -0.047685928642749786: 1, -1.0213873386383057: 1, -0.731924295425415: 1, -1.1570571660995483: 1, -1.1196062564849854: 1, -1.1970343589782715: 1, 1.324471354484558: 1, -1.2006902694702148: 1, -1.0523524284362793: 1, -0.5724524259567261: 1, -0.7811231017112732: 1, -1.199570655822754: 1, -1.1873440742492676: 1, -0.45086827874183655: 1, 0.7527873516082764: 1, 1.7851945161819458: 1, -0.6876721978187561: 1, 1.6284458637237549: 1, 1.9067574739456177: 1, 0.7951827049255371: 1, 0.07571198046207428: 1, -0.8877851366996765: 1, -1.1913617849349976: 1, 0.35307395458221436: 1, -1.07969331741333: 1, 0.6321052312850952: 1, 1.3204209804534912: 1, -0.20395949482917786: 1, -0.8662946820259094: 1, 1.7208062410354614: 1, -0.4105451703071594: 1, 1.6493014097213745: 1, -1.102870225906372: 1, -0.9806917309761047: 1, 0.454349547624588: 1, 1.2889325618743896: 1, 0.35712626576423645: 1, 1.9024626016616821: 1, -1.0555497407913208: 1, -1.088794231414795: 1, -1.2016189098358154: 1, 0.10260368138551712: 1, 1.4280599355697632: 1, 1.2709132432937622: 1, 0.804813027381897: 1, 0.9758270978927612: 1, 1.7166484594345093: 1, -1.0994056463241577: 1, 1.901644229888916: 1, 0.5713223814964294: 1, 1.1523829698562622: 1, 1.8881990909576416: 1, 0.9834553003311157: 1, 1.3033519983291626: 1, 0.9554869532585144: 1, -0.6814844608306885: 1, 0.4076603651046753: 1, -0.10395042598247528: 1, 1.865704894065857: 1, 1.7069745063781738: 1, 1.7395148277282715: 1, 0.9889004230499268: 1, 0.5330069065093994: 1, 1.612230896949768: 1, -0.3399538993835449: 1, 1.7850080728530884: 1, -1.2016271352767944: 1, -0.2531147599220276: 1, 0.9327585697174072: 1, 0.27008119225502014: 1, 0.24442099034786224: 1, 0.7021965980529785: 1, -0.025178229436278343: 1, -0.2232077419757843: 1, 0.014584558084607124: 1, 0.8425688147544861: 1, 1.76934015750885: 1, 1.6013163328170776: 1, 0.3571586012840271: 1, 1.8800233602523804: 1, 1.2595564126968384: 1, -0.655949056148529: 1, 1.7461694478988647: 1, -1.201613187789917: 1, 1.8291547298431396: 1, 1.3174397945404053: 1, 1.16111421585083: 1, 0.8071705102920532: 1, 1.3225599527359009: 1, 1.7376213073730469: 1, -1.0571757555007935: 1, 0.7080846428871155: 1, -0.5870760679244995: 1, 1.0934252738952637: 1, -0.7379482984542847: 1, -0.5476839542388916: 1, 1.110692024230957: 1, 1.7776927947998047: 1, 1.8261455297470093: 1, -0.6896651983261108: 1, 1.5496872663497925: 1, 0.4616207182407379: 1, 1.8480027914047241: 1, -0.24759358167648315: 1, 1.8067305088043213: 1, 1.8354456424713135: 1, 0.1478143036365509: 1, -1.2016080617904663: 1, -0.08804576843976974: 1, 0.8335886001586914: 1, 0.6508381366729736: 1, 1.6676998138427734: 1, 1.6221997737884521: 1, 1.8416829109191895: 1, 1.7511584758758545: 1, 0.63859623670578: 1, -0.907404899597168: 1, 1.2843722105026245: 1, 1.473099708557129: 1, 1.0176738500595093: 1, 0.9968640804290771: 1, 1.0276689529418945: 1, 1.7602899074554443: 1, 1.8266348838806152: 1, 1.058468222618103: 1, 1.7661612033843994: 1, -1.156775951385498: 1, -0.4194781482219696: 1, 0.19334031641483307: 1, 1.590468406677246: 1, 1.5324621200561523: 1, 0.29917436838150024: 1, 1.569981575012207: 1, 0.9121710062026978: 1, 0.006228437647223473: 1, 1.542358636856079: 1, -0.6440175771713257: 1, -0.3422335684299469: 1, -0.032225385308265686: 1, -0.014680324122309685: 1, -0.03591597080230713: 1, -0.48933231830596924: 1, -1.1891201734542847: 1, -0.06991042196750641: 1, 1.3399251699447632: 1, 0.15484686195850372: 1, -0.5629591941833496: 1, 1.3363116979599: 1, -0.06596078723669052: 1, 1.8792171478271484: 1, -0.0695866122841835: 1, -0.22242674231529236: 1, 1.7963730096817017: 1, -0.24450848996639252: 1, 1.5233625173568726: 1, 0.7159720659255981: 1, 0.03225273638963699: 1, -0.8833669424057007: 1, -0.07945768535137177: 1, -0.1308683305978775: 1, 0.4303920269012451: 1, -0.13842415809631348: 1, 1.4534815549850464: 1, -0.30685290694236755: 1, -0.4605187475681305: 1, 0.3248298168182373: 1, -0.879592776298523: 1, -0.288830429315567: 1, 0.6150393486022949: 1, -0.023513898253440857: 1, -0.4835919141769409: 1, -1.1766016483306885: 1, 0.9130324721336365: 1, 0.7212937474250793: 1, 0.04851381108164787: 1, -0.21868036687374115: 1, 1.2261123657226562: 1, -0.49563685059547424: 1, 1.3037792444229126: 1, -0.36514076590538025: 1, -0.37186357378959656: 1, -0.7983001470565796: 1, -1.2009780406951904: 1, 0.8171444535255432: 1, -0.3135724663734436: 1, 0.12007596343755722: 1, 0.5530001521110535: 1, 1.0318180322647095: 1, -0.24634599685668945: 1, -1.2015986442565918: 1, 0.3141234815120697: 1, 0.6405824422836304: 1, -0.8678878545761108: 1, 0.5500550270080566: 1, 0.46222802996635437: 1, -0.9465891718864441: 1, 0.7334970831871033: 1, -0.2089931219816208: 1, -0.024080123752355576: 1, -0.6431723237037659: 1, -0.6276612877845764: 1, 0.24924464523792267: 1, -0.7293344736099243: 1, -0.22210338711738586: 1, 0.9173278212547302: 1, 0.14515815675258636: 1, -0.5802597403526306: 1, 0.3640252351760864: 1, 0.04068145155906677: 1, 2.0270323753356934: 1, -0.26783594489097595: 1, -0.29477736353874207: 1, -0.7296833395957947: 1, 0.7230984568595886: 1, -1.0483030080795288: 1, -1.1406184434890747: 1, -0.9465845823287964: 1, -1.1635701656341553: 1, 4.112330913543701: 1, -1.201536774635315: 1, -1.1448140144348145: 1, 4.5118513107299805: 1, -1.1515345573425293: 1, -0.13113076984882355: 1, -1.1624175310134888: 1, -0.2223142683506012: 1, -1.1232612133026123: 1, -1.188301682472229: 1, -1.0625981092453003: 1, -0.6904935240745544: 1, -1.1205850839614868: 1, -1.1716605424880981: 1, -1.1851680278778076: 1, 1.2598285675048828: 1, -1.15325927734375: 1, -0.861803412437439: 1, -1.1379677057266235: 1, -0.17077305912971497: 1, 0.02189079485833645: 1, -1.1981743574142456: 1, -0.16389766335487366: 1, 1.488932728767395: 1, -1.2001534700393677: 1, -0.5369265675544739: 1, -0.7482910752296448: 1, 0.9220188856124878: 1, -1.192414402961731: 1, 0.4001394212245941: 1, 0.6273993253707886: 1, -1.20045006275177: 1, -0.03975825384259224: 1, -1.2010724544525146: 1, 0.9156388640403748: 1, -0.14771254360675812: 1, -0.05808640271425247: 1, -0.881252646446228: 1, -0.04230939969420433: 1, -0.6031686067581177: 1, -0.32589226961135864: 1, -1.1878859996795654: 1, -0.5142630338668823: 1, -0.4774172008037567: 1, -0.3503449857234955: 1, -1.2002339363098145: 1, 0.9924389719963074: 1, -0.39197561144828796: 1, -1.1276121139526367: 1, -0.6867349743843079: 1, -1.2015101909637451: 1, -1.1760457754135132: 1, -0.8366453051567078: 1, -0.002722974168136716: 1, -0.06760650873184204: 1, -1.1580485105514526: 1, 1.0215860605239868: 1, 0.3778545558452606: 1, 1.1708914041519165: 1, -0.15606260299682617: 1, 0.8677871227264404: 1, 0.5890273451805115: 1, 0.8621184229850769: 1, -0.7514510154724121: 1, -1.09720778465271: 1, -1.192280888557434: 1, 0.6161129474639893: 1, -1.2016304731369019: 1, -0.19042982161045074: 1, -1.198868989944458: 1, 1.1278204917907715: 1, 0.008013189770281315: 1, -0.7380070686340332: 1, 0.6649335622787476: 1, -1.2016348838806152: 1, 0.9105262160301208: 1, 0.15262554585933685: 1, -0.09158548712730408: 1, -0.1383906453847885: 1, 0.028185075148940086: 1, 0.8519235253334045: 1, -0.27063482999801636: 1, 0.5615567564964294: 1, -0.5160067081451416: 1, -1.0983095169067383: 1, 0.21761490404605865: 1, 0.02654222585260868: 1, -0.253052294254303: 1, -0.8852211833000183: 1, 0.12959904968738556: 1, -0.35746997594833374: 1, -0.8245752453804016: 1, -1.1966403722763062: 1, -0.003650385420769453: 1, 0.01674770377576351: 1, 1.1214849948883057: 1, 1.0808086395263672: 1, -0.9566242694854736: 1, -0.999508798122406: 1, -0.05626079440116882: 1, 1.0766181945800781: 1, 1.030333161354065: 1, 0.5070648193359375: 1, -1.0697368383407593: 1, -0.23161835968494415: 1, 0.048983871936798096: 1, -0.6019781231880188: 1, -0.15634003281593323: 1, -0.34825557470321655: 1, -1.1586220264434814: 1, -0.42906203866004944: 1, -0.32544630765914917: 1, 1.30207359790802: 1, 0.6744810342788696: 1, 0.9348665475845337: 1, -1.0131398439407349: 1, -1.1690752506256104: 1, 0.821479320526123: 1, -1.2016373872756958: 1, -1.1969208717346191: 1, -0.9714952111244202: 1, -1.0810678005218506: 1, -0.8829452991485596: 1, -1.1685314178466797: 1, -1.0068711042404175: 1, -1.126016616821289: 1, -1.1558103561401367: 1, -1.1563661098480225: 1, -0.9732658267021179: 1, -1.1349726915359497: 1, -0.5632508397102356: 1, -1.2015451192855835: 1, -1.2016375064849854: 1, -1.1857632398605347: 1, -0.40758535265922546: 1, -1.2014809846878052: 1, -0.6185922622680664: 1, -0.8899372816085815: 1, -0.7500604391098022: 1, -1.1936330795288086: 1, -1.2015275955200195: 1, -1.1405699253082275: 1, -1.191677212715149: 1, -1.1744723320007324: 1, -1.17979097366333: 1, -1.201562523841858: 1, 5.05051851272583: 1, 5.037554740905762: 1, -1.1486388444900513: 1, -0.9225814342498779: 1, -0.9449849724769592: 1, -1.0933367013931274: 1, -1.1434556245803833: 1, -0.4437340795993805: 1, -0.9824236631393433: 1, -1.2014120817184448: 1, -0.6967374682426453: 1, -1.0416630506515503: 1, -1.1946135759353638: 1, -1.1150031089782715: 1, -0.7968862652778625: 1, -0.5409148931503296: 1, -0.9129625558853149: 1, -0.41631850600242615: 1, -1.1062737703323364: 1, -0.2650972902774811: 1, -1.193282127380371: 1, -1.0521938800811768: 1, -1.1066040992736816: 1, -0.9499993324279785: 1, -1.2016305923461914: 1, 1.6532078981399536: 1, 0.5944019556045532: 1, 1.8255465030670166: 1, -0.44575440883636475: 1, 1.2443398237228394: 1, 1.0823159217834473: 1, 0.5601547360420227: 1, -0.8054187893867493: 1, -1.201583743095398: 1, -0.411021888256073: 1, 0.5780434608459473: 1, -1.076475739479065: 1, -0.32108673453330994: 1, -0.1892606019973755: 1, -1.2016363143920898: 1, -0.9352316856384277: 1, 0.6975425481796265: 1, -1.1707801818847656: 1, -1.084214448928833: 1, -0.8594576716423035: 1, 0.369517058134079: 1, 1.2405850887298584: 1, -1.0877141952514648: 1, 1.0043728351593018: 1, 1.6066374778747559: 1, -0.23552395403385162: 1, -1.2009553909301758: 1, -1.1019858121871948: 1, -1.0865159034729004: 1, -1.1970044374465942: 1, -1.1610828638076782: 1, -0.6870049834251404: 1, -1.1963162422180176: 1, -1.201351284980774: 1, -1.1365838050842285: 1, -1.171040415763855: 1, -0.40274444222450256: 1, -1.198758602142334: 1, -1.1924408674240112: 1, -0.638097882270813: 1, -1.1971783638000488: 1, -1.2015913724899292: 1, -1.016434669494629: 1, 0.18603742122650146: 1, 0.7324214577674866: 1, -0.15851348638534546: 1, -1.1721446514129639: 1, 0.31131237745285034: 1, 1.1650327444076538: 1, 5.001932144165039: 1, 0.4515918791294098: 1, 0.06557659059762955: 1, 0.3338538706302643: 1, 1.914624810218811: 1, 0.2561040222644806: 1, 0.6983891129493713: 1, 0.30889958143234253: 1, -0.10326965153217316: 1, 0.12041562795639038: 1, 0.05425111949443817: 1, 0.3626178801059723: 1, 1.622856616973877: 1, 0.33931559324264526: 1, 1.3110328912734985: 1, 1.561331033706665: 1, -0.13019442558288574: 1, 0.14948004484176636: 1, -0.1196289211511612: 1, 0.11391282081604004: 1, 1.5044559240341187: 1, 1.1449819803237915: 1, 0.7352478504180908: 1, 0.592692494392395: 1, 0.11681299656629562: 1, -0.6209532618522644: 1, 1.1985303163528442: 1, 0.8642333745956421: 1, -1.2016342878341675: 1, -1.201610803604126: 1, -1.2014681100845337: 1, 0.3153885304927826: 1, 0.31476086378097534: 1, -0.16844011843204498: 1, -0.22581757605075836: 1, -0.4890022277832031: 1, 0.021963730454444885: 1, 1.466652274131775: 1, -0.5888111591339111: 1, 0.22853031754493713: 1, 0.5367603302001953: 1, 1.126368761062622: 1, 1.2197668552398682: 1, 1.0209075212478638: 1, -0.5517370104789734: 1, -0.1758507341146469: 1, 0.7778939008712769: 1, 0.753506600856781: 1, 1.55558443069458: 1, 0.22794750332832336: 1, -0.6031805276870728: 1, 1.6643083095550537: 1, 0.2801852524280548: 1, 1.5557059049606323: 1, -0.4644731879234314: 1, 0.35628634691238403: 1, 0.9953116178512573: 1, 1.3597513437271118: 1, 1.5239653587341309: 1, 0.5069756507873535: 1, 0.8997297883033752: 1, 0.8847638368606567: 1, 1.2778698205947876: 1, -0.19494549930095673: 1, 0.2642950117588043: 1, 1.4599251747131348: 1, 1.3463716506958008: 1, 0.3431006968021393: 1, 0.80833899974823: 1, 0.9344565272331238: 1, 1.1921144723892212: 1, 1.4580349922180176: 1, 0.2692132890224457: 1, 0.008224710822105408: 1, -0.30212122201919556: 1, -0.009973025880753994: 1, 1.5063830614089966: 1, 0.2925977110862732: 1, 1.5686708688735962: 1, 1.5599995851516724: 1, 1.3920340538024902: 1, 0.44933900237083435: 1, -0.5775644183158875: 1, 0.2826254069805145: 1, -0.3440307080745697: 1, 0.04298178479075432: 1, 0.30982303619384766: 1, 1.4639532566070557: 1, 1.2591054439544678: 1, -0.15476621687412262: 1, -0.10053509473800659: 1, 0.39806419610977173: 1, 1.473071813583374: 1, 0.39437854290008545: 1, 0.2846507132053375: 1, 0.3661552369594574: 1, 1.622040867805481: 1, 0.98076331615448: 1, -0.09898030012845993: 1, 0.29443448781967163: 1, 0.9952307939529419: 1, -1.2016353607177734: 1, -1.201634168624878: 1, -1.2016048431396484: 1, 0.8952587246894836: 1, 0.5795131325721741: 1, -0.8759819269180298: 1, 0.009196557104587555: 1, -0.8705235719680786: 1, 0.8532567620277405: 1, 1.185150146484375: 1, 0.938378632068634: 1, 0.9840258359909058: 1, 0.008470889180898666: 1, -0.2164342701435089: 1, 0.4259852468967438: 1, 0.04932570457458496: 1, 0.6872115731239319: 1, 1.2229245901107788: 1, -1.2015588283538818: 1, -1.0965174436569214: 1, -1.1905653476715088: 1, -0.6103007793426514: 1, 0.029840881004929543: 1, 0.8218473196029663: 1, -0.1427413374185562: 1, 1.2064505815505981: 1, -1.201636552810669: 1, -0.42487016320228577: 1, -1.2004797458648682: 1, 0.4542813301086426: 1, 0.0796559751033783: 1, 0.004906347021460533: 1, -0.23942992091178894: 1, -0.11078709363937378: 1, -1.095828652381897: 1, -1.0349785089492798: 1, -0.9308528304100037: 1, -0.9307324290275574: 1, -0.39703771471977234: 1, -1.0583271980285645: 1, 0.1316474825143814: 1, -0.24153171479701996: 1, -0.8713244199752808: 1, -0.2098180055618286: 1, -0.1759326308965683: 1, -0.19223442673683167: 1, 0.7244752645492554: 1, -0.9900954961776733: 1, -0.1855221837759018: 1, -1.1928194761276245: 1, 1.179612159729004: 1, 0.18457916378974915: 1, -1.1107620000839233: 1, -1.2016196250915527: 1, -1.1849600076675415: 1, -1.201633095741272: 1, -1.1967262029647827: 1, -1.2016335725784302: 1, -1.193373441696167: 1, 0.3015405535697937: 1, 0.2464127391576767: 1, -1.2016191482543945: 1, -1.201630711555481: 1, -0.4132004976272583: 1, -0.4687884449958801: 1, -0.08719541132450104: 1, 0.9265954494476318: 1, 0.1322988122701645: 1, -1.2016332149505615: 1, -1.0628424882888794: 1, -0.6758065819740295: 1, -1.2016379833221436: 1, -1.2016324996948242: 1, 0.07849415391683578: 1, 0.8498935103416443: 1, 0.15111742913722992: 1, -0.009843221865594387: 1, 0.9705496430397034: 1, -0.14289136230945587: 1, -1.2015916109085083: 1, -1.1943039894104004: 1, -0.011501064524054527: 1, -0.3390061855316162: 1, 1.0769490003585815: 1, 0.8965467214584351: 1, -1.0558182001113892: 1, -0.34107181429862976: 1, 1.1995047330856323: 1, 1.0520529747009277: 1, -0.10290281474590302: 1, -0.2904500663280487: 1, -0.2126571536064148: 1, -1.1586899757385254: 1, -1.2016273736953735: 1, -1.2015702724456787: 1, -1.201588749885559: 1, -1.201620101928711: 1, 0.0891273021697998: 1, -1.201523780822754: 1, -1.2016347646713257: 1, -0.5937166213989258: 1, 4.363720893859863: 1, -0.7542659640312195: 1}
INFO     Network type: REG
INFO     Model: "sequential_1"
INFO     _________________________________________________________________
INFO     Layer (type)                 Output Shape              Param #
INFO     =================================================================
INFO     dense_5 (Dense)              (None, 1024)              2098176
INFO     _________________________________________________________________
INFO     dropout_4 (Dropout)          (None, 1024)              0
INFO     _________________________________________________________________
INFO     dense_6 (Dense)              (None, 512)               524800
INFO     _________________________________________________________________
INFO     dropout_5 (Dropout)          (None, 512)               0
INFO     _________________________________________________________________
INFO     dense_7 (Dense)              (None, 256)               131328
INFO     _________________________________________________________________
INFO     dropout_6 (Dropout)          (None, 256)               0
INFO     _________________________________________________________________
INFO     dense_8 (Dense)              (None, 128)               32896
INFO     _________________________________________________________________
INFO     dropout_7 (Dropout)          (None, 128)               0
INFO     _________________________________________________________________
INFO     dense_9 (Dense)              (None, 1)                 129
INFO     =================================================================
INFO     Total params: 2,787,329
INFO     Trainable params: 2,787,329
INFO     Non-trainable params: 0
INFO     _________________________________________________________________
WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.
WARNING  `period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.
2024-07-15 18:38:20.657894: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0
2024-07-15 18:38:20.658112: I tensorflow/core/grappler/clusters/single_machine.cc:357] Starting new session
2024-07-15 18:38:20.660274: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:1137] Optimization results for grappler item: graph_to_optimize
  function_optimizer: function_optimizer did nothing. time = 0.006ms.
  function_optimizer: function_optimizer did nothing. time = 0.001ms.
26/26 - 2s - loss: 4.7072 - val_loss: 4.6919
Epoch 2/5000
26/26 - 1s - loss: 4.6513 - val_loss: 4.6486
Epoch 3/5000
26/26 - 1s - loss: 4.6142 - val_loss: 4.6213
Epoch 4/5000
26/26 - 1s - loss: 4.5857 - val_loss: 4.5988
Epoch 5/5000
26/26 - 1s - loss: 4.5613 - val_loss: 4.5777
Epoch 6/5000
26/26 - 1s - loss: 4.5365 - val_loss: 4.5572
Epoch 7/5000
26/26 - 1s - loss: 4.5126 - val_loss: 4.5366
Epoch 8/5000
26/26 - 1s - loss: 4.4876 - val_loss: 4.5170
Epoch 9/5000
26/26 - 1s - loss: 4.4664 - val_loss: 4.5003
Epoch 10/5000
26/26 - 1s - loss: 4.4458 - val_loss: 4.4839
Epoch 00010: val_loss improved from inf to 4.48393, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 11/5000
26/26 - 1s - loss: 4.4277 - val_loss: 4.4689
Epoch 12/5000
26/26 - 1s - loss: 4.4094 - val_loss: 4.4547
Epoch 13/5000
26/26 - 1s - loss: 4.3926 - val_loss: 4.4407
Epoch 14/5000
26/26 - 1s - loss: 4.3735 - val_loss: 4.4266
Epoch 15/5000
26/26 - 1s - loss: 4.3580 - val_loss: 4.4132
Epoch 16/5000
26/26 - 1s - loss: 4.3418 - val_loss: 4.3998
Epoch 17/5000
26/26 - 1s - loss: 4.3246 - val_loss: 4.3865
Epoch 18/5000
26/26 - 1s - loss: 4.3081 - val_loss: 4.3737
Epoch 19/5000
26/26 - 1s - loss: 4.2922 - val_loss: 4.3604
Epoch 20/5000
26/26 - 1s - loss: 4.2766 - val_loss: 4.3480
Epoch 00020: val_loss improved from 4.48393 to 4.34802, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 21/5000
26/26 - 1s - loss: 4.2578 - val_loss: 4.3355
Epoch 22/5000
26/26 - 1s - loss: 4.2448 - val_loss: 4.3231
Epoch 23/5000
26/26 - 1s - loss: 4.2274 - val_loss: 4.3111
Epoch 24/5000
26/26 - 1s - loss: 4.2132 - val_loss: 4.2992
Epoch 25/5000
26/26 - 1s - loss: 4.1978 - val_loss: 4.2879
Epoch 26/5000
26/26 - 1s - loss: 4.1816 - val_loss: 4.2764
Epoch 27/5000
26/26 - 1s - loss: 4.1674 - val_loss: 4.2649
Epoch 28/5000
26/26 - 1s - loss: 4.1518 - val_loss: 4.2534
Epoch 29/5000
26/26 - 1s - loss: 4.1370 - val_loss: 4.2417
Epoch 30/5000
26/26 - 1s - loss: 4.1228 - val_loss: 4.2305
Epoch 00030: val_loss improved from 4.34802 to 4.23050, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 31/5000
26/26 - 1s - loss: 4.1073 - val_loss: 4.2196
Epoch 32/5000
26/26 - 1s - loss: 4.0940 - val_loss: 4.2092
Epoch 33/5000
26/26 - 1s - loss: 4.0777 - val_loss: 4.1986
Epoch 34/5000
26/26 - 1s - loss: 4.0652 - val_loss: 4.1880
Epoch 35/5000
26/26 - 1s - loss: 4.0499 - val_loss: 4.1770
Epoch 36/5000
26/26 - 1s - loss: 4.0364 - val_loss: 4.1668
Epoch 37/5000
26/26 - 1s - loss: 4.0229 - val_loss: 4.1571
Epoch 38/5000
26/26 - 1s - loss: 4.0104 - val_loss: 4.1470
Epoch 39/5000
26/26 - 1s - loss: 3.9967 - val_loss: 4.1376
Epoch 40/5000
26/26 - 1s - loss: 3.9859 - val_loss: 4.1285
Epoch 00040: val_loss improved from 4.23050 to 4.12851, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 41/5000
26/26 - 1s - loss: 3.9710 - val_loss: 4.1189
Epoch 42/5000
26/26 - 1s - loss: 3.9579 - val_loss: 4.1093
Epoch 43/5000
26/26 - 1s - loss: 3.9452 - val_loss: 4.1001
Epoch 44/5000
26/26 - 1s - loss: 3.9312 - val_loss: 4.0914
Epoch 45/5000
26/26 - 1s - loss: 3.9210 - val_loss: 4.0820
Epoch 46/5000
26/26 - 1s - loss: 3.9081 - val_loss: 4.0736
Epoch 47/5000
26/26 - 1s - loss: 3.8953 - val_loss: 4.0640
Epoch 48/5000
26/26 - 1s - loss: 3.8841 - val_loss: 4.0555
Epoch 49/5000
26/26 - 1s - loss: 3.8736 - val_loss: 4.0481
Epoch 50/5000
26/26 - 1s - loss: 3.8593 - val_loss: 4.0398
Epoch 00050: val_loss improved from 4.12851 to 4.03975, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 51/5000
26/26 - 1s - loss: 3.8494 - val_loss: 4.0325
Epoch 52/5000
26/26 - 1s - loss: 3.8367 - val_loss: 4.0236
Epoch 53/5000
26/26 - 1s - loss: 3.8265 - val_loss: 4.0155
Epoch 54/5000
26/26 - 1s - loss: 3.8166 - val_loss: 4.0082
Epoch 55/5000
26/26 - 1s - loss: 3.8028 - val_loss: 3.9999
Epoch 56/5000
26/26 - 1s - loss: 3.7922 - val_loss: 3.9919
Epoch 57/5000
26/26 - 1s - loss: 3.7831 - val_loss: 3.9837
Epoch 58/5000
26/26 - 1s - loss: 3.7720 - val_loss: 3.9772
Epoch 59/5000
26/26 - 1s - loss: 3.7629 - val_loss: 3.9697
Epoch 60/5000
26/26 - 1s - loss: 3.7528 - val_loss: 3.9628
Epoch 00060: val_loss improved from 4.03975 to 3.96284, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 61/5000
26/26 - 2s - loss: 3.7407 - val_loss: 3.9557
Epoch 62/5000
26/26 - 1s - loss: 3.7311 - val_loss: 3.9499
Epoch 63/5000
26/26 - 1s - loss: 3.7214 - val_loss: 3.9410
Epoch 64/5000
26/26 - 1s - loss: 3.7123 - val_loss: 3.9339
Epoch 65/5000
26/26 - 1s - loss: 3.7057 - val_loss: 3.9281
Epoch 66/5000
26/26 - 1s - loss: 3.6950 - val_loss: 3.9197
Epoch 67/5000
26/26 - 1s - loss: 3.6846 - val_loss: 3.9125
Epoch 68/5000
26/26 - 1s - loss: 3.6740 - val_loss: 3.9061
Epoch 69/5000
26/26 - 1s - loss: 3.6653 - val_loss: 3.8987
Epoch 70/5000
26/26 - 2s - loss: 3.6559 - val_loss: 3.8925
Epoch 00070: val_loss improved from 3.96284 to 3.89247, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 71/5000
26/26 - 1s - loss: 3.6483 - val_loss: 3.8863
Epoch 72/5000
26/26 - 1s - loss: 3.6388 - val_loss: 3.8795
Epoch 73/5000
26/26 - 1s - loss: 3.6279 - val_loss: 3.8743
Epoch 74/5000
26/26 - 1s - loss: 3.6232 - val_loss: 3.8673
Epoch 75/5000
26/26 - 1s - loss: 3.6147 - val_loss: 3.8619
Epoch 76/5000
26/26 - 1s - loss: 3.6022 - val_loss: 3.8558
Epoch 77/5000
26/26 - 1s - loss: 3.5948 - val_loss: 3.8500
Epoch 78/5000
26/26 - 1s - loss: 3.5876 - val_loss: 3.8439
Epoch 79/5000
26/26 - 1s - loss: 3.5810 - val_loss: 3.8383
Epoch 80/5000
26/26 - 1s - loss: 3.5711 - val_loss: 3.8320
Epoch 00080: val_loss improved from 3.89247 to 3.83196, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 81/5000
26/26 - 1s - loss: 3.5628 - val_loss: 3.8249
Epoch 82/5000
26/26 - 1s - loss: 3.5536 - val_loss: 3.8187
Epoch 83/5000
26/26 - 1s - loss: 3.5488 - val_loss: 3.8129
Epoch 84/5000
26/26 - 1s - loss: 3.5390 - val_loss: 3.8069
Epoch 85/5000
26/26 - 1s - loss: 3.5281 - val_loss: 3.8015
Epoch 86/5000
26/26 - 1s - loss: 3.5249 - val_loss: 3.7959
Epoch 87/5000
26/26 - 1s - loss: 3.5164 - val_loss: 3.7894
Epoch 88/5000
26/26 - 1s - loss: 3.5093 - val_loss: 3.7844
Epoch 89/5000
26/26 - 1s - loss: 3.4995 - val_loss: 3.7821
Epoch 90/5000
26/26 - 1s - loss: 3.4948 - val_loss: 3.7732
Epoch 00090: val_loss improved from 3.83196 to 3.77320, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 91/5000
26/26 - 1s - loss: 3.4898 - val_loss: 3.7682
Epoch 92/5000
26/26 - 1s - loss: 3.4812 - val_loss: 3.7638
Epoch 93/5000
26/26 - 1s - loss: 3.4741 - val_loss: 3.7575
Epoch 94/5000
26/26 - 1s - loss: 3.4660 - val_loss: 3.7507
Epoch 95/5000
26/26 - 1s - loss: 3.4594 - val_loss: 3.7440
Epoch 96/5000
26/26 - 1s - loss: 3.4519 - val_loss: 3.7385
Epoch 97/5000
26/26 - 1s - loss: 3.4430 - val_loss: 3.7333
Epoch 98/5000
26/26 - 1s - loss: 3.4375 - val_loss: 3.7269
Epoch 99/5000
26/26 - 1s - loss: 3.4309 - val_loss: 3.7223
Epoch 100/5000
26/26 - 1s - loss: 3.4240 - val_loss: 3.7190
Epoch 00100: val_loss improved from 3.77320 to 3.71901, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 101/5000
26/26 - 1s - loss: 3.4165 - val_loss: 3.7112
Epoch 102/5000
26/26 - 1s - loss: 3.4073 - val_loss: 3.7066
Epoch 103/5000
26/26 - 1s - loss: 3.4031 - val_loss: 3.6998
Epoch 104/5000
26/26 - 1s - loss: 3.3963 - val_loss: 3.6944
Epoch 105/5000
26/26 - 1s - loss: 3.3897 - val_loss: 3.6888
Epoch 106/5000
26/26 - 1s - loss: 3.3854 - val_loss: 3.6840
Epoch 107/5000
26/26 - 1s - loss: 3.3774 - val_loss: 3.6787
Epoch 108/5000
26/26 - 1s - loss: 3.3699 - val_loss: 3.6743
Epoch 109/5000
26/26 - 1s - loss: 3.3647 - val_loss: 3.6687
Epoch 110/5000
26/26 - 1s - loss: 3.3610 - val_loss: 3.6633
Epoch 00110: val_loss improved from 3.71901 to 3.66328, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 111/5000
26/26 - 1s - loss: 3.3543 - val_loss: 3.6585
Epoch 112/5000
26/26 - 1s - loss: 3.3474 - val_loss: 3.6539
Epoch 113/5000
26/26 - 1s - loss: 3.3430 - val_loss: 3.6479
Epoch 114/5000
26/26 - 1s - loss: 3.3357 - val_loss: 3.6423
Epoch 115/5000
26/26 - 1s - loss: 3.3272 - val_loss: 3.6369
Epoch 116/5000
26/26 - 1s - loss: 3.3220 - val_loss: 3.6333
Epoch 117/5000
26/26 - 1s - loss: 3.3184 - val_loss: 3.6295
Epoch 118/5000
26/26 - 1s - loss: 3.3108 - val_loss: 3.6253
Epoch 119/5000
26/26 - 1s - loss: 3.3045 - val_loss: 3.6202
Epoch 120/5000
26/26 - 1s - loss: 3.2990 - val_loss: 3.6153
Epoch 00120: val_loss improved from 3.66328 to 3.61533, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 121/5000
26/26 - 1s - loss: 3.2904 - val_loss: 3.6108
Epoch 122/5000
26/26 - 1s - loss: 3.2874 - val_loss: 3.6049
Epoch 123/5000
26/26 - 1s - loss: 3.2832 - val_loss: 3.5995
Epoch 124/5000
26/26 - 1s - loss: 3.2757 - val_loss: 3.5975
Epoch 125/5000
26/26 - 1s - loss: 3.2695 - val_loss: 3.5898
Epoch 126/5000
26/26 - 1s - loss: 3.2633 - val_loss: 3.5853
Epoch 127/5000
26/26 - 1s - loss: 3.2581 - val_loss: 3.5813
Epoch 128/5000
26/26 - 1s - loss: 3.2527 - val_loss: 3.5757
Epoch 129/5000
26/26 - 1s - loss: 3.2481 - val_loss: 3.5699
Epoch 130/5000
26/26 - 1s - loss: 3.2458 - val_loss: 3.5655
Epoch 00130: val_loss improved from 3.61533 to 3.56553, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 131/5000
26/26 - 1s - loss: 3.2348 - val_loss: 3.5603
Epoch 132/5000
26/26 - 1s - loss: 3.2297 - val_loss: 3.5549
Epoch 133/5000
26/26 - 1s - loss: 3.2282 - val_loss: 3.5505
Epoch 134/5000
26/26 - 1s - loss: 3.2200 - val_loss: 3.5463
Epoch 135/5000
26/26 - 1s - loss: 3.2166 - val_loss: 3.5419
Epoch 136/5000
26/26 - 1s - loss: 3.2133 - val_loss: 3.5370
Epoch 137/5000
26/26 - 1s - loss: 3.2052 - val_loss: 3.5329
Epoch 138/5000
26/26 - 1s - loss: 3.1986 - val_loss: 3.5279
Epoch 139/5000
26/26 - 1s - loss: 3.1952 - val_loss: 3.5233
Epoch 140/5000
26/26 - 1s - loss: 3.1923 - val_loss: 3.5195
Epoch 00140: val_loss improved from 3.56553 to 3.51946, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 141/5000
26/26 - 1s - loss: 3.1869 - val_loss: 3.5151
Epoch 142/5000
26/26 - 1s - loss: 3.1805 - val_loss: 3.5110
Epoch 143/5000
26/26 - 1s - loss: 3.1754 - val_loss: 3.5067
Epoch 144/5000
26/26 - 1s - loss: 3.1684 - val_loss: 3.5021
Epoch 145/5000
26/26 - 1s - loss: 3.1650 - val_loss: 3.4970
Epoch 146/5000
26/26 - 1s - loss: 3.1611 - val_loss: 3.4936
Epoch 147/5000
26/26 - 1s - loss: 3.1547 - val_loss: 3.4899
Epoch 148/5000
26/26 - 1s - loss: 3.1529 - val_loss: 3.4848
Epoch 149/5000
26/26 - 1s - loss: 3.1445 - val_loss: 3.4796
Epoch 150/5000
26/26 - 1s - loss: 3.1403 - val_loss: 3.4748
Epoch 00150: val_loss improved from 3.51946 to 3.47484, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 151/5000
26/26 - 1s - loss: 3.1359 - val_loss: 3.4704
Epoch 152/5000
26/26 - 1s - loss: 3.1313 - val_loss: 3.4680
Epoch 153/5000
26/26 - 1s - loss: 3.1277 - val_loss: 3.4633
Epoch 154/5000
26/26 - 1s - loss: 3.1218 - val_loss: 3.4587
Epoch 155/5000
26/26 - 1s - loss: 3.1143 - val_loss: 3.4552
Epoch 156/5000
26/26 - 1s - loss: 3.1094 - val_loss: 3.4502
Epoch 157/5000
26/26 - 1s - loss: 3.1061 - val_loss: 3.4455
Epoch 158/5000
26/26 - 1s - loss: 3.1019 - val_loss: 3.4415
Epoch 159/5000
26/26 - 1s - loss: 3.0947 - val_loss: 3.4368
Epoch 160/5000
26/26 - 1s - loss: 3.0941 - val_loss: 3.4325
Epoch 00160: val_loss improved from 3.47484 to 3.43251, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 161/5000
26/26 - 1s - loss: 3.0892 - val_loss: 3.4286
Epoch 162/5000
26/26 - 1s - loss: 3.0830 - val_loss: 3.4240
Epoch 163/5000
26/26 - 1s - loss: 3.0814 - val_loss: 3.4196
Epoch 164/5000
26/26 - 1s - loss: 3.0756 - val_loss: 3.4164
Epoch 165/5000
26/26 - 1s - loss: 3.0692 - val_loss: 3.4136
Epoch 166/5000
26/26 - 1s - loss: 3.0671 - val_loss: 3.4087
Epoch 167/5000
26/26 - 1s - loss: 3.0593 - val_loss: 3.4050
Epoch 168/5000
26/26 - 1s - loss: 3.0563 - val_loss: 3.4014
Epoch 169/5000
26/26 - 1s - loss: 3.0539 - val_loss: 3.3998
Epoch 170/5000
26/26 - 1s - loss: 3.0469 - val_loss: 3.3922
Epoch 00170: val_loss improved from 3.43251 to 3.39222, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 171/5000
26/26 - 1s - loss: 3.0435 - val_loss: 3.3887
Epoch 172/5000
26/26 - 1s - loss: 3.0360 - val_loss: 3.3846
Epoch 173/5000
26/26 - 1s - loss: 3.0343 - val_loss: 3.3815
Epoch 174/5000
26/26 - 1s - loss: 3.0296 - val_loss: 3.3766
Epoch 175/5000
26/26 - 1s - loss: 3.0283 - val_loss: 3.3748
Epoch 176/5000
26/26 - 1s - loss: 3.0249 - val_loss: 3.3696
Epoch 177/5000
26/26 - 1s - loss: 3.0166 - val_loss: 3.3652
Epoch 178/5000
26/26 - 1s - loss: 3.0111 - val_loss: 3.3625
Epoch 179/5000
26/26 - 1s - loss: 3.0081 - val_loss: 3.3599
Epoch 180/5000
26/26 - 1s - loss: 3.0030 - val_loss: 3.3547
Epoch 00180: val_loss improved from 3.39222 to 3.35467, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 181/5000
26/26 - 1s - loss: 2.9989 - val_loss: 3.3508
Epoch 182/5000
26/26 - 1s - loss: 2.9980 - val_loss: 3.3481
Epoch 183/5000
26/26 - 1s - loss: 2.9927 - val_loss: 3.3428
Epoch 184/5000
26/26 - 1s - loss: 2.9898 - val_loss: 3.3401
Epoch 185/5000
26/26 - 1s - loss: 2.9846 - val_loss: 3.3376
Epoch 186/5000
26/26 - 1s - loss: 2.9783 - val_loss: 3.3333
Epoch 187/5000
26/26 - 1s - loss: 2.9752 - val_loss: 3.3295
Epoch 188/5000
26/26 - 1s - loss: 2.9707 - val_loss: 3.3239
Epoch 189/5000
26/26 - 1s - loss: 2.9677 - val_loss: 3.3210
Epoch 190/5000
26/26 - 1s - loss: 2.9648 - val_loss: 3.3186
Epoch 00190: val_loss improved from 3.35467 to 3.31864, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 191/5000
26/26 - 1s - loss: 2.9601 - val_loss: 3.3141
Epoch 192/5000
26/26 - 1s - loss: 2.9546 - val_loss: 3.3098
Epoch 193/5000
26/26 - 1s - loss: 2.9537 - val_loss: 3.3057
Epoch 194/5000
26/26 - 1s - loss: 2.9460 - val_loss: 3.3019
Epoch 195/5000
26/26 - 1s - loss: 2.9424 - val_loss: 3.2982
Epoch 196/5000
26/26 - 2s - loss: 2.9392 - val_loss: 3.2941
Epoch 197/5000
26/26 - 1s - loss: 2.9334 - val_loss: 3.2909
Epoch 198/5000
26/26 - 1s - loss: 2.9302 - val_loss: 3.2873
Epoch 199/5000
26/26 - 1s - loss: 2.9278 - val_loss: 3.2837
Epoch 200/5000
26/26 - 1s - loss: 2.9209 - val_loss: 3.2794
Epoch 00200: val_loss improved from 3.31864 to 3.27944, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 201/5000
26/26 - 1s - loss: 2.9186 - val_loss: 3.2774
Epoch 202/5000
26/26 - 1s - loss: 2.9176 - val_loss: 3.2737
Epoch 203/5000
26/26 - 1s - loss: 2.9107 - val_loss: 3.2702
Epoch 204/5000
26/26 - 1s - loss: 2.9084 - val_loss: 3.2659
Epoch 205/5000
26/26 - 1s - loss: 2.9045 - val_loss: 3.2628
Epoch 206/5000
26/26 - 1s - loss: 2.9006 - val_loss: 3.2589
Epoch 207/5000
26/26 - 1s - loss: 2.8960 - val_loss: 3.2560
Epoch 208/5000
26/26 - 1s - loss: 2.8934 - val_loss: 3.2524
Epoch 209/5000
26/26 - 1s - loss: 2.8891 - val_loss: 3.2488
Epoch 210/5000
26/26 - 1s - loss: 2.8844 - val_loss: 3.2459
Epoch 00210: val_loss improved from 3.27944 to 3.24593, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 211/5000
26/26 - 1s - loss: 2.8799 - val_loss: 3.2431
Epoch 212/5000
26/26 - 1s - loss: 2.8775 - val_loss: 3.2387
Epoch 213/5000
26/26 - 1s - loss: 2.8750 - val_loss: 3.2365
Epoch 214/5000
26/26 - 1s - loss: 2.8723 - val_loss: 3.2337
Epoch 215/5000
26/26 - 1s - loss: 2.8684 - val_loss: 3.2283
Epoch 216/5000
26/26 - 1s - loss: 2.8620 - val_loss: 3.2263
Epoch 217/5000
26/26 - 1s - loss: 2.8600 - val_loss: 3.2231
Epoch 218/5000
26/26 - 1s - loss: 2.8556 - val_loss: 3.2197
Epoch 219/5000
26/26 - 1s - loss: 2.8512 - val_loss: 3.2148
Epoch 220/5000
26/26 - 1s - loss: 2.8474 - val_loss: 3.2102
Epoch 00220: val_loss improved from 3.24593 to 3.21025, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 221/5000
26/26 - 1s - loss: 2.8429 - val_loss: 3.2080
Epoch 222/5000
26/26 - 1s - loss: 2.8392 - val_loss: 3.2043
Epoch 223/5000
26/26 - 1s - loss: 2.8375 - val_loss: 3.2010
Epoch 224/5000
26/26 - 1s - loss: 2.8360 - val_loss: 3.1978
Epoch 225/5000
26/26 - 1s - loss: 2.8298 - val_loss: 3.1954
Epoch 226/5000
26/26 - 1s - loss: 2.8262 - val_loss: 3.1912
Epoch 227/5000
26/26 - 1s - loss: 2.8238 - val_loss: 3.1868
Epoch 228/5000
26/26 - 1s - loss: 2.8187 - val_loss: 3.1830
Epoch 229/5000
26/26 - 1s - loss: 2.8132 - val_loss: 3.1807
Epoch 230/5000
26/26 - 1s - loss: 2.8126 - val_loss: 3.1771
Epoch 00230: val_loss improved from 3.21025 to 3.17712, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 231/5000
26/26 - 1s - loss: 2.8086 - val_loss: 3.1730
Epoch 232/5000
26/26 - 1s - loss: 2.8041 - val_loss: 3.1708
Epoch 233/5000
26/26 - 1s - loss: 2.8004 - val_loss: 3.1681
Epoch 234/5000
26/26 - 1s - loss: 2.7966 - val_loss: 3.1648
Epoch 235/5000
26/26 - 2s - loss: 2.7965 - val_loss: 3.1639
Epoch 236/5000
26/26 - 1s - loss: 2.7913 - val_loss: 3.1580
Epoch 237/5000
26/26 - 1s - loss: 2.7858 - val_loss: 3.1557
Epoch 238/5000
26/26 - 1s - loss: 2.7851 - val_loss: 3.1537
Epoch 239/5000
26/26 - 1s - loss: 2.7804 - val_loss: 3.1490
Epoch 240/5000
26/26 - 1s - loss: 2.7779 - val_loss: 3.1459
Epoch 00240: val_loss improved from 3.17712 to 3.14589, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 241/5000
26/26 - 1s - loss: 2.7741 - val_loss: 3.1428
Epoch 242/5000
26/26 - 1s - loss: 2.7706 - val_loss: 3.1389
Epoch 243/5000
26/26 - 1s - loss: 2.7672 - val_loss: 3.1354
Epoch 244/5000
26/26 - 1s - loss: 2.7625 - val_loss: 3.1323
Epoch 245/5000
26/26 - 1s - loss: 2.7592 - val_loss: 3.1290
Epoch 246/5000
26/26 - 1s - loss: 2.7567 - val_loss: 3.1271
Epoch 247/5000
26/26 - 1s - loss: 2.7535 - val_loss: 3.1250
Epoch 248/5000
26/26 - 1s - loss: 2.7509 - val_loss: 3.1219
Epoch 249/5000
26/26 - 1s - loss: 2.7461 - val_loss: 3.1191
Epoch 250/5000
26/26 - 1s - loss: 2.7441 - val_loss: 3.1165
Epoch 00250: val_loss improved from 3.14589 to 3.11649, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 251/5000
26/26 - 1s - loss: 2.7431 - val_loss: 3.1151
Epoch 252/5000
26/26 - 1s - loss: 2.7371 - val_loss: 3.1101
Epoch 253/5000
26/26 - 1s - loss: 2.7332 - val_loss: 3.1077
Epoch 254/5000
26/26 - 1s - loss: 2.7324 - val_loss: 3.1032
Epoch 255/5000
26/26 - 1s - loss: 2.7281 - val_loss: 3.0994
Epoch 256/5000
26/26 - 1s - loss: 2.7251 - val_loss: 3.0962
Epoch 257/5000
26/26 - 1s - loss: 2.7196 - val_loss: 3.0930
Epoch 258/5000
26/26 - 1s - loss: 2.7183 - val_loss: 3.0893
Epoch 259/5000
26/26 - 1s - loss: 2.7139 - val_loss: 3.0863
Epoch 260/5000
26/26 - 1s - loss: 2.7114 - val_loss: 3.0833
Epoch 00260: val_loss improved from 3.11649 to 3.08334, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 261/5000
26/26 - 1s - loss: 2.7107 - val_loss: 3.0795
Epoch 262/5000
26/26 - 1s - loss: 2.7045 - val_loss: 3.0762
Epoch 263/5000
26/26 - 1s - loss: 2.7007 - val_loss: 3.0747
Epoch 264/5000
26/26 - 1s - loss: 2.6977 - val_loss: 3.0710
Epoch 265/5000
26/26 - 1s - loss: 2.6952 - val_loss: 3.0675
Epoch 266/5000
26/26 - 1s - loss: 2.6942 - val_loss: 3.0660
Epoch 267/5000
26/26 - 1s - loss: 2.6879 - val_loss: 3.0648
Epoch 268/5000
26/26 - 1s - loss: 2.6854 - val_loss: 3.0597
Epoch 269/5000
26/26 - 1s - loss: 2.6826 - val_loss: 3.0572
Epoch 270/5000
26/26 - 1s - loss: 2.6787 - val_loss: 3.0551
Epoch 00270: val_loss improved from 3.08334 to 3.05507, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 271/5000
26/26 - 1s - loss: 2.6763 - val_loss: 3.0518
Epoch 272/5000
26/26 - 1s - loss: 2.6732 - val_loss: 3.0488
Epoch 273/5000
26/26 - 1s - loss: 2.6717 - val_loss: 3.0472
Epoch 274/5000
26/26 - 2s - loss: 2.6683 - val_loss: 3.0419
Epoch 275/5000
26/26 - 1s - loss: 2.6618 - val_loss: 3.0398
Epoch 276/5000
26/26 - 1s - loss: 2.6612 - val_loss: 3.0371
Epoch 277/5000
26/26 - 1s - loss: 2.6594 - val_loss: 3.0332
Epoch 278/5000
26/26 - 1s - loss: 2.6555 - val_loss: 3.0304
Epoch 279/5000
26/26 - 1s - loss: 2.6481 - val_loss: 3.0275
Epoch 280/5000
26/26 - 1s - loss: 2.6482 - val_loss: 3.0244
Epoch 00280: val_loss improved from 3.05507 to 3.02438, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 281/5000
26/26 - 1s - loss: 2.6455 - val_loss: 3.0234
Epoch 282/5000
26/26 - 1s - loss: 2.6416 - val_loss: 3.0194
Epoch 283/5000
26/26 - 1s - loss: 2.6391 - val_loss: 3.0181
Epoch 284/5000
26/26 - 1s - loss: 2.6380 - val_loss: 3.0154
Epoch 285/5000
26/26 - 1s - loss: 2.6340 - val_loss: 3.0117
Epoch 286/5000
26/26 - 1s - loss: 2.6320 - val_loss: 3.0086
Epoch 287/5000
26/26 - 1s - loss: 2.6271 - val_loss: 3.0067
Epoch 288/5000
26/26 - 1s - loss: 2.6267 - val_loss: 3.0043
Epoch 289/5000
26/26 - 1s - loss: 2.6206 - val_loss: 3.0046
Epoch 290/5000
26/26 - 1s - loss: 2.6198 - val_loss: 2.9978
Epoch 00290: val_loss improved from 3.02438 to 2.99778, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 291/5000
26/26 - 1s - loss: 2.6136 - val_loss: 2.9957
Epoch 292/5000
26/26 - 1s - loss: 2.6107 - val_loss: 2.9927
Epoch 293/5000
26/26 - 1s - loss: 2.6101 - val_loss: 2.9908
Epoch 294/5000
26/26 - 1s - loss: 2.6078 - val_loss: 2.9855
Epoch 295/5000
26/26 - 1s - loss: 2.6014 - val_loss: 2.9831
Epoch 296/5000
26/26 - 1s - loss: 2.5997 - val_loss: 2.9805
Epoch 297/5000
26/26 - 1s - loss: 2.5993 - val_loss: 2.9785
Epoch 298/5000
26/26 - 1s - loss: 2.5941 - val_loss: 2.9738
Epoch 299/5000
26/26 - 1s - loss: 2.5931 - val_loss: 2.9706
Epoch 300/5000
26/26 - 1s - loss: 2.5886 - val_loss: 2.9681
Epoch 00300: val_loss improved from 2.99778 to 2.96809, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 301/5000
26/26 - 1s - loss: 2.5831 - val_loss: 2.9665
Epoch 302/5000
26/26 - 1s - loss: 2.5816 - val_loss: 2.9629
Epoch 303/5000
26/26 - 1s - loss: 2.5792 - val_loss: 2.9614
Epoch 304/5000
26/26 - 1s - loss: 2.5769 - val_loss: 2.9580
Epoch 305/5000
26/26 - 1s - loss: 2.5738 - val_loss: 2.9540
Epoch 306/5000
26/26 - 1s - loss: 2.5713 - val_loss: 2.9518
Epoch 307/5000
26/26 - 1s - loss: 2.5684 - val_loss: 2.9500
Epoch 308/5000
26/26 - 1s - loss: 2.5637 - val_loss: 2.9457
Epoch 309/5000
26/26 - 1s - loss: 2.5611 - val_loss: 2.9430
Epoch 310/5000
26/26 - 1s - loss: 2.5580 - val_loss: 2.9409
Epoch 00310: val_loss improved from 2.96809 to 2.94094, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 311/5000
26/26 - 1s - loss: 2.5559 - val_loss: 2.9381
Epoch 312/5000
26/26 - 1s - loss: 2.5535 - val_loss: 2.9355
Epoch 313/5000
26/26 - 1s - loss: 2.5507 - val_loss: 2.9328
Epoch 314/5000
26/26 - 1s - loss: 2.5477 - val_loss: 2.9297
Epoch 315/5000
26/26 - 1s - loss: 2.5451 - val_loss: 2.9270
Epoch 316/5000
26/26 - 1s - loss: 2.5432 - val_loss: 2.9253
Epoch 317/5000
26/26 - 1s - loss: 2.5381 - val_loss: 2.9214
Epoch 318/5000
26/26 - 1s - loss: 2.5362 - val_loss: 2.9193
Epoch 319/5000
26/26 - 1s - loss: 2.5328 - val_loss: 2.9160
Epoch 320/5000
26/26 - 1s - loss: 2.5291 - val_loss: 2.9135
Epoch 00320: val_loss improved from 2.94094 to 2.91348, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 321/5000
26/26 - 1s - loss: 2.5293 - val_loss: 2.9120
Epoch 322/5000
26/26 - 1s - loss: 2.5261 - val_loss: 2.9096
Epoch 323/5000
26/26 - 1s - loss: 2.5227 - val_loss: 2.9056
Epoch 324/5000
26/26 - 1s - loss: 2.5201 - val_loss: 2.9013
Epoch 325/5000
26/26 - 1s - loss: 2.5168 - val_loss: 2.8986
Epoch 326/5000
26/26 - 1s - loss: 2.5138 - val_loss: 2.8977
Epoch 327/5000
26/26 - 1s - loss: 2.5129 - val_loss: 2.8949
Epoch 328/5000
26/26 - 1s - loss: 2.5093 - val_loss: 2.8931
Epoch 329/5000
26/26 - 1s - loss: 2.5071 - val_loss: 2.8903
Epoch 330/5000
26/26 - 1s - loss: 2.5014 - val_loss: 2.8869
Epoch 00330: val_loss improved from 2.91348 to 2.88688, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 331/5000
26/26 - 1s - loss: 2.5016 - val_loss: 2.8857
Epoch 332/5000
26/26 - 1s - loss: 2.4976 - val_loss: 2.8838
Epoch 333/5000
26/26 - 1s - loss: 2.4939 - val_loss: 2.8807
Epoch 334/5000
26/26 - 1s - loss: 2.4927 - val_loss: 2.8775
Epoch 335/5000
26/26 - 1s - loss: 2.4882 - val_loss: 2.8744
Epoch 336/5000
26/26 - 1s - loss: 2.4903 - val_loss: 2.8715
Epoch 337/5000
26/26 - 1s - loss: 2.4834 - val_loss: 2.8692
Epoch 338/5000
26/26 - 1s - loss: 2.4813 - val_loss: 2.8662
Epoch 339/5000
26/26 - 1s - loss: 2.4814 - val_loss: 2.8639
Epoch 340/5000
26/26 - 1s - loss: 2.4760 - val_loss: 2.8611
Epoch 00340: val_loss improved from 2.88688 to 2.86106, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 341/5000
26/26 - 1s - loss: 2.4724 - val_loss: 2.8582
Epoch 342/5000
26/26 - 1s - loss: 2.4720 - val_loss: 2.8554
Epoch 343/5000
26/26 - 1s - loss: 2.4678 - val_loss: 2.8528
Epoch 344/5000
26/26 - 1s - loss: 2.4656 - val_loss: 2.8505
Epoch 345/5000
26/26 - 1s - loss: 2.4621 - val_loss: 2.8462
Epoch 346/5000
26/26 - 1s - loss: 2.4610 - val_loss: 2.8440
Epoch 347/5000
26/26 - 1s - loss: 2.4597 - val_loss: 2.8434
Epoch 348/5000
26/26 - 1s - loss: 2.4542 - val_loss: 2.8411
Epoch 349/5000
26/26 - 1s - loss: 2.4528 - val_loss: 2.8375
Epoch 350/5000
26/26 - 1s - loss: 2.4502 - val_loss: 2.8353
Epoch 00350: val_loss improved from 2.86106 to 2.83529, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 351/5000
26/26 - 1s - loss: 2.4486 - val_loss: 2.8326
Epoch 352/5000
26/26 - 1s - loss: 2.4428 - val_loss: 2.8309
Epoch 353/5000
26/26 - 1s - loss: 2.4434 - val_loss: 2.8283
Epoch 354/5000
26/26 - 1s - loss: 2.4408 - val_loss: 2.8260
Epoch 355/5000
26/26 - 1s - loss: 2.4389 - val_loss: 2.8215
Epoch 356/5000
26/26 - 1s - loss: 2.4338 - val_loss: 2.8190
Epoch 357/5000
26/26 - 1s - loss: 2.4324 - val_loss: 2.8177
Epoch 358/5000
26/26 - 1s - loss: 2.4304 - val_loss: 2.8160
Epoch 359/5000
26/26 - 1s - loss: 2.4248 - val_loss: 2.8140
Epoch 360/5000
26/26 - 1s - loss: 2.4225 - val_loss: 2.8118
Epoch 00360: val_loss improved from 2.83529 to 2.81180, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 361/5000
26/26 - 1s - loss: 2.4223 - val_loss: 2.8095
Epoch 362/5000
26/26 - 1s - loss: 2.4184 - val_loss: 2.8077
Epoch 363/5000
26/26 - 1s - loss: 2.4147 - val_loss: 2.8060
Epoch 364/5000
26/26 - 1s - loss: 2.4143 - val_loss: 2.8032
Epoch 365/5000
26/26 - 1s - loss: 2.4117 - val_loss: 2.8005
Epoch 366/5000
26/26 - 1s - loss: 2.4094 - val_loss: 2.7967
Epoch 367/5000
26/26 - 1s - loss: 2.4063 - val_loss: 2.7950
Epoch 368/5000
26/26 - 1s - loss: 2.4025 - val_loss: 2.7948
Epoch 369/5000
26/26 - 1s - loss: 2.4008 - val_loss: 2.7901
Epoch 370/5000
26/26 - 1s - loss: 2.4003 - val_loss: 2.7881
Epoch 00370: val_loss improved from 2.81180 to 2.78812, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 371/5000
26/26 - 1s - loss: 2.3947 - val_loss: 2.7860
Epoch 372/5000
26/26 - 1s - loss: 2.3909 - val_loss: 2.7838
Epoch 373/5000
26/26 - 1s - loss: 2.3910 - val_loss: 2.7811
Epoch 374/5000
26/26 - 1s - loss: 2.3860 - val_loss: 2.7788
Epoch 375/5000
26/26 - 1s - loss: 2.3864 - val_loss: 2.7756
Epoch 376/5000
26/26 - 1s - loss: 2.3835 - val_loss: 2.7716
Epoch 377/5000
26/26 - 1s - loss: 2.3808 - val_loss: 2.7696
Epoch 378/5000
26/26 - 1s - loss: 2.3786 - val_loss: 2.7684
Epoch 379/5000
26/26 - 1s - loss: 2.3758 - val_loss: 2.7684
Epoch 380/5000
26/26 - 1s - loss: 2.3752 - val_loss: 2.7637
Epoch 00380: val_loss improved from 2.78812 to 2.76370, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 381/5000
26/26 - 1s - loss: 2.3707 - val_loss: 2.7613
Epoch 382/5000
26/26 - 1s - loss: 2.3676 - val_loss: 2.7595
Epoch 383/5000
26/26 - 1s - loss: 2.3668 - val_loss: 2.7575
Epoch 384/5000
26/26 - 1s - loss: 2.3599 - val_loss: 2.7551
Epoch 385/5000
26/26 - 1s - loss: 2.3591 - val_loss: 2.7537
Epoch 386/5000
26/26 - 1s - loss: 2.3579 - val_loss: 2.7493
Epoch 387/5000
26/26 - 1s - loss: 2.3551 - val_loss: 2.7463
Epoch 388/5000
26/26 - 1s - loss: 2.3539 - val_loss: 2.7428
Epoch 389/5000
26/26 - 1s - loss: 2.3500 - val_loss: 2.7408
Epoch 390/5000
26/26 - 1s - loss: 2.3470 - val_loss: 2.7380
Epoch 00390: val_loss improved from 2.76370 to 2.73803, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 391/5000
26/26 - 1s - loss: 2.3459 - val_loss: 2.7363
Epoch 392/5000
26/26 - 1s - loss: 2.3444 - val_loss: 2.7340
Epoch 393/5000
26/26 - 1s - loss: 2.3408 - val_loss: 2.7315
Epoch 394/5000
26/26 - 1s - loss: 2.3395 - val_loss: 2.7288
Epoch 395/5000
26/26 - 1s - loss: 2.3343 - val_loss: 2.7263
Epoch 396/5000
26/26 - 1s - loss: 2.3329 - val_loss: 2.7256
Epoch 397/5000
26/26 - 1s - loss: 2.3320 - val_loss: 2.7228
Epoch 398/5000
26/26 - 1s - loss: 2.3288 - val_loss: 2.7189
Epoch 399/5000
26/26 - 1s - loss: 2.3240 - val_loss: 2.7171
Epoch 400/5000
26/26 - 1s - loss: 2.3238 - val_loss: 2.7171
Epoch 00400: val_loss improved from 2.73803 to 2.71711, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 401/5000
26/26 - 1s - loss: 2.3216 - val_loss: 2.7139
Epoch 402/5000
26/26 - 1s - loss: 2.3172 - val_loss: 2.7121
Epoch 403/5000
26/26 - 1s - loss: 2.3154 - val_loss: 2.7093
Epoch 404/5000
26/26 - 1s - loss: 2.3144 - val_loss: 2.7060
Epoch 405/5000
26/26 - 1s - loss: 2.3119 - val_loss: 2.7050
Epoch 406/5000
26/26 - 2s - loss: 2.3082 - val_loss: 2.7021
Epoch 407/5000
26/26 - 1s - loss: 2.3066 - val_loss: 2.6999
Epoch 408/5000
26/26 - 1s - loss: 2.3046 - val_loss: 2.6980
Epoch 409/5000
26/26 - 1s - loss: 2.3025 - val_loss: 2.6935
Epoch 410/5000
26/26 - 1s - loss: 2.3016 - val_loss: 2.6921
Epoch 00410: val_loss improved from 2.71711 to 2.69213, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 411/5000
26/26 - 1s - loss: 2.2957 - val_loss: 2.6908
Epoch 412/5000
26/26 - 1s - loss: 2.2956 - val_loss: 2.6888
Epoch 413/5000
26/26 - 1s - loss: 2.2906 - val_loss: 2.6861
Epoch 414/5000
26/26 - 1s - loss: 2.2871 - val_loss: 2.6839
Epoch 415/5000
26/26 - 1s - loss: 2.2884 - val_loss: 2.6818
Epoch 416/5000
26/26 - 1s - loss: 2.2866 - val_loss: 2.6791
Epoch 417/5000
26/26 - 1s - loss: 2.2836 - val_loss: 2.6773
Epoch 418/5000
26/26 - 1s - loss: 2.2809 - val_loss: 2.6747
Epoch 419/5000
26/26 - 1s - loss: 2.2795 - val_loss: 2.6720
Epoch 420/5000
26/26 - 1s - loss: 2.2767 - val_loss: 2.6697
Epoch 00420: val_loss improved from 2.69213 to 2.66970, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 421/5000
26/26 - 1s - loss: 2.2715 - val_loss: 2.6676
Epoch 422/5000
26/26 - 1s - loss: 2.2717 - val_loss: 2.6648
Epoch 423/5000
26/26 - 1s - loss: 2.2704 - val_loss: 2.6627
Epoch 424/5000
26/26 - 2s - loss: 2.2644 - val_loss: 2.6595
Epoch 425/5000
26/26 - 2s - loss: 2.2649 - val_loss: 2.6583
Epoch 426/5000
26/26 - 2s - loss: 2.2644 - val_loss: 2.6558
Epoch 427/5000
26/26 - 1s - loss: 2.2619 - val_loss: 2.6550
Epoch 428/5000
26/26 - 1s - loss: 2.2561 - val_loss: 2.6520
Epoch 429/5000
26/26 - 1s - loss: 2.2553 - val_loss: 2.6504
Epoch 430/5000
26/26 - 1s - loss: 2.2511 - val_loss: 2.6470
Epoch 00430: val_loss improved from 2.66970 to 2.64704, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 431/5000
26/26 - 1s - loss: 2.2528 - val_loss: 2.6452
Epoch 432/5000
26/26 - 1s - loss: 2.2497 - val_loss: 2.6436
Epoch 433/5000
26/26 - 1s - loss: 2.2463 - val_loss: 2.6422
Epoch 434/5000
26/26 - 1s - loss: 2.2429 - val_loss: 2.6392
Epoch 435/5000
26/26 - 1s - loss: 2.2423 - val_loss: 2.6377
Epoch 436/5000
26/26 - 1s - loss: 2.2387 - val_loss: 2.6355
Epoch 437/5000
26/26 - 1s - loss: 2.2372 - val_loss: 2.6331
Epoch 438/5000
26/26 - 1s - loss: 2.2357 - val_loss: 2.6312
Epoch 439/5000
26/26 - 1s - loss: 2.2322 - val_loss: 2.6293
Epoch 440/5000
26/26 - 1s - loss: 2.2310 - val_loss: 2.6272
Epoch 00440: val_loss improved from 2.64704 to 2.62725, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 441/5000
26/26 - 1s - loss: 2.2293 - val_loss: 2.6258
Epoch 442/5000
26/26 - 1s - loss: 2.2258 - val_loss: 2.6216
Epoch 443/5000
26/26 - 1s - loss: 2.2243 - val_loss: 2.6194
Epoch 444/5000
26/26 - 1s - loss: 2.2216 - val_loss: 2.6186
Epoch 445/5000
26/26 - 1s - loss: 2.2183 - val_loss: 2.6168
Epoch 446/5000
26/26 - 1s - loss: 2.2159 - val_loss: 2.6127
Epoch 447/5000
26/26 - 1s - loss: 2.2164 - val_loss: 2.6119
Epoch 448/5000
26/26 - 1s - loss: 2.2111 - val_loss: 2.6094
Epoch 449/5000
26/26 - 1s - loss: 2.2115 - val_loss: 2.6076
Epoch 450/5000
26/26 - 1s - loss: 2.2064 - val_loss: 2.6053
Epoch 00450: val_loss improved from 2.62725 to 2.60525, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 451/5000
26/26 - 1s - loss: 2.2062 - val_loss: 2.6030
Epoch 452/5000
26/26 - 1s - loss: 2.2050 - val_loss: 2.6015
Epoch 453/5000
26/26 - 1s - loss: 2.1999 - val_loss: 2.5991
Epoch 454/5000
26/26 - 1s - loss: 2.1997 - val_loss: 2.5955
Epoch 455/5000
26/26 - 1s - loss: 2.1975 - val_loss: 2.5935
Epoch 456/5000
26/26 - 1s - loss: 2.1964 - val_loss: 2.5926
Epoch 457/5000
26/26 - 1s - loss: 2.1916 - val_loss: 2.5914
Epoch 458/5000
26/26 - 1s - loss: 2.1908 - val_loss: 2.5883
Epoch 459/5000
26/26 - 1s - loss: 2.1884 - val_loss: 2.5881
Epoch 460/5000
26/26 - 1s - loss: 2.1847 - val_loss: 2.5849
Epoch 00460: val_loss improved from 2.60525 to 2.58489, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 461/5000
26/26 - 1s - loss: 2.1829 - val_loss: 2.5831
Epoch 462/5000
26/26 - 1s - loss: 2.1842 - val_loss: 2.5782
Epoch 463/5000
26/26 - 1s - loss: 2.1794 - val_loss: 2.5780
Epoch 464/5000
26/26 - 1s - loss: 2.1787 - val_loss: 2.5755
Epoch 465/5000
26/26 - 1s - loss: 2.1748 - val_loss: 2.5742
Epoch 466/5000
26/26 - 1s - loss: 2.1730 - val_loss: 2.5721
Epoch 467/5000
26/26 - 1s - loss: 2.1704 - val_loss: 2.5690
Epoch 468/5000
26/26 - 1s - loss: 2.1707 - val_loss: 2.5689
Epoch 469/5000
26/26 - 1s - loss: 2.1678 - val_loss: 2.5672
Epoch 470/5000
26/26 - 1s - loss: 2.1645 - val_loss: 2.5655
Epoch 00470: val_loss improved from 2.58489 to 2.56550, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 471/5000
26/26 - 1s - loss: 2.1635 - val_loss: 2.5619
Epoch 472/5000
26/26 - 1s - loss: 2.1633 - val_loss: 2.5602
Epoch 473/5000
26/26 - 1s - loss: 2.1572 - val_loss: 2.5598
Epoch 474/5000
26/26 - 1s - loss: 2.1554 - val_loss: 2.5570
Epoch 475/5000
26/26 - 1s - loss: 2.1540 - val_loss: 2.5544
Epoch 476/5000
26/26 - 1s - loss: 2.1494 - val_loss: 2.5525
Epoch 477/5000
26/26 - 1s - loss: 2.1495 - val_loss: 2.5493
Epoch 478/5000
26/26 - 1s - loss: 2.1472 - val_loss: 2.5468
Epoch 479/5000
26/26 - 1s - loss: 2.1444 - val_loss: 2.5458
Epoch 480/5000
26/26 - 1s - loss: 2.1438 - val_loss: 2.5441
Epoch 00480: val_loss improved from 2.56550 to 2.54410, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 481/5000
26/26 - 1s - loss: 2.1408 - val_loss: 2.5415
Epoch 482/5000
26/26 - 1s - loss: 2.1374 - val_loss: 2.5401
Epoch 483/5000
26/26 - 1s - loss: 2.1369 - val_loss: 2.5389
Epoch 484/5000
26/26 - 1s - loss: 2.1357 - val_loss: 2.5345
Epoch 485/5000
26/26 - 1s - loss: 2.1324 - val_loss: 2.5313
Epoch 486/5000
26/26 - 1s - loss: 2.1301 - val_loss: 2.5295
Epoch 487/5000
26/26 - 1s - loss: 2.1310 - val_loss: 2.5279
Epoch 488/5000
26/26 - 2s - loss: 2.1267 - val_loss: 2.5251
Epoch 489/5000
26/26 - 1s - loss: 2.1244 - val_loss: 2.5241
Epoch 490/5000
26/26 - 1s - loss: 2.1235 - val_loss: 2.5218
Epoch 00490: val_loss improved from 2.54410 to 2.52177, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 491/5000
26/26 - 1s - loss: 2.1199 - val_loss: 2.5204
Epoch 492/5000
26/26 - 1s - loss: 2.1198 - val_loss: 2.5191
Epoch 493/5000
26/26 - 1s - loss: 2.1175 - val_loss: 2.5166
Epoch 494/5000
26/26 - 1s - loss: 2.1139 - val_loss: 2.5156
Epoch 495/5000
26/26 - 1s - loss: 2.1106 - val_loss: 2.5142
Epoch 496/5000
26/26 - 1s - loss: 2.1104 - val_loss: 2.5102
Epoch 497/5000
26/26 - 1s - loss: 2.1044 - val_loss: 2.5074
Epoch 498/5000
26/26 - 1s - loss: 2.1086 - val_loss: 2.5100
Epoch 499/5000
26/26 - 1s - loss: 2.1034 - val_loss: 2.5046
Epoch 500/5000
26/26 - 1s - loss: 2.1029 - val_loss: 2.5041
Epoch 00500: val_loss improved from 2.52177 to 2.50408, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 501/5000
26/26 - 1s - loss: 2.1001 - val_loss: 2.5008
Epoch 502/5000
26/26 - 1s - loss: 2.0970 - val_loss: 2.5011
Epoch 503/5000
26/26 - 1s - loss: 2.0957 - val_loss: 2.4971
Epoch 504/5000
26/26 - 1s - loss: 2.0924 - val_loss: 2.4948
Epoch 505/5000
26/26 - 1s - loss: 2.0916 - val_loss: 2.4927
Epoch 506/5000
26/26 - 2s - loss: 2.0882 - val_loss: 2.4912
Epoch 507/5000
26/26 - 1s - loss: 2.0886 - val_loss: 2.4899
Epoch 508/5000
26/26 - 1s - loss: 2.0861 - val_loss: 2.4886
Epoch 509/5000
26/26 - 1s - loss: 2.0849 - val_loss: 2.4868
Epoch 510/5000
26/26 - 1s - loss: 2.0822 - val_loss: 2.4838
Epoch 00510: val_loss improved from 2.50408 to 2.48384, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 511/5000
26/26 - 2s - loss: 2.0774 - val_loss: 2.4810
Epoch 512/5000
26/26 - 1s - loss: 2.0806 - val_loss: 2.4790
Epoch 513/5000
26/26 - 1s - loss: 2.0761 - val_loss: 2.4778
Epoch 514/5000
26/26 - 1s - loss: 2.0760 - val_loss: 2.4773
Epoch 515/5000
26/26 - 1s - loss: 2.0720 - val_loss: 2.4754
Epoch 516/5000
26/26 - 1s - loss: 2.0694 - val_loss: 2.4726
Epoch 517/5000
26/26 - 1s - loss: 2.0689 - val_loss: 2.4703
Epoch 518/5000
26/26 - 1s - loss: 2.0672 - val_loss: 2.4694
Epoch 519/5000
26/26 - 1s - loss: 2.0617 - val_loss: 2.4666
Epoch 520/5000
26/26 - 1s - loss: 2.0610 - val_loss: 2.4638
Epoch 00520: val_loss improved from 2.48384 to 2.46377, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 521/5000
26/26 - 1s - loss: 2.0590 - val_loss: 2.4631
Epoch 522/5000
26/26 - 1s - loss: 2.0570 - val_loss: 2.4608
Epoch 523/5000
26/26 - 1s - loss: 2.0546 - val_loss: 2.4603
Epoch 524/5000
26/26 - 1s - loss: 2.0532 - val_loss: 2.4583
Epoch 525/5000
26/26 - 1s - loss: 2.0513 - val_loss: 2.4561
Epoch 526/5000
26/26 - 1s - loss: 2.0534 - val_loss: 2.4538
Epoch 527/5000
26/26 - 1s - loss: 2.0494 - val_loss: 2.4518
Epoch 528/5000
26/26 - 1s - loss: 2.0476 - val_loss: 2.4496
Epoch 529/5000
26/26 - 1s - loss: 2.0446 - val_loss: 2.4469
Epoch 530/5000
26/26 - 1s - loss: 2.0410 - val_loss: 2.4457
Epoch 00530: val_loss improved from 2.46377 to 2.44568, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 531/5000
26/26 - 1s - loss: 2.0393 - val_loss: 2.4443
Epoch 532/5000
26/26 - 1s - loss: 2.0406 - val_loss: 2.4436
Epoch 533/5000
26/26 - 1s - loss: 2.0369 - val_loss: 2.4413
Epoch 534/5000
26/26 - 1s - loss: 2.0345 - val_loss: 2.4395
Epoch 535/5000
26/26 - 2s - loss: 2.0311 - val_loss: 2.4376
Epoch 536/5000
26/26 - 1s - loss: 2.0302 - val_loss: 2.4361
Epoch 537/5000
26/26 - 1s - loss: 2.0290 - val_loss: 2.4347
Epoch 538/5000
26/26 - 1s - loss: 2.0267 - val_loss: 2.4310
Epoch 539/5000
26/26 - 1s - loss: 2.0234 - val_loss: 2.4290
Epoch 540/5000
26/26 - 2s - loss: 2.0216 - val_loss: 2.4280
Epoch 00540: val_loss improved from 2.44568 to 2.42797, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 541/5000
26/26 - 1s - loss: 2.0217 - val_loss: 2.4263
Epoch 542/5000
26/26 - 1s - loss: 2.0196 - val_loss: 2.4251
Epoch 543/5000
26/26 - 1s - loss: 2.0169 - val_loss: 2.4218
Epoch 544/5000
26/26 - 1s - loss: 2.0170 - val_loss: 2.4199
Epoch 545/5000
26/26 - 1s - loss: 2.0122 - val_loss: 2.4186
Epoch 546/5000
26/26 - 1s - loss: 2.0109 - val_loss: 2.4169
Epoch 547/5000
26/26 - 1s - loss: 2.0086 - val_loss: 2.4142
Epoch 548/5000
26/26 - 1s - loss: 2.0095 - val_loss: 2.4125
Epoch 549/5000
26/26 - 1s - loss: 2.0067 - val_loss: 2.4099
Epoch 550/5000
26/26 - 1s - loss: 2.0046 - val_loss: 2.4077
Epoch 00550: val_loss improved from 2.42797 to 2.40767, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 551/5000
26/26 - 1s - loss: 2.0022 - val_loss: 2.4069
Epoch 552/5000
26/26 - 1s - loss: 2.0016 - val_loss: 2.4044
Epoch 553/5000
26/26 - 1s - loss: 1.9988 - val_loss: 2.4020
Epoch 554/5000
26/26 - 1s - loss: 1.9944 - val_loss: 2.3997
Epoch 555/5000
26/26 - 1s - loss: 1.9927 - val_loss: 2.3982
Epoch 556/5000
26/26 - 1s - loss: 1.9913 - val_loss: 2.3962
Epoch 557/5000
26/26 - 1s - loss: 1.9900 - val_loss: 2.3935
Epoch 558/5000
26/26 - 1s - loss: 1.9880 - val_loss: 2.3921
Epoch 559/5000
26/26 - 1s - loss: 1.9871 - val_loss: 2.3907
Epoch 560/5000
26/26 - 1s - loss: 1.9861 - val_loss: 2.3900
Epoch 00560: val_loss improved from 2.40767 to 2.39000, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 561/5000
26/26 - 1s - loss: 1.9824 - val_loss: 2.3891
Epoch 562/5000
26/26 - 1s - loss: 1.9818 - val_loss: 2.3883
Epoch 563/5000
26/26 - 1s - loss: 1.9800 - val_loss: 2.3838
Epoch 564/5000
26/26 - 1s - loss: 1.9781 - val_loss: 2.3813
Epoch 565/5000
26/26 - 1s - loss: 1.9752 - val_loss: 2.3817
Epoch 566/5000
26/26 - 1s - loss: 1.9736 - val_loss: 2.3794
Epoch 567/5000
26/26 - 1s - loss: 1.9731 - val_loss: 2.3772
Epoch 568/5000
26/26 - 1s - loss: 1.9704 - val_loss: 2.3748
Epoch 569/5000
26/26 - 1s - loss: 1.9689 - val_loss: 2.3727
Epoch 570/5000
26/26 - 1s - loss: 1.9668 - val_loss: 2.3720
Epoch 00570: val_loss improved from 2.39000 to 2.37195, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 571/5000
26/26 - 2s - loss: 1.9648 - val_loss: 2.3687
Epoch 572/5000
26/26 - 1s - loss: 1.9652 - val_loss: 2.3690
Epoch 573/5000
26/26 - 1s - loss: 1.9621 - val_loss: 2.3682
Epoch 574/5000
26/26 - 1s - loss: 1.9582 - val_loss: 2.3658
Epoch 575/5000
26/26 - 1s - loss: 1.9581 - val_loss: 2.3632
Epoch 576/5000
26/26 - 1s - loss: 1.9572 - val_loss: 2.3608
Epoch 577/5000
26/26 - 1s - loss: 1.9528 - val_loss: 2.3613
Epoch 578/5000
26/26 - 1s - loss: 1.9511 - val_loss: 2.3574
Epoch 579/5000
26/26 - 1s - loss: 1.9503 - val_loss: 2.3557
Epoch 580/5000
26/26 - 1s - loss: 1.9486 - val_loss: 2.3540
Epoch 00580: val_loss improved from 2.37195 to 2.35400, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 581/5000
26/26 - 1s - loss: 1.9450 - val_loss: 2.3532
Epoch 582/5000
26/26 - 1s - loss: 1.9445 - val_loss: 2.3515
Epoch 583/5000
26/26 - 1s - loss: 1.9423 - val_loss: 2.3486
Epoch 584/5000
26/26 - 1s - loss: 1.9416 - val_loss: 2.3465
Epoch 585/5000
26/26 - 1s - loss: 1.9392 - val_loss: 2.3455
Epoch 586/5000
26/26 - 1s - loss: 1.9379 - val_loss: 2.3436
Epoch 587/5000
26/26 - 1s - loss: 1.9358 - val_loss: 2.3407
Epoch 588/5000
26/26 - 1s - loss: 1.9350 - val_loss: 2.3394
Epoch 589/5000
26/26 - 1s - loss: 1.9337 - val_loss: 2.3377
Epoch 590/5000
26/26 - 1s - loss: 1.9301 - val_loss: 2.3359
Epoch 00590: val_loss improved from 2.35400 to 2.33593, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 591/5000
26/26 - 1s - loss: 1.9293 - val_loss: 2.3348
Epoch 592/5000
26/26 - 1s - loss: 1.9265 - val_loss: 2.3334
Epoch 593/5000
26/26 - 1s - loss: 1.9266 - val_loss: 2.3312
Epoch 594/5000
26/26 - 1s - loss: 1.9221 - val_loss: 2.3303
Epoch 595/5000
26/26 - 1s - loss: 1.9200 - val_loss: 2.3274
Epoch 596/5000
26/26 - 1s - loss: 1.9196 - val_loss: 2.3251
Epoch 597/5000
26/26 - 1s - loss: 1.9182 - val_loss: 2.3235
Epoch 598/5000
26/26 - 1s - loss: 1.9162 - val_loss: 2.3215
Epoch 599/5000
26/26 - 1s - loss: 1.9154 - val_loss: 2.3206
Epoch 600/5000
26/26 - 1s - loss: 1.9122 - val_loss: 2.3177
Epoch 00600: val_loss improved from 2.33593 to 2.31769, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 601/5000
26/26 - 1s - loss: 1.9120 - val_loss: 2.3168
Epoch 602/5000
26/26 - 1s - loss: 1.9086 - val_loss: 2.3150
Epoch 603/5000
26/26 - 1s - loss: 1.9076 - val_loss: 2.3125
Epoch 604/5000
26/26 - 1s - loss: 1.9069 - val_loss: 2.3111
Epoch 605/5000
26/26 - 1s - loss: 1.9058 - val_loss: 2.3093
Epoch 606/5000
26/26 - 1s - loss: 1.9034 - val_loss: 2.3077
Epoch 607/5000
26/26 - 1s - loss: 1.9009 - val_loss: 2.3061
Epoch 608/5000
26/26 - 1s - loss: 1.9009 - val_loss: 2.3050
Epoch 609/5000
26/26 - 1s - loss: 1.8990 - val_loss: 2.3033
Epoch 610/5000
26/26 - 1s - loss: 1.8962 - val_loss: 2.3005
Epoch 00610: val_loss improved from 2.31769 to 2.30046, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 611/5000
26/26 - 1s - loss: 1.8938 - val_loss: 2.3007
Epoch 612/5000
26/26 - 1s - loss: 1.8924 - val_loss: 2.2996
Epoch 613/5000
26/26 - 2s - loss: 1.8890 - val_loss: 2.2969
Epoch 614/5000
26/26 - 1s - loss: 1.8875 - val_loss: 2.2953
Epoch 615/5000
26/26 - 1s - loss: 1.8878 - val_loss: 2.2930
Epoch 616/5000
26/26 - 1s - loss: 1.8857 - val_loss: 2.2934
Epoch 617/5000
26/26 - 1s - loss: 1.8847 - val_loss: 2.2920
Epoch 618/5000
26/26 - 1s - loss: 1.8809 - val_loss: 2.2901
Epoch 619/5000
26/26 - 1s - loss: 1.8806 - val_loss: 2.2896
Epoch 620/5000
26/26 - 1s - loss: 1.8781 - val_loss: 2.2861
Epoch 00620: val_loss improved from 2.30046 to 2.28611, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 621/5000
26/26 - 1s - loss: 1.8770 - val_loss: 2.2850
Epoch 622/5000
26/26 - 1s - loss: 1.8760 - val_loss: 2.2837
Epoch 623/5000
26/26 - 1s - loss: 1.8731 - val_loss: 2.2814
Epoch 624/5000
26/26 - 1s - loss: 1.8728 - val_loss: 2.2798
Epoch 625/5000
26/26 - 1s - loss: 1.8699 - val_loss: 2.2787
Epoch 626/5000
26/26 - 1s - loss: 1.8676 - val_loss: 2.2771
Epoch 627/5000
26/26 - 1s - loss: 1.8665 - val_loss: 2.2762
Epoch 628/5000
26/26 - 1s - loss: 1.8643 - val_loss: 2.2731
Epoch 629/5000
26/26 - 1s - loss: 1.8641 - val_loss: 2.2720
Epoch 630/5000
26/26 - 1s - loss: 1.8609 - val_loss: 2.2708
Epoch 00630: val_loss improved from 2.28611 to 2.27080, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 631/5000
26/26 - 1s - loss: 1.8607 - val_loss: 2.2704
Epoch 632/5000
26/26 - 1s - loss: 1.8603 - val_loss: 2.2684
Epoch 633/5000
26/26 - 1s - loss: 1.8571 - val_loss: 2.2658
Epoch 634/5000
26/26 - 1s - loss: 1.8543 - val_loss: 2.2659
Epoch 635/5000
26/26 - 1s - loss: 1.8517 - val_loss: 2.2632
Epoch 636/5000
26/26 - 1s - loss: 1.8531 - val_loss: 2.2628
Epoch 637/5000
26/26 - 1s - loss: 1.8517 - val_loss: 2.2597
Epoch 638/5000
26/26 - 1s - loss: 1.8481 - val_loss: 2.2583
Epoch 639/5000
26/26 - 1s - loss: 1.8475 - val_loss: 2.2575
Epoch 640/5000
26/26 - 1s - loss: 1.8475 - val_loss: 2.2546
Epoch 00640: val_loss improved from 2.27080 to 2.25462, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 641/5000
26/26 - 1s - loss: 1.8435 - val_loss: 2.2524
Epoch 642/5000
26/26 - 1s - loss: 1.8411 - val_loss: 2.2500
Epoch 643/5000
26/26 - 1s - loss: 1.8393 - val_loss: 2.2499
Epoch 644/5000
26/26 - 1s - loss: 1.8392 - val_loss: 2.2484
Epoch 645/5000
26/26 - 1s - loss: 1.8366 - val_loss: 2.2470
Epoch 646/5000
26/26 - 1s - loss: 1.8358 - val_loss: 2.2449
Epoch 647/5000
26/26 - 1s - loss: 1.8333 - val_loss: 2.2440
Epoch 648/5000
26/26 - 1s - loss: 1.8318 - val_loss: 2.2429
Epoch 649/5000
26/26 - 1s - loss: 1.8295 - val_loss: 2.2411
Epoch 650/5000
26/26 - 2s - loss: 1.8305 - val_loss: 2.2378
Epoch 00650: val_loss improved from 2.25462 to 2.23779, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 651/5000
26/26 - 1s - loss: 1.8268 - val_loss: 2.2369
Epoch 652/5000
26/26 - 1s - loss: 1.8245 - val_loss: 2.2343
Epoch 653/5000
26/26 - 1s - loss: 1.8240 - val_loss: 2.2326
Epoch 654/5000
26/26 - 2s - loss: 1.8249 - val_loss: 2.2308
Epoch 655/5000
26/26 - 1s - loss: 1.8191 - val_loss: 2.2307
Epoch 656/5000
26/26 - 1s - loss: 1.8188 - val_loss: 2.2299
Epoch 657/5000
26/26 - 1s - loss: 1.8190 - val_loss: 2.2267
Epoch 658/5000
26/26 - 1s - loss: 1.8148 - val_loss: 2.2271
Epoch 659/5000
26/26 - 1s - loss: 1.8157 - val_loss: 2.2250
Epoch 660/5000
26/26 - 1s - loss: 1.8138 - val_loss: 2.2235
Epoch 00660: val_loss improved from 2.23779 to 2.22349, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 661/5000
26/26 - 1s - loss: 1.8144 - val_loss: 2.2196
Epoch 662/5000
26/26 - 1s - loss: 1.8116 - val_loss: 2.2189
Epoch 663/5000
26/26 - 1s - loss: 1.8093 - val_loss: 2.2182
Epoch 664/5000
26/26 - 1s - loss: 1.8070 - val_loss: 2.2171
Epoch 665/5000
26/26 - 1s - loss: 1.8065 - val_loss: 2.2145
Epoch 666/5000
26/26 - 1s - loss: 1.8049 - val_loss: 2.2138
Epoch 667/5000
26/26 - 1s - loss: 1.8020 - val_loss: 2.2126
Epoch 668/5000
26/26 - 1s - loss: 1.8004 - val_loss: 2.2109
Epoch 669/5000
26/26 - 1s - loss: 1.8003 - val_loss: 2.2086
Epoch 670/5000
26/26 - 1s - loss: 1.7979 - val_loss: 2.2088
Epoch 00670: val_loss improved from 2.22349 to 2.20877, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 671/5000
26/26 - 1s - loss: 1.7960 - val_loss: 2.2065
Epoch 672/5000
26/26 - 1s - loss: 1.7952 - val_loss: 2.2071
Epoch 673/5000
26/26 - 1s - loss: 1.7934 - val_loss: 2.2042
Epoch 674/5000
26/26 - 1s - loss: 1.7908 - val_loss: 2.2022
Epoch 675/5000
26/26 - 2s - loss: 1.7889 - val_loss: 2.1988
Epoch 676/5000
26/26 - 1s - loss: 1.7883 - val_loss: 2.1994
Epoch 677/5000
26/26 - 1s - loss: 1.7841 - val_loss: 2.1990
Epoch 678/5000
26/26 - 1s - loss: 1.7847 - val_loss: 2.1960
Epoch 679/5000
26/26 - 1s - loss: 1.7852 - val_loss: 2.1947
Epoch 680/5000
26/26 - 1s - loss: 1.7805 - val_loss: 2.1927
Epoch 00680: val_loss improved from 2.20877 to 2.19273, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 681/5000
26/26 - 1s - loss: 1.7785 - val_loss: 2.1917
Epoch 682/5000
26/26 - 1s - loss: 1.7778 - val_loss: 2.1885
Epoch 683/5000
26/26 - 1s - loss: 1.7775 - val_loss: 2.1877
Epoch 684/5000
26/26 - 1s - loss: 1.7746 - val_loss: 2.1865
Epoch 685/5000
26/26 - 1s - loss: 1.7740 - val_loss: 2.1840
Epoch 686/5000
26/26 - 1s - loss: 1.7706 - val_loss: 2.1836
Epoch 687/5000
26/26 - 1s - loss: 1.7714 - val_loss: 2.1820
Epoch 688/5000
26/26 - 1s - loss: 1.7702 - val_loss: 2.1800
Epoch 689/5000
26/26 - 1s - loss: 1.7669 - val_loss: 2.1787
Epoch 690/5000
26/26 - 1s - loss: 1.7674 - val_loss: 2.1778
Epoch 00690: val_loss improved from 2.19273 to 2.17780, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 691/5000
26/26 - 1s - loss: 1.7637 - val_loss: 2.1751
Epoch 692/5000
26/26 - 1s - loss: 1.7630 - val_loss: 2.1759
Epoch 693/5000
26/26 - 1s - loss: 1.7638 - val_loss: 2.1731
Epoch 694/5000
26/26 - 1s - loss: 1.7597 - val_loss: 2.1732
Epoch 695/5000
26/26 - 1s - loss: 1.7597 - val_loss: 2.1712
Epoch 696/5000
26/26 - 1s - loss: 1.7562 - val_loss: 2.1680
Epoch 697/5000
26/26 - 1s - loss: 1.7545 - val_loss: 2.1658
Epoch 698/5000
26/26 - 1s - loss: 1.7544 - val_loss: 2.1657
Epoch 699/5000
26/26 - 1s - loss: 1.7531 - val_loss: 2.1650
Epoch 700/5000
26/26 - 1s - loss: 1.7521 - val_loss: 2.1624
Epoch 00700: val_loss improved from 2.17780 to 2.16238, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 701/5000
26/26 - 1s - loss: 1.7512 - val_loss: 2.1615
Epoch 702/5000
26/26 - 1s - loss: 1.7482 - val_loss: 2.1598
Epoch 703/5000
26/26 - 1s - loss: 1.7438 - val_loss: 2.1568
Epoch 704/5000
26/26 - 1s - loss: 1.7457 - val_loss: 2.1567
Epoch 705/5000
26/26 - 1s - loss: 1.7447 - val_loss: 2.1543
Epoch 706/5000
26/26 - 1s - loss: 1.7415 - val_loss: 2.1536
Epoch 707/5000
26/26 - 1s - loss: 1.7404 - val_loss: 2.1528
Epoch 708/5000
26/26 - 1s - loss: 1.7390 - val_loss: 2.1529
Epoch 709/5000
26/26 - 1s - loss: 1.7366 - val_loss: 2.1502
Epoch 710/5000
26/26 - 1s - loss: 1.7360 - val_loss: 2.1465
Epoch 00710: val_loss improved from 2.16238 to 2.14653, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 711/5000
26/26 - 1s - loss: 1.7336 - val_loss: 2.1468
Epoch 712/5000
26/26 - 1s - loss: 1.7321 - val_loss: 2.1450
Epoch 713/5000
26/26 - 1s - loss: 1.7319 - val_loss: 2.1439
Epoch 714/5000
26/26 - 1s - loss: 1.7300 - val_loss: 2.1421
Epoch 715/5000
26/26 - 1s - loss: 1.7286 - val_loss: 2.1409
Epoch 716/5000
26/26 - 1s - loss: 1.7264 - val_loss: 2.1394
Epoch 717/5000
26/26 - 1s - loss: 1.7269 - val_loss: 2.1385
Epoch 718/5000
26/26 - 1s - loss: 1.7244 - val_loss: 2.1371
Epoch 719/5000
26/26 - 1s - loss: 1.7247 - val_loss: 2.1358
Epoch 720/5000
26/26 - 1s - loss: 1.7197 - val_loss: 2.1324
Epoch 00720: val_loss improved from 2.14653 to 2.13239, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 721/5000
26/26 - 1s - loss: 1.7195 - val_loss: 2.1318
Epoch 722/5000
26/26 - 1s - loss: 1.7194 - val_loss: 2.1308
Epoch 723/5000
26/26 - 1s - loss: 1.7148 - val_loss: 2.1292
Epoch 724/5000
26/26 - 2s - loss: 1.7167 - val_loss: 2.1268
Epoch 725/5000
26/26 - 1s - loss: 1.7159 - val_loss: 2.1256
Epoch 726/5000
26/26 - 1s - loss: 1.7133 - val_loss: 2.1257
Epoch 727/5000
26/26 - 1s - loss: 1.7114 - val_loss: 2.1228
Epoch 728/5000
26/26 - 1s - loss: 1.7099 - val_loss: 2.1199
Epoch 729/5000
26/26 - 1s - loss: 1.7080 - val_loss: 2.1197
Epoch 730/5000
26/26 - 1s - loss: 1.7054 - val_loss: 2.1185
Epoch 00730: val_loss improved from 2.13239 to 2.11850, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 731/5000
26/26 - 1s - loss: 1.7058 - val_loss: 2.1155
Epoch 732/5000
26/26 - 1s - loss: 1.7032 - val_loss: 2.1142
Epoch 733/5000
26/26 - 1s - loss: 1.7024 - val_loss: 2.1139
Epoch 734/5000
26/26 - 1s - loss: 1.7028 - val_loss: 2.1129
Epoch 735/5000
26/26 - 1s - loss: 1.7000 - val_loss: 2.1131
Epoch 736/5000
26/26 - 1s - loss: 1.6991 - val_loss: 2.1116
Epoch 737/5000
26/26 - 2s - loss: 1.6964 - val_loss: 2.1107
Epoch 738/5000
26/26 - 1s - loss: 1.6952 - val_loss: 2.1094
Epoch 739/5000
26/26 - 1s - loss: 1.6950 - val_loss: 2.1083
Epoch 740/5000
26/26 - 1s - loss: 1.6893 - val_loss: 2.1049
Epoch 00740: val_loss improved from 2.11850 to 2.10485, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 741/5000
26/26 - 1s - loss: 1.6919 - val_loss: 2.1047
Epoch 742/5000
26/26 - 1s - loss: 1.6909 - val_loss: 2.1026
Epoch 743/5000
26/26 - 1s - loss: 1.6892 - val_loss: 2.1008
Epoch 744/5000
26/26 - 1s - loss: 1.6859 - val_loss: 2.0981
Epoch 745/5000
26/26 - 1s - loss: 1.6855 - val_loss: 2.0976
Epoch 746/5000
26/26 - 1s - loss: 1.6834 - val_loss: 2.0959
Epoch 747/5000
26/26 - 1s - loss: 1.6848 - val_loss: 2.0947
Epoch 748/5000
26/26 - 1s - loss: 1.6801 - val_loss: 2.0937
Epoch 749/5000
26/26 - 1s - loss: 1.6790 - val_loss: 2.0936
Epoch 750/5000
26/26 - 1s - loss: 1.6781 - val_loss: 2.0915
Epoch 00750: val_loss improved from 2.10485 to 2.09152, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 751/5000
26/26 - 1s - loss: 1.6771 - val_loss: 2.0883
Epoch 752/5000
26/26 - 1s - loss: 1.6770 - val_loss: 2.0867
Epoch 753/5000
26/26 - 1s - loss: 1.6734 - val_loss: 2.0863
Epoch 754/5000
26/26 - 1s - loss: 1.6725 - val_loss: 2.0874
Epoch 755/5000
26/26 - 1s - loss: 1.6720 - val_loss: 2.0839
Epoch 756/5000
26/26 - 1s - loss: 1.6697 - val_loss: 2.0833
Epoch 757/5000
26/26 - 1s - loss: 1.6688 - val_loss: 2.0818
Epoch 758/5000
26/26 - 1s - loss: 1.6682 - val_loss: 2.0817
Epoch 759/5000
26/26 - 1s - loss: 1.6650 - val_loss: 2.0791
Epoch 760/5000
26/26 - 1s - loss: 1.6640 - val_loss: 2.0768
Epoch 00760: val_loss improved from 2.09152 to 2.07680, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 761/5000
26/26 - 1s - loss: 1.6646 - val_loss: 2.0756
Epoch 762/5000
26/26 - 1s - loss: 1.6614 - val_loss: 2.0752
Epoch 763/5000
26/26 - 1s - loss: 1.6592 - val_loss: 2.0736
Epoch 764/5000
26/26 - 1s - loss: 1.6600 - val_loss: 2.0724
Epoch 765/5000
26/26 - 1s - loss: 1.6589 - val_loss: 2.0705
Epoch 766/5000
26/26 - 1s - loss: 1.6576 - val_loss: 2.0706
Epoch 767/5000
26/26 - 1s - loss: 1.6560 - val_loss: 2.0681
Epoch 768/5000
26/26 - 1s - loss: 1.6546 - val_loss: 2.0657
Epoch 769/5000
26/26 - 1s - loss: 1.6512 - val_loss: 2.0653
Epoch 770/5000
26/26 - 1s - loss: 1.6497 - val_loss: 2.0634
Epoch 00770: val_loss improved from 2.07680 to 2.06342, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 771/5000
26/26 - 1s - loss: 1.6492 - val_loss: 2.0617
Epoch 772/5000
26/26 - 1s - loss: 1.6488 - val_loss: 2.0608
Epoch 773/5000
26/26 - 1s - loss: 1.6475 - val_loss: 2.0590
Epoch 774/5000
26/26 - 1s - loss: 1.6454 - val_loss: 2.0588
Epoch 775/5000
26/26 - 1s - loss: 1.6429 - val_loss: 2.0560
Epoch 776/5000
26/26 - 1s - loss: 1.6421 - val_loss: 2.0547
Epoch 777/5000
26/26 - 1s - loss: 1.6407 - val_loss: 2.0533
Epoch 778/5000
26/26 - 2s - loss: 1.6381 - val_loss: 2.0530
Epoch 779/5000
26/26 - 1s - loss: 1.6391 - val_loss: 2.0518
Epoch 780/5000
26/26 - 1s - loss: 1.6376 - val_loss: 2.0523
Epoch 00780: val_loss improved from 2.06342 to 2.05233, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 781/5000
26/26 - 1s - loss: 1.6360 - val_loss: 2.0505
Epoch 782/5000
26/26 - 1s - loss: 1.6336 - val_loss: 2.0492
Epoch 783/5000
26/26 - 1s - loss: 1.6320 - val_loss: 2.0472
Epoch 784/5000
26/26 - 2s - loss: 1.6313 - val_loss: 2.0445
Epoch 785/5000
26/26 - 1s - loss: 1.6293 - val_loss: 2.0440
Epoch 786/5000
26/26 - 1s - loss: 1.6294 - val_loss: 2.0408
Epoch 787/5000
26/26 - 1s - loss: 1.6271 - val_loss: 2.0409
Epoch 788/5000
26/26 - 1s - loss: 1.6250 - val_loss: 2.0389
Epoch 789/5000
26/26 - 1s - loss: 1.6244 - val_loss: 2.0380
Epoch 790/5000
26/26 - 1s - loss: 1.6237 - val_loss: 2.0363
Epoch 00790: val_loss improved from 2.05233 to 2.03630, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 791/5000
26/26 - 1s - loss: 1.6190 - val_loss: 2.0361
Epoch 792/5000
26/26 - 1s - loss: 1.6201 - val_loss: 2.0353
Epoch 793/5000
26/26 - 1s - loss: 1.6204 - val_loss: 2.0332
Epoch 794/5000
26/26 - 1s - loss: 1.6180 - val_loss: 2.0317
Epoch 795/5000
26/26 - 1s - loss: 1.6178 - val_loss: 2.0302
Epoch 796/5000
26/26 - 1s - loss: 1.6144 - val_loss: 2.0303
Epoch 797/5000
26/26 - 1s - loss: 1.6166 - val_loss: 2.0287
Epoch 798/5000
26/26 - 1s - loss: 1.6134 - val_loss: 2.0271
Epoch 799/5000
26/26 - 1s - loss: 1.6103 - val_loss: 2.0267
Epoch 800/5000
26/26 - 1s - loss: 1.6106 - val_loss: 2.0246
Epoch 00800: val_loss improved from 2.03630 to 2.02457, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 801/5000
26/26 - 1s - loss: 1.6085 - val_loss: 2.0237
Epoch 802/5000
26/26 - 1s - loss: 1.6070 - val_loss: 2.0220
Epoch 803/5000
26/26 - 1s - loss: 1.6074 - val_loss: 2.0208
Epoch 804/5000
26/26 - 1s - loss: 1.6039 - val_loss: 2.0181
Epoch 805/5000
26/26 - 1s - loss: 1.6032 - val_loss: 2.0181
Epoch 806/5000
26/26 - 1s - loss: 1.6009 - val_loss: 2.0159
Epoch 807/5000
26/26 - 1s - loss: 1.6017 - val_loss: 2.0153
Epoch 808/5000
26/26 - 1s - loss: 1.6012 - val_loss: 2.0135
Epoch 809/5000
26/26 - 1s - loss: 1.5969 - val_loss: 2.0123
Epoch 810/5000
26/26 - 1s - loss: 1.5968 - val_loss: 2.0125
Epoch 00810: val_loss improved from 2.02457 to 2.01255, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 811/5000
26/26 - 1s - loss: 1.5951 - val_loss: 2.0103
Epoch 812/5000
26/26 - 1s - loss: 1.5945 - val_loss: 2.0099
Epoch 813/5000
26/26 - 1s - loss: 1.5918 - val_loss: 2.0073
Epoch 814/5000
26/26 - 1s - loss: 1.5907 - val_loss: 2.0040
Epoch 815/5000
26/26 - 1s - loss: 1.5915 - val_loss: 2.0057
Epoch 816/5000
26/26 - 1s - loss: 1.5889 - val_loss: 2.0038
Epoch 817/5000
26/26 - 1s - loss: 1.5869 - val_loss: 2.0026
Epoch 818/5000
26/26 - 1s - loss: 1.5867 - val_loss: 2.0015
Epoch 819/5000
26/26 - 1s - loss: 1.5861 - val_loss: 2.0015
Epoch 820/5000
26/26 - 2s - loss: 1.5853 - val_loss: 2.0003
Epoch 00820: val_loss improved from 2.01255 to 2.00030, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 821/5000
26/26 - 1s - loss: 1.5835 - val_loss: 1.9969
Epoch 822/5000
26/26 - 1s - loss: 1.5816 - val_loss: 1.9960
Epoch 823/5000
26/26 - 1s - loss: 1.5814 - val_loss: 1.9964
Epoch 824/5000
26/26 - 1s - loss: 1.5787 - val_loss: 1.9966
Epoch 825/5000
26/26 - 1s - loss: 1.5791 - val_loss: 1.9931
Epoch 826/5000
26/26 - 1s - loss: 1.5769 - val_loss: 1.9904
Epoch 827/5000
26/26 - 1s - loss: 1.5738 - val_loss: 1.9899
Epoch 828/5000
26/26 - 1s - loss: 1.5734 - val_loss: 1.9885
Epoch 829/5000
26/26 - 1s - loss: 1.5737 - val_loss: 1.9883
Epoch 830/5000
26/26 - 1s - loss: 1.5713 - val_loss: 1.9857
Epoch 00830: val_loss improved from 2.00030 to 1.98574, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 831/5000
26/26 - 1s - loss: 1.5687 - val_loss: 1.9850
Epoch 832/5000
26/26 - 1s - loss: 1.5694 - val_loss: 1.9866
Epoch 833/5000
26/26 - 1s - loss: 1.5695 - val_loss: 1.9843
Epoch 834/5000
26/26 - 1s - loss: 1.5665 - val_loss: 1.9811
Epoch 835/5000
26/26 - 1s - loss: 1.5656 - val_loss: 1.9796
Epoch 836/5000
26/26 - 1s - loss: 1.5645 - val_loss: 1.9785
Epoch 837/5000
26/26 - 1s - loss: 1.5627 - val_loss: 1.9786
Epoch 838/5000
26/26 - 1s - loss: 1.5628 - val_loss: 1.9774
Epoch 839/5000
26/26 - 1s - loss: 1.5598 - val_loss: 1.9761
Epoch 840/5000
26/26 - 1s - loss: 1.5606 - val_loss: 1.9748
Epoch 00840: val_loss improved from 1.98574 to 1.97479, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 841/5000
26/26 - 1s - loss: 1.5581 - val_loss: 1.9723
Epoch 842/5000
26/26 - 1s - loss: 1.5553 - val_loss: 1.9700
Epoch 843/5000
26/26 - 1s - loss: 1.5525 - val_loss: 1.9702
Epoch 844/5000
26/26 - 2s - loss: 1.5524 - val_loss: 1.9693
Epoch 845/5000
26/26 - 1s - loss: 1.5526 - val_loss: 1.9673
Epoch 846/5000
26/26 - 1s - loss: 1.5511 - val_loss: 1.9663
Epoch 847/5000
26/26 - 1s - loss: 1.5507 - val_loss: 1.9642
Epoch 848/5000
26/26 - 1s - loss: 1.5490 - val_loss: 1.9642
Epoch 849/5000
26/26 - 1s - loss: 1.5500 - val_loss: 1.9617
Epoch 850/5000
26/26 - 1s - loss: 1.5465 - val_loss: 1.9610
Epoch 00850: val_loss improved from 1.97479 to 1.96096, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 851/5000
26/26 - 1s - loss: 1.5462 - val_loss: 1.9612
Epoch 852/5000
26/26 - 1s - loss: 1.5450 - val_loss: 1.9608
Epoch 853/5000
26/26 - 1s - loss: 1.5426 - val_loss: 1.9569
Epoch 854/5000
26/26 - 1s - loss: 1.5425 - val_loss: 1.9573
Epoch 855/5000
26/26 - 1s - loss: 1.5409 - val_loss: 1.9556
Epoch 856/5000
26/26 - 1s - loss: 1.5393 - val_loss: 1.9544
Epoch 857/5000
26/26 - 1s - loss: 1.5401 - val_loss: 1.9545
Epoch 858/5000
26/26 - 1s - loss: 1.5386 - val_loss: 1.9513
Epoch 859/5000
26/26 - 1s - loss: 1.5362 - val_loss: 1.9512
Epoch 860/5000
26/26 - 1s - loss: 1.5359 - val_loss: 1.9498
Epoch 00860: val_loss improved from 1.96096 to 1.94976, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 861/5000
26/26 - 1s - loss: 1.5309 - val_loss: 1.9494
Epoch 862/5000
26/26 - 1s - loss: 1.5315 - val_loss: 1.9483
Epoch 863/5000
26/26 - 1s - loss: 1.5298 - val_loss: 1.9459
Epoch 864/5000
26/26 - 1s - loss: 1.5305 - val_loss: 1.9453
Epoch 865/5000
26/26 - 1s - loss: 1.5294 - val_loss: 1.9426
Epoch 866/5000
26/26 - 1s - loss: 1.5258 - val_loss: 1.9420
Epoch 867/5000
26/26 - 1s - loss: 1.5259 - val_loss: 1.9419
Epoch 868/5000
26/26 - 1s - loss: 1.5271 - val_loss: 1.9405
Epoch 869/5000
26/26 - 1s - loss: 1.5244 - val_loss: 1.9397
Epoch 870/5000
26/26 - 1s - loss: 1.5233 - val_loss: 1.9388
Epoch 00870: val_loss improved from 1.94976 to 1.93880, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 871/5000
26/26 - 1s - loss: 1.5219 - val_loss: 1.9390
Epoch 872/5000
26/26 - 1s - loss: 1.5202 - val_loss: 1.9352
Epoch 873/5000
26/26 - 1s - loss: 1.5189 - val_loss: 1.9342
Epoch 874/5000
26/26 - 1s - loss: 1.5168 - val_loss: 1.9339
Epoch 875/5000
26/26 - 1s - loss: 1.5165 - val_loss: 1.9310
Epoch 876/5000
26/26 - 1s - loss: 1.5145 - val_loss: 1.9309
Epoch 877/5000
26/26 - 1s - loss: 1.5123 - val_loss: 1.9299
Epoch 878/5000
26/26 - 1s - loss: 1.5149 - val_loss: 1.9274
Epoch 879/5000
26/26 - 1s - loss: 1.5120 - val_loss: 1.9273
Epoch 880/5000
26/26 - 1s - loss: 1.5098 - val_loss: 1.9263
Epoch 00880: val_loss improved from 1.93880 to 1.92627, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 881/5000
26/26 - 1s - loss: 1.5109 - val_loss: 1.9258
Epoch 882/5000
26/26 - 1s - loss: 1.5066 - val_loss: 1.9243
Epoch 883/5000
26/26 - 1s - loss: 1.5084 - val_loss: 1.9221
Epoch 884/5000
26/26 - 1s - loss: 1.5055 - val_loss: 1.9207
Epoch 885/5000
26/26 - 1s - loss: 1.5033 - val_loss: 1.9210
Epoch 886/5000
26/26 - 1s - loss: 1.5023 - val_loss: 1.9190
Epoch 887/5000
26/26 - 1s - loss: 1.5012 - val_loss: 1.9169
Epoch 888/5000
26/26 - 1s - loss: 1.4999 - val_loss: 1.9151
Epoch 889/5000
26/26 - 1s - loss: 1.4988 - val_loss: 1.9146
Epoch 890/5000
26/26 - 1s - loss: 1.4983 - val_loss: 1.9128
Epoch 00890: val_loss improved from 1.92627 to 1.91275, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 891/5000
26/26 - 1s - loss: 1.4960 - val_loss: 1.9130
Epoch 892/5000
26/26 - 1s - loss: 1.4966 - val_loss: 1.9119
Epoch 893/5000
26/26 - 1s - loss: 1.4949 - val_loss: 1.9115
Epoch 894/5000
26/26 - 1s - loss: 1.4942 - val_loss: 1.9106
Epoch 895/5000
26/26 - 1s - loss: 1.4900 - val_loss: 1.9094
Epoch 896/5000
26/26 - 1s - loss: 1.4922 - val_loss: 1.9075
Epoch 897/5000
26/26 - 1s - loss: 1.4907 - val_loss: 1.9062
Epoch 898/5000
26/26 - 1s - loss: 1.4914 - val_loss: 1.9053
Epoch 899/5000
26/26 - 1s - loss: 1.4865 - val_loss: 1.9070
Epoch 900/5000
26/26 - 1s - loss: 1.4855 - val_loss: 1.9040
Epoch 00900: val_loss improved from 1.91275 to 1.90399, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 901/5000
26/26 - 1s - loss: 1.4852 - val_loss: 1.9033
Epoch 902/5000
26/26 - 1s - loss: 1.4835 - val_loss: 1.9000
Epoch 903/5000
26/26 - 2s - loss: 1.4826 - val_loss: 1.8985
Epoch 904/5000
26/26 - 1s - loss: 1.4835 - val_loss: 1.8984
Epoch 905/5000
26/26 - 1s - loss: 1.4810 - val_loss: 1.8971
Epoch 906/5000
26/26 - 1s - loss: 1.4791 - val_loss: 1.8964
Epoch 907/5000
26/26 - 1s - loss: 1.4810 - val_loss: 1.8943
Epoch 908/5000
26/26 - 1s - loss: 1.4782 - val_loss: 1.8933
Epoch 909/5000
26/26 - 1s - loss: 1.4762 - val_loss: 1.8922
Epoch 910/5000
26/26 - 1s - loss: 1.4755 - val_loss: 1.8919
Epoch 00910: val_loss improved from 1.90399 to 1.89189, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 911/5000
26/26 - 1s - loss: 1.4761 - val_loss: 1.8900
Epoch 912/5000
26/26 - 1s - loss: 1.4737 - val_loss: 1.8890
Epoch 913/5000
26/26 - 1s - loss: 1.4716 - val_loss: 1.8869
Epoch 914/5000
26/26 - 2s - loss: 1.4707 - val_loss: 1.8868
Epoch 915/5000
26/26 - 1s - loss: 1.4684 - val_loss: 1.8852
Epoch 916/5000
26/26 - 1s - loss: 1.4701 - val_loss: 1.8843
Epoch 917/5000
26/26 - 1s - loss: 1.4672 - val_loss: 1.8834
Epoch 918/5000
26/26 - 1s - loss: 1.4643 - val_loss: 1.8815
Epoch 919/5000
26/26 - 1s - loss: 1.4663 - val_loss: 1.8804
Epoch 920/5000
26/26 - 1s - loss: 1.4637 - val_loss: 1.8804
Epoch 00920: val_loss improved from 1.89189 to 1.88038, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 921/5000
26/26 - 1s - loss: 1.4623 - val_loss: 1.8775
Epoch 922/5000
26/26 - 1s - loss: 1.4598 - val_loss: 1.8782
Epoch 923/5000
26/26 - 1s - loss: 1.4599 - val_loss: 1.8761
Epoch 924/5000
26/26 - 1s - loss: 1.4599 - val_loss: 1.8748
Epoch 925/5000
26/26 - 1s - loss: 1.4577 - val_loss: 1.8750
Epoch 926/5000
26/26 - 1s - loss: 1.4582 - val_loss: 1.8745
Epoch 927/5000
26/26 - 1s - loss: 1.4545 - val_loss: 1.8712
Epoch 928/5000
26/26 - 1s - loss: 1.4543 - val_loss: 1.8706
Epoch 929/5000
26/26 - 1s - loss: 1.4550 - val_loss: 1.8696
Epoch 930/5000
26/26 - 1s - loss: 1.4530 - val_loss: 1.8679
Epoch 00930: val_loss improved from 1.88038 to 1.86791, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 931/5000
26/26 - 1s - loss: 1.4510 - val_loss: 1.8677
Epoch 932/5000
26/26 - 1s - loss: 1.4491 - val_loss: 1.8655
Epoch 933/5000
26/26 - 1s - loss: 1.4482 - val_loss: 1.8649
Epoch 934/5000
26/26 - 1s - loss: 1.4491 - val_loss: 1.8656
Epoch 935/5000
26/26 - 1s - loss: 1.4477 - val_loss: 1.8644
Epoch 936/5000
26/26 - 1s - loss: 1.4473 - val_loss: 1.8635
Epoch 937/5000
26/26 - 1s - loss: 1.4439 - val_loss: 1.8639
Epoch 938/5000
26/26 - 1s - loss: 1.4430 - val_loss: 1.8614
Epoch 939/5000
26/26 - 1s - loss: 1.4412 - val_loss: 1.8594
Epoch 940/5000
26/26 - 1s - loss: 1.4427 - val_loss: 1.8581
Epoch 00940: val_loss improved from 1.86791 to 1.85812, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 941/5000
26/26 - 1s - loss: 1.4397 - val_loss: 1.8571
Epoch 942/5000
26/26 - 1s - loss: 1.4406 - val_loss: 1.8561
Epoch 943/5000
26/26 - 1s - loss: 1.4376 - val_loss: 1.8545
Epoch 944/5000
26/26 - 1s - loss: 1.4371 - val_loss: 1.8543
Epoch 945/5000
26/26 - 1s - loss: 1.4363 - val_loss: 1.8541
Epoch 946/5000
26/26 - 1s - loss: 1.4359 - val_loss: 1.8519
Epoch 947/5000
26/26 - 1s - loss: 1.4353 - val_loss: 1.8510
Epoch 948/5000
26/26 - 1s - loss: 1.4318 - val_loss: 1.8486
Epoch 949/5000
26/26 - 1s - loss: 1.4328 - val_loss: 1.8481
Epoch 950/5000
26/26 - 1s - loss: 1.4297 - val_loss: 1.8470
Epoch 00950: val_loss improved from 1.85812 to 1.84700, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 951/5000
26/26 - 1s - loss: 1.4307 - val_loss: 1.8459
Epoch 952/5000
26/26 - 1s - loss: 1.4281 - val_loss: 1.8439
Epoch 953/5000
26/26 - 1s - loss: 1.4277 - val_loss: 1.8451
Epoch 954/5000
26/26 - 1s - loss: 1.4264 - val_loss: 1.8433
Epoch 955/5000
26/26 - 1s - loss: 1.4247 - val_loss: 1.8433
Epoch 956/5000
26/26 - 1s - loss: 1.4237 - val_loss: 1.8411
Epoch 957/5000
26/26 - 1s - loss: 1.4233 - val_loss: 1.8392
Epoch 958/5000
26/26 - 1s - loss: 1.4234 - val_loss: 1.8390
Epoch 959/5000
26/26 - 1s - loss: 1.4199 - val_loss: 1.8381
Epoch 960/5000
26/26 - 1s - loss: 1.4211 - val_loss: 1.8373
Epoch 00960: val_loss improved from 1.84700 to 1.83730, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 961/5000
26/26 - 1s - loss: 1.4188 - val_loss: 1.8357
Epoch 962/5000
26/26 - 1s - loss: 1.4179 - val_loss: 1.8355
Epoch 963/5000
26/26 - 1s - loss: 1.4168 - val_loss: 1.8339
Epoch 964/5000
26/26 - 1s - loss: 1.4170 - val_loss: 1.8341
Epoch 965/5000
26/26 - 1s - loss: 1.4140 - val_loss: 1.8323
Epoch 966/5000
26/26 - 1s - loss: 1.4134 - val_loss: 1.8300
Epoch 967/5000
26/26 - 1s - loss: 1.4121 - val_loss: 1.8310
Epoch 968/5000
26/26 - 1s - loss: 1.4104 - val_loss: 1.8291
Epoch 969/5000
26/26 - 1s - loss: 1.4091 - val_loss: 1.8283
Epoch 970/5000
26/26 - 1s - loss: 1.4083 - val_loss: 1.8275
Epoch 00970: val_loss improved from 1.83730 to 1.82753, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 971/5000
26/26 - 1s - loss: 1.4072 - val_loss: 1.8256
Epoch 972/5000
26/26 - 1s - loss: 1.4058 - val_loss: 1.8254
Epoch 973/5000
26/26 - 1s - loss: 1.4041 - val_loss: 1.8251
Epoch 974/5000
26/26 - 1s - loss: 1.4044 - val_loss: 1.8237
Epoch 975/5000
26/26 - 1s - loss: 1.4042 - val_loss: 1.8218
Epoch 976/5000
26/26 - 1s - loss: 1.4017 - val_loss: 1.8199
Epoch 977/5000
26/26 - 1s - loss: 1.4020 - val_loss: 1.8188
Epoch 978/5000
26/26 - 1s - loss: 1.4006 - val_loss: 1.8177
Epoch 979/5000
26/26 - 1s - loss: 1.3981 - val_loss: 1.8179
Epoch 980/5000
26/26 - 1s - loss: 1.3974 - val_loss: 1.8178
Epoch 00980: val_loss improved from 1.82753 to 1.81779, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 981/5000
26/26 - 1s - loss: 1.3986 - val_loss: 1.8157
Epoch 982/5000
26/26 - 1s - loss: 1.3949 - val_loss: 1.8139
Epoch 983/5000
26/26 - 1s - loss: 1.3949 - val_loss: 1.8128
Epoch 984/5000
26/26 - 1s - loss: 1.3951 - val_loss: 1.8116
Epoch 985/5000
26/26 - 1s - loss: 1.3933 - val_loss: 1.8117
Epoch 986/5000
26/26 - 2s - loss: 1.3925 - val_loss: 1.8109
Epoch 987/5000
26/26 - 1s - loss: 1.3924 - val_loss: 1.8099
Epoch 988/5000
26/26 - 1s - loss: 1.3918 - val_loss: 1.8087
Epoch 989/5000
26/26 - 1s - loss: 1.3916 - val_loss: 1.8062
Epoch 990/5000
26/26 - 1s - loss: 1.3880 - val_loss: 1.8048
Epoch 00990: val_loss improved from 1.81779 to 1.80477, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 991/5000
26/26 - 1s - loss: 1.3873 - val_loss: 1.8043
Epoch 992/5000
26/26 - 1s - loss: 1.3862 - val_loss: 1.8034
Epoch 993/5000
26/26 - 1s - loss: 1.3837 - val_loss: 1.8021
Epoch 994/5000
26/26 - 1s - loss: 1.3839 - val_loss: 1.8016
Epoch 995/5000
26/26 - 1s - loss: 1.3825 - val_loss: 1.8011
Epoch 996/5000
26/26 - 1s - loss: 1.3823 - val_loss: 1.8006
Epoch 997/5000
26/26 - 1s - loss: 1.3814 - val_loss: 1.7991
Epoch 998/5000
26/26 - 1s - loss: 1.3793 - val_loss: 1.7988
Epoch 999/5000
26/26 - 1s - loss: 1.3782 - val_loss: 1.7963
Epoch 1000/5000
26/26 - 1s - loss: 1.3759 - val_loss: 1.7943
Epoch 01000: val_loss improved from 1.80477 to 1.79426, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1001/5000
26/26 - 1s - loss: 1.3768 - val_loss: 1.7935
Epoch 1002/5000
26/26 - 1s - loss: 1.3749 - val_loss: 1.7927
Epoch 1003/5000
26/26 - 1s - loss: 1.3744 - val_loss: 1.7935
Epoch 1004/5000
26/26 - 2s - loss: 1.3714 - val_loss: 1.7924
Epoch 1005/5000
26/26 - 1s - loss: 1.3719 - val_loss: 1.7900
Epoch 1006/5000
26/26 - 1s - loss: 1.3736 - val_loss: 1.7896
Epoch 1007/5000
26/26 - 1s - loss: 1.3703 - val_loss: 1.7886
Epoch 1008/5000
26/26 - 1s - loss: 1.3687 - val_loss: 1.7873
Epoch 1009/5000
26/26 - 1s - loss: 1.3678 - val_loss: 1.7893
Epoch 1010/5000
26/26 - 1s - loss: 1.3678 - val_loss: 1.7862
Epoch 01010: val_loss improved from 1.79426 to 1.78624, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1011/5000
26/26 - 1s - loss: 1.3671 - val_loss: 1.7831
Epoch 1012/5000
26/26 - 1s - loss: 1.3655 - val_loss: 1.7837
Epoch 1013/5000
26/26 - 1s - loss: 1.3638 - val_loss: 1.7820
Epoch 1014/5000
26/26 - 1s - loss: 1.3630 - val_loss: 1.7805
Epoch 1015/5000
26/26 - 1s - loss: 1.3631 - val_loss: 1.7797
Epoch 1016/5000
26/26 - 1s - loss: 1.3606 - val_loss: 1.7792
Epoch 1017/5000
26/26 - 1s - loss: 1.3598 - val_loss: 1.7784
Epoch 1018/5000
26/26 - 1s - loss: 1.3590 - val_loss: 1.7781
Epoch 1019/5000
26/26 - 1s - loss: 1.3598 - val_loss: 1.7772
Epoch 1020/5000
26/26 - 1s - loss: 1.3580 - val_loss: 1.7744
Epoch 01020: val_loss improved from 1.78624 to 1.77439, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1021/5000
26/26 - 1s - loss: 1.3560 - val_loss: 1.7739
Epoch 1022/5000
26/26 - 1s - loss: 1.3544 - val_loss: 1.7740
Epoch 1023/5000
26/26 - 1s - loss: 1.3532 - val_loss: 1.7724
Epoch 1024/5000
26/26 - 1s - loss: 1.3538 - val_loss: 1.7726
Epoch 1025/5000
26/26 - 2s - loss: 1.3517 - val_loss: 1.7728
Epoch 1026/5000
26/26 - 1s - loss: 1.3502 - val_loss: 1.7717
Epoch 1027/5000
26/26 - 1s - loss: 1.3512 - val_loss: 1.7699
Epoch 1028/5000
26/26 - 1s - loss: 1.3500 - val_loss: 1.7683
Epoch 1029/5000
26/26 - 1s - loss: 1.3485 - val_loss: 1.7669
Epoch 1030/5000
26/26 - 1s - loss: 1.3494 - val_loss: 1.7669
Epoch 01030: val_loss improved from 1.77439 to 1.76692, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1031/5000
26/26 - 1s - loss: 1.3455 - val_loss: 1.7652
Epoch 1032/5000
26/26 - 1s - loss: 1.3456 - val_loss: 1.7637
Epoch 1033/5000
26/26 - 1s - loss: 1.3458 - val_loss: 1.7635
Epoch 1034/5000
26/26 - 1s - loss: 1.3439 - val_loss: 1.7614
Epoch 1035/5000
26/26 - 1s - loss: 1.3425 - val_loss: 1.7612
Epoch 1036/5000
26/26 - 1s - loss: 1.3417 - val_loss: 1.7619
Epoch 1037/5000
26/26 - 1s - loss: 1.3409 - val_loss: 1.7608
Epoch 1038/5000
26/26 - 1s - loss: 1.3393 - val_loss: 1.7577
Epoch 1039/5000
26/26 - 1s - loss: 1.3380 - val_loss: 1.7563
Epoch 1040/5000
26/26 - 1s - loss: 1.3355 - val_loss: 1.7556
Epoch 01040: val_loss improved from 1.76692 to 1.75563, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1041/5000
26/26 - 1s - loss: 1.3363 - val_loss: 1.7551
Epoch 1042/5000
26/26 - 1s - loss: 1.3353 - val_loss: 1.7547
Epoch 1043/5000
26/26 - 2s - loss: 1.3348 - val_loss: 1.7540
Epoch 1044/5000
26/26 - 1s - loss: 1.3333 - val_loss: 1.7539
Epoch 1045/5000
26/26 - 1s - loss: 1.3320 - val_loss: 1.7525
Epoch 1046/5000
26/26 - 1s - loss: 1.3320 - val_loss: 1.7499
Epoch 1047/5000
26/26 - 1s - loss: 1.3306 - val_loss: 1.7493
Epoch 1048/5000
26/26 - 1s - loss: 1.3279 - val_loss: 1.7471
Epoch 1049/5000
26/26 - 1s - loss: 1.3274 - val_loss: 1.7456
Epoch 1050/5000
26/26 - 1s - loss: 1.3282 - val_loss: 1.7445
Epoch 01050: val_loss improved from 1.75563 to 1.74448, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1051/5000
26/26 - 1s - loss: 1.3271 - val_loss: 1.7451
Epoch 1052/5000
26/26 - 1s - loss: 1.3246 - val_loss: 1.7444
Epoch 1053/5000
26/26 - 1s - loss: 1.3249 - val_loss: 1.7425
Epoch 1054/5000
26/26 - 1s - loss: 1.3242 - val_loss: 1.7409
Epoch 1055/5000
26/26 - 1s - loss: 1.3235 - val_loss: 1.7423
Epoch 1056/5000
26/26 - 1s - loss: 1.3240 - val_loss: 1.7410
Epoch 1057/5000
26/26 - 1s - loss: 1.3209 - val_loss: 1.7391
Epoch 1058/5000
26/26 - 1s - loss: 1.3204 - val_loss: 1.7390
Epoch 1059/5000
26/26 - 1s - loss: 1.3201 - val_loss: 1.7376
Epoch 1060/5000
26/26 - 1s - loss: 1.3187 - val_loss: 1.7360
Epoch 01060: val_loss improved from 1.74448 to 1.73599, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1061/5000
26/26 - 1s - loss: 1.3171 - val_loss: 1.7362
Epoch 1062/5000
26/26 - 1s - loss: 1.3187 - val_loss: 1.7345
Epoch 1063/5000
26/26 - 1s - loss: 1.3160 - val_loss: 1.7340
Epoch 1064/5000
26/26 - 1s - loss: 1.3145 - val_loss: 1.7329
Epoch 1065/5000
26/26 - 1s - loss: 1.3157 - val_loss: 1.7316
Epoch 1066/5000
26/26 - 1s - loss: 1.3121 - val_loss: 1.7304
Epoch 1067/5000
26/26 - 1s - loss: 1.3095 - val_loss: 1.7298
Epoch 1068/5000
26/26 - 1s - loss: 1.3116 - val_loss: 1.7297
Epoch 1069/5000
26/26 - 2s - loss: 1.3087 - val_loss: 1.7291
Epoch 1070/5000
26/26 - 1s - loss: 1.3078 - val_loss: 1.7277
Epoch 01070: val_loss improved from 1.73599 to 1.72769, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1071/5000
26/26 - 1s - loss: 1.3083 - val_loss: 1.7270
Epoch 1072/5000
26/26 - 1s - loss: 1.3075 - val_loss: 1.7268
Epoch 1073/5000
26/26 - 1s - loss: 1.3063 - val_loss: 1.7254
Epoch 1074/5000
26/26 - 1s - loss: 1.3047 - val_loss: 1.7241
Epoch 1075/5000
26/26 - 1s - loss: 1.3047 - val_loss: 1.7226
Epoch 1076/5000
26/26 - 1s - loss: 1.3041 - val_loss: 1.7223
Epoch 1077/5000
26/26 - 1s - loss: 1.3014 - val_loss: 1.7222
Epoch 1078/5000
26/26 - 1s - loss: 1.3010 - val_loss: 1.7209
Epoch 1079/5000
26/26 - 1s - loss: 1.2998 - val_loss: 1.7208
Epoch 1080/5000
26/26 - 1s - loss: 1.3005 - val_loss: 1.7195
Epoch 01080: val_loss improved from 1.72769 to 1.71948, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1081/5000
26/26 - 1s - loss: 1.2977 - val_loss: 1.7190
Epoch 1082/5000
26/26 - 1s - loss: 1.2959 - val_loss: 1.7162
Epoch 1083/5000
26/26 - 1s - loss: 1.2962 - val_loss: 1.7172
Epoch 1084/5000
26/26 - 1s - loss: 1.2960 - val_loss: 1.7155
Epoch 1085/5000
26/26 - 1s - loss: 1.2944 - val_loss: 1.7149
Epoch 1086/5000
26/26 - 1s - loss: 1.2931 - val_loss: 1.7145
Epoch 1087/5000
26/26 - 1s - loss: 1.2902 - val_loss: 1.7127
Epoch 1088/5000
26/26 - 1s - loss: 1.2927 - val_loss: 1.7119
Epoch 1089/5000
26/26 - 1s - loss: 1.2933 - val_loss: 1.7103
Epoch 1090/5000
26/26 - 1s - loss: 1.2899 - val_loss: 1.7098
Epoch 01090: val_loss improved from 1.71948 to 1.70981, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1091/5000
26/26 - 1s - loss: 1.2895 - val_loss: 1.7093
Epoch 1092/5000
26/26 - 1s - loss: 1.2860 - val_loss: 1.7096
Epoch 1093/5000
26/26 - 1s - loss: 1.2884 - val_loss: 1.7086
Epoch 1094/5000
26/26 - 1s - loss: 1.2849 - val_loss: 1.7078
Epoch 1095/5000
26/26 - 1s - loss: 1.2843 - val_loss: 1.7055
Epoch 1096/5000
26/26 - 1s - loss: 1.2848 - val_loss: 1.7040
Epoch 1097/5000
26/26 - 1s - loss: 1.2845 - val_loss: 1.7030
Epoch 1098/5000
26/26 - 1s - loss: 1.2825 - val_loss: 1.7029
Epoch 1099/5000
26/26 - 1s - loss: 1.2820 - val_loss: 1.7004
Epoch 1100/5000
26/26 - 1s - loss: 1.2796 - val_loss: 1.6991
Epoch 01100: val_loss improved from 1.70981 to 1.69908, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1101/5000
26/26 - 1s - loss: 1.2796 - val_loss: 1.6991
Epoch 1102/5000
26/26 - 1s - loss: 1.2778 - val_loss: 1.6992
Epoch 1103/5000
26/26 - 2s - loss: 1.2773 - val_loss: 1.6983
Epoch 1104/5000
26/26 - 1s - loss: 1.2778 - val_loss: 1.6968
Epoch 1105/5000
26/26 - 1s - loss: 1.2778 - val_loss: 1.6961
Epoch 1106/5000
26/26 - 1s - loss: 1.2764 - val_loss: 1.6952
Epoch 1107/5000
26/26 - 1s - loss: 1.2728 - val_loss: 1.6953
Epoch 1108/5000
26/26 - 1s - loss: 1.2738 - val_loss: 1.6941
Epoch 1109/5000
26/26 - 1s - loss: 1.2720 - val_loss: 1.6934
Epoch 1110/5000
26/26 - 1s - loss: 1.2728 - val_loss: 1.6908
Epoch 01110: val_loss improved from 1.69908 to 1.69079, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1111/5000
26/26 - 1s - loss: 1.2697 - val_loss: 1.6893
Epoch 1112/5000
26/26 - 1s - loss: 1.2698 - val_loss: 1.6905
Epoch 1113/5000
26/26 - 1s - loss: 1.2679 - val_loss: 1.6898
Epoch 1114/5000
26/26 - 1s - loss: 1.2667 - val_loss: 1.6895
Epoch 1115/5000
26/26 - 1s - loss: 1.2683 - val_loss: 1.6870
Epoch 1116/5000
26/26 - 1s - loss: 1.2663 - val_loss: 1.6865
Epoch 1117/5000
26/26 - 1s - loss: 1.2664 - val_loss: 1.6865
Epoch 1118/5000
26/26 - 1s - loss: 1.2655 - val_loss: 1.6849
Epoch 1119/5000
26/26 - 1s - loss: 1.2634 - val_loss: 1.6829
Epoch 1120/5000
26/26 - 1s - loss: 1.2643 - val_loss: 1.6824
Epoch 01120: val_loss improved from 1.69079 to 1.68238, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1121/5000
26/26 - 1s - loss: 1.2614 - val_loss: 1.6835
Epoch 1122/5000
26/26 - 1s - loss: 1.2624 - val_loss: 1.6807
Epoch 1123/5000
26/26 - 1s - loss: 1.2611 - val_loss: 1.6791
Epoch 1124/5000
26/26 - 1s - loss: 1.2575 - val_loss: 1.6792
Epoch 1125/5000
26/26 - 2s - loss: 1.2582 - val_loss: 1.6787
Epoch 1126/5000
26/26 - 1s - loss: 1.2594 - val_loss: 1.6757
Epoch 1127/5000
26/26 - 1s - loss: 1.2561 - val_loss: 1.6773
Epoch 1128/5000
26/26 - 1s - loss: 1.2556 - val_loss: 1.6757
Epoch 1129/5000
26/26 - 1s - loss: 1.2548 - val_loss: 1.6747
Epoch 1130/5000
26/26 - 1s - loss: 1.2542 - val_loss: 1.6728
Epoch 01130: val_loss improved from 1.68238 to 1.67282, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1131/5000
26/26 - 1s - loss: 1.2522 - val_loss: 1.6719
Epoch 1132/5000
26/26 - 1s - loss: 1.2529 - val_loss: 1.6712
Epoch 1133/5000
26/26 - 2s - loss: 1.2509 - val_loss: 1.6721
Epoch 1134/5000
26/26 - 1s - loss: 1.2501 - val_loss: 1.6703
Epoch 1135/5000
26/26 - 1s - loss: 1.2496 - val_loss: 1.6702
Epoch 1136/5000
26/26 - 1s - loss: 1.2470 - val_loss: 1.6690
Epoch 1137/5000
26/26 - 1s - loss: 1.2485 - val_loss: 1.6670
Epoch 1138/5000
26/26 - 1s - loss: 1.2474 - val_loss: 1.6667
Epoch 1139/5000
26/26 - 1s - loss: 1.2469 - val_loss: 1.6652
Epoch 1140/5000
26/26 - 1s - loss: 1.2457 - val_loss: 1.6639
Epoch 01140: val_loss improved from 1.67282 to 1.66386, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1141/5000
26/26 - 1s - loss: 1.2433 - val_loss: 1.6636
Epoch 1142/5000
26/26 - 1s - loss: 1.2457 - val_loss: 1.6613
Epoch 1143/5000
26/26 - 1s - loss: 1.2440 - val_loss: 1.6617
Epoch 1144/5000
26/26 - 1s - loss: 1.2418 - val_loss: 1.6617
Epoch 1145/5000
26/26 - 1s - loss: 1.2398 - val_loss: 1.6590
Epoch 1146/5000
26/26 - 1s - loss: 1.2390 - val_loss: 1.6601
Epoch 1147/5000
26/26 - 1s - loss: 1.2402 - val_loss: 1.6584
Epoch 1148/5000
26/26 - 1s - loss: 1.2393 - val_loss: 1.6581
Epoch 1149/5000
26/26 - 2s - loss: 1.2367 - val_loss: 1.6570
Epoch 1150/5000
26/26 - 1s - loss: 1.2362 - val_loss: 1.6554
Epoch 01150: val_loss improved from 1.66386 to 1.65542, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1151/5000
26/26 - 1s - loss: 1.2341 - val_loss: 1.6557
Epoch 1152/5000
26/26 - 2s - loss: 1.2355 - val_loss: 1.6558
Epoch 1153/5000
26/26 - 1s - loss: 1.2330 - val_loss: 1.6550
Epoch 1154/5000
26/26 - 1s - loss: 1.2329 - val_loss: 1.6530
Epoch 1155/5000
26/26 - 1s - loss: 1.2330 - val_loss: 1.6525
Epoch 1156/5000
26/26 - 1s - loss: 1.2304 - val_loss: 1.6516
Epoch 1157/5000
26/26 - 1s - loss: 1.2304 - val_loss: 1.6495
Epoch 1158/5000
26/26 - 1s - loss: 1.2293 - val_loss: 1.6502
Epoch 1159/5000
26/26 - 2s - loss: 1.2281 - val_loss: 1.6498
Epoch 1160/5000
26/26 - 1s - loss: 1.2289 - val_loss: 1.6486
Epoch 01160: val_loss improved from 1.65542 to 1.64864, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1161/5000
26/26 - 1s - loss: 1.2283 - val_loss: 1.6469
Epoch 1162/5000
26/26 - 1s - loss: 1.2255 - val_loss: 1.6463
Epoch 1163/5000
26/26 - 1s - loss: 1.2249 - val_loss: 1.6457
Epoch 1164/5000
26/26 - 1s - loss: 1.2242 - val_loss: 1.6471
Epoch 1165/5000
26/26 - 1s - loss: 1.2248 - val_loss: 1.6448
Epoch 1166/5000
26/26 - 1s - loss: 1.2218 - val_loss: 1.6422
Epoch 1167/5000
26/26 - 1s - loss: 1.2212 - val_loss: 1.6422
Epoch 1168/5000
26/26 - 1s - loss: 1.2219 - val_loss: 1.6415
Epoch 1169/5000
26/26 - 1s - loss: 1.2202 - val_loss: 1.6404
Epoch 1170/5000
26/26 - 1s - loss: 1.2193 - val_loss: 1.6391
Epoch 01170: val_loss improved from 1.64864 to 1.63906, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1171/5000
26/26 - 1s - loss: 1.2182 - val_loss: 1.6382
Epoch 1172/5000
26/26 - 1s - loss: 1.2194 - val_loss: 1.6376
Epoch 1173/5000
26/26 - 1s - loss: 1.2179 - val_loss: 1.6370
Epoch 1174/5000
26/26 - 1s - loss: 1.2171 - val_loss: 1.6365
Epoch 1175/5000
26/26 - 1s - loss: 1.2152 - val_loss: 1.6344
Epoch 1176/5000
26/26 - 1s - loss: 1.2139 - val_loss: 1.6350
Epoch 1177/5000
26/26 - 1s - loss: 1.2135 - val_loss: 1.6332
Epoch 1178/5000
26/26 - 1s - loss: 1.2138 - val_loss: 1.6328
Epoch 1179/5000
26/26 - 1s - loss: 1.2117 - val_loss: 1.6323
Epoch 1180/5000
26/26 - 1s - loss: 1.2118 - val_loss: 1.6311
Epoch 01180: val_loss improved from 1.63906 to 1.63110, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1181/5000
26/26 - 1s - loss: 1.2100 - val_loss: 1.6306
Epoch 1182/5000
26/26 - 1s - loss: 1.2089 - val_loss: 1.6278
Epoch 1183/5000
26/26 - 1s - loss: 1.2101 - val_loss: 1.6290
Epoch 1184/5000
26/26 - 1s - loss: 1.2082 - val_loss: 1.6290
Epoch 1185/5000
26/26 - 1s - loss: 1.2065 - val_loss: 1.6277
Epoch 1186/5000
26/26 - 1s - loss: 1.2071 - val_loss: 1.6254
Epoch 1187/5000
26/26 - 1s - loss: 1.2064 - val_loss: 1.6256
Epoch 1188/5000
26/26 - 1s - loss: 1.2068 - val_loss: 1.6247
Epoch 1189/5000
26/26 - 1s - loss: 1.2049 - val_loss: 1.6226
Epoch 1190/5000
26/26 - 2s - loss: 1.2018 - val_loss: 1.6236
Epoch 01190: val_loss improved from 1.63110 to 1.62358, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1191/5000
26/26 - 1s - loss: 1.2025 - val_loss: 1.6229
Epoch 1192/5000
26/26 - 1s - loss: 1.1991 - val_loss: 1.6217
Epoch 1193/5000
26/26 - 1s - loss: 1.2006 - val_loss: 1.6215
Epoch 1194/5000
26/26 - 1s - loss: 1.2003 - val_loss: 1.6198
Epoch 1195/5000
26/26 - 1s - loss: 1.1989 - val_loss: 1.6181
Epoch 1196/5000
26/26 - 1s - loss: 1.1977 - val_loss: 1.6164
Epoch 1197/5000
26/26 - 1s - loss: 1.1971 - val_loss: 1.6178
Epoch 1198/5000
26/26 - 1s - loss: 1.1955 - val_loss: 1.6155
Epoch 1199/5000
26/26 - 1s - loss: 1.1947 - val_loss: 1.6149
Epoch 1200/5000
26/26 - 1s - loss: 1.1949 - val_loss: 1.6150
Epoch 01200: val_loss improved from 1.62358 to 1.61505, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1201/5000
26/26 - 1s - loss: 1.1948 - val_loss: 1.6148
Epoch 1202/5000
26/26 - 1s - loss: 1.1928 - val_loss: 1.6148
Epoch 1203/5000
26/26 - 1s - loss: 1.1927 - val_loss: 1.6148
Epoch 1204/5000
26/26 - 1s - loss: 1.1914 - val_loss: 1.6131
Epoch 1205/5000
26/26 - 1s - loss: 1.1915 - val_loss: 1.6112
Epoch 1206/5000
26/26 - 1s - loss: 1.1898 - val_loss: 1.6098
Epoch 1207/5000
26/26 - 1s - loss: 1.1872 - val_loss: 1.6088
Epoch 1208/5000
26/26 - 1s - loss: 1.1868 - val_loss: 1.6081
Epoch 1209/5000
26/26 - 1s - loss: 1.1868 - val_loss: 1.6078
Epoch 1210/5000
26/26 - 1s - loss: 1.1855 - val_loss: 1.6068
Epoch 01210: val_loss improved from 1.61505 to 1.60681, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1211/5000
26/26 - 1s - loss: 1.1861 - val_loss: 1.6062
Epoch 1212/5000
26/26 - 1s - loss: 1.1845 - val_loss: 1.6062
Epoch 1213/5000
26/26 - 1s - loss: 1.1843 - val_loss: 1.6044
Epoch 1214/5000
26/26 - 1s - loss: 1.1824 - val_loss: 1.6059
Epoch 1215/5000
26/26 - 2s - loss: 1.1821 - val_loss: 1.6028
Epoch 1216/5000
26/26 - 1s - loss: 1.1815 - val_loss: 1.6020
Epoch 1217/5000
26/26 - 1s - loss: 1.1818 - val_loss: 1.6017
Epoch 1218/5000
26/26 - 1s - loss: 1.1807 - val_loss: 1.6004
Epoch 1219/5000
26/26 - 1s - loss: 1.1806 - val_loss: 1.5992
Epoch 1220/5000
26/26 - 1s - loss: 1.1795 - val_loss: 1.5977
Epoch 01220: val_loss improved from 1.60681 to 1.59774, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1221/5000
26/26 - 1s - loss: 1.1782 - val_loss: 1.5976
Epoch 1222/5000
26/26 - 2s - loss: 1.1770 - val_loss: 1.5975
Epoch 1223/5000
26/26 - 1s - loss: 1.1766 - val_loss: 1.5966
Epoch 1224/5000
26/26 - 1s - loss: 1.1761 - val_loss: 1.5938
Epoch 1225/5000
26/26 - 1s - loss: 1.1731 - val_loss: 1.5938
Epoch 1226/5000
26/26 - 1s - loss: 1.1729 - val_loss: 1.5944
Epoch 1227/5000
26/26 - 1s - loss: 1.1736 - val_loss: 1.5927
Epoch 1228/5000
26/26 - 1s - loss: 1.1725 - val_loss: 1.5927
Epoch 1229/5000
26/26 - 2s - loss: 1.1712 - val_loss: 1.5920
Epoch 1230/5000
26/26 - 1s - loss: 1.1708 - val_loss: 1.5913
Epoch 01230: val_loss improved from 1.59774 to 1.59128, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1231/5000
26/26 - 1s - loss: 1.1709 - val_loss: 1.5909
Epoch 1232/5000
26/26 - 1s - loss: 1.1687 - val_loss: 1.5894
Epoch 1233/5000
26/26 - 1s - loss: 1.1696 - val_loss: 1.5879
Epoch 1234/5000
26/26 - 1s - loss: 1.1674 - val_loss: 1.5883
Epoch 1235/5000
26/26 - 1s - loss: 1.1662 - val_loss: 1.5859
Epoch 1236/5000
26/26 - 1s - loss: 1.1647 - val_loss: 1.5872
Epoch 1237/5000
26/26 - 1s - loss: 1.1652 - val_loss: 1.5848
Epoch 1238/5000
26/26 - 1s - loss: 1.1638 - val_loss: 1.5853
Epoch 1239/5000
26/26 - 2s - loss: 1.1646 - val_loss: 1.5841
Epoch 1240/5000
26/26 - 1s - loss: 1.1629 - val_loss: 1.5833
Epoch 01240: val_loss improved from 1.59128 to 1.58326, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1241/5000
26/26 - 1s - loss: 1.1635 - val_loss: 1.5836
Epoch 1242/5000
26/26 - 1s - loss: 1.1615 - val_loss: 1.5825
Epoch 1243/5000
26/26 - 1s - loss: 1.1603 - val_loss: 1.5822
Epoch 1244/5000
26/26 - 1s - loss: 1.1601 - val_loss: 1.5794
Epoch 1245/5000
26/26 - 1s - loss: 1.1601 - val_loss: 1.5790
Epoch 1246/5000
26/26 - 1s - loss: 1.1574 - val_loss: 1.5768
Epoch 1247/5000
26/26 - 1s - loss: 1.1582 - val_loss: 1.5771
Epoch 1248/5000
26/26 - 1s - loss: 1.1559 - val_loss: 1.5779
Epoch 1249/5000
26/26 - 1s - loss: 1.1577 - val_loss: 1.5754
Epoch 1250/5000
26/26 - 1s - loss: 1.1558 - val_loss: 1.5749
Epoch 01250: val_loss improved from 1.58326 to 1.57490, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1251/5000
26/26 - 1s - loss: 1.1551 - val_loss: 1.5754
Epoch 1252/5000
26/26 - 1s - loss: 1.1545 - val_loss: 1.5742
Epoch 1253/5000
26/26 - 1s - loss: 1.1513 - val_loss: 1.5727
Epoch 1254/5000
26/26 - 1s - loss: 1.1516 - val_loss: 1.5748
Epoch 1255/5000
26/26 - 1s - loss: 1.1519 - val_loss: 1.5747
Epoch 1256/5000
26/26 - 1s - loss: 1.1507 - val_loss: 1.5724
Epoch 1257/5000
26/26 - 1s - loss: 1.1508 - val_loss: 1.5719
Epoch 1258/5000
26/26 - 1s - loss: 1.1480 - val_loss: 1.5706
Epoch 1259/5000
26/26 - 1s - loss: 1.1496 - val_loss: 1.5703
Epoch 1260/5000
26/26 - 1s - loss: 1.1481 - val_loss: 1.5693
Epoch 01260: val_loss improved from 1.57490 to 1.56933, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1261/5000
26/26 - 1s - loss: 1.1468 - val_loss: 1.5686
Epoch 1262/5000
26/26 - 1s - loss: 1.1463 - val_loss: 1.5686
Epoch 1263/5000
26/26 - 1s - loss: 1.1455 - val_loss: 1.5676
Epoch 1264/5000
26/26 - 1s - loss: 1.1442 - val_loss: 1.5657
Epoch 1265/5000
26/26 - 1s - loss: 1.1431 - val_loss: 1.5671
Epoch 1266/5000
26/26 - 1s - loss: 1.1444 - val_loss: 1.5648
Epoch 1267/5000
26/26 - 1s - loss: 1.1421 - val_loss: 1.5629
Epoch 1268/5000
26/26 - 1s - loss: 1.1418 - val_loss: 1.5628
Epoch 1269/5000
26/26 - 1s - loss: 1.1398 - val_loss: 1.5616
Epoch 1270/5000
26/26 - 1s - loss: 1.1418 - val_loss: 1.5607
Epoch 01270: val_loss improved from 1.56933 to 1.56075, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1271/5000
26/26 - 1s - loss: 1.1396 - val_loss: 1.5591
Epoch 1272/5000
26/26 - 1s - loss: 1.1395 - val_loss: 1.5585
Epoch 1273/5000
26/26 - 1s - loss: 1.1399 - val_loss: 1.5590
Epoch 1274/5000
26/26 - 2s - loss: 1.1370 - val_loss: 1.5586
Epoch 1275/5000
26/26 - 1s - loss: 1.1357 - val_loss: 1.5573
Epoch 1276/5000
26/26 - 2s - loss: 1.1343 - val_loss: 1.5551
Epoch 1277/5000
26/26 - 2s - loss: 1.1339 - val_loss: 1.5545
Epoch 1278/5000
26/26 - 1s - loss: 1.1329 - val_loss: 1.5546
Epoch 1279/5000
26/26 - 1s - loss: 1.1346 - val_loss: 1.5531
Epoch 1280/5000
26/26 - 1s - loss: 1.1335 - val_loss: 1.5538
Epoch 01280: val_loss improved from 1.56075 to 1.55382, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1281/5000
26/26 - 1s - loss: 1.1318 - val_loss: 1.5531
Epoch 1282/5000
26/26 - 1s - loss: 1.1304 - val_loss: 1.5525
Epoch 1283/5000
26/26 - 1s - loss: 1.1303 - val_loss: 1.5513
Epoch 1284/5000
26/26 - 1s - loss: 1.1309 - val_loss: 1.5509
Epoch 1285/5000
26/26 - 2s - loss: 1.1287 - val_loss: 1.5509
Epoch 1286/5000
26/26 - 1s - loss: 1.1272 - val_loss: 1.5501
Epoch 1287/5000
26/26 - 1s - loss: 1.1273 - val_loss: 1.5510
Epoch 1288/5000
26/26 - 1s - loss: 1.1261 - val_loss: 1.5481
Epoch 1289/5000
26/26 - 1s - loss: 1.1259 - val_loss: 1.5469
Epoch 1290/5000
26/26 - 1s - loss: 1.1265 - val_loss: 1.5483
Epoch 01290: val_loss improved from 1.55382 to 1.54834, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1291/5000
26/26 - 1s - loss: 1.1230 - val_loss: 1.5460
Epoch 1292/5000
26/26 - 1s - loss: 1.1237 - val_loss: 1.5442
Epoch 1293/5000
26/26 - 1s - loss: 1.1248 - val_loss: 1.5435
Epoch 1294/5000
26/26 - 2s - loss: 1.1223 - val_loss: 1.5425
Epoch 1295/5000
26/26 - 1s - loss: 1.1218 - val_loss: 1.5430
Epoch 1296/5000
26/26 - 1s - loss: 1.1193 - val_loss: 1.5415
Epoch 1297/5000
26/26 - 1s - loss: 1.1204 - val_loss: 1.5400
Epoch 1298/5000
26/26 - 1s - loss: 1.1195 - val_loss: 1.5395
Epoch 1299/5000
26/26 - 1s - loss: 1.1191 - val_loss: 1.5402
Epoch 1300/5000
26/26 - 1s - loss: 1.1176 - val_loss: 1.5394
Epoch 01300: val_loss improved from 1.54834 to 1.53938, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1301/5000
26/26 - 1s - loss: 1.1184 - val_loss: 1.5387
Epoch 1302/5000
26/26 - 1s - loss: 1.1172 - val_loss: 1.5374
Epoch 1303/5000
26/26 - 1s - loss: 1.1156 - val_loss: 1.5375
Epoch 1304/5000
26/26 - 1s - loss: 1.1141 - val_loss: 1.5362
Epoch 1305/5000
26/26 - 1s - loss: 1.1137 - val_loss: 1.5351
Epoch 1306/5000
26/26 - 1s - loss: 1.1137 - val_loss: 1.5360
Epoch 1307/5000
26/26 - 1s - loss: 1.1133 - val_loss: 1.5357
Epoch 1308/5000
26/26 - 1s - loss: 1.1128 - val_loss: 1.5338
Epoch 1309/5000
26/26 - 1s - loss: 1.1106 - val_loss: 1.5345
Epoch 1310/5000
26/26 - 1s - loss: 1.1105 - val_loss: 1.5330
Epoch 01310: val_loss improved from 1.53938 to 1.53304, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1311/5000
26/26 - 1s - loss: 1.1110 - val_loss: 1.5320
Epoch 1312/5000
26/26 - 1s - loss: 1.1095 - val_loss: 1.5309
Epoch 1313/5000
26/26 - 1s - loss: 1.1110 - val_loss: 1.5302
Epoch 1314/5000
26/26 - 1s - loss: 1.1089 - val_loss: 1.5285
Epoch 1315/5000
26/26 - 1s - loss: 1.1075 - val_loss: 1.5276
Epoch 1316/5000
26/26 - 1s - loss: 1.1052 - val_loss: 1.5278
Epoch 1317/5000
26/26 - 1s - loss: 1.1058 - val_loss: 1.5260
Epoch 1318/5000
26/26 - 2s - loss: 1.1054 - val_loss: 1.5255
Epoch 1319/5000
26/26 - 1s - loss: 1.1046 - val_loss: 1.5248
Epoch 1320/5000
26/26 - 1s - loss: 1.1049 - val_loss: 1.5243
Epoch 01320: val_loss improved from 1.53304 to 1.52434, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1321/5000
26/26 - 1s - loss: 1.1035 - val_loss: 1.5229
Epoch 1322/5000
26/26 - 1s - loss: 1.1038 - val_loss: 1.5220
Epoch 1323/5000
26/26 - 1s - loss: 1.1030 - val_loss: 1.5226
Epoch 1324/5000
26/26 - 1s - loss: 1.1009 - val_loss: 1.5217
Epoch 1325/5000
26/26 - 1s - loss: 1.0996 - val_loss: 1.5216
Epoch 1326/5000
26/26 - 2s - loss: 1.1004 - val_loss: 1.5215
Epoch 1327/5000
26/26 - 1s - loss: 1.0990 - val_loss: 1.5200
Epoch 1328/5000
26/26 - 1s - loss: 1.0972 - val_loss: 1.5188
Epoch 1329/5000
26/26 - 1s - loss: 1.0979 - val_loss: 1.5196
Epoch 1330/5000
26/26 - 2s - loss: 1.0959 - val_loss: 1.5191
Epoch 01330: val_loss improved from 1.52434 to 1.51910, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1331/5000
26/26 - 1s - loss: 1.0979 - val_loss: 1.5171
Epoch 1332/5000
26/26 - 1s - loss: 1.0962 - val_loss: 1.5168
Epoch 1333/5000
26/26 - 1s - loss: 1.0944 - val_loss: 1.5170
Epoch 1334/5000
26/26 - 1s - loss: 1.0926 - val_loss: 1.5137
Epoch 1335/5000
26/26 - 1s - loss: 1.0932 - val_loss: 1.5138
Epoch 1336/5000
26/26 - 1s - loss: 1.0932 - val_loss: 1.5132
Epoch 1337/5000
26/26 - 1s - loss: 1.0903 - val_loss: 1.5128
Epoch 1338/5000
26/26 - 1s - loss: 1.0921 - val_loss: 1.5128
Epoch 1339/5000
26/26 - 1s - loss: 1.0915 - val_loss: 1.5122
Epoch 1340/5000
26/26 - 1s - loss: 1.0898 - val_loss: 1.5115
Epoch 01340: val_loss improved from 1.51910 to 1.51146, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1341/5000
26/26 - 1s - loss: 1.0887 - val_loss: 1.5117
Epoch 1342/5000
26/26 - 1s - loss: 1.0888 - val_loss: 1.5107
Epoch 1343/5000
26/26 - 1s - loss: 1.0879 - val_loss: 1.5078
Epoch 1344/5000
26/26 - 1s - loss: 1.0878 - val_loss: 1.5061
Epoch 1345/5000
26/26 - 1s - loss: 1.0859 - val_loss: 1.5063
Epoch 1346/5000
26/26 - 1s - loss: 1.0854 - val_loss: 1.5065
Epoch 1347/5000
26/26 - 1s - loss: 1.0854 - val_loss: 1.5051
Epoch 1348/5000
26/26 - 1s - loss: 1.0847 - val_loss: 1.5054
Epoch 1349/5000
26/26 - 1s - loss: 1.0823 - val_loss: 1.5036
Epoch 1350/5000
26/26 - 1s - loss: 1.0828 - val_loss: 1.5034
Epoch 01350: val_loss improved from 1.51146 to 1.50337, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1351/5000
26/26 - 1s - loss: 1.0816 - val_loss: 1.5044
Epoch 1352/5000
26/26 - 1s - loss: 1.0828 - val_loss: 1.5023
Epoch 1353/5000
26/26 - 1s - loss: 1.0806 - val_loss: 1.5024
Epoch 1354/5000
26/26 - 1s - loss: 1.0803 - val_loss: 1.5029
Epoch 1355/5000
26/26 - 1s - loss: 1.0777 - val_loss: 1.5016
Epoch 1356/5000
26/26 - 1s - loss: 1.0790 - val_loss: 1.5002
Epoch 1357/5000
26/26 - 1s - loss: 1.0755 - val_loss: 1.4987
Epoch 1358/5000
26/26 - 1s - loss: 1.0770 - val_loss: 1.4981
Epoch 1359/5000
26/26 - 1s - loss: 1.0787 - val_loss: 1.4982
Epoch 1360/5000
26/26 - 1s - loss: 1.0769 - val_loss: 1.4975
Epoch 01360: val_loss improved from 1.50337 to 1.49747, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1361/5000
26/26 - 1s - loss: 1.0756 - val_loss: 1.4960
Epoch 1362/5000
26/26 - 1s - loss: 1.0755 - val_loss: 1.4956
Epoch 1363/5000
26/26 - 1s - loss: 1.0734 - val_loss: 1.4954
Epoch 1364/5000
26/26 - 1s - loss: 1.0724 - val_loss: 1.4949
Epoch 1365/5000
26/26 - 1s - loss: 1.0729 - val_loss: 1.4952
Epoch 1366/5000
26/26 - 1s - loss: 1.0713 - val_loss: 1.4926
Epoch 1367/5000
26/26 - 1s - loss: 1.0723 - val_loss: 1.4923
Epoch 1368/5000
26/26 - 1s - loss: 1.0691 - val_loss: 1.4914
Epoch 1369/5000
26/26 - 1s - loss: 1.0678 - val_loss: 1.4903
Epoch 1370/5000
26/26 - 1s - loss: 1.0691 - val_loss: 1.4899
Epoch 01370: val_loss improved from 1.49747 to 1.48989, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1371/5000
26/26 - 1s - loss: 1.0672 - val_loss: 1.4890
Epoch 1372/5000
26/26 - 1s - loss: 1.0685 - val_loss: 1.4897
Epoch 1373/5000
26/26 - 1s - loss: 1.0687 - val_loss: 1.4894
Epoch 1374/5000
26/26 - 1s - loss: 1.0670 - val_loss: 1.4878
Epoch 1375/5000
26/26 - 1s - loss: 1.0654 - val_loss: 1.4888
Epoch 1376/5000
26/26 - 1s - loss: 1.0669 - val_loss: 1.4869
Epoch 1377/5000
26/26 - 2s - loss: 1.0642 - val_loss: 1.4863
Epoch 1378/5000
26/26 - 1s - loss: 1.0642 - val_loss: 1.4857
Epoch 1379/5000
26/26 - 1s - loss: 1.0632 - val_loss: 1.4846
Epoch 1380/5000
26/26 - 1s - loss: 1.0609 - val_loss: 1.4848
Epoch 01380: val_loss improved from 1.48989 to 1.48482, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1381/5000
26/26 - 1s - loss: 1.0608 - val_loss: 1.4843
Epoch 1382/5000
26/26 - 1s - loss: 1.0612 - val_loss: 1.4854
Epoch 1383/5000
26/26 - 1s - loss: 1.0599 - val_loss: 1.4825
Epoch 1384/5000
26/26 - 1s - loss: 1.0585 - val_loss: 1.4810
Epoch 1385/5000
26/26 - 1s - loss: 1.0579 - val_loss: 1.4815
Epoch 1386/5000
26/26 - 2s - loss: 1.0579 - val_loss: 1.4802
Epoch 1387/5000
26/26 - 1s - loss: 1.0571 - val_loss: 1.4803
Epoch 1388/5000
26/26 - 1s - loss: 1.0558 - val_loss: 1.4801
Epoch 1389/5000
26/26 - 1s - loss: 1.0571 - val_loss: 1.4775
Epoch 1390/5000
26/26 - 1s - loss: 1.0567 - val_loss: 1.4823
Epoch 01390: val_loss improved from 1.48482 to 1.48228, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1391/5000
26/26 - 1s - loss: 1.0570 - val_loss: 1.4784
Epoch 1392/5000
26/26 - 1s - loss: 1.0562 - val_loss: 1.4779
Epoch 1393/5000
26/26 - 1s - loss: 1.0523 - val_loss: 1.4766
Epoch 1394/5000
26/26 - 1s - loss: 1.0533 - val_loss: 1.4767
Epoch 1395/5000
26/26 - 1s - loss: 1.0524 - val_loss: 1.4743
Epoch 1396/5000
26/26 - 1s - loss: 1.0508 - val_loss: 1.4735
Epoch 1397/5000
26/26 - 1s - loss: 1.0497 - val_loss: 1.4731
Epoch 1398/5000
26/26 - 1s - loss: 1.0509 - val_loss: 1.4717
Epoch 1399/5000
26/26 - 1s - loss: 1.0501 - val_loss: 1.4718
Epoch 1400/5000
26/26 - 1s - loss: 1.0497 - val_loss: 1.4710
Epoch 01400: val_loss improved from 1.48228 to 1.47104, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1401/5000
26/26 - 1s - loss: 1.0482 - val_loss: 1.4714
Epoch 1402/5000
26/26 - 1s - loss: 1.0474 - val_loss: 1.4702
Epoch 1403/5000
26/26 - 1s - loss: 1.0463 - val_loss: 1.4694
Epoch 1404/5000
26/26 - 1s - loss: 1.0464 - val_loss: 1.4708
Epoch 1405/5000
26/26 - 1s - loss: 1.0468 - val_loss: 1.4703
Epoch 1406/5000
26/26 - 1s - loss: 1.0456 - val_loss: 1.4674
Epoch 1407/5000
26/26 - 1s - loss: 1.0455 - val_loss: 1.4679
Epoch 1408/5000
26/26 - 1s - loss: 1.0446 - val_loss: 1.4664
Epoch 1409/5000
26/26 - 1s - loss: 1.0426 - val_loss: 1.4661
Epoch 1410/5000
26/26 - 1s - loss: 1.0441 - val_loss: 1.4650
Epoch 01410: val_loss improved from 1.47104 to 1.46496, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1411/5000
26/26 - 1s - loss: 1.0432 - val_loss: 1.4660
Epoch 1412/5000
26/26 - 1s - loss: 1.0412 - val_loss: 1.4645
Epoch 1413/5000
26/26 - 1s - loss: 1.0408 - val_loss: 1.4642
Epoch 1414/5000
26/26 - 1s - loss: 1.0411 - val_loss: 1.4627
Epoch 1415/5000
26/26 - 1s - loss: 1.0391 - val_loss: 1.4611
Epoch 1416/5000
26/26 - 1s - loss: 1.0397 - val_loss: 1.4604
Epoch 1417/5000
26/26 - 1s - loss: 1.0401 - val_loss: 1.4591
Epoch 1418/5000
26/26 - 1s - loss: 1.0378 - val_loss: 1.4590
Epoch 1419/5000
26/26 - 1s - loss: 1.0384 - val_loss: 1.4582
Epoch 1420/5000
26/26 - 1s - loss: 1.0369 - val_loss: 1.4594
Epoch 01420: val_loss improved from 1.46496 to 1.45939, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1421/5000
26/26 - 1s - loss: 1.0358 - val_loss: 1.4575
Epoch 1422/5000
26/26 - 1s - loss: 1.0351 - val_loss: 1.4547
Epoch 1423/5000
26/26 - 1s - loss: 1.0346 - val_loss: 1.4570
Epoch 1424/5000
26/26 - 1s - loss: 1.0344 - val_loss: 1.4554
Epoch 1425/5000
26/26 - 1s - loss: 1.0329 - val_loss: 1.4542
Epoch 1426/5000
26/26 - 1s - loss: 1.0333 - val_loss: 1.4536
Epoch 1427/5000
26/26 - 1s - loss: 1.0309 - val_loss: 1.4553
Epoch 1428/5000
26/26 - 1s - loss: 1.0295 - val_loss: 1.4542
Epoch 1429/5000
26/26 - 1s - loss: 1.0309 - val_loss: 1.4528
Epoch 1430/5000
26/26 - 1s - loss: 1.0311 - val_loss: 1.4526
Epoch 01430: val_loss improved from 1.45939 to 1.45261, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1431/5000
26/26 - 1s - loss: 1.0317 - val_loss: 1.4504
Epoch 1432/5000
26/26 - 1s - loss: 1.0299 - val_loss: 1.4507
Epoch 1433/5000
26/26 - 1s - loss: 1.0289 - val_loss: 1.4503
Epoch 1434/5000
26/26 - 1s - loss: 1.0262 - val_loss: 1.4495
Epoch 1435/5000
26/26 - 1s - loss: 1.0279 - val_loss: 1.4497
Epoch 1436/5000
26/26 - 1s - loss: 1.0267 - val_loss: 1.4480
Epoch 1437/5000
26/26 - 1s - loss: 1.0257 - val_loss: 1.4481
Epoch 1438/5000
26/26 - 1s - loss: 1.0243 - val_loss: 1.4483
Epoch 1439/5000
26/26 - 1s - loss: 1.0238 - val_loss: 1.4461
Epoch 1440/5000
26/26 - 1s - loss: 1.0240 - val_loss: 1.4468
Epoch 01440: val_loss improved from 1.45261 to 1.44679, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1441/5000
26/26 - 1s - loss: 1.0233 - val_loss: 1.4462
Epoch 1442/5000
26/26 - 2s - loss: 1.0218 - val_loss: 1.4458
Epoch 1443/5000
26/26 - 1s - loss: 1.0246 - val_loss: 1.4438
Epoch 1444/5000
26/26 - 1s - loss: 1.0212 - val_loss: 1.4450
Epoch 1445/5000
26/26 - 1s - loss: 1.0210 - val_loss: 1.4449
Epoch 1446/5000
26/26 - 1s - loss: 1.0221 - val_loss: 1.4434
Epoch 1447/5000
26/26 - 1s - loss: 1.0200 - val_loss: 1.4428
Epoch 1448/5000
26/26 - 1s - loss: 1.0205 - val_loss: 1.4417
Epoch 1449/5000
26/26 - 1s - loss: 1.0187 - val_loss: 1.4421
Epoch 1450/5000
26/26 - 1s - loss: 1.0177 - val_loss: 1.4396
Epoch 01450: val_loss improved from 1.44679 to 1.43961, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1451/5000
26/26 - 1s - loss: 1.0175 - val_loss: 1.4409
Epoch 1452/5000
26/26 - 1s - loss: 1.0158 - val_loss: 1.4390
Epoch 1453/5000
26/26 - 1s - loss: 1.0164 - val_loss: 1.4382
Epoch 1454/5000
26/26 - 1s - loss: 1.0156 - val_loss: 1.4375
Epoch 1455/5000
26/26 - 1s - loss: 1.0145 - val_loss: 1.4377
Epoch 1456/5000
26/26 - 1s - loss: 1.0153 - val_loss: 1.4368
Epoch 1457/5000
26/26 - 1s - loss: 1.0128 - val_loss: 1.4359
Epoch 1458/5000
26/26 - 2s - loss: 1.0130 - val_loss: 1.4370
Epoch 1459/5000
26/26 - 1s - loss: 1.0123 - val_loss: 1.4356
Epoch 1460/5000
26/26 - 1s - loss: 1.0124 - val_loss: 1.4361
Epoch 01460: val_loss improved from 1.43961 to 1.43611, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1461/5000
26/26 - 1s - loss: 1.0116 - val_loss: 1.4351
Epoch 1462/5000
26/26 - 1s - loss: 1.0123 - val_loss: 1.4340
Epoch 1463/5000
26/26 - 1s - loss: 1.0096 - val_loss: 1.4322
Epoch 1464/5000
26/26 - 1s - loss: 1.0089 - val_loss: 1.4324
Epoch 1465/5000
26/26 - 1s - loss: 1.0087 - val_loss: 1.4314
Epoch 1466/5000
26/26 - 1s - loss: 1.0089 - val_loss: 1.4309
Epoch 1467/5000
26/26 - 1s - loss: 1.0072 - val_loss: 1.4301
Epoch 1468/5000
26/26 - 1s - loss: 1.0090 - val_loss: 1.4285
Epoch 1469/5000
26/26 - 1s - loss: 1.0060 - val_loss: 1.4303
Epoch 1470/5000
26/26 - 1s - loss: 1.0052 - val_loss: 1.4300
Epoch 01470: val_loss improved from 1.43611 to 1.42998, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1471/5000
26/26 - 1s - loss: 1.0058 - val_loss: 1.4277
Epoch 1472/5000
26/26 - 1s - loss: 1.0046 - val_loss: 1.4273
Epoch 1473/5000
26/26 - 1s - loss: 1.0049 - val_loss: 1.4263
Epoch 1474/5000
26/26 - 1s - loss: 1.0040 - val_loss: 1.4258
Epoch 1475/5000
26/26 - 1s - loss: 1.0043 - val_loss: 1.4257
Epoch 1476/5000
26/26 - 1s - loss: 1.0023 - val_loss: 1.4260
Epoch 1477/5000
26/26 - 1s - loss: 1.0019 - val_loss: 1.4241
Epoch 1478/5000
26/26 - 1s - loss: 1.0013 - val_loss: 1.4243
Epoch 1479/5000
26/26 - 1s - loss: 0.9998 - val_loss: 1.4237
Epoch 1480/5000
26/26 - 1s - loss: 1.0007 - val_loss: 1.4228
Epoch 01480: val_loss improved from 1.42998 to 1.42280, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1481/5000
26/26 - 1s - loss: 0.9984 - val_loss: 1.4228
Epoch 1482/5000
26/26 - 1s - loss: 0.9994 - val_loss: 1.4216
Epoch 1483/5000
26/26 - 1s - loss: 1.0008 - val_loss: 1.4207
Epoch 1484/5000
26/26 - 1s - loss: 0.9981 - val_loss: 1.4199
Epoch 1485/5000
26/26 - 1s - loss: 0.9968 - val_loss: 1.4188
Epoch 1486/5000
26/26 - 1s - loss: 0.9944 - val_loss: 1.4185
Epoch 1487/5000
26/26 - 1s - loss: 0.9969 - val_loss: 1.4183
Epoch 1488/5000
26/26 - 1s - loss: 0.9951 - val_loss: 1.4192
Epoch 1489/5000
26/26 - 1s - loss: 0.9939 - val_loss: 1.4185
Epoch 1490/5000
26/26 - 1s - loss: 0.9942 - val_loss: 1.4175
Epoch 01490: val_loss improved from 1.42280 to 1.41753, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1491/5000
26/26 - 1s - loss: 0.9938 - val_loss: 1.4183
Epoch 1492/5000
26/26 - 2s - loss: 0.9920 - val_loss: 1.4180
Epoch 1493/5000
26/26 - 1s - loss: 0.9925 - val_loss: 1.4160
Epoch 1494/5000
26/26 - 1s - loss: 0.9917 - val_loss: 1.4166
Epoch 1495/5000
26/26 - 1s - loss: 0.9912 - val_loss: 1.4160
Epoch 1496/5000
26/26 - 2s - loss: 0.9914 - val_loss: 1.4140
Epoch 1497/5000
26/26 - 1s - loss: 0.9898 - val_loss: 1.4140
Epoch 1498/5000
26/26 - 1s - loss: 0.9895 - val_loss: 1.4121
Epoch 1499/5000
26/26 - 1s - loss: 0.9878 - val_loss: 1.4140
Epoch 1500/5000
26/26 - 1s - loss: 0.9870 - val_loss: 1.4127
Epoch 01500: val_loss improved from 1.41753 to 1.41269, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1501/5000
26/26 - 1s - loss: 0.9890 - val_loss: 1.4127
Epoch 1502/5000
26/26 - 1s - loss: 0.9864 - val_loss: 1.4113
Epoch 1503/5000
26/26 - 1s - loss: 0.9855 - val_loss: 1.4118
Epoch 1504/5000
26/26 - 1s - loss: 0.9860 - val_loss: 1.4099
Epoch 1505/5000
26/26 - 1s - loss: 0.9861 - val_loss: 1.4086
Epoch 1506/5000
26/26 - 1s - loss: 0.9833 - val_loss: 1.4084
Epoch 1507/5000
26/26 - 1s - loss: 0.9841 - val_loss: 1.4087
Epoch 1508/5000
26/26 - 1s - loss: 0.9829 - val_loss: 1.4067
Epoch 1509/5000
26/26 - 1s - loss: 0.9858 - val_loss: 1.4057
Epoch 1510/5000
26/26 - 1s - loss: 0.9827 - val_loss: 1.4046
Epoch 01510: val_loss improved from 1.41269 to 1.40463, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1511/5000
26/26 - 1s - loss: 0.9822 - val_loss: 1.4048
Epoch 1512/5000
26/26 - 1s - loss: 0.9801 - val_loss: 1.4044
Epoch 1513/5000
26/26 - 1s - loss: 0.9796 - val_loss: 1.4055
Epoch 1514/5000
26/26 - 1s - loss: 0.9804 - val_loss: 1.4035
Epoch 1515/5000
26/26 - 1s - loss: 0.9817 - val_loss: 1.4026
Epoch 1516/5000
26/26 - 1s - loss: 0.9794 - val_loss: 1.4029
Epoch 1517/5000
26/26 - 1s - loss: 0.9776 - val_loss: 1.4026
Epoch 1518/5000
26/26 - 1s - loss: 0.9768 - val_loss: 1.4023
Epoch 1519/5000
26/26 - 1s - loss: 0.9770 - val_loss: 1.4019
Epoch 1520/5000
26/26 - 1s - loss: 0.9779 - val_loss: 1.4001
Epoch 01520: val_loss improved from 1.40463 to 1.40013, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1521/5000
26/26 - 1s - loss: 0.9749 - val_loss: 1.4000
Epoch 1522/5000
26/26 - 1s - loss: 0.9763 - val_loss: 1.3992
Epoch 1523/5000
26/26 - 1s - loss: 0.9758 - val_loss: 1.3995
Epoch 1524/5000
26/26 - 1s - loss: 0.9755 - val_loss: 1.3977
Epoch 1525/5000
26/26 - 1s - loss: 0.9726 - val_loss: 1.3963
Epoch 1526/5000
26/26 - 1s - loss: 0.9735 - val_loss: 1.3958
Epoch 1527/5000
26/26 - 1s - loss: 0.9716 - val_loss: 1.3982
Epoch 1528/5000
26/26 - 1s - loss: 0.9716 - val_loss: 1.3960
Epoch 1529/5000
26/26 - 1s - loss: 0.9710 - val_loss: 1.3962
Epoch 1530/5000
26/26 - 1s - loss: 0.9733 - val_loss: 1.3950
Epoch 01530: val_loss improved from 1.40013 to 1.39505, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1531/5000
26/26 - 1s - loss: 0.9730 - val_loss: 1.3942
Epoch 1532/5000
26/26 - 1s - loss: 0.9714 - val_loss: 1.3927
Epoch 1533/5000
26/26 - 1s - loss: 0.9704 - val_loss: 1.3920
Epoch 1534/5000
26/26 - 1s - loss: 0.9702 - val_loss: 1.3930
Epoch 1535/5000
26/26 - 1s - loss: 0.9689 - val_loss: 1.3921
Epoch 1536/5000
26/26 - 1s - loss: 0.9691 - val_loss: 1.3922
Epoch 1537/5000
26/26 - 1s - loss: 0.9667 - val_loss: 1.3908
Epoch 1538/5000
26/26 - 1s - loss: 0.9667 - val_loss: 1.3897
Epoch 1539/5000
26/26 - 1s - loss: 0.9664 - val_loss: 1.3898
Epoch 1540/5000
26/26 - 1s - loss: 0.9665 - val_loss: 1.3892
Epoch 01540: val_loss improved from 1.39505 to 1.38925, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1541/5000
26/26 - 1s - loss: 0.9652 - val_loss: 1.3892
Epoch 1542/5000
26/26 - 1s - loss: 0.9647 - val_loss: 1.3893
Epoch 1543/5000
26/26 - 1s - loss: 0.9633 - val_loss: 1.3881
Epoch 1544/5000
26/26 - 1s - loss: 0.9645 - val_loss: 1.3874
Epoch 1545/5000
26/26 - 1s - loss: 0.9625 - val_loss: 1.3873
Epoch 1546/5000
26/26 - 1s - loss: 0.9631 - val_loss: 1.3868
Epoch 1547/5000
26/26 - 1s - loss: 0.9614 - val_loss: 1.3856
Epoch 1548/5000
26/26 - 1s - loss: 0.9628 - val_loss: 1.3865
Epoch 1549/5000
26/26 - 1s - loss: 0.9607 - val_loss: 1.3851
Epoch 1550/5000
26/26 - 1s - loss: 0.9600 - val_loss: 1.3833
Epoch 01550: val_loss improved from 1.38925 to 1.38334, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1551/5000
26/26 - 1s - loss: 0.9595 - val_loss: 1.3830
Epoch 1552/5000
26/26 - 1s - loss: 0.9579 - val_loss: 1.3838
Epoch 1553/5000
26/26 - 1s - loss: 0.9590 - val_loss: 1.3820
Epoch 1554/5000
26/26 - 1s - loss: 0.9570 - val_loss: 1.3795
Epoch 1555/5000
26/26 - 1s - loss: 0.9563 - val_loss: 1.3799
Epoch 1556/5000
26/26 - 1s - loss: 0.9569 - val_loss: 1.3783
Epoch 1557/5000
26/26 - 1s - loss: 0.9562 - val_loss: 1.3795
Epoch 1558/5000
26/26 - 1s - loss: 0.9570 - val_loss: 1.3797
Epoch 1559/5000
26/26 - 1s - loss: 0.9552 - val_loss: 1.3805
Epoch 1560/5000
26/26 - 1s - loss: 0.9554 - val_loss: 1.3798
Epoch 01560: val_loss improved from 1.38334 to 1.37978, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1561/5000
26/26 - 1s - loss: 0.9530 - val_loss: 1.3788
Epoch 1562/5000
26/26 - 1s - loss: 0.9534 - val_loss: 1.3783
Epoch 1563/5000
26/26 - 1s - loss: 0.9541 - val_loss: 1.3771
Epoch 1564/5000
26/26 - 1s - loss: 0.9522 - val_loss: 1.3766
Epoch 1565/5000
26/26 - 1s - loss: 0.9507 - val_loss: 1.3751
Epoch 1566/5000
26/26 - 1s - loss: 0.9516 - val_loss: 1.3763
Epoch 1567/5000
26/26 - 1s - loss: 0.9510 - val_loss: 1.3747
Epoch 1568/5000
26/26 - 1s - loss: 0.9512 - val_loss: 1.3738
Epoch 1569/5000
26/26 - 1s - loss: 0.9483 - val_loss: 1.3753
Epoch 1570/5000
26/26 - 1s - loss: 0.9494 - val_loss: 1.3744
Epoch 01570: val_loss improved from 1.37978 to 1.37440, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1571/5000
26/26 - 1s - loss: 0.9504 - val_loss: 1.3736
Epoch 1572/5000
26/26 - 1s - loss: 0.9489 - val_loss: 1.3724
Epoch 1573/5000
26/26 - 1s - loss: 0.9469 - val_loss: 1.3714
Epoch 1574/5000
26/26 - 2s - loss: 0.9469 - val_loss: 1.3703
Epoch 1575/5000
26/26 - 1s - loss: 0.9473 - val_loss: 1.3709
Epoch 1576/5000
26/26 - 1s - loss: 0.9448 - val_loss: 1.3703
Epoch 1577/5000
26/26 - 1s - loss: 0.9460 - val_loss: 1.3701
Epoch 1578/5000
26/26 - 1s - loss: 0.9458 - val_loss: 1.3686
Epoch 1579/5000
26/26 - 1s - loss: 0.9436 - val_loss: 1.3685
Epoch 1580/5000
26/26 - 1s - loss: 0.9453 - val_loss: 1.3679
Epoch 01580: val_loss improved from 1.37440 to 1.36793, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1581/5000
26/26 - 1s - loss: 0.9435 - val_loss: 1.3667
Epoch 1582/5000
26/26 - 1s - loss: 0.9426 - val_loss: 1.3673
Epoch 1583/5000
26/26 - 1s - loss: 0.9419 - val_loss: 1.3653
Epoch 1584/5000
26/26 - 1s - loss: 0.9418 - val_loss: 1.3669
Epoch 1585/5000
26/26 - 1s - loss: 0.9429 - val_loss: 1.3676
Epoch 1586/5000
26/26 - 1s - loss: 0.9407 - val_loss: 1.3658
Epoch 1587/5000
26/26 - 1s - loss: 0.9411 - val_loss: 1.3655
Epoch 1588/5000
26/26 - 1s - loss: 0.9396 - val_loss: 1.3640
Epoch 1589/5000
26/26 - 1s - loss: 0.9379 - val_loss: 1.3621
Epoch 1590/5000
26/26 - 1s - loss: 0.9387 - val_loss: 1.3640
Epoch 01590: val_loss improved from 1.36793 to 1.36405, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1591/5000
26/26 - 1s - loss: 0.9388 - val_loss: 1.3631
Epoch 1592/5000
26/26 - 2s - loss: 0.9374 - val_loss: 1.3633
Epoch 1593/5000
26/26 - 1s - loss: 0.9371 - val_loss: 1.3618
Epoch 1594/5000
26/26 - 1s - loss: 0.9375 - val_loss: 1.3619
Epoch 1595/5000
26/26 - 1s - loss: 0.9366 - val_loss: 1.3606
Epoch 1596/5000
26/26 - 1s - loss: 0.9357 - val_loss: 1.3595
Epoch 1597/5000
26/26 - 1s - loss: 0.9335 - val_loss: 1.3611
Epoch 1598/5000
26/26 - 1s - loss: 0.9350 - val_loss: 1.3592
Epoch 1599/5000
26/26 - 1s - loss: 0.9349 - val_loss: 1.3591
Epoch 1600/5000
26/26 - 1s - loss: 0.9322 - val_loss: 1.3585
Epoch 01600: val_loss improved from 1.36405 to 1.35850, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1601/5000
26/26 - 1s - loss: 0.9329 - val_loss: 1.3570
Epoch 1602/5000
26/26 - 1s - loss: 0.9321 - val_loss: 1.3567
Epoch 1603/5000
26/26 - 1s - loss: 0.9326 - val_loss: 1.3560
Epoch 1604/5000
26/26 - 1s - loss: 0.9314 - val_loss: 1.3553
Epoch 1605/5000
26/26 - 1s - loss: 0.9317 - val_loss: 1.3545
Epoch 1606/5000
26/26 - 1s - loss: 0.9305 - val_loss: 1.3536
Epoch 1607/5000
26/26 - 1s - loss: 0.9302 - val_loss: 1.3549
Epoch 1608/5000
26/26 - 1s - loss: 0.9294 - val_loss: 1.3545
Epoch 1609/5000
26/26 - 1s - loss: 0.9307 - val_loss: 1.3537
Epoch 1610/5000
26/26 - 1s - loss: 0.9280 - val_loss: 1.3532
Epoch 01610: val_loss improved from 1.35850 to 1.35321, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1611/5000
26/26 - 1s - loss: 0.9278 - val_loss: 1.3525
Epoch 1612/5000
26/26 - 1s - loss: 0.9260 - val_loss: 1.3527
Epoch 1613/5000
26/26 - 1s - loss: 0.9262 - val_loss: 1.3516
Epoch 1614/5000
26/26 - 1s - loss: 0.9261 - val_loss: 1.3525
Epoch 1615/5000
26/26 - 1s - loss: 0.9242 - val_loss: 1.3511
Epoch 1616/5000
26/26 - 1s - loss: 0.9233 - val_loss: 1.3508
Epoch 1617/5000
26/26 - 1s - loss: 0.9249 - val_loss: 1.3490
Epoch 1618/5000
26/26 - 1s - loss: 0.9233 - val_loss: 1.3491
Epoch 1619/5000
26/26 - 1s - loss: 0.9233 - val_loss: 1.3473
Epoch 1620/5000
26/26 - 1s - loss: 0.9225 - val_loss: 1.3466
Epoch 01620: val_loss improved from 1.35321 to 1.34664, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1621/5000
26/26 - 1s - loss: 0.9223 - val_loss: 1.3465
Epoch 1622/5000
26/26 - 1s - loss: 0.9211 - val_loss: 1.3464
Epoch 1623/5000
26/26 - 1s - loss: 0.9200 - val_loss: 1.3471
Epoch 1624/5000
26/26 - 1s - loss: 0.9234 - val_loss: 1.3472
Epoch 1625/5000
26/26 - 2s - loss: 0.9220 - val_loss: 1.3462
Epoch 1626/5000
26/26 - 1s - loss: 0.9188 - val_loss: 1.3446
Epoch 1627/5000
26/26 - 1s - loss: 0.9186 - val_loss: 1.3444
Epoch 1628/5000
26/26 - 1s - loss: 0.9212 - val_loss: 1.3440
Epoch 1629/5000
26/26 - 1s - loss: 0.9187 - val_loss: 1.3438
Epoch 1630/5000
26/26 - 1s - loss: 0.9175 - val_loss: 1.3435
Epoch 01630: val_loss improved from 1.34664 to 1.34349, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1631/5000
26/26 - 1s - loss: 0.9176 - val_loss: 1.3418
Epoch 1632/5000
26/26 - 1s - loss: 0.9174 - val_loss: 1.3420
Epoch 1633/5000
26/26 - 1s - loss: 0.9158 - val_loss: 1.3434
Epoch 1634/5000
26/26 - 1s - loss: 0.9156 - val_loss: 1.3412
Epoch 1635/5000
26/26 - 1s - loss: 0.9147 - val_loss: 1.3419
Epoch 1636/5000
26/26 - 1s - loss: 0.9152 - val_loss: 1.3404
Epoch 1637/5000
26/26 - 1s - loss: 0.9148 - val_loss: 1.3407
Epoch 1638/5000
26/26 - 1s - loss: 0.9151 - val_loss: 1.3403
Epoch 1639/5000
26/26 - 1s - loss: 0.9142 - val_loss: 1.3387
Epoch 1640/5000
26/26 - 1s - loss: 0.9130 - val_loss: 1.3391
Epoch 01640: val_loss improved from 1.34349 to 1.33907, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1641/5000
26/26 - 1s - loss: 0.9131 - val_loss: 1.3366
Epoch 1642/5000
26/26 - 1s - loss: 0.9140 - val_loss: 1.3382
Epoch 1643/5000
26/26 - 1s - loss: 0.9122 - val_loss: 1.3382
Epoch 1644/5000
26/26 - 1s - loss: 0.9113 - val_loss: 1.3358
Epoch 1645/5000
26/26 - 1s - loss: 0.9093 - val_loss: 1.3348
Epoch 1646/5000
26/26 - 1s - loss: 0.9113 - val_loss: 1.3337
Epoch 1647/5000
26/26 - 1s - loss: 0.9093 - val_loss: 1.3347
Epoch 1648/5000
26/26 - 1s - loss: 0.9088 - val_loss: 1.3349
Epoch 1649/5000
26/26 - 1s - loss: 0.9091 - val_loss: 1.3337
Epoch 1650/5000
26/26 - 1s - loss: 0.9084 - val_loss: 1.3329
Epoch 01650: val_loss improved from 1.33907 to 1.33286, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1651/5000
26/26 - 1s - loss: 0.9082 - val_loss: 1.3327
Epoch 1652/5000
26/26 - 1s - loss: 0.9083 - val_loss: 1.3305
Epoch 1653/5000
26/26 - 1s - loss: 0.9090 - val_loss: 1.3300
Epoch 1654/5000
26/26 - 1s - loss: 0.9058 - val_loss: 1.3316
Epoch 1655/5000
26/26 - 1s - loss: 0.9062 - val_loss: 1.3312
Epoch 1656/5000
26/26 - 1s - loss: 0.9059 - val_loss: 1.3298
Epoch 1657/5000
26/26 - 1s - loss: 0.9050 - val_loss: 1.3310
Epoch 1658/5000
26/26 - 1s - loss: 0.9039 - val_loss: 1.3291
Epoch 1659/5000
26/26 - 1s - loss: 0.9029 - val_loss: 1.3279
Epoch 1660/5000
26/26 - 1s - loss: 0.9034 - val_loss: 1.3296
Epoch 01660: val_loss improved from 1.33286 to 1.32962, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1661/5000
26/26 - 1s - loss: 0.9026 - val_loss: 1.3280
Epoch 1662/5000
26/26 - 1s - loss: 0.9033 - val_loss: 1.3265
Epoch 1663/5000
26/26 - 1s - loss: 0.9009 - val_loss: 1.3274
Epoch 1664/5000
26/26 - 1s - loss: 0.9016 - val_loss: 1.3272
Epoch 1665/5000
26/26 - 1s - loss: 0.9011 - val_loss: 1.3263
Epoch 1666/5000
26/26 - 1s - loss: 0.9006 - val_loss: 1.3283
Epoch 1667/5000
26/26 - 1s - loss: 0.8996 - val_loss: 1.3256
Epoch 1668/5000
26/26 - 1s - loss: 0.8984 - val_loss: 1.3246
Epoch 1669/5000
26/26 - 1s - loss: 0.8993 - val_loss: 1.3240
Epoch 1670/5000
26/26 - 1s - loss: 0.8971 - val_loss: 1.3241
Epoch 01670: val_loss improved from 1.32962 to 1.32414, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1671/5000
26/26 - 1s - loss: 0.8974 - val_loss: 1.3234
Epoch 1672/5000
26/26 - 1s - loss: 0.8960 - val_loss: 1.3235
Epoch 1673/5000
26/26 - 1s - loss: 0.8969 - val_loss: 1.3226
Epoch 1674/5000
26/26 - 1s - loss: 0.8953 - val_loss: 1.3205
Epoch 1675/5000
26/26 - 1s - loss: 0.8961 - val_loss: 1.3206
Epoch 1676/5000
26/26 - 1s - loss: 0.8955 - val_loss: 1.3202
Epoch 1677/5000
26/26 - 1s - loss: 0.8944 - val_loss: 1.3210
Epoch 1678/5000
26/26 - 1s - loss: 0.8950 - val_loss: 1.3191
Epoch 1679/5000
26/26 - 1s - loss: 0.8937 - val_loss: 1.3196
Epoch 1680/5000
26/26 - 1s - loss: 0.8920 - val_loss: 1.3183
Epoch 01680: val_loss improved from 1.32414 to 1.31827, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1681/5000
26/26 - 1s - loss: 0.8922 - val_loss: 1.3190
Epoch 1682/5000
26/26 - 1s - loss: 0.8923 - val_loss: 1.3191
Epoch 1683/5000
26/26 - 1s - loss: 0.8920 - val_loss: 1.3168
Epoch 1684/5000
26/26 - 1s - loss: 0.8905 - val_loss: 1.3173
Epoch 1685/5000
26/26 - 1s - loss: 0.8911 - val_loss: 1.3162
Epoch 1686/5000
26/26 - 1s - loss: 0.8908 - val_loss: 1.3147
Epoch 1687/5000
26/26 - 1s - loss: 0.8886 - val_loss: 1.3167
Epoch 1688/5000
26/26 - 1s - loss: 0.8890 - val_loss: 1.3143
Epoch 1689/5000
26/26 - 1s - loss: 0.8893 - val_loss: 1.3142
Epoch 1690/5000
26/26 - 1s - loss: 0.8890 - val_loss: 1.3130
Epoch 01690: val_loss improved from 1.31827 to 1.31301, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1691/5000
26/26 - 1s - loss: 0.8887 - val_loss: 1.3120
Epoch 1692/5000
26/26 - 1s - loss: 0.8881 - val_loss: 1.3125
Epoch 1693/5000
26/26 - 2s - loss: 0.8871 - val_loss: 1.3128
Epoch 1694/5000
26/26 - 1s - loss: 0.8879 - val_loss: 1.3127
Epoch 1695/5000
26/26 - 1s - loss: 0.8860 - val_loss: 1.3110
Epoch 1696/5000
26/26 - 1s - loss: 0.8871 - val_loss: 1.3108
Epoch 1697/5000
26/26 - 1s - loss: 0.8869 - val_loss: 1.3109
Epoch 1698/5000
26/26 - 1s - loss: 0.8845 - val_loss: 1.3102
Epoch 1699/5000
26/26 - 1s - loss: 0.8836 - val_loss: 1.3100
Epoch 1700/5000
26/26 - 2s - loss: 0.8844 - val_loss: 1.3092
Epoch 01700: val_loss improved from 1.31301 to 1.30915, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1701/5000
26/26 - 1s - loss: 0.8837 - val_loss: 1.3087
Epoch 1702/5000
26/26 - 1s - loss: 0.8812 - val_loss: 1.3087
Epoch 1703/5000
26/26 - 1s - loss: 0.8844 - val_loss: 1.3091
Epoch 1704/5000
26/26 - 1s - loss: 0.8835 - val_loss: 1.3087
Epoch 1705/5000
26/26 - 1s - loss: 0.8817 - val_loss: 1.3085
Epoch 1706/5000
26/26 - 1s - loss: 0.8818 - val_loss: 1.3071
Epoch 1707/5000
26/26 - 1s - loss: 0.8809 - val_loss: 1.3062
Epoch 1708/5000
26/26 - 1s - loss: 0.8810 - val_loss: 1.3071
Epoch 1709/5000
26/26 - 1s - loss: 0.8807 - val_loss: 1.3039
Epoch 1710/5000
26/26 - 1s - loss: 0.8790 - val_loss: 1.3028
Epoch 01710: val_loss improved from 1.30915 to 1.30278, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1711/5000
26/26 - 1s - loss: 0.8789 - val_loss: 1.3044
Epoch 1712/5000
26/26 - 1s - loss: 0.8793 - val_loss: 1.3034
Epoch 1713/5000
26/26 - 1s - loss: 0.8772 - val_loss: 1.3051
Epoch 1714/5000
26/26 - 1s - loss: 0.8782 - val_loss: 1.3035
Epoch 1715/5000
26/26 - 1s - loss: 0.8778 - val_loss: 1.3032
Epoch 1716/5000
26/26 - 1s - loss: 0.8762 - val_loss: 1.3040
Epoch 1717/5000
26/26 - 1s - loss: 0.8740 - val_loss: 1.3024
Epoch 1718/5000
26/26 - 1s - loss: 0.8752 - val_loss: 1.3010
Epoch 1719/5000
26/26 - 1s - loss: 0.8747 - val_loss: 1.2995
Epoch 1720/5000
26/26 - 1s - loss: 0.8747 - val_loss: 1.2999
Epoch 01720: val_loss improved from 1.30278 to 1.29994, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1721/5000
26/26 - 1s - loss: 0.8761 - val_loss: 1.2991
Epoch 1722/5000
26/26 - 1s - loss: 0.8720 - val_loss: 1.2991
Epoch 1723/5000
26/26 - 1s - loss: 0.8731 - val_loss: 1.2996
Epoch 1724/5000
26/26 - 1s - loss: 0.8727 - val_loss: 1.2989
Epoch 1725/5000
26/26 - 1s - loss: 0.8718 - val_loss: 1.2988
Epoch 1726/5000
26/26 - 1s - loss: 0.8722 - val_loss: 1.2977
Epoch 1727/5000
26/26 - 1s - loss: 0.8716 - val_loss: 1.2955
Epoch 1728/5000
26/26 - 1s - loss: 0.8710 - val_loss: 1.2972
Epoch 1729/5000
26/26 - 1s - loss: 0.8705 - val_loss: 1.2955
Epoch 1730/5000
26/26 - 1s - loss: 0.8699 - val_loss: 1.2952
Epoch 01730: val_loss improved from 1.29994 to 1.29524, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1731/5000
26/26 - 1s - loss: 0.8693 - val_loss: 1.2940
Epoch 1732/5000
26/26 - 1s - loss: 0.8699 - val_loss: 1.2944
Epoch 1733/5000
26/26 - 1s - loss: 0.8691 - val_loss: 1.2939
Epoch 1734/5000
26/26 - 1s - loss: 0.8683 - val_loss: 1.2947
Epoch 1735/5000
26/26 - 2s - loss: 0.8687 - val_loss: 1.2932
Epoch 1736/5000
26/26 - 1s - loss: 0.8670 - val_loss: 1.2937
Epoch 1737/5000
26/26 - 1s - loss: 0.8672 - val_loss: 1.2916
Epoch 1738/5000
26/26 - 1s - loss: 0.8688 - val_loss: 1.2913
Epoch 1739/5000
26/26 - 1s - loss: 0.8668 - val_loss: 1.2918
Epoch 1740/5000
26/26 - 1s - loss: 0.8658 - val_loss: 1.2909
Epoch 01740: val_loss improved from 1.29524 to 1.29086, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1741/5000
26/26 - 1s - loss: 0.8664 - val_loss: 1.2908
Epoch 1742/5000
26/26 - 1s - loss: 0.8645 - val_loss: 1.2902
Epoch 1743/5000
26/26 - 1s - loss: 0.8647 - val_loss: 1.2920
Epoch 1744/5000
26/26 - 1s - loss: 0.8633 - val_loss: 1.2912
Epoch 1745/5000
26/26 - 1s - loss: 0.8629 - val_loss: 1.2894
Epoch 1746/5000
26/26 - 1s - loss: 0.8639 - val_loss: 1.2905
Epoch 1747/5000
26/26 - 1s - loss: 0.8615 - val_loss: 1.2883
Epoch 1748/5000
26/26 - 1s - loss: 0.8609 - val_loss: 1.2866
Epoch 1749/5000
26/26 - 1s - loss: 0.8622 - val_loss: 1.2867
Epoch 1750/5000
26/26 - 1s - loss: 0.8604 - val_loss: 1.2867
Epoch 01750: val_loss improved from 1.29086 to 1.28666, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1751/5000
26/26 - 1s - loss: 0.8617 - val_loss: 1.2859
Epoch 1752/5000
26/26 - 1s - loss: 0.8600 - val_loss: 1.2846
Epoch 1753/5000
26/26 - 1s - loss: 0.8608 - val_loss: 1.2853
Epoch 1754/5000
26/26 - 1s - loss: 0.8584 - val_loss: 1.2858
Epoch 1755/5000
26/26 - 1s - loss: 0.8607 - val_loss: 1.2848
Epoch 1756/5000
26/26 - 1s - loss: 0.8580 - val_loss: 1.2843
Epoch 1757/5000
26/26 - 1s - loss: 0.8596 - val_loss: 1.2827
Epoch 1758/5000
26/26 - 1s - loss: 0.8571 - val_loss: 1.2829
Epoch 1759/5000
26/26 - 1s - loss: 0.8565 - val_loss: 1.2835
Epoch 1760/5000
26/26 - 1s - loss: 0.8565 - val_loss: 1.2811
Epoch 01760: val_loss improved from 1.28666 to 1.28114, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1761/5000
26/26 - 1s - loss: 0.8569 - val_loss: 1.2803
Epoch 1762/5000
26/26 - 1s - loss: 0.8565 - val_loss: 1.2802
Epoch 1763/5000
26/26 - 1s - loss: 0.8544 - val_loss: 1.2804
Epoch 1764/5000
26/26 - 1s - loss: 0.8545 - val_loss: 1.2805
Epoch 1765/5000
26/26 - 2s - loss: 0.8549 - val_loss: 1.2804
Epoch 1766/5000
26/26 - 1s - loss: 0.8545 - val_loss: 1.2807
Epoch 1767/5000
26/26 - 1s - loss: 0.8537 - val_loss: 1.2798
Epoch 1768/5000
26/26 - 1s - loss: 0.8533 - val_loss: 1.2796
Epoch 1769/5000
26/26 - 1s - loss: 0.8532 - val_loss: 1.2794
Epoch 1770/5000
26/26 - 1s - loss: 0.8530 - val_loss: 1.2787
Epoch 01770: val_loss improved from 1.28114 to 1.27871, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1771/5000
26/26 - 1s - loss: 0.8509 - val_loss: 1.2780
Epoch 1772/5000
26/26 - 1s - loss: 0.8505 - val_loss: 1.2778
Epoch 1773/5000
26/26 - 1s - loss: 0.8521 - val_loss: 1.2773
Epoch 1774/5000
26/26 - 1s - loss: 0.8514 - val_loss: 1.2772
Epoch 1775/5000
26/26 - 1s - loss: 0.8496 - val_loss: 1.2769
Epoch 1776/5000
26/26 - 1s - loss: 0.8486 - val_loss: 1.2759
Epoch 1777/5000
26/26 - 1s - loss: 0.8486 - val_loss: 1.2755
Epoch 1778/5000
26/26 - 1s - loss: 0.8470 - val_loss: 1.2755
Epoch 1779/5000
26/26 - 1s - loss: 0.8477 - val_loss: 1.2764
Epoch 1780/5000
26/26 - 1s - loss: 0.8496 - val_loss: 1.2744
Epoch 01780: val_loss improved from 1.27871 to 1.27444, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1781/5000
26/26 - 1s - loss: 0.8481 - val_loss: 1.2739
Epoch 1782/5000
26/26 - 1s - loss: 0.8481 - val_loss: 1.2726
Epoch 1783/5000
26/26 - 1s - loss: 0.8472 - val_loss: 1.2720
Epoch 1784/5000
26/26 - 1s - loss: 0.8475 - val_loss: 1.2716
Epoch 1785/5000
26/26 - 1s - loss: 0.8460 - val_loss: 1.2724
Epoch 1786/5000
26/26 - 1s - loss: 0.8452 - val_loss: 1.2726
Epoch 1787/5000
26/26 - 1s - loss: 0.8439 - val_loss: 1.2713
Epoch 1788/5000
26/26 - 1s - loss: 0.8435 - val_loss: 1.2694
Epoch 1789/5000
26/26 - 1s - loss: 0.8443 - val_loss: 1.2700
Epoch 1790/5000
26/26 - 1s - loss: 0.8438 - val_loss: 1.2710
Epoch 01790: val_loss improved from 1.27444 to 1.27098, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1791/5000
26/26 - 1s - loss: 0.8430 - val_loss: 1.2692
Epoch 1792/5000
26/26 - 1s - loss: 0.8429 - val_loss: 1.2682
Epoch 1793/5000
26/26 - 1s - loss: 0.8424 - val_loss: 1.2689
Epoch 1794/5000
26/26 - 1s - loss: 0.8424 - val_loss: 1.2676
Epoch 1795/5000
26/26 - 1s - loss: 0.8415 - val_loss: 1.2679
Epoch 1796/5000
26/26 - 1s - loss: 0.8396 - val_loss: 1.2659
Epoch 1797/5000
26/26 - 1s - loss: 0.8400 - val_loss: 1.2680
Epoch 1798/5000
26/26 - 1s - loss: 0.8401 - val_loss: 1.2662
Epoch 1799/5000
26/26 - 1s - loss: 0.8394 - val_loss: 1.2676
Epoch 1800/5000
26/26 - 1s - loss: 0.8391 - val_loss: 1.2678
Epoch 01800: val_loss improved from 1.27098 to 1.26776, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1801/5000
26/26 - 2s - loss: 0.8384 - val_loss: 1.2659
Epoch 1802/5000
26/26 - 1s - loss: 0.8381 - val_loss: 1.2646
Epoch 1803/5000
26/26 - 1s - loss: 0.8365 - val_loss: 1.2641
Epoch 1804/5000
26/26 - 1s - loss: 0.8372 - val_loss: 1.2643
Epoch 1805/5000
26/26 - 1s - loss: 0.8379 - val_loss: 1.2634
Epoch 1806/5000
26/26 - 1s - loss: 0.8364 - val_loss: 1.2642
Epoch 1807/5000
26/26 - 1s - loss: 0.8362 - val_loss: 1.2624
Epoch 1808/5000
26/26 - 1s - loss: 0.8364 - val_loss: 1.2628
Epoch 1809/5000
26/26 - 1s - loss: 0.8351 - val_loss: 1.2615
Epoch 1810/5000
26/26 - 1s - loss: 0.8351 - val_loss: 1.2604
Epoch 01810: val_loss improved from 1.26776 to 1.26037, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1811/5000
26/26 - 1s - loss: 0.8356 - val_loss: 1.2621
Epoch 1812/5000
26/26 - 1s - loss: 0.8348 - val_loss: 1.2602
Epoch 1813/5000
26/26 - 1s - loss: 0.8336 - val_loss: 1.2602
Epoch 1814/5000
26/26 - 1s - loss: 0.8327 - val_loss: 1.2602
Epoch 1815/5000
26/26 - 1s - loss: 0.8331 - val_loss: 1.2596
Epoch 1816/5000
26/26 - 1s - loss: 0.8321 - val_loss: 1.2592
Epoch 1817/5000
26/26 - 1s - loss: 0.8324 - val_loss: 1.2580
Epoch 1818/5000
26/26 - 1s - loss: 0.8320 - val_loss: 1.2584
Epoch 1819/5000
26/26 - 1s - loss: 0.8332 - val_loss: 1.2583
Epoch 1820/5000
26/26 - 1s - loss: 0.8305 - val_loss: 1.2562
Epoch 01820: val_loss improved from 1.26037 to 1.25623, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1821/5000
26/26 - 1s - loss: 0.8309 - val_loss: 1.2563
Epoch 1822/5000
26/26 - 1s - loss: 0.8298 - val_loss: 1.2557
Epoch 1823/5000
26/26 - 1s - loss: 0.8302 - val_loss: 1.2548
Epoch 1824/5000
26/26 - 2s - loss: 0.8294 - val_loss: 1.2555
Epoch 1825/5000
26/26 - 1s - loss: 0.8289 - val_loss: 1.2558
Epoch 1826/5000
26/26 - 1s - loss: 0.8292 - val_loss: 1.2562
Epoch 1827/5000
26/26 - 1s - loss: 0.8282 - val_loss: 1.2534
Epoch 1828/5000
26/26 - 1s - loss: 0.8284 - val_loss: 1.2538
Epoch 1829/5000
26/26 - 1s - loss: 0.8274 - val_loss: 1.2524
Epoch 1830/5000
26/26 - 1s - loss: 0.8274 - val_loss: 1.2520
Epoch 01830: val_loss improved from 1.25623 to 1.25197, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1831/5000
26/26 - 1s - loss: 0.8272 - val_loss: 1.2521
Epoch 1832/5000
26/26 - 1s - loss: 0.8264 - val_loss: 1.2519
Epoch 1833/5000
26/26 - 1s - loss: 0.8260 - val_loss: 1.2518
Epoch 1834/5000
26/26 - 1s - loss: 0.8248 - val_loss: 1.2510
Epoch 1835/5000
26/26 - 1s - loss: 0.8243 - val_loss: 1.2521
Epoch 1836/5000
26/26 - 1s - loss: 0.8243 - val_loss: 1.2497
Epoch 1837/5000
26/26 - 1s - loss: 0.8252 - val_loss: 1.2506
Epoch 1838/5000
26/26 - 2s - loss: 0.8231 - val_loss: 1.2496
Epoch 1839/5000
26/26 - 1s - loss: 0.8230 - val_loss: 1.2486
Epoch 1840/5000
26/26 - 1s - loss: 0.8225 - val_loss: 1.2477
Epoch 01840: val_loss improved from 1.25197 to 1.24772, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1841/5000
26/26 - 1s - loss: 0.8224 - val_loss: 1.2488
Epoch 1842/5000
26/26 - 1s - loss: 0.8223 - val_loss: 1.2494
Epoch 1843/5000
26/26 - 1s - loss: 0.8212 - val_loss: 1.2484
Epoch 1844/5000
26/26 - 1s - loss: 0.8204 - val_loss: 1.2464
Epoch 1845/5000
26/26 - 1s - loss: 0.8200 - val_loss: 1.2466
Epoch 1846/5000
26/26 - 1s - loss: 0.8205 - val_loss: 1.2463
Epoch 1847/5000
26/26 - 1s - loss: 0.8212 - val_loss: 1.2459
Epoch 1848/5000
26/26 - 1s - loss: 0.8206 - val_loss: 1.2458
Epoch 1849/5000
26/26 - 1s - loss: 0.8187 - val_loss: 1.2442
Epoch 1850/5000
26/26 - 1s - loss: 0.8185 - val_loss: 1.2437
Epoch 01850: val_loss improved from 1.24772 to 1.24372, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1851/5000
26/26 - 1s - loss: 0.8168 - val_loss: 1.2458
Epoch 1852/5000
26/26 - 1s - loss: 0.8177 - val_loss: 1.2453
Epoch 1853/5000
26/26 - 1s - loss: 0.8177 - val_loss: 1.2434
Epoch 1854/5000
26/26 - 1s - loss: 0.8165 - val_loss: 1.2437
Epoch 1855/5000
26/26 - 1s - loss: 0.8168 - val_loss: 1.2437
Epoch 1856/5000
26/26 - 1s - loss: 0.8165 - val_loss: 1.2421
Epoch 1857/5000
26/26 - 1s - loss: 0.8160 - val_loss: 1.2425
Epoch 1858/5000
26/26 - 1s - loss: 0.8146 - val_loss: 1.2429
Epoch 1859/5000
26/26 - 2s - loss: 0.8142 - val_loss: 1.2416
Epoch 1860/5000
26/26 - 1s - loss: 0.8143 - val_loss: 1.2419
Epoch 01860: val_loss improved from 1.24372 to 1.24194, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1861/5000
26/26 - 1s - loss: 0.8141 - val_loss: 1.2403
Epoch 1862/5000
26/26 - 1s - loss: 0.8140 - val_loss: 1.2404
Epoch 1863/5000
26/26 - 1s - loss: 0.8135 - val_loss: 1.2391
Epoch 1864/5000
26/26 - 1s - loss: 0.8123 - val_loss: 1.2409
Epoch 1865/5000
26/26 - 1s - loss: 0.8138 - val_loss: 1.2389
Epoch 1866/5000
26/26 - 1s - loss: 0.8119 - val_loss: 1.2392
Epoch 1867/5000
26/26 - 1s - loss: 0.8121 - val_loss: 1.2380
Epoch 1868/5000
26/26 - 1s - loss: 0.8116 - val_loss: 1.2388
Epoch 1869/5000
26/26 - 1s - loss: 0.8117 - val_loss: 1.2376
Epoch 1870/5000
26/26 - 1s - loss: 0.8109 - val_loss: 1.2375
Epoch 01870: val_loss improved from 1.24194 to 1.23749, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1871/5000
26/26 - 2s - loss: 0.8098 - val_loss: 1.2365
Epoch 1872/5000
26/26 - 1s - loss: 0.8106 - val_loss: 1.2358
Epoch 1873/5000
26/26 - 1s - loss: 0.8081 - val_loss: 1.2368
Epoch 1874/5000
26/26 - 1s - loss: 0.8079 - val_loss: 1.2356
Epoch 1875/5000
26/26 - 1s - loss: 0.8091 - val_loss: 1.2358
Epoch 1876/5000
26/26 - 1s - loss: 0.8089 - val_loss: 1.2346
Epoch 1877/5000
26/26 - 1s - loss: 0.8094 - val_loss: 1.2338
Epoch 1878/5000
26/26 - 1s - loss: 0.8069 - val_loss: 1.2342
Epoch 1879/5000
26/26 - 1s - loss: 0.8065 - val_loss: 1.2327
Epoch 1880/5000
26/26 - 1s - loss: 0.8060 - val_loss: 1.2327
Epoch 01880: val_loss improved from 1.23749 to 1.23269, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1881/5000
26/26 - 1s - loss: 0.8051 - val_loss: 1.2327
Epoch 1882/5000
26/26 - 1s - loss: 0.8056 - val_loss: 1.2316
Epoch 1883/5000
26/26 - 1s - loss: 0.8047 - val_loss: 1.2336
Epoch 1884/5000
26/26 - 2s - loss: 0.8054 - val_loss: 1.2307
Epoch 1885/5000
26/26 - 1s - loss: 0.8043 - val_loss: 1.2303
Epoch 1886/5000
26/26 - 1s - loss: 0.8042 - val_loss: 1.2302
Epoch 1887/5000
26/26 - 1s - loss: 0.8041 - val_loss: 1.2299
Epoch 1888/5000
26/26 - 1s - loss: 0.8029 - val_loss: 1.2315
Epoch 1889/5000
26/26 - 1s - loss: 0.8034 - val_loss: 1.2294
Epoch 1890/5000
26/26 - 1s - loss: 0.8028 - val_loss: 1.2291
Epoch 01890: val_loss improved from 1.23269 to 1.22905, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1891/5000
26/26 - 1s - loss: 0.8030 - val_loss: 1.2294
Epoch 1892/5000
26/26 - 1s - loss: 0.8011 - val_loss: 1.2279
Epoch 1893/5000
26/26 - 1s - loss: 0.8008 - val_loss: 1.2281
Epoch 1894/5000
26/26 - 1s - loss: 0.8003 - val_loss: 1.2284
Epoch 1895/5000
26/26 - 1s - loss: 0.8010 - val_loss: 1.2272
Epoch 1896/5000
26/26 - 1s - loss: 0.8016 - val_loss: 1.2255
Epoch 1897/5000
26/26 - 1s - loss: 0.8013 - val_loss: 1.2259
Epoch 1898/5000
26/26 - 1s - loss: 0.7989 - val_loss: 1.2263
Epoch 1899/5000
26/26 - 1s - loss: 0.7984 - val_loss: 1.2254
Epoch 1900/5000
26/26 - 2s - loss: 0.7997 - val_loss: 1.2245
Epoch 01900: val_loss improved from 1.22905 to 1.22452, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1901/5000
26/26 - 1s - loss: 0.7978 - val_loss: 1.2249
Epoch 1902/5000
26/26 - 1s - loss: 0.7986 - val_loss: 1.2242
Epoch 1903/5000
26/26 - 1s - loss: 0.7980 - val_loss: 1.2232
Epoch 1904/5000
26/26 - 1s - loss: 0.7978 - val_loss: 1.2223
Epoch 1905/5000
26/26 - 1s - loss: 0.7967 - val_loss: 1.2244
Epoch 1906/5000
26/26 - 1s - loss: 0.7976 - val_loss: 1.2235
Epoch 1907/5000
26/26 - 1s - loss: 0.7967 - val_loss: 1.2231
Epoch 1908/5000
26/26 - 1s - loss: 0.7963 - val_loss: 1.2225
Epoch 1909/5000
26/26 - 2s - loss: 0.7961 - val_loss: 1.2228
Epoch 1910/5000
26/26 - 1s - loss: 0.7961 - val_loss: 1.2221
Epoch 01910: val_loss improved from 1.22452 to 1.22214, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1911/5000
26/26 - 1s - loss: 0.7940 - val_loss: 1.2215
Epoch 1912/5000
26/26 - 2s - loss: 0.7947 - val_loss: 1.2204
Epoch 1913/5000
26/26 - 1s - loss: 0.7931 - val_loss: 1.2216
Epoch 1914/5000
26/26 - 1s - loss: 0.7937 - val_loss: 1.2205
Epoch 1915/5000
26/26 - 1s - loss: 0.7935 - val_loss: 1.2213
Epoch 1916/5000
26/26 - 1s - loss: 0.7921 - val_loss: 1.2193
Epoch 1917/5000
26/26 - 1s - loss: 0.7922 - val_loss: 1.2204
Epoch 1918/5000
26/26 - 1s - loss: 0.7924 - val_loss: 1.2191
Epoch 1919/5000
26/26 - 1s - loss: 0.7928 - val_loss: 1.2184
Epoch 1920/5000
26/26 - 1s - loss: 0.7920 - val_loss: 1.2183
Epoch 01920: val_loss improved from 1.22214 to 1.21829, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1921/5000
26/26 - 1s - loss: 0.7898 - val_loss: 1.2182
Epoch 1922/5000
26/26 - 1s - loss: 0.7914 - val_loss: 1.2176
Epoch 1923/5000
26/26 - 1s - loss: 0.7894 - val_loss: 1.2169
Epoch 1924/5000
26/26 - 1s - loss: 0.7890 - val_loss: 1.2167
Epoch 1925/5000
26/26 - 1s - loss: 0.7892 - val_loss: 1.2173
Epoch 1926/5000
26/26 - 1s - loss: 0.7903 - val_loss: 1.2164
Epoch 1927/5000
26/26 - 1s - loss: 0.7892 - val_loss: 1.2156
Epoch 1928/5000
26/26 - 1s - loss: 0.7872 - val_loss: 1.2141
Epoch 1929/5000
26/26 - 1s - loss: 0.7898 - val_loss: 1.2149
Epoch 1930/5000
26/26 - 1s - loss: 0.7880 - val_loss: 1.2127
Epoch 01930: val_loss improved from 1.21829 to 1.21271, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1931/5000
26/26 - 1s - loss: 0.7879 - val_loss: 1.2137
Epoch 1932/5000
26/26 - 1s - loss: 0.7885 - val_loss: 1.2138
Epoch 1933/5000
26/26 - 2s - loss: 0.7859 - val_loss: 1.2140
Epoch 1934/5000
26/26 - 1s - loss: 0.7853 - val_loss: 1.2139
Epoch 1935/5000
26/26 - 1s - loss: 0.7855 - val_loss: 1.2136
Epoch 1936/5000
26/26 - 1s - loss: 0.7847 - val_loss: 1.2141
Epoch 1937/5000
26/26 - 1s - loss: 0.7858 - val_loss: 1.2128
Epoch 1938/5000
26/26 - 1s - loss: 0.7843 - val_loss: 1.2116
Epoch 1939/5000
26/26 - 1s - loss: 0.7845 - val_loss: 1.2107
Epoch 1940/5000
26/26 - 1s - loss: 0.7844 - val_loss: 1.2099
Epoch 01940: val_loss improved from 1.21271 to 1.20986, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1941/5000
26/26 - 1s - loss: 0.7836 - val_loss: 1.2110
Epoch 1942/5000
26/26 - 1s - loss: 0.7833 - val_loss: 1.2102
Epoch 1943/5000
26/26 - 1s - loss: 0.7842 - val_loss: 1.2099
Epoch 1944/5000
26/26 - 1s - loss: 0.7832 - val_loss: 1.2095
Epoch 1945/5000
26/26 - 1s - loss: 0.7822 - val_loss: 1.2096
Epoch 1946/5000
26/26 - 1s - loss: 0.7812 - val_loss: 1.2082
Epoch 1947/5000
26/26 - 1s - loss: 0.7828 - val_loss: 1.2084
Epoch 1948/5000
26/26 - 1s - loss: 0.7813 - val_loss: 1.2074
Epoch 1949/5000
26/26 - 1s - loss: 0.7798 - val_loss: 1.2070
Epoch 1950/5000
26/26 - 1s - loss: 0.7799 - val_loss: 1.2056
Epoch 01950: val_loss improved from 1.20986 to 1.20561, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1951/5000
26/26 - 1s - loss: 0.7814 - val_loss: 1.2074
Epoch 1952/5000
26/26 - 1s - loss: 0.7786 - val_loss: 1.2072
Epoch 1953/5000
26/26 - 1s - loss: 0.7792 - val_loss: 1.2047
Epoch 1954/5000
26/26 - 1s - loss: 0.7787 - val_loss: 1.2047
Epoch 1955/5000
26/26 - 1s - loss: 0.7792 - val_loss: 1.2028
Epoch 1956/5000
26/26 - 1s - loss: 0.7785 - val_loss: 1.2045
Epoch 1957/5000
26/26 - 1s - loss: 0.7772 - val_loss: 1.2044
Epoch 1958/5000
26/26 - 1s - loss: 0.7789 - val_loss: 1.2030
Epoch 1959/5000
26/26 - 1s - loss: 0.7776 - val_loss: 1.2026
Epoch 1960/5000
26/26 - 1s - loss: 0.7763 - val_loss: 1.2044
Epoch 01960: val_loss improved from 1.20561 to 1.20441, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1961/5000
26/26 - 1s - loss: 0.7770 - val_loss: 1.2040
Epoch 1962/5000
26/26 - 1s - loss: 0.7761 - val_loss: 1.2037
Epoch 1963/5000
26/26 - 1s - loss: 0.7765 - val_loss: 1.2026
Epoch 1964/5000
26/26 - 1s - loss: 0.7752 - val_loss: 1.2029
Epoch 1965/5000
26/26 - 1s - loss: 0.7750 - val_loss: 1.2019
Epoch 1966/5000
26/26 - 1s - loss: 0.7738 - val_loss: 1.2021
Epoch 1967/5000
26/26 - 1s - loss: 0.7752 - val_loss: 1.1999
Epoch 1968/5000
26/26 - 1s - loss: 0.7734 - val_loss: 1.2025
Epoch 1969/5000
26/26 - 1s - loss: 0.7756 - val_loss: 1.2004
Epoch 1970/5000
26/26 - 1s - loss: 0.7736 - val_loss: 1.2006
Epoch 01970: val_loss improved from 1.20441 to 1.20055, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1971/5000
26/26 - 1s - loss: 0.7727 - val_loss: 1.1988
Epoch 1972/5000
26/26 - 1s - loss: 0.7732 - val_loss: 1.1998
Epoch 1973/5000
26/26 - 1s - loss: 0.7740 - val_loss: 1.2005
Epoch 1974/5000
26/26 - 2s - loss: 0.7708 - val_loss: 1.1982
Epoch 1975/5000
26/26 - 1s - loss: 0.7704 - val_loss: 1.1991
Epoch 1976/5000
26/26 - 1s - loss: 0.7716 - val_loss: 1.1981
Epoch 1977/5000
26/26 - 1s - loss: 0.7720 - val_loss: 1.1981
Epoch 1978/5000
26/26 - 1s - loss: 0.7706 - val_loss: 1.1984
Epoch 1979/5000
26/26 - 1s - loss: 0.7694 - val_loss: 1.1970
Epoch 1980/5000
26/26 - 1s - loss: 0.7694 - val_loss: 1.1987
Epoch 01980: val_loss improved from 1.20055 to 1.19871, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1981/5000
26/26 - 1s - loss: 0.7692 - val_loss: 1.1966
Epoch 1982/5000
26/26 - 2s - loss: 0.7684 - val_loss: 1.1948
Epoch 1983/5000
26/26 - 1s - loss: 0.7687 - val_loss: 1.1968
Epoch 1984/5000
26/26 - 1s - loss: 0.7690 - val_loss: 1.1955
Epoch 1985/5000
26/26 - 1s - loss: 0.7681 - val_loss: 1.1949
Epoch 1986/5000
26/26 - 1s - loss: 0.7682 - val_loss: 1.1946
Epoch 1987/5000
26/26 - 1s - loss: 0.7673 - val_loss: 1.1949
Epoch 1988/5000
26/26 - 1s - loss: 0.7676 - val_loss: 1.1959
Epoch 1989/5000
26/26 - 1s - loss: 0.7677 - val_loss: 1.1937
Epoch 1990/5000
26/26 - 1s - loss: 0.7668 - val_loss: 1.1935
Epoch 01990: val_loss improved from 1.19871 to 1.19351, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 1991/5000
26/26 - 1s - loss: 0.7668 - val_loss: 1.1934
Epoch 1992/5000
26/26 - 1s - loss: 0.7658 - val_loss: 1.1933
Epoch 1993/5000
26/26 - 1s - loss: 0.7662 - val_loss: 1.1922
Epoch 1994/5000
26/26 - 1s - loss: 0.7627 - val_loss: 1.1935
Epoch 1995/5000
26/26 - 1s - loss: 0.7631 - val_loss: 1.1921
Epoch 1996/5000
26/26 - 1s - loss: 0.7644 - val_loss: 1.1914
Epoch 1997/5000
26/26 - 1s - loss: 0.7649 - val_loss: 1.1897
Epoch 1998/5000
26/26 - 1s - loss: 0.7632 - val_loss: 1.1918
Epoch 1999/5000
26/26 - 1s - loss: 0.7632 - val_loss: 1.1914
Epoch 2000/5000
26/26 - 1s - loss: 0.7638 - val_loss: 1.1889
Epoch 02000: val_loss improved from 1.19351 to 1.18895, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 2001/5000
26/26 - 1s - loss: 0.7639 - val_loss: 1.1901
Epoch 2002/5000
26/26 - 1s - loss: 0.7629 - val_loss: 1.1898
Epoch 2003/5000
26/26 - 1s - loss: 0.7609 - val_loss: 1.1900
Epoch 2004/5000
26/26 - 1s - loss: 0.7632 - val_loss: 1.1886
Epoch 2005/5000
26/26 - 1s - loss: 0.7613 - val_loss: 1.1880
Epoch 2006/5000
26/26 - 1s - loss: 0.7603 - val_loss: 1.1884
Epoch 2007/5000
26/26 - 1s - loss: 0.7592 - val_loss: 1.1884
Epoch 2008/5000
26/26 - 1s - loss: 0.7591 - val_loss: 1.1879
Epoch 2009/5000
26/26 - 2s - loss: 0.7605 - val_loss: 1.1872
Epoch 2010/5000
26/26 - 1s - loss: 0.7597 - val_loss: 1.1857
Epoch 02010: val_loss improved from 1.18895 to 1.18574, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 2011/5000
26/26 - 1s - loss: 0.7593 - val_loss: 1.1868
Epoch 2012/5000
26/26 - 1s - loss: 0.7592 - val_loss: 1.1858
Epoch 2013/5000
26/26 - 1s - loss: 0.7587 - val_loss: 1.1872
Epoch 2014/5000
26/26 - 1s - loss: 0.7589 - val_loss: 1.1859
Epoch 2015/5000
26/26 - 1s - loss: 0.7581 - val_loss: 1.1854
Epoch 2016/5000
26/26 - 1s - loss: 0.7560 - val_loss: 1.1837
Epoch 2017/5000
26/26 - 1s - loss: 0.7560 - val_loss: 1.1843
Epoch 2018/5000
26/26 - 1s - loss: 0.7576 - val_loss: 1.1844
Epoch 2019/5000
26/26 - 1s - loss: 0.7573 - val_loss: 1.1840
Epoch 2020/5000
26/26 - 1s - loss: 0.7562 - val_loss: 1.1836
Epoch 02020: val_loss improved from 1.18574 to 1.18356, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 2021/5000
26/26 - 1s - loss: 0.7561 - val_loss: 1.1830
Epoch 2022/5000
26/26 - 1s - loss: 0.7560 - val_loss: 1.1827
Epoch 2023/5000
26/26 - 1s - loss: 0.7545 - val_loss: 1.1834
Epoch 2024/5000
26/26 - 1s - loss: 0.7546 - val_loss: 1.1824
Epoch 2025/5000
26/26 - 1s - loss: 0.7544 - val_loss: 1.1820
Epoch 2026/5000
26/26 - 1s - loss: 0.7544 - val_loss: 1.1812
Epoch 2027/5000
26/26 - 1s - loss: 0.7535 - val_loss: 1.1807
Epoch 2028/5000
26/26 - 1s - loss: 0.7521 - val_loss: 1.1813
Epoch 2029/5000
26/26 - 1s - loss: 0.7525 - val_loss: 1.1796
Epoch 2030/5000
26/26 - 1s - loss: 0.7529 - val_loss: 1.1802
Epoch 02030: val_loss improved from 1.18356 to 1.18017, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 2031/5000
26/26 - 1s - loss: 0.7520 - val_loss: 1.1782
Epoch 2032/5000
26/26 - 1s - loss: 0.7520 - val_loss: 1.1794
Epoch 2033/5000
26/26 - 1s - loss: 0.7517 - val_loss: 1.1783
Epoch 2034/5000
26/26 - 1s - loss: 0.7514 - val_loss: 1.1778
Epoch 2035/5000
26/26 - 1s - loss: 0.7522 - val_loss: 1.1775
Epoch 2036/5000
26/26 - 1s - loss: 0.7509 - val_loss: 1.1793
Epoch 2037/5000
26/26 - 1s - loss: 0.7494 - val_loss: 1.1794
Epoch 2038/5000
26/26 - 1s - loss: 0.7503 - val_loss: 1.1764
Epoch 2039/5000
26/26 - 1s - loss: 0.7488 - val_loss: 1.1771
Epoch 2040/5000
26/26 - 1s - loss: 0.7491 - val_loss: 1.1766
Epoch 02040: val_loss improved from 1.18017 to 1.17658, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 2041/5000
26/26 - 1s - loss: 0.7486 - val_loss: 1.1754
Epoch 2042/5000
26/26 - 1s - loss: 0.7487 - val_loss: 1.1754
Epoch 2043/5000
26/26 - 1s - loss: 0.7493 - val_loss: 1.1752
Epoch 2044/5000
26/26 - 1s - loss: 0.7465 - val_loss: 1.1748
Epoch 2045/5000
26/26 - 1s - loss: 0.7472 - val_loss: 1.1735
Epoch 2046/5000
26/26 - 1s - loss: 0.7495 - val_loss: 1.1743
Epoch 2047/5000
26/26 - 1s - loss: 0.7476 - val_loss: 1.1744
Epoch 2048/5000
26/26 - 1s - loss: 0.7474 - val_loss: 1.1746
Epoch 2049/5000
26/26 - 1s - loss: 0.7464 - val_loss: 1.1738
Epoch 2050/5000
26/26 - 1s - loss: 0.7456 - val_loss: 1.1742
Epoch 02050: val_loss improved from 1.17658 to 1.17423, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 2051/5000
26/26 - 1s - loss: 0.7443 - val_loss: 1.1718
Epoch 2052/5000
26/26 - 1s - loss: 0.7460 - val_loss: 1.1731
Epoch 2053/5000
26/26 - 1s - loss: 0.7459 - val_loss: 1.1722
Epoch 2054/5000
26/26 - 1s - loss: 0.7457 - val_loss: 1.1730
Epoch 2055/5000
26/26 - 1s - loss: 0.7439 - val_loss: 1.1728
Epoch 2056/5000
26/26 - 1s - loss: 0.7444 - val_loss: 1.1710
Epoch 2057/5000
26/26 - 1s - loss: 0.7435 - val_loss: 1.1706
Epoch 2058/5000
26/26 - 1s - loss: 0.7433 - val_loss: 1.1712
Epoch 2059/5000
26/26 - 1s - loss: 0.7440 - val_loss: 1.1717
Epoch 2060/5000
26/26 - 1s - loss: 0.7427 - val_loss: 1.1702
Epoch 02060: val_loss improved from 1.17423 to 1.17016, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 2061/5000
26/26 - 1s - loss: 0.7427 - val_loss: 1.1697
Epoch 2062/5000
26/26 - 1s - loss: 0.7422 - val_loss: 1.1696
Epoch 2063/5000
26/26 - 1s - loss: 0.7429 - val_loss: 1.1692
Epoch 2064/5000
26/26 - 1s - loss: 0.7406 - val_loss: 1.1682
Epoch 2065/5000
26/26 - 2s - loss: 0.7411 - val_loss: 1.1686
Epoch 2066/5000
26/26 - 1s - loss: 0.7407 - val_loss: 1.1695
Epoch 2067/5000
26/26 - 1s - loss: 0.7399 - val_loss: 1.1685
Epoch 2068/5000
26/26 - 1s - loss: 0.7406 - val_loss: 1.1675
Epoch 2069/5000
26/26 - 1s - loss: 0.7404 - val_loss: 1.1671
Epoch 2070/5000
26/26 - 1s - loss: 0.7405 - val_loss: 1.1674
Epoch 02070: val_loss improved from 1.17016 to 1.16739, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 2071/5000
26/26 - 1s - loss: 0.7401 - val_loss: 1.1663
Epoch 2072/5000
26/26 - 1s - loss: 0.7393 - val_loss: 1.1669
Epoch 2073/5000
26/26 - 1s - loss: 0.7393 - val_loss: 1.1665
Epoch 2074/5000
26/26 - 1s - loss: 0.7383 - val_loss: 1.1665
Epoch 2075/5000
26/26 - 1s - loss: 0.7381 - val_loss: 1.1650
Epoch 2076/5000
26/26 - 1s - loss: 0.7386 - val_loss: 1.1653
Epoch 2077/5000
26/26 - 1s - loss: 0.7369 - val_loss: 1.1670
Epoch 2078/5000
26/26 - 1s - loss: 0.7356 - val_loss: 1.1654
Epoch 2079/5000
26/26 - 1s - loss: 0.7362 - val_loss: 1.1649
Epoch 2080/5000
26/26 - 1s - loss: 0.7360 - val_loss: 1.1640
Epoch 02080: val_loss improved from 1.16739 to 1.16396, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 2081/5000
26/26 - 1s - loss: 0.7360 - val_loss: 1.1633
Epoch 2082/5000
26/26 - 1s - loss: 0.7365 - val_loss: 1.1635
Epoch 2083/5000
26/26 - 1s - loss: 0.7344 - val_loss: 1.1625
Epoch 2084/5000
26/26 - 1s - loss: 0.7344 - val_loss: 1.1619
Epoch 2085/5000
26/26 - 1s - loss: 0.7349 - val_loss: 1.1612
Epoch 2086/5000
26/26 - 1s - loss: 0.7343 - val_loss: 1.1617
Epoch 2087/5000
26/26 - 1s - loss: 0.7338 - val_loss: 1.1611
Epoch 2088/5000
26/26 - 1s - loss: 0.7341 - val_loss: 1.1617
Epoch 2089/5000
26/26 - 1s - loss: 0.7329 - val_loss: 1.1610
Epoch 2090/5000
26/26 - 1s - loss: 0.7326 - val_loss: 1.1614
Epoch 02090: val_loss improved from 1.16396 to 1.16138, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 2091/5000
26/26 - 1s - loss: 0.7319 - val_loss: 1.1608
Epoch 2092/5000
26/26 - 1s - loss: 0.7335 - val_loss: 1.1598
Epoch 2093/5000
26/26 - 1s - loss: 0.7329 - val_loss: 1.1613
Epoch 2094/5000
26/26 - 1s - loss: 0.7318 - val_loss: 1.1594
Epoch 2095/5000
26/26 - 1s - loss: 0.7311 - val_loss: 1.1594
Epoch 2096/5000
26/26 - 1s - loss: 0.7308 - val_loss: 1.1597
Epoch 2097/5000
26/26 - 1s - loss: 0.7306 - val_loss: 1.1599
Epoch 2098/5000
26/26 - 1s - loss: 0.7289 - val_loss: 1.1588
Epoch 2099/5000
26/26 - 1s - loss: 0.7310 - val_loss: 1.1581
Epoch 2100/5000
26/26 - 1s - loss: 0.7311 - val_loss: 1.1582
Epoch 02100: val_loss improved from 1.16138 to 1.15823, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 2101/5000
26/26 - 1s - loss: 0.7294 - val_loss: 1.1576
Epoch 2102/5000
26/26 - 1s - loss: 0.7284 - val_loss: 1.1567
Epoch 2103/5000
26/26 - 1s - loss: 0.7294 - val_loss: 1.1570
Epoch 2104/5000
26/26 - 1s - loss: 0.7293 - val_loss: 1.1582
Epoch 2105/5000
26/26 - 1s - loss: 0.7290 - val_loss: 1.1561
Epoch 2106/5000
26/26 - 2s - loss: 0.7292 - val_loss: 1.1568
Epoch 2107/5000
26/26 - 1s - loss: 0.7284 - val_loss: 1.1558
Epoch 2108/5000
26/26 - 1s - loss: 0.7276 - val_loss: 1.1546
Epoch 2109/5000
26/26 - 1s - loss: 0.7270 - val_loss: 1.1551
Epoch 2110/5000
26/26 - 1s - loss: 0.7272 - val_loss: 1.1541
Epoch 02110: val_loss improved from 1.15823 to 1.15414, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 2111/5000
26/26 - 1s - loss: 0.7281 - val_loss: 1.1529
Epoch 2112/5000
26/26 - 1s - loss: 0.7262 - val_loss: 1.1543
Epoch 2113/5000
26/26 - 1s - loss: 0.7255 - val_loss: 1.1542
Epoch 2114/5000
26/26 - 1s - loss: 0.7256 - val_loss: 1.1521
Epoch 2115/5000
26/26 - 1s - loss: 0.7249 - val_loss: 1.1524
Epoch 2116/5000
26/26 - 1s - loss: 0.7253 - val_loss: 1.1528
Epoch 2117/5000
26/26 - 1s - loss: 0.7261 - val_loss: 1.1520
Epoch 2118/5000
26/26 - 1s - loss: 0.7234 - val_loss: 1.1521
Epoch 2119/5000
26/26 - 1s - loss: 0.7245 - val_loss: 1.1515
Epoch 2120/5000
26/26 - 1s - loss: 0.7233 - val_loss: 1.1511
Epoch 02120: val_loss improved from 1.15414 to 1.15115, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 2121/5000
26/26 - 1s - loss: 0.7239 - val_loss: 1.1514
Epoch 2122/5000
26/26 - 1s - loss: 0.7249 - val_loss: 1.1530
Epoch 2123/5000
26/26 - 1s - loss: 0.7235 - val_loss: 1.1506
Epoch 2124/5000
26/26 - 1s - loss: 0.7233 - val_loss: 1.1515
Epoch 2125/5000
26/26 - 1s - loss: 0.7214 - val_loss: 1.1512
Epoch 2126/5000
26/26 - 1s - loss: 0.7227 - val_loss: 1.1488
Epoch 2127/5000
26/26 - 1s - loss: 0.7218 - val_loss: 1.1488
Epoch 2128/5000
26/26 - 1s - loss: 0.7207 - val_loss: 1.1486
Epoch 2129/5000
26/26 - 1s - loss: 0.7197 - val_loss: 1.1480
Epoch 2130/5000
26/26 - 1s - loss: 0.7222 - val_loss: 1.1488
Epoch 02130: val_loss improved from 1.15115 to 1.14878, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 2131/5000
26/26 - 1s - loss: 0.7200 - val_loss: 1.1480
Epoch 2132/5000
26/26 - 1s - loss: 0.7197 - val_loss: 1.1480
Epoch 2133/5000
26/26 - 1s - loss: 0.7193 - val_loss: 1.1473
Epoch 2134/5000
26/26 - 1s - loss: 0.7194 - val_loss: 1.1470
Epoch 2135/5000
26/26 - 1s - loss: 0.7186 - val_loss: 1.1472
Epoch 2136/5000
26/26 - 1s - loss: 0.7205 - val_loss: 1.1477
Epoch 2137/5000
26/26 - 1s - loss: 0.7186 - val_loss: 1.1462
Epoch 2138/5000
26/26 - 1s - loss: 0.7187 - val_loss: 1.1462
Epoch 2139/5000
26/26 - 1s - loss: 0.7189 - val_loss: 1.1460
Epoch 2140/5000
26/26 - 1s - loss: 0.7177 - val_loss: 1.1442
Epoch 02140: val_loss improved from 1.14878 to 1.14418, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 2141/5000
26/26 - 1s - loss: 0.7191 - val_loss: 1.1449
Epoch 2142/5000
26/26 - 1s - loss: 0.7168 - val_loss: 1.1440
Epoch 2143/5000
26/26 - 1s - loss: 0.7173 - val_loss: 1.1441
Epoch 2144/5000
26/26 - 1s - loss: 0.7162 - val_loss: 1.1436
Epoch 2145/5000
26/26 - 1s - loss: 0.7170 - val_loss: 1.1434
Epoch 2146/5000
26/26 - 1s - loss: 0.7168 - val_loss: 1.1427
Epoch 2147/5000
26/26 - 1s - loss: 0.7157 - val_loss: 1.1427
Epoch 2148/5000
26/26 - 2s - loss: 0.7149 - val_loss: 1.1428
Epoch 2149/5000
26/26 - 1s - loss: 0.7147 - val_loss: 1.1428
Epoch 2150/5000
26/26 - 2s - loss: 0.7143 - val_loss: 1.1421
Epoch 02150: val_loss improved from 1.14418 to 1.14209, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 2151/5000
26/26 - 1s - loss: 0.7132 - val_loss: 1.1416
Epoch 2152/5000
26/26 - 1s - loss: 0.7141 - val_loss: 1.1419
Epoch 2153/5000
26/26 - 1s - loss: 0.7132 - val_loss: 1.1405
Epoch 2154/5000
26/26 - 1s - loss: 0.7133 - val_loss: 1.1401
Epoch 2155/5000
26/26 - 1s - loss: 0.7129 - val_loss: 1.1396
Epoch 2156/5000
26/26 - 1s - loss: 0.7119 - val_loss: 1.1395
Epoch 2157/5000
26/26 - 1s - loss: 0.7128 - val_loss: 1.1394
Epoch 2158/5000
26/26 - 1s - loss: 0.7130 - val_loss: 1.1402
Epoch 2159/5000
26/26 - 1s - loss: 0.7118 - val_loss: 1.1397
Epoch 2160/5000
26/26 - 1s - loss: 0.7129 - val_loss: 1.1396
Epoch 02160: val_loss improved from 1.14209 to 1.13964, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 2161/5000
26/26 - 1s - loss: 0.7112 - val_loss: 1.1394
Epoch 2162/5000
26/26 - 1s - loss: 0.7114 - val_loss: 1.1392
Epoch 2163/5000
26/26 - 1s - loss: 0.7101 - val_loss: 1.1395
Epoch 2164/5000
26/26 - 1s - loss: 0.7108 - val_loss: 1.1393
Epoch 2165/5000
26/26 - 1s - loss: 0.7102 - val_loss: 1.1389
Epoch 2166/5000
26/26 - 1s - loss: 0.7095 - val_loss: 1.1375
Epoch 2167/5000
26/26 - 1s - loss: 0.7099 - val_loss: 1.1375
Epoch 2168/5000
26/26 - 1s - loss: 0.7104 - val_loss: 1.1377
Epoch 2169/5000
26/26 - 1s - loss: 0.7104 - val_loss: 1.1371
Epoch 2170/5000
26/26 - 1s - loss: 0.7091 - val_loss: 1.1376
Epoch 02170: val_loss improved from 1.13964 to 1.13761, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 2171/5000
26/26 - 1s - loss: 0.7090 - val_loss: 1.1374
Epoch 2172/5000
26/26 - 1s - loss: 0.7098 - val_loss: 1.1371
Epoch 2173/5000
26/26 - 1s - loss: 0.7069 - val_loss: 1.1363
Epoch 2174/5000
26/26 - 1s - loss: 0.7085 - val_loss: 1.1377
Epoch 2175/5000
26/26 - 1s - loss: 0.7063 - val_loss: 1.1351
Epoch 2176/5000
26/26 - 1s - loss: 0.7062 - val_loss: 1.1340
Epoch 2177/5000
26/26 - 1s - loss: 0.7074 - val_loss: 1.1340
Epoch 2178/5000
26/26 - 1s - loss: 0.7063 - val_loss: 1.1338
Epoch 2179/5000
26/26 - 1s - loss: 0.7063 - val_loss: 1.1326
Epoch 2180/5000
26/26 - 1s - loss: 0.7058 - val_loss: 1.1349
Epoch 02180: val_loss improved from 1.13761 to 1.13489, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 2181/5000
26/26 - 1s - loss: 0.7055 - val_loss: 1.1337
Epoch 2182/5000
26/26 - 1s - loss: 0.7066 - val_loss: 1.1331
Epoch 2183/5000
26/26 - 1s - loss: 0.7037 - val_loss: 1.1329
Epoch 2184/5000
26/26 - 1s - loss: 0.7049 - val_loss: 1.1312
Epoch 2185/5000
26/26 - 1s - loss: 0.7049 - val_loss: 1.1318
Epoch 2186/5000
26/26 - 1s - loss: 0.7036 - val_loss: 1.1314
Epoch 2187/5000
26/26 - 1s - loss: 0.7034 - val_loss: 1.1317
Epoch 2188/5000
26/26 - 1s - loss: 0.7035 - val_loss: 1.1313
Epoch 2189/5000
26/26 - 2s - loss: 0.7023 - val_loss: 1.1320
Epoch 2190/5000
26/26 - 1s - loss: 0.7032 - val_loss: 1.1320
Epoch 02190: val_loss improved from 1.13489 to 1.13197, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 2191/5000
26/26 - 1s - loss: 0.7015 - val_loss: 1.1317
Epoch 2192/5000
26/26 - 1s - loss: 0.7023 - val_loss: 1.1307
Epoch 2193/5000
26/26 - 1s - loss: 0.7022 - val_loss: 1.1320
Epoch 2194/5000
26/26 - 1s - loss: 0.7015 - val_loss: 1.1304
Epoch 2195/5000
26/26 - 1s - loss: 0.7020 - val_loss: 1.1310
Epoch 2196/5000
26/26 - 1s - loss: 0.7003 - val_loss: 1.1295
Epoch 2197/5000
26/26 - 1s - loss: 0.6999 - val_loss: 1.1292
Epoch 2198/5000
26/26 - 1s - loss: 0.7019 - val_loss: 1.1294
Epoch 2199/5000
26/26 - 1s - loss: 0.6998 - val_loss: 1.1282
Epoch 2200/5000
26/26 - 1s - loss: 0.6975 - val_loss: 1.1286
Epoch 02200: val_loss improved from 1.13197 to 1.12861, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 2201/5000
26/26 - 1s - loss: 0.6989 - val_loss: 1.1286
Epoch 2202/5000
26/26 - 1s - loss: 0.6990 - val_loss: 1.1274
Epoch 2203/5000
26/26 - 1s - loss: 0.6999 - val_loss: 1.1281
Epoch 2204/5000
26/26 - 2s - loss: 0.6996 - val_loss: 1.1260
Epoch 2205/5000
26/26 - 1s - loss: 0.6988 - val_loss: 1.1270
Epoch 2206/5000
26/26 - 1s - loss: 0.6986 - val_loss: 1.1270
Epoch 2207/5000
26/26 - 1s - loss: 0.6992 - val_loss: 1.1243
Epoch 2208/5000
26/26 - 1s - loss: 0.6982 - val_loss: 1.1255
Epoch 2209/5000
26/26 - 1s - loss: 0.6974 - val_loss: 1.1256
Epoch 2210/5000
26/26 - 1s - loss: 0.6978 - val_loss: 1.1240
Epoch 02210: val_loss improved from 1.12861 to 1.12403, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 2211/5000
26/26 - 1s - loss: 0.6963 - val_loss: 1.1245
Epoch 2212/5000
26/26 - 1s - loss: 0.6953 - val_loss: 1.1244
Epoch 2213/5000
26/26 - 1s - loss: 0.6973 - val_loss: 1.1239
Epoch 2214/5000
26/26 - 1s - loss: 0.6964 - val_loss: 1.1237
Epoch 2215/5000
26/26 - 1s - loss: 0.6954 - val_loss: 1.1248
Epoch 2216/5000
26/26 - 1s - loss: 0.6955 - val_loss: 1.1241
Epoch 2217/5000
26/26 - 1s - loss: 0.6942 - val_loss: 1.1222
Epoch 2218/5000
26/26 - 1s - loss: 0.6960 - val_loss: 1.1237
Epoch 2219/5000
26/26 - 1s - loss: 0.6938 - val_loss: 1.1225
Epoch 2220/5000
26/26 - 1s - loss: 0.6958 - val_loss: 1.1233
Epoch 02220: val_loss improved from 1.12403 to 1.12331, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 2221/5000
26/26 - 1s - loss: 0.6945 - val_loss: 1.1223
Epoch 2222/5000
26/26 - 2s - loss: 0.6930 - val_loss: 1.1219
Epoch 2223/5000
26/26 - 1s - loss: 0.6930 - val_loss: 1.1222
Epoch 2224/5000
26/26 - 1s - loss: 0.6913 - val_loss: 1.1213
Epoch 2225/5000
26/26 - 1s - loss: 0.6937 - val_loss: 1.1212
Epoch 2226/5000
26/26 - 1s - loss: 0.6930 - val_loss: 1.1195
Epoch 2227/5000
26/26 - 1s - loss: 0.6937 - val_loss: 1.1194
Epoch 2228/5000
26/26 - 1s - loss: 0.6932 - val_loss: 1.1200
Epoch 2229/5000
26/26 - 1s - loss: 0.6924 - val_loss: 1.1199
Epoch 2230/5000
26/26 - 2s - loss: 0.6907 - val_loss: 1.1200
Epoch 02230: val_loss improved from 1.12331 to 1.11998, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 2231/5000
26/26 - 1s - loss: 0.6914 - val_loss: 1.1192
Epoch 2232/5000
26/26 - 1s - loss: 0.6910 - val_loss: 1.1188
Epoch 2233/5000
26/26 - 1s - loss: 0.6904 - val_loss: 1.1177
Epoch 2234/5000
26/26 - 1s - loss: 0.6915 - val_loss: 1.1186
Epoch 2235/5000
26/26 - 1s - loss: 0.6906 - val_loss: 1.1193
Epoch 2236/5000
26/26 - 1s - loss: 0.6896 - val_loss: 1.1170
Epoch 2237/5000
26/26 - 1s - loss: 0.6894 - val_loss: 1.1191
Epoch 2238/5000
26/26 - 1s - loss: 0.6904 - val_loss: 1.1186
Epoch 2239/5000
26/26 - 2s - loss: 0.6887 - val_loss: 1.1174
Epoch 2240/5000
26/26 - 1s - loss: 0.6881 - val_loss: 1.1174
Epoch 02240: val_loss improved from 1.11998 to 1.11738, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 2241/5000
26/26 - 1s - loss: 0.6880 - val_loss: 1.1165
Epoch 2242/5000
26/26 - 1s - loss: 0.6879 - val_loss: 1.1151
Epoch 2243/5000
26/26 - 1s - loss: 0.6891 - val_loss: 1.1164
Epoch 2244/5000
26/26 - 1s - loss: 0.6887 - val_loss: 1.1163
Epoch 2245/5000
26/26 - 1s - loss: 0.6865 - val_loss: 1.1163
Epoch 2246/5000
26/26 - 1s - loss: 0.6867 - val_loss: 1.1151
Epoch 2247/5000
26/26 - 1s - loss: 0.6867 - val_loss: 1.1149
Epoch 2248/5000
26/26 - 1s - loss: 0.6855 - val_loss: 1.1153
Epoch 2249/5000
26/26 - 1s - loss: 0.6864 - val_loss: 1.1157
Epoch 2250/5000
26/26 - 1s - loss: 0.6855 - val_loss: 1.1149
Epoch 02250: val_loss improved from 1.11738 to 1.11489, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 2251/5000
26/26 - 1s - loss: 0.6859 - val_loss: 1.1148
Epoch 2252/5000
26/26 - 1s - loss: 0.6863 - val_loss: 1.1145
Epoch 2253/5000
26/26 - 1s - loss: 0.6864 - val_loss: 1.1140
Epoch 2254/5000
26/26 - 2s - loss: 0.6849 - val_loss: 1.1142
Epoch 2255/5000
26/26 - 1s - loss: 0.6853 - val_loss: 1.1136
Epoch 2256/5000
26/26 - 1s - loss: 0.6840 - val_loss: 1.1133
Epoch 2257/5000
26/26 - 1s - loss: 0.6851 - val_loss: 1.1133
Epoch 2258/5000
26/26 - 1s - loss: 0.6832 - val_loss: 1.1113
Epoch 2259/5000
26/26 - 1s - loss: 0.6838 - val_loss: 1.1119
Epoch 2260/5000
26/26 - 1s - loss: 0.6829 - val_loss: 1.1120
Epoch 02260: val_loss improved from 1.11489 to 1.11195, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 2261/5000
26/26 - 1s - loss: 0.6841 - val_loss: 1.1120
Epoch 2262/5000
26/26 - 1s - loss: 0.6823 - val_loss: 1.1117
Epoch 2263/5000
26/26 - 1s - loss: 0.6839 - val_loss: 1.1104
Epoch 2264/5000
26/26 - 1s - loss: 0.6816 - val_loss: 1.1105
Epoch 2265/5000
26/26 - 1s - loss: 0.6824 - val_loss: 1.1104
Epoch 2266/5000
26/26 - 1s - loss: 0.6822 - val_loss: 1.1097
Epoch 2267/5000
26/26 - 1s - loss: 0.6816 - val_loss: 1.1100
Epoch 2268/5000
26/26 - 1s - loss: 0.6811 - val_loss: 1.1101
Epoch 2269/5000
26/26 - 1s - loss: 0.6804 - val_loss: 1.1101
Epoch 2270/5000
26/26 - 1s - loss: 0.6807 - val_loss: 1.1076
Epoch 02270: val_loss improved from 1.11195 to 1.10757, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 2271/5000
26/26 - 1s - loss: 0.6808 - val_loss: 1.1088
Epoch 2272/5000
26/26 - 1s - loss: 0.6805 - val_loss: 1.1085
Epoch 2273/5000
26/26 - 1s - loss: 0.6806 - val_loss: 1.1089
Epoch 2274/5000
26/26 - 1s - loss: 0.6795 - val_loss: 1.1087
Epoch 2275/5000
26/26 - 1s - loss: 0.6793 - val_loss: 1.1076
Epoch 2276/5000
26/26 - 1s - loss: 0.6801 - val_loss: 1.1077
Epoch 2277/5000
26/26 - 1s - loss: 0.6786 - val_loss: 1.1071
Epoch 2278/5000
26/26 - 1s - loss: 0.6766 - val_loss: 1.1070
Epoch 2279/5000
26/26 - 1s - loss: 0.6773 - val_loss: 1.1075
Epoch 2280/5000
26/26 - 1s - loss: 0.6788 - val_loss: 1.1065
Epoch 02280: val_loss improved from 1.10757 to 1.10650, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 2281/5000
26/26 - 1s - loss: 0.6771 - val_loss: 1.1076
Epoch 2282/5000
26/26 - 1s - loss: 0.6768 - val_loss: 1.1062
Epoch 2283/5000
26/26 - 1s - loss: 0.6784 - val_loss: 1.1065
Epoch 2284/5000
26/26 - 1s - loss: 0.6781 - val_loss: 1.1062
Epoch 2285/5000
26/26 - 1s - loss: 0.6776 - val_loss: 1.1072
Epoch 2286/5000
26/26 - 1s - loss: 0.6770 - val_loss: 1.1055
Epoch 2287/5000
26/26 - 1s - loss: 0.6773 - val_loss: 1.1052
Epoch 2288/5000
26/26 - 1s - loss: 0.6757 - val_loss: 1.1048
Epoch 2289/5000
26/26 - 1s - loss: 0.6756 - val_loss: 1.1052
Epoch 2290/5000
26/26 - 1s - loss: 0.6760 - val_loss: 1.1044
Epoch 02290: val_loss improved from 1.10650 to 1.10442, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 2291/5000
26/26 - 1s - loss: 0.6740 - val_loss: 1.1039
Epoch 2292/5000
26/26 - 1s - loss: 0.6751 - val_loss: 1.1037
Epoch 2293/5000
26/26 - 1s - loss: 0.6748 - val_loss: 1.1045
Epoch 2294/5000
26/26 - 1s - loss: 0.6754 - val_loss: 1.1034
Epoch 2295/5000
26/26 - 1s - loss: 0.6746 - val_loss: 1.1045
Epoch 2296/5000
26/26 - 1s - loss: 0.6740 - val_loss: 1.1035
Epoch 2297/5000
26/26 - 1s - loss: 0.6733 - val_loss: 1.1032
Epoch 2298/5000
26/26 - 1s - loss: 0.6729 - val_loss: 1.1022
Epoch 2299/5000
26/26 - 1s - loss: 0.6739 - val_loss: 1.1026
Epoch 2300/5000
26/26 - 1s - loss: 0.6732 - val_loss: 1.1018
Epoch 02300: val_loss improved from 1.10442 to 1.10184, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 2301/5000
26/26 - 1s - loss: 0.6726 - val_loss: 1.1017
Epoch 2302/5000
26/26 - 1s - loss: 0.6731 - val_loss: 1.1015
Epoch 2303/5000
26/26 - 1s - loss: 0.6707 - val_loss: 1.1013
Epoch 2304/5000
26/26 - 1s - loss: 0.6717 - val_loss: 1.1006
Epoch 2305/5000
26/26 - 1s - loss: 0.6728 - val_loss: 1.1007
Epoch 2306/5000
26/26 - 1s - loss: 0.6705 - val_loss: 1.1002
Epoch 2307/5000
26/26 - 1s - loss: 0.6711 - val_loss: 1.1012
Epoch 2308/5000
26/26 - 1s - loss: 0.6694 - val_loss: 1.1003
Epoch 2309/5000
26/26 - 1s - loss: 0.6692 - val_loss: 1.1003
Epoch 2310/5000
26/26 - 1s - loss: 0.6698 - val_loss: 1.0992
Epoch 02310: val_loss improved from 1.10184 to 1.09923, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 2311/5000
26/26 - 1s - loss: 0.6701 - val_loss: 1.0979
Epoch 2312/5000
26/26 - 1s - loss: 0.6706 - val_loss: 1.0984
Epoch 2313/5000
26/26 - 2s - loss: 0.6691 - val_loss: 1.0983
Epoch 2314/5000
26/26 - 1s - loss: 0.6696 - val_loss: 1.0986
Epoch 2315/5000
26/26 - 1s - loss: 0.6698 - val_loss: 1.0990
Epoch 2316/5000
26/26 - 1s - loss: 0.6685 - val_loss: 1.0985
Epoch 2317/5000
26/26 - 1s - loss: 0.6683 - val_loss: 1.0975
Epoch 2318/5000
26/26 - 1s - loss: 0.6677 - val_loss: 1.0973
Epoch 2319/5000
26/26 - 1s - loss: 0.6682 - val_loss: 1.0983
Epoch 2320/5000
26/26 - 1s - loss: 0.6680 - val_loss: 1.0974
Epoch 02320: val_loss improved from 1.09923 to 1.09737, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 2321/5000
26/26 - 1s - loss: 0.6687 - val_loss: 1.0975
Epoch 2322/5000
26/26 - 1s - loss: 0.6678 - val_loss: 1.0959
Epoch 2323/5000
26/26 - 1s - loss: 0.6671 - val_loss: 1.0950
Epoch 2324/5000
26/26 - 1s - loss: 0.6674 - val_loss: 1.0961
Epoch 2325/5000
26/26 - 1s - loss: 0.6679 - val_loss: 1.0953
Epoch 2326/5000
26/26 - 1s - loss: 0.6654 - val_loss: 1.0949
Epoch 2327/5000
26/26 - 1s - loss: 0.6666 - val_loss: 1.0946
Epoch 2328/5000
26/26 - 1s - loss: 0.6664 - val_loss: 1.0948
Epoch 2329/5000
26/26 - 1s - loss: 0.6653 - val_loss: 1.0945
Epoch 2330/5000
26/26 - 1s - loss: 0.6653 - val_loss: 1.0949
Epoch 02330: val_loss improved from 1.09737 to 1.09487, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 2331/5000
26/26 - 1s - loss: 0.6658 - val_loss: 1.0930
Epoch 2332/5000
26/26 - 1s - loss: 0.6649 - val_loss: 1.0937
Epoch 2333/5000
26/26 - 1s - loss: 0.6643 - val_loss: 1.0926
Epoch 2334/5000
26/26 - 1s - loss: 0.6630 - val_loss: 1.0941
Epoch 2335/5000
26/26 - 1s - loss: 0.6650 - val_loss: 1.0937
Epoch 2336/5000
26/26 - 1s - loss: 0.6632 - val_loss: 1.0939
Epoch 2337/5000
26/26 - 1s - loss: 0.6643 - val_loss: 1.0929
Epoch 2338/5000
26/26 - 1s - loss: 0.6623 - val_loss: 1.0923
Epoch 2339/5000
26/26 - 1s - loss: 0.6630 - val_loss: 1.0910
Epoch 2340/5000
26/26 - 1s - loss: 0.6622 - val_loss: 1.0903
Epoch 02340: val_loss improved from 1.09487 to 1.09032, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 2341/5000
26/26 - 1s - loss: 0.6631 - val_loss: 1.0903
Epoch 2342/5000
26/26 - 1s - loss: 0.6637 - val_loss: 1.0907
Epoch 2343/5000
26/26 - 1s - loss: 0.6626 - val_loss: 1.0911
Epoch 2344/5000
26/26 - 1s - loss: 0.6623 - val_loss: 1.0912
Epoch 2345/5000
26/26 - 1s - loss: 0.6602 - val_loss: 1.0901
Epoch 2346/5000
26/26 - 1s - loss: 0.6608 - val_loss: 1.0905
Epoch 2347/5000
26/26 - 1s - loss: 0.6613 - val_loss: 1.0892
Epoch 2348/5000
26/26 - 1s - loss: 0.6611 - val_loss: 1.0881
Epoch 2349/5000
26/26 - 1s - loss: 0.6600 - val_loss: 1.0890
Epoch 2350/5000
26/26 - 1s - loss: 0.6606 - val_loss: 1.0887
Epoch 02350: val_loss improved from 1.09032 to 1.08865, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 2351/5000
26/26 - 1s - loss: 0.6586 - val_loss: 1.0882
Epoch 2352/5000
26/26 - 1s - loss: 0.6596 - val_loss: 1.0879
Epoch 2353/5000
26/26 - 1s - loss: 0.6598 - val_loss: 1.0879
Epoch 2354/5000
26/26 - 1s - loss: 0.6599 - val_loss: 1.0872
Epoch 2355/5000
26/26 - 2s - loss: 0.6580 - val_loss: 1.0880
Epoch 2356/5000
26/26 - 1s - loss: 0.6583 - val_loss: 1.0866
Epoch 2357/5000
26/26 - 1s - loss: 0.6586 - val_loss: 1.0869
Epoch 2358/5000
26/26 - 1s - loss: 0.6572 - val_loss: 1.0874
Epoch 2359/5000
26/26 - 1s - loss: 0.6581 - val_loss: 1.0868
Epoch 2360/5000
26/26 - 1s - loss: 0.6579 - val_loss: 1.0865
Epoch 02360: val_loss improved from 1.08865 to 1.08651, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 2361/5000
26/26 - 1s - loss: 0.6566 - val_loss: 1.0874
Epoch 2362/5000
26/26 - 1s - loss: 0.6582 - val_loss: 1.0877
Epoch 2363/5000
26/26 - 1s - loss: 0.6574 - val_loss: 1.0856
Epoch 2364/5000
26/26 - 1s - loss: 0.6580 - val_loss: 1.0869
Epoch 2365/5000
26/26 - 1s - loss: 0.6573 - val_loss: 1.0858
Epoch 2366/5000
26/26 - 1s - loss: 0.6571 - val_loss: 1.0857
Epoch 2367/5000
26/26 - 1s - loss: 0.6577 - val_loss: 1.0858
Epoch 2368/5000
26/26 - 1s - loss: 0.6565 - val_loss: 1.0860
Epoch 2369/5000
26/26 - 1s - loss: 0.6550 - val_loss: 1.0841
Epoch 2370/5000
26/26 - 1s - loss: 0.6558 - val_loss: 1.0844
Epoch 02370: val_loss improved from 1.08651 to 1.08442, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 2371/5000
26/26 - 1s - loss: 0.6551 - val_loss: 1.0856
Epoch 2372/5000
26/26 - 1s - loss: 0.6560 - val_loss: 1.0851
Epoch 2373/5000
26/26 - 1s - loss: 0.6551 - val_loss: 1.0842
Epoch 2374/5000
26/26 - 1s - loss: 0.6540 - val_loss: 1.0832
Epoch 2375/5000
26/26 - 1s - loss: 0.6545 - val_loss: 1.0840
Epoch 2376/5000
26/26 - 1s - loss: 0.6541 - val_loss: 1.0823
Epoch 2377/5000
26/26 - 1s - loss: 0.6544 - val_loss: 1.0833
Epoch 2378/5000
26/26 - 1s - loss: 0.6553 - val_loss: 1.0829
Epoch 2379/5000
26/26 - 1s - loss: 0.6540 - val_loss: 1.0815
Epoch 2380/5000
26/26 - 1s - loss: 0.6541 - val_loss: 1.0809
Epoch 02380: val_loss improved from 1.08442 to 1.08086, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 2381/5000
26/26 - 1s - loss: 0.6521 - val_loss: 1.0805
Epoch 2382/5000
26/26 - 2s - loss: 0.6519 - val_loss: 1.0813
Epoch 2383/5000
26/26 - 1s - loss: 0.6529 - val_loss: 1.0821
Epoch 2384/5000
26/26 - 1s - loss: 0.6517 - val_loss: 1.0817
Epoch 2385/5000
26/26 - 1s - loss: 0.6520 - val_loss: 1.0825
Epoch 2386/5000
26/26 - 1s - loss: 0.6512 - val_loss: 1.0801
Epoch 2387/5000
26/26 - 1s - loss: 0.6510 - val_loss: 1.0797
Epoch 2388/5000
26/26 - 1s - loss: 0.6510 - val_loss: 1.0786
Epoch 2389/5000
26/26 - 1s - loss: 0.6497 - val_loss: 1.0798
Epoch 2390/5000
26/26 - 1s - loss: 0.6515 - val_loss: 1.0788
Epoch 02390: val_loss improved from 1.08086 to 1.07879, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 2391/5000
26/26 - 1s - loss: 0.6499 - val_loss: 1.0796
Epoch 2392/5000
26/26 - 1s - loss: 0.6501 - val_loss: 1.0787
Epoch 2393/5000
26/26 - 1s - loss: 0.6493 - val_loss: 1.0801
Epoch 2394/5000
26/26 - 1s - loss: 0.6507 - val_loss: 1.0797
Epoch 2395/5000
26/26 - 1s - loss: 0.6492 - val_loss: 1.0786
Epoch 2396/5000
26/26 - 2s - loss: 0.6492 - val_loss: 1.0789
Epoch 2397/5000
26/26 - 1s - loss: 0.6490 - val_loss: 1.0780
Epoch 2398/5000
26/26 - 1s - loss: 0.6480 - val_loss: 1.0772
Epoch 2399/5000
26/26 - 1s - loss: 0.6484 - val_loss: 1.0768
Epoch 2400/5000
26/26 - 1s - loss: 0.6482 - val_loss: 1.0779
Epoch 02400: val_loss improved from 1.07879 to 1.07793, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 2401/5000
26/26 - 1s - loss: 0.6493 - val_loss: 1.0782
Epoch 2402/5000
26/26 - 1s - loss: 0.6483 - val_loss: 1.0770
Epoch 2403/5000
26/26 - 1s - loss: 0.6469 - val_loss: 1.0769
Epoch 2404/5000
26/26 - 1s - loss: 0.6456 - val_loss: 1.0764
Epoch 2405/5000
26/26 - 1s - loss: 0.6469 - val_loss: 1.0760
Epoch 2406/5000
26/26 - 1s - loss: 0.6478 - val_loss: 1.0760
Epoch 2407/5000
26/26 - 1s - loss: 0.6473 - val_loss: 1.0771
Epoch 2408/5000
26/26 - 1s - loss: 0.6471 - val_loss: 1.0767
Epoch 2409/5000
26/26 - 1s - loss: 0.6475 - val_loss: 1.0761
Epoch 2410/5000
26/26 - 1s - loss: 0.6455 - val_loss: 1.0746
Epoch 02410: val_loss improved from 1.07793 to 1.07457, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 2411/5000
26/26 - 1s - loss: 0.6448 - val_loss: 1.0758
Epoch 2412/5000
26/26 - 1s - loss: 0.6457 - val_loss: 1.0761
Epoch 2413/5000
26/26 - 1s - loss: 0.6438 - val_loss: 1.0754
Epoch 2414/5000
26/26 - 1s - loss: 0.6453 - val_loss: 1.0764
Epoch 2415/5000
26/26 - 1s - loss: 0.6455 - val_loss: 1.0753
Epoch 2416/5000
26/26 - 1s - loss: 0.6451 - val_loss: 1.0739
Epoch 2417/5000
26/26 - 1s - loss: 0.6457 - val_loss: 1.0739
Epoch 2418/5000
26/26 - 1s - loss: 0.6439 - val_loss: 1.0734
Epoch 2419/5000
26/26 - 1s - loss: 0.6435 - val_loss: 1.0737
Epoch 2420/5000
26/26 - 1s - loss: 0.6430 - val_loss: 1.0745
Epoch 02420: val_loss improved from 1.07457 to 1.07446, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 2421/5000
26/26 - 1s - loss: 0.6426 - val_loss: 1.0721
Epoch 2422/5000
26/26 - 1s - loss: 0.6430 - val_loss: 1.0718
Epoch 2423/5000
26/26 - 1s - loss: 0.6429 - val_loss: 1.0730
Epoch 2424/5000
26/26 - 1s - loss: 0.6437 - val_loss: 1.0726
Epoch 2425/5000
26/26 - 1s - loss: 0.6418 - val_loss: 1.0719
Epoch 2426/5000
26/26 - 1s - loss: 0.6424 - val_loss: 1.0719
Epoch 2427/5000
26/26 - 1s - loss: 0.6421 - val_loss: 1.0713
Epoch 2428/5000
26/26 - 1s - loss: 0.6419 - val_loss: 1.0710
Epoch 2429/5000
26/26 - 1s - loss: 0.6405 - val_loss: 1.0709
Epoch 2430/5000
26/26 - 1s - loss: 0.6406 - val_loss: 1.0707
Epoch 02430: val_loss improved from 1.07446 to 1.07075, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 2431/5000
26/26 - 1s - loss: 0.6409 - val_loss: 1.0700
Epoch 2432/5000
26/26 - 1s - loss: 0.6401 - val_loss: 1.0689
Epoch 2433/5000
26/26 - 1s - loss: 0.6403 - val_loss: 1.0692
Epoch 2434/5000
26/26 - 1s - loss: 0.6406 - val_loss: 1.0690
Epoch 2435/5000
26/26 - 1s - loss: 0.6401 - val_loss: 1.0700
Epoch 2436/5000
26/26 - 1s - loss: 0.6410 - val_loss: 1.0696
Epoch 2437/5000
26/26 - 2s - loss: 0.6410 - val_loss: 1.0686
Epoch 2438/5000
26/26 - 1s - loss: 0.6396 - val_loss: 1.0682
Epoch 2439/5000
26/26 - 1s - loss: 0.6393 - val_loss: 1.0689
Epoch 2440/5000
26/26 - 1s - loss: 0.6385 - val_loss: 1.0686
Epoch 02440: val_loss improved from 1.07075 to 1.06863, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 2441/5000
26/26 - 1s - loss: 0.6388 - val_loss: 1.0689
Epoch 2442/5000
26/26 - 1s - loss: 0.6387 - val_loss: 1.0687
Epoch 2443/5000
26/26 - 1s - loss: 0.6389 - val_loss: 1.0665
Epoch 2444/5000
26/26 - 1s - loss: 0.6380 - val_loss: 1.0673
Epoch 2445/5000
26/26 - 1s - loss: 0.6374 - val_loss: 1.0671
Epoch 2446/5000
26/26 - 1s - loss: 0.6381 - val_loss: 1.0668
Epoch 2447/5000
26/26 - 1s - loss: 0.6378 - val_loss: 1.0677
Epoch 2448/5000
26/26 - 1s - loss: 0.6363 - val_loss: 1.0674
Epoch 2449/5000
26/26 - 1s - loss: 0.6370 - val_loss: 1.0667
Epoch 2450/5000
26/26 - 1s - loss: 0.6368 - val_loss: 1.0668
Epoch 02450: val_loss improved from 1.06863 to 1.06676, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 2451/5000
26/26 - 1s - loss: 0.6367 - val_loss: 1.0648
Epoch 2452/5000
26/26 - 1s - loss: 0.6356 - val_loss: 1.0665
Epoch 2453/5000
26/26 - 1s - loss: 0.6365 - val_loss: 1.0664
Epoch 2454/5000
26/26 - 1s - loss: 0.6352 - val_loss: 1.0650
Epoch 2455/5000
26/26 - 1s - loss: 0.6362 - val_loss: 1.0655
Epoch 2456/5000
26/26 - 1s - loss: 0.6343 - val_loss: 1.0650
Epoch 2457/5000
26/26 - 1s - loss: 0.6349 - val_loss: 1.0653
Epoch 2458/5000
26/26 - 1s - loss: 0.6360 - val_loss: 1.0639
Epoch 2459/5000
26/26 - 1s - loss: 0.6347 - val_loss: 1.0627
Epoch 2460/5000
26/26 - 1s - loss: 0.6350 - val_loss: 1.0635
Epoch 02460: val_loss improved from 1.06676 to 1.06352, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 2461/5000
26/26 - 1s - loss: 0.6346 - val_loss: 1.0631
Epoch 2462/5000
26/26 - 1s - loss: 0.6348 - val_loss: 1.0631
Epoch 2463/5000
26/26 - 1s - loss: 0.6334 - val_loss: 1.0638
Epoch 2464/5000
26/26 - 1s - loss: 0.6341 - val_loss: 1.0627
Epoch 2465/5000
26/26 - 1s - loss: 0.6324 - val_loss: 1.0614
Epoch 2466/5000
26/26 - 1s - loss: 0.6340 - val_loss: 1.0624
Epoch 2467/5000
26/26 - 1s - loss: 0.6323 - val_loss: 1.0615
Epoch 2468/5000
26/26 - 1s - loss: 0.6329 - val_loss: 1.0601
Epoch 2469/5000
26/26 - 1s - loss: 0.6326 - val_loss: 1.0613
Epoch 2470/5000
26/26 - 1s - loss: 0.6320 - val_loss: 1.0624
Epoch 02470: val_loss improved from 1.06352 to 1.06242, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 2471/5000
26/26 - 1s - loss: 0.6333 - val_loss: 1.0610
Epoch 2472/5000
26/26 - 1s - loss: 0.6325 - val_loss: 1.0602
Epoch 2473/5000
26/26 - 1s - loss: 0.6313 - val_loss: 1.0613
Epoch 2474/5000
26/26 - 1s - loss: 0.6317 - val_loss: 1.0600
Epoch 2475/5000
26/26 - 1s - loss: 0.6321 - val_loss: 1.0607
Epoch 2476/5000
26/26 - 1s - loss: 0.6312 - val_loss: 1.0603
Epoch 2477/5000
26/26 - 1s - loss: 0.6307 - val_loss: 1.0608
Epoch 2478/5000
26/26 - 1s - loss: 0.6304 - val_loss: 1.0603
Epoch 2479/5000
26/26 - 2s - loss: 0.6303 - val_loss: 1.0600
Epoch 2480/5000
26/26 - 1s - loss: 0.6297 - val_loss: 1.0581
Epoch 02480: val_loss improved from 1.06242 to 1.05810, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 2481/5000
26/26 - 1s - loss: 0.6305 - val_loss: 1.0593
Epoch 2482/5000
26/26 - 1s - loss: 0.6298 - val_loss: 1.0600
Epoch 2483/5000
26/26 - 1s - loss: 0.6288 - val_loss: 1.0590
Epoch 2484/5000
26/26 - 1s - loss: 0.6288 - val_loss: 1.0579
Epoch 2485/5000
26/26 - 1s - loss: 0.6291 - val_loss: 1.0581
Epoch 2486/5000
26/26 - 1s - loss: 0.6290 - val_loss: 1.0579
Epoch 2487/5000
26/26 - 1s - loss: 0.6286 - val_loss: 1.0584
Epoch 2488/5000
26/26 - 1s - loss: 0.6272 - val_loss: 1.0575
Epoch 2489/5000
26/26 - 1s - loss: 0.6289 - val_loss: 1.0573
Epoch 2490/5000
26/26 - 1s - loss: 0.6281 - val_loss: 1.0576
Epoch 02490: val_loss improved from 1.05810 to 1.05762, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 2491/5000
26/26 - 1s - loss: 0.6285 - val_loss: 1.0576
Epoch 2492/5000
26/26 - 1s - loss: 0.6266 - val_loss: 1.0559
Epoch 2493/5000
26/26 - 1s - loss: 0.6268 - val_loss: 1.0572
Epoch 2494/5000
26/26 - 1s - loss: 0.6269 - val_loss: 1.0565
Epoch 2495/5000
26/26 - 1s - loss: 0.6273 - val_loss: 1.0558
Epoch 2496/5000
26/26 - 1s - loss: 0.6278 - val_loss: 1.0555
Epoch 2497/5000
26/26 - 1s - loss: 0.6282 - val_loss: 1.0554
Epoch 2498/5000
26/26 - 1s - loss: 0.6272 - val_loss: 1.0557
Epoch 2499/5000
26/26 - 1s - loss: 0.6268 - val_loss: 1.0555
Epoch 2500/5000
26/26 - 1s - loss: 0.6259 - val_loss: 1.0553
Epoch 02500: val_loss improved from 1.05762 to 1.05533, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 2501/5000
26/26 - 1s - loss: 0.6252 - val_loss: 1.0548
Epoch 2502/5000
26/26 - 2s - loss: 0.6253 - val_loss: 1.0554
Epoch 2503/5000
26/26 - 2s - loss: 0.6252 - val_loss: 1.0544
Epoch 2504/5000
26/26 - 1s - loss: 0.6252 - val_loss: 1.0535
Epoch 2505/5000
26/26 - 1s - loss: 0.6246 - val_loss: 1.0550
Epoch 2506/5000
26/26 - 1s - loss: 0.6250 - val_loss: 1.0535
Epoch 2507/5000
26/26 - 2s - loss: 0.6252 - val_loss: 1.0550
Epoch 2508/5000
26/26 - 2s - loss: 0.6253 - val_loss: 1.0545
Epoch 2509/5000
26/26 - 1s - loss: 0.6233 - val_loss: 1.0530
Epoch 2510/5000
26/26 - 1s - loss: 0.6221 - val_loss: 1.0540
Epoch 02510: val_loss improved from 1.05533 to 1.05402, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 2511/5000
26/26 - 1s - loss: 0.6222 - val_loss: 1.0536
Epoch 2512/5000
26/26 - 1s - loss: 0.6225 - val_loss: 1.0525
Epoch 2513/5000
26/26 - 1s - loss: 0.6228 - val_loss: 1.0527
Epoch 2514/5000
26/26 - 2s - loss: 0.6233 - val_loss: 1.0518
Epoch 2515/5000
26/26 - 1s - loss: 0.6229 - val_loss: 1.0517
Epoch 2516/5000
26/26 - 1s - loss: 0.6218 - val_loss: 1.0514
Epoch 2517/5000
26/26 - 1s - loss: 0.6213 - val_loss: 1.0505
Epoch 2518/5000
26/26 - 1s - loss: 0.6229 - val_loss: 1.0505
Epoch 2519/5000
26/26 - 1s - loss: 0.6217 - val_loss: 1.0513
Epoch 2520/5000
26/26 - 2s - loss: 0.6223 - val_loss: 1.0504
Epoch 02520: val_loss improved from 1.05402 to 1.05035, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 2521/5000
26/26 - 1s - loss: 0.6208 - val_loss: 1.0510
Epoch 2522/5000
26/26 - 1s - loss: 0.6218 - val_loss: 1.0511
Epoch 2523/5000
26/26 - 1s - loss: 0.6208 - val_loss: 1.0506
Epoch 2524/5000
26/26 - 1s - loss: 0.6223 - val_loss: 1.0510
Epoch 2525/5000
26/26 - 1s - loss: 0.6203 - val_loss: 1.0503
Epoch 2526/5000
26/26 - 1s - loss: 0.6199 - val_loss: 1.0511
Epoch 2527/5000
26/26 - 1s - loss: 0.6202 - val_loss: 1.0500
Epoch 2528/5000
26/26 - 2s - loss: 0.6193 - val_loss: 1.0500
Epoch 2529/5000
26/26 - 1s - loss: 0.6198 - val_loss: 1.0495
Epoch 2530/5000
26/26 - 1s - loss: 0.6200 - val_loss: 1.0500
Epoch 02530: val_loss improved from 1.05035 to 1.05001, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 2531/5000
26/26 - 1s - loss: 0.6193 - val_loss: 1.0504
Epoch 2532/5000
26/26 - 2s - loss: 0.6192 - val_loss: 1.0488
Epoch 2533/5000
26/26 - 1s - loss: 0.6185 - val_loss: 1.0470
Epoch 2534/5000
26/26 - 1s - loss: 0.6181 - val_loss: 1.0482
Epoch 2535/5000
26/26 - 1s - loss: 0.6179 - val_loss: 1.0479
Epoch 2536/5000
26/26 - 1s - loss: 0.6189 - val_loss: 1.0487
Epoch 2537/5000
26/26 - 1s - loss: 0.6188 - val_loss: 1.0485
Epoch 2538/5000
26/26 - 1s - loss: 0.6172 - val_loss: 1.0490
Epoch 2539/5000
26/26 - 1s - loss: 0.6177 - val_loss: 1.0475
Epoch 2540/5000
26/26 - 1s - loss: 0.6179 - val_loss: 1.0469
Epoch 02540: val_loss improved from 1.05001 to 1.04687, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 2541/5000
26/26 - 1s - loss: 0.6158 - val_loss: 1.0462
Epoch 2542/5000
26/26 - 1s - loss: 0.6167 - val_loss: 1.0465
Epoch 2543/5000
26/26 - 1s - loss: 0.6159 - val_loss: 1.0459
Epoch 2544/5000
26/26 - 1s - loss: 0.6167 - val_loss: 1.0470
Epoch 2545/5000
26/26 - 1s - loss: 0.6156 - val_loss: 1.0445
Epoch 2546/5000
26/26 - 1s - loss: 0.6149 - val_loss: 1.0459
Epoch 2547/5000
26/26 - 2s - loss: 0.6179 - val_loss: 1.0443
Epoch 2548/5000
26/26 - 1s - loss: 0.6149 - val_loss: 1.0449
Epoch 2549/5000
26/26 - 1s - loss: 0.6148 - val_loss: 1.0451
Epoch 2550/5000
26/26 - 1s - loss: 0.6161 - val_loss: 1.0447
Epoch 02550: val_loss improved from 1.04687 to 1.04472, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 2551/5000
26/26 - 1s - loss: 0.6148 - val_loss: 1.0440
Epoch 2552/5000
26/26 - 1s - loss: 0.6155 - val_loss: 1.0435
Epoch 2553/5000
26/26 - 1s - loss: 0.6139 - val_loss: 1.0448
Epoch 2554/5000
26/26 - 1s - loss: 0.6143 - val_loss: 1.0437
Epoch 2555/5000
26/26 - 2s - loss: 0.6138 - val_loss: 1.0443
Epoch 2556/5000
26/26 - 1s - loss: 0.6146 - val_loss: 1.0435
Epoch 2557/5000
26/26 - 1s - loss: 0.6140 - val_loss: 1.0433
Epoch 2558/5000
26/26 - 1s - loss: 0.6140 - val_loss: 1.0429
Epoch 2559/5000
26/26 - 1s - loss: 0.6132 - val_loss: 1.0444
Epoch 2560/5000
26/26 - 1s - loss: 0.6132 - val_loss: 1.0433
Epoch 02560: val_loss improved from 1.04472 to 1.04329, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 2561/5000
26/26 - 1s - loss: 0.6139 - val_loss: 1.0438
Epoch 2562/5000
26/26 - 1s - loss: 0.6123 - val_loss: 1.0427
Epoch 2563/5000
26/26 - 1s - loss: 0.6122 - val_loss: 1.0417
Epoch 2564/5000
26/26 - 1s - loss: 0.6125 - val_loss: 1.0412
Epoch 2565/5000
26/26 - 2s - loss: 0.6124 - val_loss: 1.0423
Epoch 2566/5000
26/26 - 1s - loss: 0.6125 - val_loss: 1.0434
Epoch 2567/5000
26/26 - 2s - loss: 0.6130 - val_loss: 1.0411
Epoch 2568/5000
26/26 - 1s - loss: 0.6107 - val_loss: 1.0415
Epoch 2569/5000
26/26 - 1s - loss: 0.6114 - val_loss: 1.0402
Epoch 2570/5000
26/26 - 1s - loss: 0.6126 - val_loss: 1.0406
Epoch 02570: val_loss improved from 1.04329 to 1.04062, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 2571/5000
26/26 - 1s - loss: 0.6109 - val_loss: 1.0421
Epoch 2572/5000
26/26 - 1s - loss: 0.6106 - val_loss: 1.0415
Epoch 2573/5000
26/26 - 2s - loss: 0.6091 - val_loss: 1.0419
Epoch 2574/5000
26/26 - 1s - loss: 0.6110 - val_loss: 1.0404
Epoch 2575/5000
26/26 - 1s - loss: 0.6117 - val_loss: 1.0415
Epoch 2576/5000
26/26 - 1s - loss: 0.6112 - val_loss: 1.0399
Epoch 2577/5000
26/26 - 1s - loss: 0.6097 - val_loss: 1.0403
Epoch 2578/5000
26/26 - 1s - loss: 0.6100 - val_loss: 1.0399
Epoch 2579/5000
26/26 - 1s - loss: 0.6088 - val_loss: 1.0401
Epoch 2580/5000
26/26 - 2s - loss: 0.6084 - val_loss: 1.0409
Epoch 02580: val_loss did not improve from 1.04062
Epoch 2581/5000
26/26 - 2s - loss: 0.6090 - val_loss: 1.0397
Epoch 2582/5000
26/26 - 1s - loss: 0.6093 - val_loss: 1.0393
Epoch 2583/5000
26/26 - 1s - loss: 0.6092 - val_loss: 1.0387
Epoch 2584/5000
26/26 - 1s - loss: 0.6079 - val_loss: 1.0374
Epoch 2585/5000
26/26 - 1s - loss: 0.6082 - val_loss: 1.0375
Epoch 2586/5000
26/26 - 1s - loss: 0.6077 - val_loss: 1.0382
Epoch 2587/5000
26/26 - 2s - loss: 0.6087 - val_loss: 1.0377
Epoch 2588/5000
26/26 - 1s - loss: 0.6064 - val_loss: 1.0374
Epoch 2589/5000
26/26 - 1s - loss: 0.6071 - val_loss: 1.0381
Epoch 2590/5000
26/26 - 1s - loss: 0.6068 - val_loss: 1.0376
Epoch 02590: val_loss improved from 1.04062 to 1.03761, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 2591/5000
26/26 - 1s - loss: 0.6066 - val_loss: 1.0364
Epoch 2592/5000
26/26 - 1s - loss: 0.6067 - val_loss: 1.0372
Epoch 2593/5000
26/26 - 1s - loss: 0.6075 - val_loss: 1.0383
Epoch 2594/5000
26/26 - 1s - loss: 0.6070 - val_loss: 1.0373
Epoch 2595/5000
26/26 - 1s - loss: 0.6062 - val_loss: 1.0361
Epoch 2596/5000
26/26 - 1s - loss: 0.6069 - val_loss: 1.0379
Epoch 2597/5000
26/26 - 2s - loss: 0.6066 - val_loss: 1.0360
Epoch 2598/5000
26/26 - 1s - loss: 0.6048 - val_loss: 1.0369
Epoch 2599/5000
26/26 - 1s - loss: 0.6051 - val_loss: 1.0366
Epoch 2600/5000
26/26 - 2s - loss: 0.6067 - val_loss: 1.0357
Epoch 02600: val_loss improved from 1.03761 to 1.03566, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 2601/5000
26/26 - 1s - loss: 0.6055 - val_loss: 1.0349
Epoch 2602/5000
26/26 - 1s - loss: 0.6060 - val_loss: 1.0357
Epoch 2603/5000
26/26 - 1s - loss: 0.6043 - val_loss: 1.0350
Epoch 2604/5000
26/26 - 1s - loss: 0.6043 - val_loss: 1.0340
Epoch 2605/5000
26/26 - 1s - loss: 0.6042 - val_loss: 1.0341
Epoch 2606/5000
26/26 - 1s - loss: 0.6038 - val_loss: 1.0336
Epoch 2607/5000
26/26 - 1s - loss: 0.6039 - val_loss: 1.0335
Epoch 2608/5000
26/26 - 1s - loss: 0.6047 - val_loss: 1.0329
Epoch 2609/5000
26/26 - 1s - loss: 0.6047 - val_loss: 1.0329
Epoch 2610/5000
26/26 - 1s - loss: 0.6039 - val_loss: 1.0338
Epoch 02610: val_loss improved from 1.03566 to 1.03384, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 2611/5000
26/26 - 1s - loss: 0.6015 - val_loss: 1.0334
Epoch 2612/5000
26/26 - 2s - loss: 0.6028 - val_loss: 1.0332
Epoch 2613/5000
26/26 - 1s - loss: 0.6025 - val_loss: 1.0334
Epoch 2614/5000
26/26 - 1s - loss: 0.6017 - val_loss: 1.0331
Epoch 2615/5000
26/26 - 2s - loss: 0.6021 - val_loss: 1.0341
Epoch 2616/5000
26/26 - 1s - loss: 0.6022 - val_loss: 1.0338
Epoch 2617/5000
26/26 - 1s - loss: 0.6010 - val_loss: 1.0330
Epoch 2618/5000
26/26 - 1s - loss: 0.6027 - val_loss: 1.0323
Epoch 2619/5000
26/26 - 1s - loss: 0.6012 - val_loss: 1.0334
Epoch 2620/5000
26/26 - 2s - loss: 0.6006 - val_loss: 1.0321
Epoch 02620: val_loss improved from 1.03384 to 1.03213, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 2621/5000
26/26 - 1s - loss: 0.6018 - val_loss: 1.0305
Epoch 2622/5000
26/26 - 1s - loss: 0.6015 - val_loss: 1.0311
Epoch 2623/5000
26/26 - 1s - loss: 0.6004 - val_loss: 1.0315
Epoch 2624/5000
26/26 - 1s - loss: 0.6014 - val_loss: 1.0322
Epoch 2625/5000
26/26 - 1s - loss: 0.6010 - val_loss: 1.0311
Epoch 2626/5000
26/26 - 1s - loss: 0.5990 - val_loss: 1.0309
Epoch 2627/5000
26/26 - 1s - loss: 0.6005 - val_loss: 1.0304
Epoch 2628/5000
26/26 - 1s - loss: 0.6001 - val_loss: 1.0297
Epoch 2629/5000
26/26 - 1s - loss: 0.5995 - val_loss: 1.0291
Epoch 2630/5000
26/26 - 1s - loss: 0.5990 - val_loss: 1.0295
Epoch 02630: val_loss improved from 1.03213 to 1.02950, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 2631/5000
26/26 - 1s - loss: 0.5997 - val_loss: 1.0301
Epoch 2632/5000
26/26 - 1s - loss: 0.5997 - val_loss: 1.0288
Epoch 2633/5000
26/26 - 1s - loss: 0.5988 - val_loss: 1.0294
Epoch 2634/5000
26/26 - 1s - loss: 0.5983 - val_loss: 1.0288
Epoch 2635/5000
26/26 - 1s - loss: 0.5988 - val_loss: 1.0299
Epoch 2636/5000
26/26 - 1s - loss: 0.5980 - val_loss: 1.0283
Epoch 2637/5000
26/26 - 1s - loss: 0.5978 - val_loss: 1.0274
Epoch 2638/5000
26/26 - 1s - loss: 0.5981 - val_loss: 1.0268
Epoch 2639/5000
26/26 - 1s - loss: 0.5984 - val_loss: 1.0268
Epoch 2640/5000
26/26 - 1s - loss: 0.5980 - val_loss: 1.0274
Epoch 02640: val_loss improved from 1.02950 to 1.02743, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 2641/5000
26/26 - 1s - loss: 0.5975 - val_loss: 1.0284
Epoch 2642/5000
26/26 - 2s - loss: 0.5984 - val_loss: 1.0274
Epoch 2643/5000
26/26 - 1s - loss: 0.5972 - val_loss: 1.0280
Epoch 2644/5000
26/26 - 1s - loss: 0.5958 - val_loss: 1.0286
Epoch 2645/5000
26/26 - 1s - loss: 0.5966 - val_loss: 1.0269
Epoch 2646/5000
26/26 - 1s - loss: 0.5950 - val_loss: 1.0261
Epoch 2647/5000
26/26 - 1s - loss: 0.5963 - val_loss: 1.0276
Epoch 2648/5000
26/26 - 1s - loss: 0.5963 - val_loss: 1.0270
Epoch 2649/5000
26/26 - 1s - loss: 0.5967 - val_loss: 1.0265
Epoch 2650/5000
26/26 - 1s - loss: 0.5953 - val_loss: 1.0253
Epoch 02650: val_loss improved from 1.02743 to 1.02528, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 2651/5000
26/26 - 1s - loss: 0.5947 - val_loss: 1.0252
Epoch 2652/5000
26/26 - 2s - loss: 0.5956 - val_loss: 1.0257
Epoch 2653/5000
26/26 - 1s - loss: 0.5957 - val_loss: 1.0251
Epoch 2654/5000
26/26 - 1s - loss: 0.5935 - val_loss: 1.0248
Epoch 2655/5000
26/26 - 1s - loss: 0.5950 - val_loss: 1.0239
Epoch 2656/5000
26/26 - 1s - loss: 0.5958 - val_loss: 1.0247
Epoch 2657/5000
26/26 - 1s - loss: 0.5938 - val_loss: 1.0243
Epoch 2658/5000
26/26 - 1s - loss: 0.5940 - val_loss: 1.0245
Epoch 2659/5000
26/26 - 1s - loss: 0.5950 - val_loss: 1.0235
Epoch 2660/5000
26/26 - 1s - loss: 0.5943 - val_loss: 1.0241
Epoch 02660: val_loss improved from 1.02528 to 1.02412, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 2661/5000
26/26 - 1s - loss: 0.5944 - val_loss: 1.0234
Epoch 2662/5000
26/26 - 1s - loss: 0.5940 - val_loss: 1.0247
Epoch 2663/5000
26/26 - 2s - loss: 0.5941 - val_loss: 1.0240
Epoch 2664/5000
26/26 - 1s - loss: 0.5941 - val_loss: 1.0229
Epoch 2665/5000
26/26 - 2s - loss: 0.5933 - val_loss: 1.0233
Epoch 2666/5000
26/26 - 1s - loss: 0.5921 - val_loss: 1.0235
Epoch 2667/5000
26/26 - 1s - loss: 0.5927 - val_loss: 1.0228
Epoch 2668/5000
26/26 - 1s - loss: 0.5930 - val_loss: 1.0221
Epoch 2669/5000
26/26 - 1s - loss: 0.5916 - val_loss: 1.0225
Epoch 2670/5000
26/26 - 1s - loss: 0.5910 - val_loss: 1.0237
Epoch 02670: val_loss improved from 1.02412 to 1.02365, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 2671/5000
26/26 - 1s - loss: 0.5931 - val_loss: 1.0215
Epoch 2672/5000
26/26 - 1s - loss: 0.5909 - val_loss: 1.0230
Epoch 2673/5000
26/26 - 1s - loss: 0.5919 - val_loss: 1.0216
Epoch 2674/5000
26/26 - 1s - loss: 0.5912 - val_loss: 1.0215
Epoch 2675/5000
26/26 - 1s - loss: 0.5900 - val_loss: 1.0204
Epoch 2676/5000
26/26 - 1s - loss: 0.5916 - val_loss: 1.0211
Epoch 2677/5000
26/26 - 1s - loss: 0.5895 - val_loss: 1.0212
Epoch 2678/5000
26/26 - 1s - loss: 0.5908 - val_loss: 1.0211
Epoch 2679/5000
26/26 - 1s - loss: 0.5908 - val_loss: 1.0215
Epoch 2680/5000
26/26 - 1s - loss: 0.5913 - val_loss: 1.0202
Epoch 02680: val_loss improved from 1.02365 to 1.02021, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 2681/5000
26/26 - 1s - loss: 0.5905 - val_loss: 1.0203
Epoch 2682/5000
26/26 - 1s - loss: 0.5902 - val_loss: 1.0188
Epoch 2683/5000
26/26 - 2s - loss: 0.5895 - val_loss: 1.0192
Epoch 2684/5000
26/26 - 2s - loss: 0.5890 - val_loss: 1.0202
Epoch 2685/5000
26/26 - 1s - loss: 0.5894 - val_loss: 1.0183
Epoch 2686/5000
26/26 - 1s - loss: 0.5895 - val_loss: 1.0193
Epoch 2687/5000
26/26 - 1s - loss: 0.5893 - val_loss: 1.0166
Epoch 2688/5000
26/26 - 1s - loss: 0.5889 - val_loss: 1.0180
Epoch 2689/5000
26/26 - 1s - loss: 0.5887 - val_loss: 1.0164
Epoch 2690/5000
26/26 - 1s - loss: 0.5882 - val_loss: 1.0174
Epoch 02690: val_loss improved from 1.02021 to 1.01740, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 2691/5000
26/26 - 2s - loss: 0.5876 - val_loss: 1.0172
Epoch 2692/5000
26/26 - 1s - loss: 0.5872 - val_loss: 1.0178
Epoch 2693/5000
26/26 - 1s - loss: 0.5864 - val_loss: 1.0177
Epoch 2694/5000
26/26 - 1s - loss: 0.5871 - val_loss: 1.0174
Epoch 2695/5000
26/26 - 2s - loss: 0.5864 - val_loss: 1.0165
Epoch 2696/5000
26/26 - 1s - loss: 0.5854 - val_loss: 1.0181
Epoch 2697/5000
26/26 - 1s - loss: 0.5882 - val_loss: 1.0172
Epoch 2698/5000
26/26 - 2s - loss: 0.5871 - val_loss: 1.0163
Epoch 2699/5000
26/26 - 1s - loss: 0.5873 - val_loss: 1.0169
Epoch 2700/5000
26/26 - 1s - loss: 0.5855 - val_loss: 1.0161
Epoch 02700: val_loss improved from 1.01740 to 1.01610, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 2701/5000
26/26 - 1s - loss: 0.5862 - val_loss: 1.0174
Epoch 2702/5000
26/26 - 1s - loss: 0.5860 - val_loss: 1.0168
Epoch 2703/5000
26/26 - 1s - loss: 0.5857 - val_loss: 1.0156
Epoch 2704/5000
26/26 - 1s - loss: 0.5854 - val_loss: 1.0164
Epoch 2705/5000
26/26 - 1s - loss: 0.5857 - val_loss: 1.0163
Epoch 2706/5000
26/26 - 1s - loss: 0.5864 - val_loss: 1.0161
Epoch 2707/5000
26/26 - 1s - loss: 0.5846 - val_loss: 1.0146
Epoch 2708/5000
26/26 - 1s - loss: 0.5854 - val_loss: 1.0153
Epoch 2709/5000
26/26 - 1s - loss: 0.5846 - val_loss: 1.0150
Epoch 2710/5000
26/26 - 2s - loss: 0.5853 - val_loss: 1.0151
Epoch 02710: val_loss improved from 1.01610 to 1.01509, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 2711/5000
26/26 - 1s - loss: 0.5857 - val_loss: 1.0156
Epoch 2712/5000
26/26 - 1s - loss: 0.5846 - val_loss: 1.0142
Epoch 2713/5000
26/26 - 1s - loss: 0.5842 - val_loss: 1.0142
Epoch 2714/5000
26/26 - 1s - loss: 0.5848 - val_loss: 1.0143
Epoch 2715/5000
26/26 - 1s - loss: 0.5840 - val_loss: 1.0139
Epoch 2716/5000
26/26 - 1s - loss: 0.5841 - val_loss: 1.0144
Epoch 2717/5000
26/26 - 1s - loss: 0.5819 - val_loss: 1.0137
Epoch 2718/5000
26/26 - 1s - loss: 0.5833 - val_loss: 1.0154
Epoch 2719/5000
26/26 - 1s - loss: 0.5846 - val_loss: 1.0142
Epoch 2720/5000
26/26 - 1s - loss: 0.5845 - val_loss: 1.0154
Epoch 02720: val_loss did not improve from 1.01509
Epoch 2721/5000
26/26 - 1s - loss: 0.5830 - val_loss: 1.0146
Epoch 2722/5000
26/26 - 1s - loss: 0.5829 - val_loss: 1.0134
Epoch 2723/5000
26/26 - 1s - loss: 0.5828 - val_loss: 1.0136
Epoch 2724/5000
26/26 - 2s - loss: 0.5829 - val_loss: 1.0132
Epoch 2725/5000
26/26 - 1s - loss: 0.5833 - val_loss: 1.0140
Epoch 2726/5000
26/26 - 1s - loss: 0.5811 - val_loss: 1.0130
Epoch 2727/5000
26/26 - 1s - loss: 0.5805 - val_loss: 1.0129
Epoch 2728/5000
26/26 - 1s - loss: 0.5814 - val_loss: 1.0132
Epoch 2729/5000
26/26 - 1s - loss: 0.5810 - val_loss: 1.0132
Epoch 2730/5000
26/26 - 2s - loss: 0.5804 - val_loss: 1.0124
Epoch 02730: val_loss improved from 1.01509 to 1.01244, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 2731/5000
26/26 - 1s - loss: 0.5828 - val_loss: 1.0109
Epoch 2732/5000
26/26 - 1s - loss: 0.5806 - val_loss: 1.0108
Epoch 2733/5000
26/26 - 1s - loss: 0.5807 - val_loss: 1.0127
Epoch 2734/5000
26/26 - 2s - loss: 0.5819 - val_loss: 1.0120
Epoch 2735/5000
26/26 - 1s - loss: 0.5812 - val_loss: 1.0115
Epoch 2736/5000
26/26 - 1s - loss: 0.5809 - val_loss: 1.0100
Epoch 2737/5000
26/26 - 2s - loss: 0.5803 - val_loss: 1.0105
Epoch 2738/5000
26/26 - 1s - loss: 0.5788 - val_loss: 1.0112
Epoch 2739/5000
26/26 - 1s - loss: 0.5795 - val_loss: 1.0104
Epoch 2740/5000
26/26 - 1s - loss: 0.5802 - val_loss: 1.0106
Epoch 02740: val_loss improved from 1.01244 to 1.01055, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 2741/5000
26/26 - 1s - loss: 0.5790 - val_loss: 1.0108
Epoch 2742/5000
26/26 - 1s - loss: 0.5793 - val_loss: 1.0103
Epoch 2743/5000
26/26 - 2s - loss: 0.5782 - val_loss: 1.0112
Epoch 2744/5000
26/26 - 1s - loss: 0.5793 - val_loss: 1.0108
Epoch 2745/5000
26/26 - 1s - loss: 0.5786 - val_loss: 1.0087
Epoch 2746/5000
26/26 - 1s - loss: 0.5784 - val_loss: 1.0090
Epoch 2747/5000
26/26 - 1s - loss: 0.5780 - val_loss: 1.0092
Epoch 2748/5000
26/26 - 1s - loss: 0.5784 - val_loss: 1.0082
Epoch 2749/5000
26/26 - 1s - loss: 0.5777 - val_loss: 1.0076
Epoch 2750/5000
26/26 - 1s - loss: 0.5779 - val_loss: 1.0079
Epoch 02750: val_loss improved from 1.01055 to 1.00792, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 2751/5000
26/26 - 1s - loss: 0.5775 - val_loss: 1.0077
Epoch 2752/5000
26/26 - 1s - loss: 0.5772 - val_loss: 1.0077
Epoch 2753/5000
26/26 - 1s - loss: 0.5767 - val_loss: 1.0069
Epoch 2754/5000
26/26 - 1s - loss: 0.5783 - val_loss: 1.0071
Epoch 2755/5000
26/26 - 1s - loss: 0.5771 - val_loss: 1.0067
Epoch 2756/5000
26/26 - 1s - loss: 0.5774 - val_loss: 1.0072
Epoch 2757/5000
26/26 - 1s - loss: 0.5769 - val_loss: 1.0080
Epoch 2758/5000
26/26 - 1s - loss: 0.5765 - val_loss: 1.0068
Epoch 2759/5000
26/26 - 1s - loss: 0.5764 - val_loss: 1.0066
Epoch 2760/5000
26/26 - 1s - loss: 0.5744 - val_loss: 1.0065
Epoch 02760: val_loss improved from 1.00792 to 1.00654, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 2761/5000
26/26 - 1s - loss: 0.5761 - val_loss: 1.0067
Epoch 2762/5000
26/26 - 1s - loss: 0.5755 - val_loss: 1.0058
Epoch 2763/5000
26/26 - 1s - loss: 0.5757 - val_loss: 1.0065
Epoch 2764/5000
26/26 - 1s - loss: 0.5757 - val_loss: 1.0062
Epoch 2765/5000
26/26 - 1s - loss: 0.5753 - val_loss: 1.0078
Epoch 2766/5000
26/26 - 1s - loss: 0.5741 - val_loss: 1.0067
Epoch 2767/5000
26/26 - 1s - loss: 0.5756 - val_loss: 1.0073
Epoch 2768/5000
26/26 - 1s - loss: 0.5746 - val_loss: 1.0046
Epoch 2769/5000
26/26 - 1s - loss: 0.5741 - val_loss: 1.0049
Epoch 2770/5000
26/26 - 1s - loss: 0.5742 - val_loss: 1.0054
Epoch 02770: val_loss improved from 1.00654 to 1.00539, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 2771/5000
26/26 - 1s - loss: 0.5752 - val_loss: 1.0054
Epoch 2772/5000
26/26 - 2s - loss: 0.5728 - val_loss: 1.0044
Epoch 2773/5000
26/26 - 1s - loss: 0.5747 - val_loss: 1.0043
Epoch 2774/5000
26/26 - 1s - loss: 0.5729 - val_loss: 1.0054
Epoch 2775/5000
26/26 - 1s - loss: 0.5733 - val_loss: 1.0033
Epoch 2776/5000
26/26 - 1s - loss: 0.5734 - val_loss: 1.0025
Epoch 2777/5000
26/26 - 1s - loss: 0.5753 - val_loss: 1.0031
Epoch 2778/5000
26/26 - 1s - loss: 0.5721 - val_loss: 1.0031
Epoch 2779/5000
26/26 - 1s - loss: 0.5719 - val_loss: 1.0032
Epoch 2780/5000
26/26 - 1s - loss: 0.5722 - val_loss: 1.0032
Epoch 02780: val_loss improved from 1.00539 to 1.00319, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 2781/5000
26/26 - 1s - loss: 0.5727 - val_loss: 1.0032
Epoch 2782/5000
26/26 - 1s - loss: 0.5717 - val_loss: 1.0020
Epoch 2783/5000
26/26 - 1s - loss: 0.5715 - val_loss: 1.0038
Epoch 2784/5000
26/26 - 1s - loss: 0.5734 - val_loss: 1.0018
Epoch 2785/5000
26/26 - 1s - loss: 0.5726 - val_loss: 1.0032
Epoch 2786/5000
26/26 - 1s - loss: 0.5724 - val_loss: 1.0029
Epoch 2787/5000
26/26 - 1s - loss: 0.5700 - val_loss: 1.0013
Epoch 2788/5000
26/26 - 1s - loss: 0.5712 - val_loss: 1.0012
Epoch 2789/5000
26/26 - 1s - loss: 0.5713 - val_loss: 0.9999
Epoch 2790/5000
26/26 - 1s - loss: 0.5707 - val_loss: 1.0013
Epoch 02790: val_loss improved from 1.00319 to 1.00128, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 2791/5000
26/26 - 1s - loss: 0.5727 - val_loss: 1.0017
Epoch 2792/5000
26/26 - 1s - loss: 0.5703 - val_loss: 1.0011
Epoch 2793/5000
26/26 - 1s - loss: 0.5708 - val_loss: 1.0003
Epoch 2794/5000
26/26 - 1s - loss: 0.5707 - val_loss: 0.9996
Epoch 2795/5000
26/26 - 1s - loss: 0.5708 - val_loss: 1.0002
Epoch 2796/5000
26/26 - 1s - loss: 0.5716 - val_loss: 1.0003
Epoch 2797/5000
26/26 - 1s - loss: 0.5710 - val_loss: 1.0010
Epoch 2798/5000
26/26 - 1s - loss: 0.5697 - val_loss: 1.0001
Epoch 2799/5000
26/26 - 1s - loss: 0.5702 - val_loss: 0.9998
Epoch 2800/5000
26/26 - 1s - loss: 0.5710 - val_loss: 0.9997
Epoch 02800: val_loss improved from 1.00128 to 0.99968, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 2801/5000
26/26 - 1s - loss: 0.5700 - val_loss: 1.0008
Epoch 2802/5000
26/26 - 1s - loss: 0.5698 - val_loss: 0.9998
Epoch 2803/5000
26/26 - 1s - loss: 0.5687 - val_loss: 1.0009
Epoch 2804/5000
26/26 - 1s - loss: 0.5683 - val_loss: 1.0001
Epoch 2805/5000
26/26 - 1s - loss: 0.5676 - val_loss: 0.9995
Epoch 2806/5000
26/26 - 1s - loss: 0.5678 - val_loss: 0.9995
Epoch 2807/5000
26/26 - 1s - loss: 0.5683 - val_loss: 0.9998
Epoch 2808/5000
26/26 - 1s - loss: 0.5685 - val_loss: 0.9994
Epoch 2809/5000
26/26 - 1s - loss: 0.5679 - val_loss: 0.9987
Epoch 2810/5000
26/26 - 1s - loss: 0.5671 - val_loss: 0.9986
Epoch 02810: val_loss improved from 0.99968 to 0.99865, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 2811/5000
26/26 - 1s - loss: 0.5671 - val_loss: 0.9967
Epoch 2812/5000
26/26 - 1s - loss: 0.5678 - val_loss: 0.9974
Epoch 2813/5000
26/26 - 1s - loss: 0.5665 - val_loss: 0.9981
Epoch 2814/5000
26/26 - 2s - loss: 0.5680 - val_loss: 0.9977
Epoch 2815/5000
26/26 - 2s - loss: 0.5675 - val_loss: 0.9981
Epoch 2816/5000
26/26 - 2s - loss: 0.5665 - val_loss: 0.9981
Epoch 2817/5000
26/26 - 2s - loss: 0.5648 - val_loss: 0.9982
Epoch 2818/5000
26/26 - 2s - loss: 0.5664 - val_loss: 0.9961
Epoch 2819/5000
26/26 - 1s - loss: 0.5663 - val_loss: 0.9965
Epoch 2820/5000
26/26 - 1s - loss: 0.5676 - val_loss: 0.9969
Epoch 02820: val_loss improved from 0.99865 to 0.99692, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 2821/5000
26/26 - 1s - loss: 0.5663 - val_loss: 0.9976
Epoch 2822/5000
26/26 - 1s - loss: 0.5652 - val_loss: 0.9959
Epoch 2823/5000
26/26 - 1s - loss: 0.5657 - val_loss: 0.9954
Epoch 2824/5000
26/26 - 1s - loss: 0.5661 - val_loss: 0.9957
Epoch 2825/5000
26/26 - 1s - loss: 0.5654 - val_loss: 0.9957
Epoch 2826/5000
26/26 - 1s - loss: 0.5650 - val_loss: 0.9957
Epoch 2827/5000
26/26 - 1s - loss: 0.5666 - val_loss: 0.9950
Epoch 2828/5000
26/26 - 1s - loss: 0.5635 - val_loss: 0.9946
Epoch 2829/5000
26/26 - 1s - loss: 0.5650 - val_loss: 0.9941
Epoch 2830/5000
26/26 - 1s - loss: 0.5644 - val_loss: 0.9945
Epoch 02830: val_loss improved from 0.99692 to 0.99445, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 2831/5000
26/26 - 1s - loss: 0.5640 - val_loss: 0.9937
Epoch 2832/5000
26/26 - 1s - loss: 0.5635 - val_loss: 0.9953
Epoch 2833/5000
26/26 - 1s - loss: 0.5643 - val_loss: 0.9940
Epoch 2834/5000
26/26 - 1s - loss: 0.5633 - val_loss: 0.9941
Epoch 2835/5000
26/26 - 1s - loss: 0.5632 - val_loss: 0.9940
Epoch 2836/5000
26/26 - 1s - loss: 0.5650 - val_loss: 0.9949
Epoch 2837/5000
26/26 - 1s - loss: 0.5633 - val_loss: 0.9936
Epoch 2838/5000
26/26 - 1s - loss: 0.5637 - val_loss: 0.9938
Epoch 2839/5000
26/26 - 1s - loss: 0.5632 - val_loss: 0.9932
Epoch 2840/5000
26/26 - 1s - loss: 0.5631 - val_loss: 0.9940
Epoch 02840: val_loss improved from 0.99445 to 0.99403, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 2841/5000
26/26 - 1s - loss: 0.5627 - val_loss: 0.9937
Epoch 2842/5000
26/26 - 1s - loss: 0.5634 - val_loss: 0.9932
Epoch 2843/5000
26/26 - 1s - loss: 0.5622 - val_loss: 0.9938
Epoch 2844/5000
26/26 - 1s - loss: 0.5647 - val_loss: 0.9932
Epoch 2845/5000
26/26 - 1s - loss: 0.5619 - val_loss: 0.9922
Epoch 2846/5000
26/26 - 1s - loss: 0.5620 - val_loss: 0.9915
Epoch 2847/5000
26/26 - 1s - loss: 0.5622 - val_loss: 0.9928
Epoch 2848/5000
26/26 - 2s - loss: 0.5624 - val_loss: 0.9924
Epoch 2849/5000
26/26 - 1s - loss: 0.5631 - val_loss: 0.9922
Epoch 2850/5000
26/26 - 1s - loss: 0.5607 - val_loss: 0.9934
Epoch 02850: val_loss improved from 0.99403 to 0.99339, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 2851/5000
26/26 - 1s - loss: 0.5619 - val_loss: 0.9922
Epoch 2852/5000
26/26 - 1s - loss: 0.5609 - val_loss: 0.9911
Epoch 2853/5000
26/26 - 1s - loss: 0.5608 - val_loss: 0.9913
Epoch 2854/5000
26/26 - 2s - loss: 0.5609 - val_loss: 0.9908
Epoch 2855/5000
26/26 - 1s - loss: 0.5605 - val_loss: 0.9905
Epoch 2856/5000
26/26 - 1s - loss: 0.5616 - val_loss: 0.9912
Epoch 2857/5000
26/26 - 1s - loss: 0.5613 - val_loss: 0.9903
Epoch 2858/5000
26/26 - 1s - loss: 0.5604 - val_loss: 0.9919
Epoch 2859/5000
26/26 - 1s - loss: 0.5607 - val_loss: 0.9922
Epoch 2860/5000
26/26 - 1s - loss: 0.5608 - val_loss: 0.9901
Epoch 02860: val_loss improved from 0.99339 to 0.99008, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 2861/5000
26/26 - 1s - loss: 0.5587 - val_loss: 0.9915
Epoch 2862/5000
26/26 - 1s - loss: 0.5596 - val_loss: 0.9894
Epoch 2863/5000
26/26 - 1s - loss: 0.5595 - val_loss: 0.9887
Epoch 2864/5000
26/26 - 1s - loss: 0.5593 - val_loss: 0.9896
Epoch 2865/5000
26/26 - 1s - loss: 0.5593 - val_loss: 0.9907
Epoch 2866/5000
26/26 - 1s - loss: 0.5593 - val_loss: 0.9889
Epoch 2867/5000
26/26 - 1s - loss: 0.5585 - val_loss: 0.9892
Epoch 2868/5000
26/26 - 1s - loss: 0.5597 - val_loss: 0.9885
Epoch 2869/5000
26/26 - 1s - loss: 0.5589 - val_loss: 0.9898
Epoch 2870/5000
26/26 - 1s - loss: 0.5591 - val_loss: 0.9878
Epoch 02870: val_loss improved from 0.99008 to 0.98776, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 2871/5000
26/26 - 1s - loss: 0.5584 - val_loss: 0.9890
Epoch 2872/5000
26/26 - 1s - loss: 0.5576 - val_loss: 0.9879
Epoch 2873/5000
26/26 - 1s - loss: 0.5583 - val_loss: 0.9880
Epoch 2874/5000
26/26 - 1s - loss: 0.5583 - val_loss: 0.9883
Epoch 2875/5000
26/26 - 1s - loss: 0.5578 - val_loss: 0.9886
Epoch 2876/5000
26/26 - 1s - loss: 0.5568 - val_loss: 0.9886
Epoch 2877/5000
26/26 - 1s - loss: 0.5563 - val_loss: 0.9890
Epoch 2878/5000
26/26 - 1s - loss: 0.5575 - val_loss: 0.9882
Epoch 2879/5000
26/26 - 1s - loss: 0.5576 - val_loss: 0.9874
Epoch 2880/5000
26/26 - 1s - loss: 0.5566 - val_loss: 0.9868
Epoch 02880: val_loss improved from 0.98776 to 0.98678, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 2881/5000
26/26 - 1s - loss: 0.5580 - val_loss: 0.9872
Epoch 2882/5000
26/26 - 1s - loss: 0.5567 - val_loss: 0.9873
Epoch 2883/5000
26/26 - 1s - loss: 0.5557 - val_loss: 0.9867
Epoch 2884/5000
26/26 - 1s - loss: 0.5563 - val_loss: 0.9873
Epoch 2885/5000
26/26 - 1s - loss: 0.5563 - val_loss: 0.9862
Epoch 2886/5000
26/26 - 1s - loss: 0.5560 - val_loss: 0.9865
Epoch 2887/5000
26/26 - 1s - loss: 0.5557 - val_loss: 0.9868
Epoch 2888/5000
26/26 - 1s - loss: 0.5555 - val_loss: 0.9858
Epoch 2889/5000
26/26 - 2s - loss: 0.5551 - val_loss: 0.9870
Epoch 2890/5000
26/26 - 1s - loss: 0.5562 - val_loss: 0.9846
Epoch 02890: val_loss improved from 0.98678 to 0.98461, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 2891/5000
26/26 - 1s - loss: 0.5554 - val_loss: 0.9870
Epoch 2892/5000
26/26 - 1s - loss: 0.5552 - val_loss: 0.9859
Epoch 2893/5000
26/26 - 1s - loss: 0.5549 - val_loss: 0.9861
Epoch 2894/5000
26/26 - 1s - loss: 0.5538 - val_loss: 0.9850
Epoch 2895/5000
26/26 - 1s - loss: 0.5552 - val_loss: 0.9837
Epoch 2896/5000
26/26 - 1s - loss: 0.5539 - val_loss: 0.9854
Epoch 2897/5000
26/26 - 1s - loss: 0.5552 - val_loss: 0.9831
Epoch 2898/5000
26/26 - 1s - loss: 0.5533 - val_loss: 0.9846
Epoch 2899/5000
26/26 - 1s - loss: 0.5535 - val_loss: 0.9842
Epoch 2900/5000
26/26 - 1s - loss: 0.5535 - val_loss: 0.9852
Epoch 02900: val_loss did not improve from 0.98461
Epoch 2901/5000
26/26 - 1s - loss: 0.5533 - val_loss: 0.9852
Epoch 2902/5000
26/26 - 1s - loss: 0.5531 - val_loss: 0.9839
Epoch 2903/5000
26/26 - 1s - loss: 0.5530 - val_loss: 0.9845
Epoch 2904/5000
26/26 - 1s - loss: 0.5520 - val_loss: 0.9823
Epoch 2905/5000
26/26 - 1s - loss: 0.5539 - val_loss: 0.9845
Epoch 2906/5000
26/26 - 1s - loss: 0.5531 - val_loss: 0.9823
Epoch 2907/5000
26/26 - 1s - loss: 0.5524 - val_loss: 0.9816
Epoch 2908/5000
26/26 - 1s - loss: 0.5527 - val_loss: 0.9820
Epoch 2909/5000
26/26 - 1s - loss: 0.5523 - val_loss: 0.9830
Epoch 2910/5000
26/26 - 1s - loss: 0.5530 - val_loss: 0.9827
Epoch 02910: val_loss improved from 0.98461 to 0.98273, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 2911/5000
26/26 - 1s - loss: 0.5522 - val_loss: 0.9830
Epoch 2912/5000
26/26 - 1s - loss: 0.5528 - val_loss: 0.9822
Epoch 2913/5000
26/26 - 1s - loss: 0.5523 - val_loss: 0.9834
Epoch 2914/5000
26/26 - 1s - loss: 0.5506 - val_loss: 0.9829
Epoch 2915/5000
26/26 - 1s - loss: 0.5516 - val_loss: 0.9815
Epoch 2916/5000
26/26 - 1s - loss: 0.5525 - val_loss: 0.9825
Epoch 2917/5000
26/26 - 1s - loss: 0.5515 - val_loss: 0.9819
Epoch 2918/5000
26/26 - 1s - loss: 0.5511 - val_loss: 0.9809
Epoch 2919/5000
26/26 - 1s - loss: 0.5499 - val_loss: 0.9823
Epoch 2920/5000
26/26 - 1s - loss: 0.5512 - val_loss: 0.9808
Epoch 02920: val_loss improved from 0.98273 to 0.98076, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 2921/5000
26/26 - 1s - loss: 0.5511 - val_loss: 0.9802
Epoch 2922/5000
26/26 - 1s - loss: 0.5517 - val_loss: 0.9826
Epoch 2923/5000
26/26 - 1s - loss: 0.5501 - val_loss: 0.9805
Epoch 2924/5000
26/26 - 1s - loss: 0.5497 - val_loss: 0.9815
Epoch 2925/5000
26/26 - 1s - loss: 0.5498 - val_loss: 0.9805
Epoch 2926/5000
26/26 - 1s - loss: 0.5493 - val_loss: 0.9819
Epoch 2927/5000
26/26 - 1s - loss: 0.5494 - val_loss: 0.9815
Epoch 2928/5000
26/26 - 1s - loss: 0.5492 - val_loss: 0.9807
Epoch 2929/5000
26/26 - 1s - loss: 0.5501 - val_loss: 0.9806
Epoch 2930/5000
26/26 - 1s - loss: 0.5492 - val_loss: 0.9801
Epoch 02930: val_loss improved from 0.98076 to 0.98013, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 2931/5000
26/26 - 1s - loss: 0.5488 - val_loss: 0.9803
Epoch 2932/5000
26/26 - 2s - loss: 0.5495 - val_loss: 0.9797
Epoch 2933/5000
26/26 - 1s - loss: 0.5495 - val_loss: 0.9803
Epoch 2934/5000
26/26 - 2s - loss: 0.5497 - val_loss: 0.9808
Epoch 2935/5000
26/26 - 1s - loss: 0.5482 - val_loss: 0.9801
Epoch 2936/5000
26/26 - 1s - loss: 0.5479 - val_loss: 0.9797
Epoch 2937/5000
26/26 - 1s - loss: 0.5494 - val_loss: 0.9784
Epoch 2938/5000
26/26 - 1s - loss: 0.5483 - val_loss: 0.9794
Epoch 2939/5000
26/26 - 1s - loss: 0.5477 - val_loss: 0.9788
Epoch 2940/5000
26/26 - 1s - loss: 0.5476 - val_loss: 0.9787
Epoch 02940: val_loss improved from 0.98013 to 0.97873, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 2941/5000
26/26 - 1s - loss: 0.5468 - val_loss: 0.9780
Epoch 2942/5000
26/26 - 1s - loss: 0.5474 - val_loss: 0.9780
Epoch 2943/5000
26/26 - 1s - loss: 0.5471 - val_loss: 0.9790
Epoch 2944/5000
26/26 - 2s - loss: 0.5467 - val_loss: 0.9773
Epoch 2945/5000
26/26 - 1s - loss: 0.5463 - val_loss: 0.9790
Epoch 2946/5000
26/26 - 1s - loss: 0.5463 - val_loss: 0.9787
Epoch 2947/5000
26/26 - 1s - loss: 0.5472 - val_loss: 0.9776
Epoch 2948/5000
26/26 - 1s - loss: 0.5462 - val_loss: 0.9775
Epoch 2949/5000
26/26 - 1s - loss: 0.5468 - val_loss: 0.9775
Epoch 2950/5000
26/26 - 1s - loss: 0.5456 - val_loss: 0.9783
Epoch 02950: val_loss improved from 0.97873 to 0.97833, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 2951/5000
26/26 - 1s - loss: 0.5459 - val_loss: 0.9780
Epoch 2952/5000
26/26 - 1s - loss: 0.5467 - val_loss: 0.9771
Epoch 2953/5000
26/26 - 1s - loss: 0.5446 - val_loss: 0.9758
Epoch 2954/5000
26/26 - 1s - loss: 0.5461 - val_loss: 0.9764
Epoch 2955/5000
26/26 - 1s - loss: 0.5457 - val_loss: 0.9753
Epoch 2956/5000
26/26 - 1s - loss: 0.5462 - val_loss: 0.9767
Epoch 2957/5000
26/26 - 1s - loss: 0.5456 - val_loss: 0.9767
Epoch 2958/5000
26/26 - 1s - loss: 0.5455 - val_loss: 0.9778
Epoch 2959/5000
26/26 - 1s - loss: 0.5456 - val_loss: 0.9746
Epoch 2960/5000
26/26 - 1s - loss: 0.5455 - val_loss: 0.9760
Epoch 02960: val_loss improved from 0.97833 to 0.97604, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 2961/5000
26/26 - 1s - loss: 0.5442 - val_loss: 0.9763
Epoch 2962/5000
26/26 - 1s - loss: 0.5453 - val_loss: 0.9756
Epoch 2963/5000
26/26 - 1s - loss: 0.5432 - val_loss: 0.9761
Epoch 2964/5000
26/26 - 1s - loss: 0.5430 - val_loss: 0.9778
Epoch 2965/5000
26/26 - 1s - loss: 0.5442 - val_loss: 0.9760
Epoch 2966/5000
26/26 - 1s - loss: 0.5446 - val_loss: 0.9767
Epoch 2967/5000
26/26 - 1s - loss: 0.5434 - val_loss: 0.9756
Epoch 2968/5000
26/26 - 1s - loss: 0.5436 - val_loss: 0.9749
Epoch 2969/5000
26/26 - 1s - loss: 0.5438 - val_loss: 0.9739
Epoch 2970/5000
26/26 - 1s - loss: 0.5431 - val_loss: 0.9755
Epoch 02970: val_loss improved from 0.97604 to 0.97545, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 2971/5000
26/26 - 1s - loss: 0.5442 - val_loss: 0.9739
Epoch 2972/5000
26/26 - 1s - loss: 0.5430 - val_loss: 0.9739
Epoch 2973/5000
26/26 - 1s - loss: 0.5441 - val_loss: 0.9744
Epoch 2974/5000
26/26 - 1s - loss: 0.5434 - val_loss: 0.9744
Epoch 2975/5000
26/26 - 1s - loss: 0.5419 - val_loss: 0.9737
Epoch 2976/5000
26/26 - 1s - loss: 0.5434 - val_loss: 0.9736
Epoch 2977/5000
26/26 - 1s - loss: 0.5433 - val_loss: 0.9722
Epoch 2978/5000
26/26 - 1s - loss: 0.5422 - val_loss: 0.9717
Epoch 2979/5000
26/26 - 1s - loss: 0.5430 - val_loss: 0.9712
Epoch 2980/5000
26/26 - 1s - loss: 0.5422 - val_loss: 0.9720
Epoch 02980: val_loss improved from 0.97545 to 0.97203, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 2981/5000
26/26 - 1s - loss: 0.5427 - val_loss: 0.9722
Epoch 2982/5000
26/26 - 1s - loss: 0.5416 - val_loss: 0.9717
Epoch 2983/5000
26/26 - 1s - loss: 0.5414 - val_loss: 0.9722
Epoch 2984/5000
26/26 - 1s - loss: 0.5409 - val_loss: 0.9734
Epoch 2985/5000
26/26 - 1s - loss: 0.5402 - val_loss: 0.9720
Epoch 2986/5000
26/26 - 1s - loss: 0.5410 - val_loss: 0.9708
Epoch 2987/5000
26/26 - 1s - loss: 0.5418 - val_loss: 0.9716
Epoch 2988/5000
26/26 - 1s - loss: 0.5404 - val_loss: 0.9725
Epoch 2989/5000
26/26 - 1s - loss: 0.5416 - val_loss: 0.9722
Epoch 2990/5000
26/26 - 1s - loss: 0.5400 - val_loss: 0.9710
Epoch 02990: val_loss improved from 0.97203 to 0.97102, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 2991/5000
26/26 - 1s - loss: 0.5420 - val_loss: 0.9719
Epoch 2992/5000
26/26 - 1s - loss: 0.5401 - val_loss: 0.9706
Epoch 2993/5000
26/26 - 1s - loss: 0.5405 - val_loss: 0.9716
Epoch 2994/5000
26/26 - 1s - loss: 0.5388 - val_loss: 0.9719
Epoch 2995/5000
26/26 - 1s - loss: 0.5404 - val_loss: 0.9702
Epoch 2996/5000
26/26 - 1s - loss: 0.5396 - val_loss: 0.9708
Epoch 2997/5000
26/26 - 1s - loss: 0.5401 - val_loss: 0.9698
Epoch 2998/5000
26/26 - 1s - loss: 0.5402 - val_loss: 0.9723
Epoch 2999/5000
26/26 - 1s - loss: 0.5392 - val_loss: 0.9701
Epoch 3000/5000
26/26 - 1s - loss: 0.5394 - val_loss: 0.9703
Epoch 03000: val_loss improved from 0.97102 to 0.97034, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 3001/5000
26/26 - 1s - loss: 0.5387 - val_loss: 0.9706
Epoch 3002/5000
26/26 - 1s - loss: 0.5384 - val_loss: 0.9691
Epoch 3003/5000
26/26 - 1s - loss: 0.5390 - val_loss: 0.9700
Epoch 3004/5000
26/26 - 1s - loss: 0.5382 - val_loss: 0.9695
Epoch 3005/5000
26/26 - 1s - loss: 0.5389 - val_loss: 0.9696
Epoch 3006/5000
26/26 - 2s - loss: 0.5380 - val_loss: 0.9695
Epoch 3007/5000
26/26 - 1s - loss: 0.5371 - val_loss: 0.9694
Epoch 3008/5000
26/26 - 1s - loss: 0.5378 - val_loss: 0.9697
Epoch 3009/5000
26/26 - 1s - loss: 0.5386 - val_loss: 0.9684
Epoch 3010/5000
26/26 - 1s - loss: 0.5385 - val_loss: 0.9682
Epoch 03010: val_loss improved from 0.97034 to 0.96815, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 3011/5000
26/26 - 1s - loss: 0.5382 - val_loss: 0.9681
Epoch 3012/5000
26/26 - 1s - loss: 0.5377 - val_loss: 0.9689
Epoch 3013/5000
26/26 - 2s - loss: 0.5376 - val_loss: 0.9695
Epoch 3014/5000
26/26 - 2s - loss: 0.5387 - val_loss: 0.9688
Epoch 3015/5000
26/26 - 1s - loss: 0.5383 - val_loss: 0.9679
Epoch 3016/5000
26/26 - 1s - loss: 0.5368 - val_loss: 0.9672
Epoch 3017/5000
26/26 - 1s - loss: 0.5371 - val_loss: 0.9673
Epoch 3018/5000
26/26 - 1s - loss: 0.5371 - val_loss: 0.9674
Epoch 3019/5000
26/26 - 1s - loss: 0.5373 - val_loss: 0.9671
Epoch 3020/5000
26/26 - 1s - loss: 0.5356 - val_loss: 0.9672
Epoch 03020: val_loss improved from 0.96815 to 0.96720, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 3021/5000
26/26 - 1s - loss: 0.5374 - val_loss: 0.9669
Epoch 3022/5000
26/26 - 1s - loss: 0.5358 - val_loss: 0.9685
Epoch 3023/5000
26/26 - 1s - loss: 0.5356 - val_loss: 0.9674
Epoch 3024/5000
26/26 - 1s - loss: 0.5364 - val_loss: 0.9668
Epoch 3025/5000
26/26 - 1s - loss: 0.5358 - val_loss: 0.9671
Epoch 3026/5000
26/26 - 1s - loss: 0.5370 - val_loss: 0.9670
Epoch 3027/5000
26/26 - 1s - loss: 0.5351 - val_loss: 0.9678
Epoch 3028/5000
26/26 - 1s - loss: 0.5351 - val_loss: 0.9675
Epoch 3029/5000
26/26 - 1s - loss: 0.5343 - val_loss: 0.9665
Epoch 3030/5000
26/26 - 1s - loss: 0.5351 - val_loss: 0.9672
Epoch 03030: val_loss did not improve from 0.96720
Epoch 3031/5000
26/26 - 1s - loss: 0.5357 - val_loss: 0.9656
Epoch 3032/5000
26/26 - 1s - loss: 0.5336 - val_loss: 0.9664
Epoch 3033/5000
26/26 - 1s - loss: 0.5346 - val_loss: 0.9667
Epoch 3034/5000
26/26 - 1s - loss: 0.5361 - val_loss: 0.9671
Epoch 3035/5000
26/26 - 1s - loss: 0.5338 - val_loss: 0.9650
Epoch 3036/5000
26/26 - 1s - loss: 0.5351 - val_loss: 0.9662
Epoch 3037/5000
26/26 - 1s - loss: 0.5356 - val_loss: 0.9663
Epoch 3038/5000
26/26 - 1s - loss: 0.5345 - val_loss: 0.9652
Epoch 3039/5000
26/26 - 1s - loss: 0.5339 - val_loss: 0.9654
Epoch 3040/5000
26/26 - 1s - loss: 0.5347 - val_loss: 0.9664
Epoch 03040: val_loss improved from 0.96720 to 0.96637, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 3041/5000
26/26 - 1s - loss: 0.5344 - val_loss: 0.9654
Epoch 3042/5000
26/26 - 1s - loss: 0.5333 - val_loss: 0.9667
Epoch 3043/5000
26/26 - 1s - loss: 0.5334 - val_loss: 0.9654
Epoch 3044/5000
26/26 - 2s - loss: 0.5335 - val_loss: 0.9657
Epoch 3045/5000
26/26 - 1s - loss: 0.5323 - val_loss: 0.9644
Epoch 3046/5000
26/26 - 1s - loss: 0.5331 - val_loss: 0.9656
Epoch 3047/5000
26/26 - 1s - loss: 0.5329 - val_loss: 0.9661
Epoch 3048/5000
26/26 - 1s - loss: 0.5329 - val_loss: 0.9655
Epoch 3049/5000
26/26 - 1s - loss: 0.5322 - val_loss: 0.9633
Epoch 3050/5000
26/26 - 1s - loss: 0.5330 - val_loss: 0.9650
Epoch 03050: val_loss improved from 0.96637 to 0.96505, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 3051/5000
26/26 - 1s - loss: 0.5327 - val_loss: 0.9640
Epoch 3052/5000
26/26 - 1s - loss: 0.5321 - val_loss: 0.9634
Epoch 3053/5000
26/26 - 1s - loss: 0.5319 - val_loss: 0.9643
Epoch 3054/5000
26/26 - 2s - loss: 0.5324 - val_loss: 0.9633
Epoch 3055/5000
26/26 - 1s - loss: 0.5314 - val_loss: 0.9641
Epoch 3056/5000
26/26 - 1s - loss: 0.5325 - val_loss: 0.9632
Epoch 3057/5000
26/26 - 1s - loss: 0.5309 - val_loss: 0.9626
Epoch 3058/5000
26/26 - 1s - loss: 0.5307 - val_loss: 0.9625
Epoch 3059/5000
26/26 - 1s - loss: 0.5307 - val_loss: 0.9615
Epoch 3060/5000
26/26 - 1s - loss: 0.5302 - val_loss: 0.9627
Epoch 03060: val_loss improved from 0.96505 to 0.96272, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 3061/5000
26/26 - 1s - loss: 0.5310 - val_loss: 0.9628
Epoch 3062/5000
26/26 - 1s - loss: 0.5314 - val_loss: 0.9621
Epoch 3063/5000
26/26 - 1s - loss: 0.5312 - val_loss: 0.9616
Epoch 3064/5000
26/26 - 1s - loss: 0.5309 - val_loss: 0.9628
Epoch 3065/5000
26/26 - 1s - loss: 0.5299 - val_loss: 0.9617
Epoch 3066/5000
26/26 - 1s - loss: 0.5302 - val_loss: 0.9610
Epoch 3067/5000
26/26 - 1s - loss: 0.5309 - val_loss: 0.9614
Epoch 3068/5000
26/26 - 1s - loss: 0.5306 - val_loss: 0.9618
Epoch 3069/5000
26/26 - 1s - loss: 0.5295 - val_loss: 0.9608
Epoch 3070/5000
26/26 - 1s - loss: 0.5308 - val_loss: 0.9605
Epoch 03070: val_loss improved from 0.96272 to 0.96049, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 3071/5000
26/26 - 1s - loss: 0.5305 - val_loss: 0.9619
Epoch 3072/5000
26/26 - 1s - loss: 0.5299 - val_loss: 0.9611
Epoch 3073/5000
26/26 - 1s - loss: 0.5299 - val_loss: 0.9615
Epoch 3074/5000
26/26 - 1s - loss: 0.5300 - val_loss: 0.9615
Epoch 3075/5000
26/26 - 1s - loss: 0.5299 - val_loss: 0.9610
Epoch 3076/5000
26/26 - 1s - loss: 0.5295 - val_loss: 0.9598
Epoch 3077/5000
26/26 - 1s - loss: 0.5278 - val_loss: 0.9613
Epoch 3078/5000
26/26 - 1s - loss: 0.5293 - val_loss: 0.9605
Epoch 3079/5000
26/26 - 1s - loss: 0.5283 - val_loss: 0.9606
Epoch 3080/5000
26/26 - 1s - loss: 0.5289 - val_loss: 0.9616
Epoch 03080: val_loss did not improve from 0.96049
Epoch 3081/5000
26/26 - 1s - loss: 0.5279 - val_loss: 0.9612
Epoch 3082/5000
26/26 - 1s - loss: 0.5279 - val_loss: 0.9599
Epoch 3083/5000
26/26 - 1s - loss: 0.5268 - val_loss: 0.9603
Epoch 3084/5000
26/26 - 1s - loss: 0.5283 - val_loss: 0.9591
Epoch 3085/5000
26/26 - 1s - loss: 0.5297 - val_loss: 0.9594
Epoch 3086/5000
26/26 - 1s - loss: 0.5288 - val_loss: 0.9605
Epoch 3087/5000
26/26 - 1s - loss: 0.5276 - val_loss: 0.9592
Epoch 3088/5000
26/26 - 1s - loss: 0.5268 - val_loss: 0.9577
Epoch 3089/5000
26/26 - 1s - loss: 0.5283 - val_loss: 0.9574
Epoch 3090/5000
26/26 - 1s - loss: 0.5277 - val_loss: 0.9564
Epoch 03090: val_loss improved from 0.96049 to 0.95643, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 3091/5000
26/26 - 1s - loss: 0.5276 - val_loss: 0.9583
Epoch 3092/5000
26/26 - 1s - loss: 0.5284 - val_loss: 0.9577
Epoch 3093/5000
26/26 - 1s - loss: 0.5266 - val_loss: 0.9571
Epoch 3094/5000
26/26 - 1s - loss: 0.5273 - val_loss: 0.9573
Epoch 3095/5000
26/26 - 1s - loss: 0.5270 - val_loss: 0.9585
Epoch 3096/5000
26/26 - 2s - loss: 0.5262 - val_loss: 0.9565
Epoch 3097/5000
26/26 - 1s - loss: 0.5267 - val_loss: 0.9570
Epoch 3098/5000
26/26 - 1s - loss: 0.5259 - val_loss: 0.9577
Epoch 3099/5000
26/26 - 1s - loss: 0.5266 - val_loss: 0.9583
Epoch 3100/5000
26/26 - 1s - loss: 0.5265 - val_loss: 0.9576
Epoch 03100: val_loss did not improve from 0.95643
Epoch 3101/5000
26/26 - 1s - loss: 0.5261 - val_loss: 0.9578
Epoch 3102/5000
26/26 - 1s - loss: 0.5269 - val_loss: 0.9569
Epoch 3103/5000
26/26 - 1s - loss: 0.5255 - val_loss: 0.9578
Epoch 3104/5000
26/26 - 1s - loss: 0.5251 - val_loss: 0.9578
Epoch 3105/5000
26/26 - 1s - loss: 0.5252 - val_loss: 0.9561
Epoch 3106/5000
26/26 - 1s - loss: 0.5252 - val_loss: 0.9565
Epoch 3107/5000
26/26 - 1s - loss: 0.5246 - val_loss: 0.9561
Epoch 3108/5000
26/26 - 1s - loss: 0.5257 - val_loss: 0.9552
Epoch 3109/5000
26/26 - 1s - loss: 0.5250 - val_loss: 0.9561
Epoch 3110/5000
26/26 - 1s - loss: 0.5254 - val_loss: 0.9566
Epoch 03110: val_loss did not improve from 0.95643
Epoch 3111/5000
26/26 - 1s - loss: 0.5253 - val_loss: 0.9570
Epoch 3112/5000
26/26 - 1s - loss: 0.5241 - val_loss: 0.9550
Epoch 3113/5000
26/26 - 1s - loss: 0.5241 - val_loss: 0.9557
Epoch 3114/5000
26/26 - 1s - loss: 0.5255 - val_loss: 0.9555
Epoch 3115/5000
26/26 - 1s - loss: 0.5245 - val_loss: 0.9546
Epoch 3116/5000
26/26 - 1s - loss: 0.5240 - val_loss: 0.9557
Epoch 3117/5000
26/26 - 1s - loss: 0.5253 - val_loss: 0.9554
Epoch 3118/5000
26/26 - 1s - loss: 0.5232 - val_loss: 0.9557
Epoch 3119/5000
26/26 - 1s - loss: 0.5236 - val_loss: 0.9555
Epoch 3120/5000
26/26 - 1s - loss: 0.5235 - val_loss: 0.9557
Epoch 03120: val_loss improved from 0.95643 to 0.95573, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 3121/5000
26/26 - 1s - loss: 0.5239 - val_loss: 0.9566
Epoch 3122/5000
26/26 - 1s - loss: 0.5237 - val_loss: 0.9550
Epoch 3123/5000
26/26 - 1s - loss: 0.5237 - val_loss: 0.9556
Epoch 3124/5000
26/26 - 1s - loss: 0.5236 - val_loss: 0.9551
Epoch 3125/5000
26/26 - 1s - loss: 0.5242 - val_loss: 0.9542
Epoch 3126/5000
26/26 - 1s - loss: 0.5237 - val_loss: 0.9549
Epoch 3127/5000
26/26 - 1s - loss: 0.5227 - val_loss: 0.9550
Epoch 3128/5000
26/26 - 1s - loss: 0.5224 - val_loss: 0.9543
Epoch 3129/5000
26/26 - 1s - loss: 0.5237 - val_loss: 0.9534
Epoch 3130/5000
26/26 - 1s - loss: 0.5224 - val_loss: 0.9545
Epoch 03130: val_loss improved from 0.95573 to 0.95449, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 3131/5000
26/26 - 1s - loss: 0.5216 - val_loss: 0.9551
Epoch 3132/5000
26/26 - 1s - loss: 0.5222 - val_loss: 0.9562
Epoch 3133/5000
26/26 - 1s - loss: 0.5222 - val_loss: 0.9551
Epoch 3134/5000
26/26 - 1s - loss: 0.5221 - val_loss: 0.9537
Epoch 3135/5000
26/26 - 1s - loss: 0.5221 - val_loss: 0.9543
Epoch 3136/5000
26/26 - 1s - loss: 0.5217 - val_loss: 0.9541
Epoch 3137/5000
26/26 - 1s - loss: 0.5215 - val_loss: 0.9551
Epoch 3138/5000
26/26 - 1s - loss: 0.5214 - val_loss: 0.9541
Epoch 3139/5000
26/26 - 1s - loss: 0.5227 - val_loss: 0.9543
Epoch 3140/5000
26/26 - 1s - loss: 0.5210 - val_loss: 0.9552
Epoch 03140: val_loss did not improve from 0.95449
Epoch 3141/5000
26/26 - 1s - loss: 0.5223 - val_loss: 0.9542
Epoch 3142/5000
26/26 - 1s - loss: 0.5202 - val_loss: 0.9541
Epoch 3143/5000
26/26 - 2s - loss: 0.5205 - val_loss: 0.9543
Epoch 3144/5000
26/26 - 1s - loss: 0.5211 - val_loss: 0.9532
Epoch 3145/5000
26/26 - 1s - loss: 0.5203 - val_loss: 0.9533
Epoch 3146/5000
26/26 - 1s - loss: 0.5211 - val_loss: 0.9524
Epoch 3147/5000
26/26 - 1s - loss: 0.5197 - val_loss: 0.9539
Epoch 3148/5000
26/26 - 2s - loss: 0.5197 - val_loss: 0.9526
Epoch 3149/5000
26/26 - 1s - loss: 0.5200 - val_loss: 0.9518
Epoch 3150/5000
26/26 - 1s - loss: 0.5198 - val_loss: 0.9513
Epoch 03150: val_loss improved from 0.95449 to 0.95134, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 3151/5000
26/26 - 1s - loss: 0.5194 - val_loss: 0.9538
Epoch 3152/5000
26/26 - 1s - loss: 0.5192 - val_loss: 0.9529
Epoch 3153/5000
26/26 - 1s - loss: 0.5195 - val_loss: 0.9523
Epoch 3154/5000
26/26 - 1s - loss: 0.5199 - val_loss: 0.9525
Epoch 3155/5000
26/26 - 1s - loss: 0.5205 - val_loss: 0.9511
Epoch 3156/5000
26/26 - 1s - loss: 0.5189 - val_loss: 0.9510
Epoch 3157/5000
26/26 - 1s - loss: 0.5202 - val_loss: 0.9530
Epoch 3158/5000
26/26 - 1s - loss: 0.5192 - val_loss: 0.9526
Epoch 3159/5000
26/26 - 1s - loss: 0.5189 - val_loss: 0.9518
Epoch 3160/5000
26/26 - 1s - loss: 0.5191 - val_loss: 0.9524
Epoch 03160: val_loss did not improve from 0.95134
Epoch 3161/5000
26/26 - 1s - loss: 0.5195 - val_loss: 0.9515
Epoch 3162/5000
26/26 - 1s - loss: 0.5185 - val_loss: 0.9513
Epoch 3163/5000
26/26 - 1s - loss: 0.5182 - val_loss: 0.9521
Epoch 3164/5000
26/26 - 1s - loss: 0.5180 - val_loss: 0.9498
Epoch 3165/5000
26/26 - 1s - loss: 0.5191 - val_loss: 0.9514
Epoch 3166/5000
26/26 - 1s - loss: 0.5179 - val_loss: 0.9500
Epoch 3167/5000
26/26 - 1s - loss: 0.5176 - val_loss: 0.9494
Epoch 3168/5000
26/26 - 1s - loss: 0.5176 - val_loss: 0.9501
Epoch 3169/5000
26/26 - 1s - loss: 0.5173 - val_loss: 0.9498
Epoch 3170/5000
26/26 - 1s - loss: 0.5165 - val_loss: 0.9502
Epoch 03170: val_loss improved from 0.95134 to 0.95022, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 3171/5000
26/26 - 1s - loss: 0.5176 - val_loss: 0.9494
Epoch 3172/5000
26/26 - 1s - loss: 0.5168 - val_loss: 0.9499
Epoch 3173/5000
26/26 - 1s - loss: 0.5174 - val_loss: 0.9489
Epoch 3174/5000
26/26 - 1s - loss: 0.5169 - val_loss: 0.9493
Epoch 3175/5000
26/26 - 1s - loss: 0.5174 - val_loss: 0.9483
Epoch 3176/5000
26/26 - 1s - loss: 0.5169 - val_loss: 0.9489
Epoch 3177/5000
26/26 - 1s - loss: 0.5165 - val_loss: 0.9492
Epoch 3178/5000
26/26 - 2s - loss: 0.5167 - val_loss: 0.9489
Epoch 3179/5000
26/26 - 1s - loss: 0.5169 - val_loss: 0.9486
Epoch 3180/5000
26/26 - 1s - loss: 0.5161 - val_loss: 0.9484
Epoch 03180: val_loss improved from 0.95022 to 0.94841, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 3181/5000
26/26 - 1s - loss: 0.5146 - val_loss: 0.9492
Epoch 3182/5000
26/26 - 1s - loss: 0.5162 - val_loss: 0.9478
Epoch 3183/5000
26/26 - 1s - loss: 0.5157 - val_loss: 0.9479
Epoch 3184/5000
26/26 - 1s - loss: 0.5157 - val_loss: 0.9469
Epoch 3185/5000
26/26 - 1s - loss: 0.5158 - val_loss: 0.9484
Epoch 3186/5000
26/26 - 1s - loss: 0.5158 - val_loss: 0.9482
Epoch 3187/5000
26/26 - 1s - loss: 0.5157 - val_loss: 0.9470
Epoch 3188/5000
26/26 - 1s - loss: 0.5146 - val_loss: 0.9491
Epoch 3189/5000
26/26 - 1s - loss: 0.5150 - val_loss: 0.9479
Epoch 3190/5000
26/26 - 2s - loss: 0.5153 - val_loss: 0.9478
Epoch 03190: val_loss improved from 0.94841 to 0.94777, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 3191/5000
26/26 - 2s - loss: 0.5146 - val_loss: 0.9476
Epoch 3192/5000
26/26 - 1s - loss: 0.5149 - val_loss: 0.9484
Epoch 3193/5000
26/26 - 1s - loss: 0.5152 - val_loss: 0.9480
Epoch 3194/5000
26/26 - 1s - loss: 0.5139 - val_loss: 0.9478
Epoch 3195/5000
26/26 - 1s - loss: 0.5147 - val_loss: 0.9463
Epoch 3196/5000
26/26 - 1s - loss: 0.5151 - val_loss: 0.9477
Epoch 3197/5000
26/26 - 1s - loss: 0.5149 - val_loss: 0.9470
Epoch 3198/5000
26/26 - 1s - loss: 0.5136 - val_loss: 0.9457
Epoch 3199/5000
26/26 - 1s - loss: 0.5143 - val_loss: 0.9466
Epoch 3200/5000
26/26 - 1s - loss: 0.5143 - val_loss: 0.9462
Epoch 03200: val_loss improved from 0.94777 to 0.94621, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 3201/5000
26/26 - 1s - loss: 0.5147 - val_loss: 0.9461
Epoch 3202/5000
26/26 - 1s - loss: 0.5126 - val_loss: 0.9457
Epoch 3203/5000
26/26 - 1s - loss: 0.5134 - val_loss: 0.9475
Epoch 3204/5000
26/26 - 1s - loss: 0.5138 - val_loss: 0.9457
Epoch 3205/5000
26/26 - 1s - loss: 0.5136 - val_loss: 0.9458
Epoch 3206/5000
26/26 - 1s - loss: 0.5132 - val_loss: 0.9464
Epoch 3207/5000
26/26 - 1s - loss: 0.5134 - val_loss: 0.9469
Epoch 3208/5000
26/26 - 1s - loss: 0.5131 - val_loss: 0.9452
Epoch 3209/5000
26/26 - 1s - loss: 0.5134 - val_loss: 0.9462
Epoch 3210/5000
26/26 - 1s - loss: 0.5135 - val_loss: 0.9466
Epoch 03210: val_loss did not improve from 0.94621
Epoch 3211/5000
26/26 - 1s - loss: 0.5132 - val_loss: 0.9452
Epoch 3212/5000
26/26 - 1s - loss: 0.5124 - val_loss: 0.9439
Epoch 3213/5000
26/26 - 1s - loss: 0.5121 - val_loss: 0.9449
Epoch 3214/5000
26/26 - 2s - loss: 0.5122 - val_loss: 0.9440
Epoch 3215/5000
26/26 - 1s - loss: 0.5132 - val_loss: 0.9445
Epoch 3216/5000
26/26 - 1s - loss: 0.5120 - val_loss: 0.9446
Epoch 3217/5000
26/26 - 1s - loss: 0.5117 - val_loss: 0.9441
Epoch 3218/5000
26/26 - 1s - loss: 0.5123 - val_loss: 0.9436
Epoch 3219/5000
26/26 - 1s - loss: 0.5115 - val_loss: 0.9441
Epoch 3220/5000
26/26 - 2s - loss: 0.5131 - val_loss: 0.9432
Epoch 03220: val_loss improved from 0.94621 to 0.94323, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 3221/5000
26/26 - 1s - loss: 0.5125 - val_loss: 0.9438
Epoch 3222/5000
26/26 - 1s - loss: 0.5107 - val_loss: 0.9438
Epoch 3223/5000
26/26 - 1s - loss: 0.5111 - val_loss: 0.9437
Epoch 3224/5000
26/26 - 1s - loss: 0.5116 - val_loss: 0.9441
Epoch 3225/5000
26/26 - 1s - loss: 0.5105 - val_loss: 0.9422
Epoch 3226/5000
26/26 - 1s - loss: 0.5115 - val_loss: 0.9437
Epoch 3227/5000
26/26 - 1s - loss: 0.5109 - val_loss: 0.9428
Epoch 3228/5000
26/26 - 1s - loss: 0.5107 - val_loss: 0.9428
Epoch 3229/5000
26/26 - 1s - loss: 0.5103 - val_loss: 0.9432
Epoch 3230/5000
26/26 - 1s - loss: 0.5105 - val_loss: 0.9439
Epoch 03230: val_loss did not improve from 0.94323
Epoch 3231/5000
26/26 - 2s - loss: 0.5104 - val_loss: 0.9425
Epoch 3232/5000
26/26 - 1s - loss: 0.5099 - val_loss: 0.9431
Epoch 3233/5000
26/26 - 1s - loss: 0.5110 - val_loss: 0.9434
Epoch 3234/5000
26/26 - 1s - loss: 0.5092 - val_loss: 0.9435
Epoch 3235/5000
26/26 - 1s - loss: 0.5102 - val_loss: 0.9433
Epoch 3236/5000
26/26 - 1s - loss: 0.5106 - val_loss: 0.9434
Epoch 3237/5000
26/26 - 1s - loss: 0.5112 - val_loss: 0.9419
Epoch 3238/5000
26/26 - 1s - loss: 0.5095 - val_loss: 0.9439
Epoch 3239/5000
26/26 - 1s - loss: 0.5090 - val_loss: 0.9439
Epoch 3240/5000
26/26 - 1s - loss: 0.5092 - val_loss: 0.9420
Epoch 03240: val_loss improved from 0.94323 to 0.94202, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 3241/5000
26/26 - 1s - loss: 0.5082 - val_loss: 0.9412
Epoch 3242/5000
26/26 - 1s - loss: 0.5099 - val_loss: 0.9405
Epoch 3243/5000
26/26 - 1s - loss: 0.5091 - val_loss: 0.9413
Epoch 3244/5000
26/26 - 1s - loss: 0.5090 - val_loss: 0.9423
Epoch 3245/5000
26/26 - 1s - loss: 0.5089 - val_loss: 0.9411
Epoch 3246/5000
26/26 - 1s - loss: 0.5100 - val_loss: 0.9424
Epoch 3247/5000
26/26 - 1s - loss: 0.5090 - val_loss: 0.9413
Epoch 3248/5000
26/26 - 1s - loss: 0.5079 - val_loss: 0.9421
Epoch 3249/5000
26/26 - 1s - loss: 0.5084 - val_loss: 0.9427
Epoch 3250/5000
26/26 - 1s - loss: 0.5090 - val_loss: 0.9423
Epoch 03250: val_loss did not improve from 0.94202
Epoch 3251/5000
26/26 - 2s - loss: 0.5077 - val_loss: 0.9410
Epoch 3252/5000
26/26 - 1s - loss: 0.5082 - val_loss: 0.9419
Epoch 3253/5000
26/26 - 1s - loss: 0.5076 - val_loss: 0.9424
Epoch 3254/5000
26/26 - 1s - loss: 0.5078 - val_loss: 0.9423
Epoch 3255/5000
26/26 - 1s - loss: 0.5077 - val_loss: 0.9418
Epoch 3256/5000
26/26 - 1s - loss: 0.5069 - val_loss: 0.9422
Epoch 3257/5000
26/26 - 1s - loss: 0.5076 - val_loss: 0.9413
Epoch 3258/5000
26/26 - 1s - loss: 0.5075 - val_loss: 0.9417
Epoch 3259/5000
26/26 - 1s - loss: 0.5082 - val_loss: 0.9412
Epoch 3260/5000
26/26 - 1s - loss: 0.5065 - val_loss: 0.9402
Epoch 03260: val_loss improved from 0.94202 to 0.94023, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 3261/5000
26/26 - 1s - loss: 0.5077 - val_loss: 0.9405
Epoch 3262/5000
26/26 - 1s - loss: 0.5056 - val_loss: 0.9406
Epoch 3263/5000
26/26 - 1s - loss: 0.5068 - val_loss: 0.9400
Epoch 3264/5000
26/26 - 1s - loss: 0.5079 - val_loss: 0.9390
Epoch 3265/5000
26/26 - 1s - loss: 0.5065 - val_loss: 0.9380
Epoch 3266/5000
26/26 - 1s - loss: 0.5070 - val_loss: 0.9403
Epoch 3267/5000
26/26 - 1s - loss: 0.5056 - val_loss: 0.9385
Epoch 3268/5000
26/26 - 1s - loss: 0.5071 - val_loss: 0.9380
Epoch 3269/5000
26/26 - 1s - loss: 0.5064 - val_loss: 0.9384
Epoch 3270/5000
26/26 - 1s - loss: 0.5060 - val_loss: 0.9391
Epoch 03270: val_loss improved from 0.94023 to 0.93909, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 3271/5000
26/26 - 1s - loss: 0.5066 - val_loss: 0.9379
Epoch 3272/5000
26/26 - 1s - loss: 0.5065 - val_loss: 0.9388
Epoch 3273/5000
26/26 - 1s - loss: 0.5052 - val_loss: 0.9389
Epoch 3274/5000
26/26 - 1s - loss: 0.5055 - val_loss: 0.9378
Epoch 3275/5000
26/26 - 1s - loss: 0.5053 - val_loss: 0.9375
Epoch 3276/5000
26/26 - 1s - loss: 0.5053 - val_loss: 0.9367
Epoch 3277/5000
26/26 - 1s - loss: 0.5061 - val_loss: 0.9373
Epoch 3278/5000
26/26 - 1s - loss: 0.5046 - val_loss: 0.9365
Epoch 3279/5000
26/26 - 1s - loss: 0.5043 - val_loss: 0.9363
Epoch 3280/5000
26/26 - 1s - loss: 0.5064 - val_loss: 0.9365
Epoch 03280: val_loss improved from 0.93909 to 0.93647, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 3281/5000
26/26 - 1s - loss: 0.5065 - val_loss: 0.9369
Epoch 3282/5000
26/26 - 1s - loss: 0.5043 - val_loss: 0.9377
Epoch 3283/5000
26/26 - 1s - loss: 0.5052 - val_loss: 0.9377
Epoch 3284/5000
26/26 - 1s - loss: 0.5046 - val_loss: 0.9376
Epoch 3285/5000
26/26 - 1s - loss: 0.5043 - val_loss: 0.9370
Epoch 3286/5000
26/26 - 1s - loss: 0.5044 - val_loss: 0.9367
Epoch 3287/5000
26/26 - 1s - loss: 0.5041 - val_loss: 0.9374
Epoch 3288/5000
26/26 - 1s - loss: 0.5052 - val_loss: 0.9388
Epoch 3289/5000
26/26 - 1s - loss: 0.5044 - val_loss: 0.9371
Epoch 3290/5000
26/26 - 1s - loss: 0.5036 - val_loss: 0.9366
Epoch 03290: val_loss did not improve from 0.93647
Epoch 3291/5000
26/26 - 1s - loss: 0.5046 - val_loss: 0.9350
Epoch 3292/5000
26/26 - 1s - loss: 0.5041 - val_loss: 0.9378
Epoch 3293/5000
26/26 - 1s - loss: 0.5043 - val_loss: 0.9373
Epoch 3294/5000
26/26 - 1s - loss: 0.5043 - val_loss: 0.9373
Epoch 3295/5000
26/26 - 1s - loss: 0.5042 - val_loss: 0.9363
Epoch 3296/5000
26/26 - 1s - loss: 0.5036 - val_loss: 0.9353
Epoch 3297/5000
26/26 - 1s - loss: 0.5044 - val_loss: 0.9372
Epoch 3298/5000
26/26 - 1s - loss: 0.5041 - val_loss: 0.9359
Epoch 3299/5000
26/26 - 1s - loss: 0.5032 - val_loss: 0.9363
Epoch 3300/5000
26/26 - 1s - loss: 0.5030 - val_loss: 0.9368
Epoch 03300: val_loss did not improve from 0.93647
Epoch 3301/5000
26/26 - 1s - loss: 0.5025 - val_loss: 0.9353
Epoch 3302/5000
26/26 - 2s - loss: 0.5026 - val_loss: 0.9373
Epoch 3303/5000
26/26 - 1s - loss: 0.5027 - val_loss: 0.9362
Epoch 3304/5000
26/26 - 1s - loss: 0.5024 - val_loss: 0.9370
Epoch 3305/5000
26/26 - 1s - loss: 0.5032 - val_loss: 0.9350
Epoch 3306/5000
26/26 - 1s - loss: 0.5022 - val_loss: 0.9353
Epoch 3307/5000
26/26 - 1s - loss: 0.5026 - val_loss: 0.9350
Epoch 3308/5000
26/26 - 1s - loss: 0.5022 - val_loss: 0.9353
Epoch 3309/5000
26/26 - 1s - loss: 0.5020 - val_loss: 0.9336
Epoch 3310/5000
26/26 - 1s - loss: 0.5013 - val_loss: 0.9360
Epoch 03310: val_loss improved from 0.93647 to 0.93600, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 3311/5000
26/26 - 1s - loss: 0.5014 - val_loss: 0.9348
Epoch 3312/5000
26/26 - 1s - loss: 0.5019 - val_loss: 0.9340
Epoch 3313/5000
26/26 - 1s - loss: 0.5021 - val_loss: 0.9342
Epoch 3314/5000
26/26 - 1s - loss: 0.5007 - val_loss: 0.9347
Epoch 3315/5000
26/26 - 1s - loss: 0.5011 - val_loss: 0.9355
Epoch 3316/5000
26/26 - 1s - loss: 0.5014 - val_loss: 0.9330
Epoch 3317/5000
26/26 - 1s - loss: 0.5014 - val_loss: 0.9334
Epoch 3318/5000
26/26 - 1s - loss: 0.5002 - val_loss: 0.9333
Epoch 3319/5000
26/26 - 1s - loss: 0.5002 - val_loss: 0.9329
Epoch 3320/5000
26/26 - 1s - loss: 0.5013 - val_loss: 0.9337
Epoch 03320: val_loss improved from 0.93600 to 0.93366, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 3321/5000
26/26 - 1s - loss: 0.5013 - val_loss: 0.9327
Epoch 3322/5000
26/26 - 1s - loss: 0.4999 - val_loss: 0.9332
Epoch 3323/5000
26/26 - 1s - loss: 0.5003 - val_loss: 0.9342
Epoch 3324/5000
26/26 - 1s - loss: 0.5015 - val_loss: 0.9340
Epoch 3325/5000
26/26 - 1s - loss: 0.5007 - val_loss: 0.9337
Epoch 3326/5000
26/26 - 1s - loss: 0.4995 - val_loss: 0.9324
Epoch 3327/5000
26/26 - 1s - loss: 0.4999 - val_loss: 0.9322
Epoch 3328/5000
26/26 - 1s - loss: 0.4999 - val_loss: 0.9319
Epoch 3329/5000
26/26 - 1s - loss: 0.5002 - val_loss: 0.9322
Epoch 3330/5000
26/26 - 1s - loss: 0.5001 - val_loss: 0.9326
Epoch 03330: val_loss improved from 0.93366 to 0.93257, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 3331/5000
26/26 - 1s - loss: 0.4990 - val_loss: 0.9325
Epoch 3332/5000
26/26 - 1s - loss: 0.4993 - val_loss: 0.9316
Epoch 3333/5000
26/26 - 1s - loss: 0.4996 - val_loss: 0.9332
Epoch 3334/5000
26/26 - 1s - loss: 0.4996 - val_loss: 0.9323
Epoch 3335/5000
26/26 - 1s - loss: 0.4992 - val_loss: 0.9316
Epoch 3336/5000
26/26 - 1s - loss: 0.4987 - val_loss: 0.9323
Epoch 3337/5000
26/26 - 1s - loss: 0.4996 - val_loss: 0.9311
Epoch 3338/5000
26/26 - 1s - loss: 0.4991 - val_loss: 0.9316
Epoch 3339/5000
26/26 - 1s - loss: 0.4992 - val_loss: 0.9299
Epoch 3340/5000
26/26 - 1s - loss: 0.4994 - val_loss: 0.9324
Epoch 03340: val_loss improved from 0.93257 to 0.93237, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 3341/5000
26/26 - 1s - loss: 0.5001 - val_loss: 0.9301
Epoch 3342/5000
26/26 - 1s - loss: 0.4988 - val_loss: 0.9304
Epoch 3343/5000
26/26 - 2s - loss: 0.4987 - val_loss: 0.9311
Epoch 3344/5000
26/26 - 1s - loss: 0.4973 - val_loss: 0.9296
Epoch 3345/5000
26/26 - 1s - loss: 0.4990 - val_loss: 0.9296
Epoch 3346/5000
26/26 - 1s - loss: 0.4987 - val_loss: 0.9297
Epoch 3347/5000
26/26 - 1s - loss: 0.4979 - val_loss: 0.9304
Epoch 3348/5000
26/26 - 1s - loss: 0.4974 - val_loss: 0.9294
Epoch 3349/5000
26/26 - 1s - loss: 0.4987 - val_loss: 0.9298
Epoch 3350/5000
26/26 - 2s - loss: 0.4984 - val_loss: 0.9299
Epoch 03350: val_loss improved from 0.93237 to 0.92989, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 3351/5000
26/26 - 1s - loss: 0.4985 - val_loss: 0.9296
Epoch 3352/5000
26/26 - 2s - loss: 0.4972 - val_loss: 0.9299
Epoch 3353/5000
26/26 - 1s - loss: 0.4968 - val_loss: 0.9299
Epoch 3354/5000
26/26 - 1s - loss: 0.4966 - val_loss: 0.9285
Epoch 3355/5000
26/26 - 2s - loss: 0.4959 - val_loss: 0.9294
Epoch 3356/5000
26/26 - 1s - loss: 0.4951 - val_loss: 0.9293
Epoch 3357/5000
26/26 - 1s - loss: 0.4979 - val_loss: 0.9302
Epoch 3358/5000
26/26 - 1s - loss: 0.4971 - val_loss: 0.9292
Epoch 3359/5000
26/26 - 1s - loss: 0.4977 - val_loss: 0.9292
Epoch 3360/5000
26/26 - 1s - loss: 0.4967 - val_loss: 0.9290
Epoch 03360: val_loss improved from 0.92989 to 0.92900, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 3361/5000
26/26 - 1s - loss: 0.4970 - val_loss: 0.9289
Epoch 3362/5000
26/26 - 1s - loss: 0.4955 - val_loss: 0.9299
Epoch 3363/5000
26/26 - 1s - loss: 0.4956 - val_loss: 0.9285
Epoch 3364/5000
26/26 - 1s - loss: 0.4961 - val_loss: 0.9282
Epoch 3365/5000
26/26 - 1s - loss: 0.4966 - val_loss: 0.9282
Epoch 3366/5000
26/26 - 1s - loss: 0.4952 - val_loss: 0.9278
Epoch 3367/5000
26/26 - 1s - loss: 0.4957 - val_loss: 0.9293
Epoch 3368/5000
26/26 - 1s - loss: 0.4965 - val_loss: 0.9289
Epoch 3369/5000
26/26 - 1s - loss: 0.4956 - val_loss: 0.9286
Epoch 3370/5000
26/26 - 1s - loss: 0.4951 - val_loss: 0.9296
Epoch 03370: val_loss did not improve from 0.92900
Epoch 3371/5000
26/26 - 1s - loss: 0.4964 - val_loss: 0.9292
Epoch 3372/5000
26/26 - 1s - loss: 0.4962 - val_loss: 0.9287
Epoch 3373/5000
26/26 - 1s - loss: 0.4943 - val_loss: 0.9285
Epoch 3374/5000
26/26 - 1s - loss: 0.4946 - val_loss: 0.9281
Epoch 3375/5000
26/26 - 1s - loss: 0.4955 - val_loss: 0.9289
Epoch 3376/5000
26/26 - 1s - loss: 0.4947 - val_loss: 0.9284
Epoch 3377/5000
26/26 - 2s - loss: 0.4958 - val_loss: 0.9283
Epoch 3378/5000
26/26 - 1s - loss: 0.4944 - val_loss: 0.9262
Epoch 3379/5000
26/26 - 1s - loss: 0.4951 - val_loss: 0.9285
Epoch 3380/5000
26/26 - 1s - loss: 0.4952 - val_loss: 0.9279
Epoch 03380: val_loss improved from 0.92900 to 0.92787, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 3381/5000
26/26 - 1s - loss: 0.4936 - val_loss: 0.9266
Epoch 3382/5000
26/26 - 1s - loss: 0.4936 - val_loss: 0.9274
Epoch 3383/5000
26/26 - 1s - loss: 0.4946 - val_loss: 0.9261
Epoch 3384/5000
26/26 - 1s - loss: 0.4942 - val_loss: 0.9267
Epoch 3385/5000
26/26 - 1s - loss: 0.4934 - val_loss: 0.9263
Epoch 3386/5000
26/26 - 1s - loss: 0.4939 - val_loss: 0.9263
Epoch 3387/5000
26/26 - 1s - loss: 0.4937 - val_loss: 0.9274
Epoch 3388/5000
26/26 - 1s - loss: 0.4937 - val_loss: 0.9252
Epoch 3389/5000
26/26 - 1s - loss: 0.4935 - val_loss: 0.9261
Epoch 3390/5000
26/26 - 1s - loss: 0.4936 - val_loss: 0.9281
Epoch 03390: val_loss did not improve from 0.92787
Epoch 3391/5000
26/26 - 1s - loss: 0.4936 - val_loss: 0.9265
Epoch 3392/5000
26/26 - 1s - loss: 0.4922 - val_loss: 0.9266
Epoch 3393/5000
26/26 - 1s - loss: 0.4937 - val_loss: 0.9262
Epoch 3394/5000
26/26 - 1s - loss: 0.4940 - val_loss: 0.9254
Epoch 3395/5000
26/26 - 1s - loss: 0.4940 - val_loss: 0.9253
Epoch 3396/5000
26/26 - 1s - loss: 0.4936 - val_loss: 0.9255
Epoch 3397/5000
26/26 - 1s - loss: 0.4932 - val_loss: 0.9255
Epoch 3398/5000
26/26 - 2s - loss: 0.4918 - val_loss: 0.9262
Epoch 3399/5000
26/26 - 1s - loss: 0.4916 - val_loss: 0.9264
Epoch 3400/5000
26/26 - 1s - loss: 0.4923 - val_loss: 0.9246
Epoch 03400: val_loss improved from 0.92787 to 0.92464, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 3401/5000
26/26 - 1s - loss: 0.4929 - val_loss: 0.9263
Epoch 3402/5000
26/26 - 2s - loss: 0.4924 - val_loss: 0.9260
Epoch 3403/5000
26/26 - 1s - loss: 0.4934 - val_loss: 0.9264
Epoch 3404/5000
26/26 - 1s - loss: 0.4931 - val_loss: 0.9244
Epoch 3405/5000
26/26 - 1s - loss: 0.4921 - val_loss: 0.9263
Epoch 3406/5000
26/26 - 1s - loss: 0.4922 - val_loss: 0.9250
Epoch 3407/5000
26/26 - 1s - loss: 0.4919 - val_loss: 0.9246
Epoch 3408/5000
26/26 - 1s - loss: 0.4921 - val_loss: 0.9246
Epoch 3409/5000
26/26 - 1s - loss: 0.4918 - val_loss: 0.9250
Epoch 3410/5000
26/26 - 1s - loss: 0.4913 - val_loss: 0.9246
Epoch 03410: val_loss improved from 0.92464 to 0.92461, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 3411/5000
26/26 - 1s - loss: 0.4905 - val_loss: 0.9249
Epoch 3412/5000
26/26 - 1s - loss: 0.4915 - val_loss: 0.9240
Epoch 3413/5000
26/26 - 1s - loss: 0.4914 - val_loss: 0.9241
Epoch 3414/5000
26/26 - 1s - loss: 0.4921 - val_loss: 0.9236
Epoch 3415/5000
26/26 - 1s - loss: 0.4909 - val_loss: 0.9236
Epoch 3416/5000
26/26 - 1s - loss: 0.4917 - val_loss: 0.9238
Epoch 3417/5000
26/26 - 2s - loss: 0.4908 - val_loss: 0.9240
Epoch 3418/5000
26/26 - 1s - loss: 0.4920 - val_loss: 0.9248
Epoch 3419/5000
26/26 - 1s - loss: 0.4911 - val_loss: 0.9236
Epoch 3420/5000
26/26 - 1s - loss: 0.4911 - val_loss: 0.9226
Epoch 03420: val_loss improved from 0.92461 to 0.92256, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 3421/5000
26/26 - 1s - loss: 0.4903 - val_loss: 0.9235
Epoch 3422/5000
26/26 - 1s - loss: 0.4902 - val_loss: 0.9243
Epoch 3423/5000
26/26 - 1s - loss: 0.4903 - val_loss: 0.9232
Epoch 3424/5000
26/26 - 1s - loss: 0.4904 - val_loss: 0.9234
Epoch 3425/5000
26/26 - 1s - loss: 0.4902 - val_loss: 0.9236
Epoch 3426/5000
26/26 - 2s - loss: 0.4897 - val_loss: 0.9229
Epoch 3427/5000
26/26 - 1s - loss: 0.4902 - val_loss: 0.9244
Epoch 3428/5000
26/26 - 1s - loss: 0.4917 - val_loss: 0.9232
Epoch 3429/5000
26/26 - 1s - loss: 0.4902 - val_loss: 0.9247
Epoch 3430/5000
26/26 - 1s - loss: 0.4896 - val_loss: 0.9224
Epoch 03430: val_loss improved from 0.92256 to 0.92236, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 3431/5000
26/26 - 1s - loss: 0.4895 - val_loss: 0.9245
Epoch 3432/5000
26/26 - 1s - loss: 0.4894 - val_loss: 0.9240
Epoch 3433/5000
26/26 - 1s - loss: 0.4889 - val_loss: 0.9239
Epoch 3434/5000
26/26 - 1s - loss: 0.4893 - val_loss: 0.9235
Epoch 3435/5000
26/26 - 1s - loss: 0.4890 - val_loss: 0.9228
Epoch 3436/5000
26/26 - 1s - loss: 0.4877 - val_loss: 0.9230
Epoch 3437/5000
26/26 - 1s - loss: 0.4882 - val_loss: 0.9221
Epoch 3438/5000
26/26 - 1s - loss: 0.4891 - val_loss: 0.9234
Epoch 3439/5000
26/26 - 1s - loss: 0.4885 - val_loss: 0.9219
Epoch 3440/5000
26/26 - 1s - loss: 0.4880 - val_loss: 0.9208
Epoch 03440: val_loss improved from 0.92236 to 0.92082, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 3441/5000
26/26 - 1s - loss: 0.4904 - val_loss: 0.9221
Epoch 3442/5000
26/26 - 1s - loss: 0.4880 - val_loss: 0.9210
Epoch 3443/5000
26/26 - 1s - loss: 0.4881 - val_loss: 0.9220
Epoch 3444/5000
26/26 - 1s - loss: 0.4872 - val_loss: 0.9203
Epoch 3445/5000
26/26 - 1s - loss: 0.4889 - val_loss: 0.9215
Epoch 3446/5000
26/26 - 1s - loss: 0.4880 - val_loss: 0.9209
Epoch 3447/5000
26/26 - 1s - loss: 0.4882 - val_loss: 0.9202
Epoch 3448/5000
26/26 - 1s - loss: 0.4885 - val_loss: 0.9202
Epoch 3449/5000
26/26 - 1s - loss: 0.4882 - val_loss: 0.9207
Epoch 3450/5000
26/26 - 1s - loss: 0.4881 - val_loss: 0.9212
Epoch 03450: val_loss did not improve from 0.92082
Epoch 3451/5000
26/26 - 1s - loss: 0.4870 - val_loss: 0.9196
Epoch 3452/5000
26/26 - 1s - loss: 0.4889 - val_loss: 0.9208
Epoch 3453/5000
26/26 - 1s - loss: 0.4866 - val_loss: 0.9211
Epoch 3454/5000
26/26 - 1s - loss: 0.4881 - val_loss: 0.9217
Epoch 3455/5000
26/26 - 1s - loss: 0.4880 - val_loss: 0.9200
Epoch 3456/5000
26/26 - 1s - loss: 0.4864 - val_loss: 0.9195
Epoch 3457/5000
26/26 - 1s - loss: 0.4883 - val_loss: 0.9179
Epoch 3458/5000
26/26 - 1s - loss: 0.4870 - val_loss: 0.9212
Epoch 3459/5000
26/26 - 1s - loss: 0.4878 - val_loss: 0.9188
Epoch 3460/5000
26/26 - 1s - loss: 0.4877 - val_loss: 0.9198
Epoch 03460: val_loss improved from 0.92082 to 0.91981, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 3461/5000
26/26 - 1s - loss: 0.4862 - val_loss: 0.9203
Epoch 3462/5000
26/26 - 1s - loss: 0.4861 - val_loss: 0.9198
Epoch 3463/5000
26/26 - 1s - loss: 0.4865 - val_loss: 0.9204
Epoch 3464/5000
26/26 - 1s - loss: 0.4862 - val_loss: 0.9187
Epoch 3465/5000
26/26 - 1s - loss: 0.4863 - val_loss: 0.9191
Epoch 3466/5000
26/26 - 1s - loss: 0.4858 - val_loss: 0.9195
Epoch 3467/5000
26/26 - 2s - loss: 0.4858 - val_loss: 0.9196
Epoch 3468/5000
26/26 - 1s - loss: 0.4856 - val_loss: 0.9186
Epoch 3469/5000
26/26 - 1s - loss: 0.4861 - val_loss: 0.9187
Epoch 3470/5000
26/26 - 1s - loss: 0.4847 - val_loss: 0.9187
Epoch 03470: val_loss improved from 0.91981 to 0.91869, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-2.model.weights.hdf5
Epoch 3471/5000
26/26 - 1s - loss: 0.4859 - val_loss: 0.9185
Epoch 3472/5000
26/26 - 1s - loss: 0.4855 - val_loss: 0.9193
Epoch 3473/5000
26/26 - 1s - loss: 0.4865 - val_loss: 0.9194
Epoch 3474/5000
26/26 - 1s - loss: 0.4854 - val_loss: 0.9181
Epoch 3475/5000
26/26 - 1s - loss: 0.4851 - val_loss: 0.9181
Epoch 3476/5000
26/26 - 1s - loss: 0.4860 - val_loss: 0.9181
Epoch 3477/5000
INFO     Computation time for training the single-label model for AR: 84.4 min
INFO     Network type: REG
INFO     Model: "sequential"
INFO     _________________________________________________________________
INFO     Layer (type)                 Output Shape              Param #
INFO     =================================================================
INFO     dense (Dense)                (None, 1024)              2098176
INFO     _________________________________________________________________
INFO     dropout (Dropout)            (None, 1024)              0
INFO     _________________________________________________________________
INFO     dense_1 (Dense)              (None, 512)               524800
INFO     _________________________________________________________________
INFO     dropout_1 (Dropout)          (None, 512)               0
INFO     _________________________________________________________________
INFO     dense_2 (Dense)              (None, 256)               131328
INFO     _________________________________________________________________
INFO     dropout_2 (Dropout)          (None, 256)               0
INFO     _________________________________________________________________
INFO     dense_3 (Dense)              (None, 128)               32896
INFO     _________________________________________________________________
INFO     dropout_3 (Dropout)          (None, 128)               0
INFO     _________________________________________________________________
INFO     dense_4 (Dense)              (None, 1)                 129
INFO     =================================================================
INFO     Total params: 2,787,329
INFO     Trainable params: 2,787,329
INFO     Non-trainable params: 0
INFO     _________________________________________________________________
INFO     Evaluating trained model 'AR single-labeled Fold-2' on test data
26/26 - 1s - loss: 0.4855 - val_loss: 0.9190
Restoring model weights from the end of the best epoch.
Epoch 03477: early stopping
Epoch 1/5000
INFO     Training of fold number: 3
INFO     Training sample distribution: train data: {-1.2016366720199585: 5, -1.2016383409500122: 4, -1.20163094997406: 4, -1.2016377449035645: 4, -1.201637625694275: 3, -1.2016369104385376: 3, -1.201636791229248: 3, -1.201636552810669: 3, -1.2016324996948242: 3, -1.2016304731369019: 3, -1.2016327381134033: 3, -1.2016355991363525: 3, -1.2016363143920898: 3, -1.201635479927063: 3, -1.2016347646713257: 2, -1.201621651649475: 2, -1.2016375064849854: 2, -1.2016326189041138: 2, -1.2016253471374512: 2, -1.2016353607177734: 2, -1.2009780406951904: 2, -1.2016041278839111: 2, -1.2016315460205078: 2, -1.201629877090454: 2, -1.2016345262527466: 2, -1.201633095741272: 2, -1.2016384601593018: 2, -1.2016342878341675: 2, -1.2015819549560547: 2, -1.2016303539276123: 2, -1.2016254663467407: 2, -1.201627254486084: 2, -1.2016159296035767: 2, -1.2016351222991943: 2, -1.2016295194625854: 2, -0.37773171067237854: 1, 1.6106586456298828: 1, 1.3083291053771973: 1, -0.3040960133075714: 1, 1.8768579959869385: 1, -0.5348793864250183: 1, 0.6560998558998108: 1, 0.25592291355133057: 1, 0.20332399010658264: 1, -0.9010018706321716: 1, 0.2900018095970154: 1, -1.0873279571533203: 1, 1.6950387954711914: 1, -0.2516374886035919: 1, 1.1870161294937134: 1, 0.5256680846214294: 1, 0.6158161163330078: 1, 1.3986883163452148: 1, 1.4535586833953857: 1, 1.4237861633300781: 1, 0.6442342400550842: 1, 1.2952117919921875: 1, -0.37317758798599243: 1, 0.8825770616531372: 1, 1.5720155239105225: 1, 1.6089627742767334: 1, 0.06883639097213745: 1, 1.6063342094421387: 1, 1.605971336364746: 1, 0.2661615014076233: 1, 1.8398889303207397: 1, 0.2954026460647583: 1, -0.18418414890766144: 1, 0.8664619326591492: 1, 0.32629087567329407: 1, 1.5149681568145752: 1, 1.8254425525665283: 1, 1.7107861042022705: 1, 0.9998847246170044: 1, 0.19487899541854858: 1, 0.6321052312850952: 1, 0.21748410165309906: 1, -0.06838630884885788: 1, 0.31109386682510376: 1, 0.5247108340263367: 1, 0.8396581411361694: 1, 0.7778733968734741: 1, 0.5732383131980896: 1, -0.4307803809642792: 1, -1.201623558998108: 1, 0.6573249697685242: 1, 0.4933412969112396: 1, -0.6442912220954895: 1, -0.5330178141593933: 1, -0.008235386572778225: 1, 0.2470504194498062: 1, -0.297276109457016: 1, 1.5978947877883911: 1, -0.3203604817390442: 1, 1.1970174312591553: 1, -0.41311657428741455: 1, 1.5667779445648193: 1, 0.2669691741466522: 1, -0.29657527804374695: 1, 1.5410983562469482: 1, 0.21768306195735931: 1, 1.3818247318267822: 1, 0.21878471970558167: 1, 1.1950836181640625: 1, 1.3501269817352295: 1, 0.0860099047422409: 1, 0.7218993902206421: 1, 0.31096193194389343: 1, 1.0667099952697754: 1, -1.1963087320327759: 1, -0.2558962106704712: 1, 0.002237366745248437: 1, -1.2016195058822632: 1, -0.16027171909809113: 1, 0.02149348333477974: 1, 0.5299685597419739: 1, 0.2713952362537384: 1, 1.4458893537521362: 1, 0.06667295098304749: 1, -0.7750424146652222: 1, -0.4970245659351349: 1, 1.5455868244171143: 1, 0.3489625155925751: 1, -0.3774075210094452: 1, -0.16336557269096375: 1, -0.16630633175373077: 1, 0.08119866997003555: 1, -0.35442060232162476: 1, 0.8313724994659424: 1, 0.26908448338508606: 1, 0.8440163731575012: 1, 1.3062021732330322: 1, 0.16810350120067596: 1, -0.22351212799549103: 1, 0.5170465111732483: 1, -1.1701372861862183: 1, 0.8274267315864563: 1, -0.2711203992366791: 1, 1.457643747329712: 1, 0.3245508372783661: 1, -0.47023993730545044: 1, -0.48075738549232483: 1, 1.2379846572875977: 1, 1.088638186454773: 1, -0.10035426914691925: 1, 0.0010552277090027928: 1, -0.116549052298069: 1, 0.8569538593292236: 1, 0.6820655465126038: 1, 0.8028292059898376: 1, 0.6465133428573608: 1, -0.8897131681442261: 1, -1.0873202085494995: 1, 1.5124294757843018: 1, -0.24653010070323944: 1, 1.4755654335021973: 1, 0.17100460827350616: 1, -1.115770697593689: 1, 1.090896725654602: 1, -0.7282882928848267: 1, -0.408608615398407: 1, -0.03796708956360817: 1, -1.1950759887695312: 1, 0.34950539469718933: 1, 0.9149655103683472: 1, -0.993471086025238: 1, -0.040320102125406265: 1, 1.4500402212142944: 1, -1.1746125221252441: 1, 1.286658525466919: 1, 1.477781891822815: 1, 0.7171676754951477: 1, 0.7614589929580688: 1, 0.6386443972587585: 1, 1.0165932178497314: 1, 1.0860453844070435: 1, 0.7365891337394714: 1, 0.10738043487071991: 1, -1.160044550895691: 1, 1.8953936100006104: 1, -0.30134135484695435: 1, 0.643775999546051: 1, 1.4653488397598267: 1, 1.3042255640029907: 1, 1.571059226989746: 1, 1.2753889560699463: 1, 1.4251669645309448: 1, 0.663663923740387: 1, 0.15129715204238892: 1, -1.194837212562561: 1, -0.30124133825302124: 1, -0.3344474732875824: 1, 0.3422311544418335: 1, 1.5088152885437012: 1, 0.17204155027866364: 1, -1.009895920753479: 1, 0.04832748696208: 1, 0.2610865831375122: 1, 0.1385408341884613: 1, -0.17784874141216278: 1, 1.595760464668274: 1, 0.34191834926605225: 1, 0.015656888484954834: 1, 1.166581153869629: 1, 0.2743425965309143: 1, 1.2997812032699585: 1, 1.2994223833084106: 1, 0.5403873920440674: 1, 1.5550175905227661: 1, 0.10213274508714676: 1, -0.17338967323303223: 1, 0.9650787115097046: 1, 0.31522658467292786: 1, 0.21625953912734985: 1, -0.19042591750621796: 1, 0.7412875294685364: 1, -0.31671836972236633: 1, -1.2016119956970215: 1, 1.3204209804534912: 1, 0.29451489448547363: 1, 0.15692748129367828: 1, 0.7517246007919312: 1, 0.36155056953430176: 1, -0.026365874335169792: 1, -0.4993174076080322: 1, 0.34516385197639465: 1, 0.11128426343202591: 1, 0.007953275926411152: 1, -1.2007761001586914: 1, 0.034827083349227905: 1, -0.37911587953567505: 1, -1.1482487916946411: 1, 1.520749807357788: 1, -1.1323087215423584: 1, -1.2000701427459717: 1, -1.1476235389709473: 1, 0.5436715483665466: 1, -1.1639471054077148: 1, -1.2015410661697388: 1, -0.33821895718574524: 1, -1.045175313949585: 1, -1.1976145505905151: 1, -1.197718858718872: 1, -0.7745821475982666: 1, -0.9927355051040649: 1, -1.1987890005111694: 1, -1.196489930152893: 1, -1.1852269172668457: 1, -1.1929398775100708: 1, 0.4001854956150055: 1, -0.6864959597587585: 1, -1.1871466636657715: 1, -1.2014594078063965: 1, -1.1205617189407349: 1, -0.09781666100025177: 1, -1.2015609741210938: 1, -0.2831217646598816: 1, 0.7523799538612366: 1, -0.8503693342208862: 1, -1.2014657258987427: 1, -1.201615810394287: 1, -1.186226725578308: 1, -0.27123570442199707: 1, -1.1830931901931763: 1, 0.5888639092445374: 1, -1.1972260475158691: 1, -0.4147615134716034: 1, -1.1996524333953857: 1, 0.39954620599746704: 1, -1.1674489974975586: 1, -1.201310396194458: 1, 1.14208984375: 1, -0.9657180905342102: 1, -0.34237241744995117: 1, -1.197901725769043: 1, -0.5027830004692078: 1, 1.4029184579849243: 1, -0.09802207350730896: 1, 0.28549933433532715: 1, -0.4268537759780884: 1, 0.16395387053489685: 1, -0.29486238956451416: 1, -0.5222632884979248: 1, -0.27864202857017517: 1, -0.9769929647445679: 1, -0.5042855143547058: 1, -0.3579169511795044: 1, -0.00951747503131628: 1, -1.177890419960022: 1, 1.603068470954895: 1, -0.3334684669971466: 1, -0.2865978181362152: 1, -0.683555543422699: 1, 1.4387927055358887: 1, 0.18236172199249268: 1, 1.2996296882629395: 1, 0.5139665007591248: 1, 1.4342314004898071: 1, 1.4295574426651: 1, -1.1866750717163086: 1, -1.1269394159317017: 1, -1.1237342357635498: 1, -1.1972938776016235: 1, -1.1962217092514038: 1, -1.1939657926559448: 1, -1.1927686929702759: 1, -1.2009947299957275: 1, -1.2008297443389893: 1, -1.1962995529174805: 1, -1.1956098079681396: 1, 0.7675377726554871: 1, 0.8374537825584412: 1, -1.1961579322814941: 1, -0.15980762243270874: 1, -0.8842195272445679: 1, -0.2302703857421875: 1, 1.7539664506912231: 1, -0.005905755329877138: 1, 1.1079081296920776: 1, -1.1402051448822021: 1, -0.07565759867429733: 1, -0.4154701232910156: 1, 0.1561044156551361: 1, -1.201622486114502: 1, -1.092740535736084: 1, -1.201349139213562: 1, -0.46576231718063354: 1, -0.916256844997406: 1, 0.339458167552948: 1, -1.137963891029358: 1, -0.9888867139816284: 1, -0.9931707978248596: 1, 0.2407364845275879: 1, -1.2013576030731201: 1, -0.6085047125816345: 1, -0.6204319000244141: 1, -1.1730265617370605: 1, -0.8519163131713867: 1, 0.0290671493858099: 1, 0.6741006970405579: 1, 0.9611315131187439: 1, 0.031881630420684814: 1, -1.2016046047210693: 1, -0.4260459542274475: 1, -0.506174623966217: 1, -1.2013626098632812: 1, -0.8657602667808533: 1, -1.201596975326538: 1, 0.9703378081321716: 1, -0.24468590319156647: 1, 1.478103518486023: 1, -0.399366557598114: 1, -1.1840753555297852: 1, 1.3625078201293945: 1, 1.621954083442688: 1, 1.5633379220962524: 1, 1.593016266822815: 1, 0.32263097167015076: 1, -1.2016290426254272: 1, -1.2015366554260254: 1, -0.1421377956867218: 1, 1.4907145500183105: 1, 1.6047093868255615: 1, 1.5627902746200562: 1, 0.20390741527080536: 1, 0.007752139586955309: 1, 0.21325090527534485: 1, -1.1937233209609985: 1, -1.1647279262542725: 1, -0.3525408208370209: 1, -1.180529236793518: 1, -1.2015936374664307: 1, 1.6281145811080933: 1, -0.7713357210159302: 1, -1.2016221284866333: 1, -0.21383443474769592: 1, 0.21984633803367615: 1, -1.1999688148498535: 1, -1.2006030082702637: 1, -1.2015992403030396: 1, 0.6647039651870728: 1, -0.883184015750885: 1, 0.46835482120513916: 1, -0.19289743900299072: 1, -1.2016280889511108: 1, -1.1382324695587158: 1, -1.1506352424621582: 1, -1.201614260673523: 1, -1.2014697790145874: 1, -1.201564908027649: 1, -1.2014739513397217: 1, -0.7784246206283569: 1, -1.0070439577102661: 1, -1.2015589475631714: 1, -1.2015974521636963: 1, -1.2007629871368408: 1, -1.1959139108657837: 1, -0.6465518474578857: 1, -1.1526696681976318: 1, -1.2014310359954834: 1, -1.0348321199417114: 1, 0.603252649307251: 1, -0.4781683385372162: 1, -1.2015955448150635: 1, -0.5828713178634644: 1, -1.2016127109527588: 1, -1.201597809791565: 1, 0.10008653253316879: 1, -0.9153497219085693: 1, 0.1308048814535141: 1, -0.1883729249238968: 1, -1.2015149593353271: 1, -1.055851936340332: 1, -0.5025617480278015: 1, -1.2004907131195068: 1, -0.6686071157455444: 1, 0.670230507850647: 1, 1.7721431255340576: 1, 1.5277636051177979: 1, -1.0090559720993042: 1, 0.7494425773620605: 1, -0.8920286297798157: 1, -0.556195080280304: 1, -0.896551251411438: 1, 1.8447388410568237: 1, -1.1907743215560913: 1, 0.6965684294700623: 1, -1.1941906213760376: 1, -0.26542410254478455: 1, -0.9432769417762756: 1, 1.3791615962982178: 1, -1.0422825813293457: 1, 1.577986240386963: 1, 1.3899286985397339: 1, 1.5516252517700195: 1, 0.7257186770439148: 1, -0.3594827950000763: 1, -0.41735291481018066: 1, 1.5167564153671265: 1, -0.2154475301504135: 1, -1.111975073814392: 1, -1.0634658336639404: 1, -0.6316778063774109: 1, -1.1412583589553833: 1, -0.1989370882511139: 1, -1.193503737449646: 1, -0.923246443271637: 1, -0.8935588002204895: 1, -1.0519613027572632: 1, -1.18907630443573: 1, -1.0768063068389893: 1, -1.0751264095306396: 1, -0.7096079587936401: 1, -0.35407769680023193: 1, -0.2589719295501709: 1, 0.2903974652290344: 1, -0.930094301700592: 1, -0.9047161340713501: 1, -1.1848548650741577: 1, -0.6980454921722412: 1, -0.6311256289482117: 1, 1.192413568496704: 1, -0.9288858771324158: 1, -0.5754680633544922: 1, -0.18136551976203918: 1, -0.5884833335876465: 1, 0.7908697128295898: 1, 0.10187211632728577: 1, -0.8993450403213501: 1, -1.1138004064559937: 1, -1.1072322130203247: 1, 0.5786248445510864: 1, 0.9790754318237305: 1, 0.20924636721611023: 1, 0.05307941138744354: 1, -0.43377333879470825: 1, 0.5685755014419556: 1, 0.46832725405693054: 1, -1.1569617986679077: 1, -1.0893759727478027: 1, 0.8691079020500183: 1, -1.1004241704940796: 1, -0.26597675681114197: 1, -1.1329883337020874: 1, 1.3982725143432617: 1, -1.0213873386383057: 1, -1.1798655986785889: 1, -0.8407037854194641: 1, -0.5144169330596924: 1, 1.2280430793762207: 1, 0.16367900371551514: 1, -1.1570571660995483: 1, -0.700495719909668: 1, -0.9755853414535522: 1, -0.9736310243606567: 1, -0.9981153011322021: 1, -1.18631112575531: 1, -0.5620161294937134: 1, 1.8358889818191528: 1, -0.07864277809858322: 1, -0.04164140671491623: 1, -0.8634552955627441: 1, 0.4990178942680359: 1, -0.4610443115234375: 1, -1.0622771978378296: 1, -1.1442792415618896: 1, 0.2268732339143753: 1, -0.5990430116653442: 1, 0.15106460452079773: 1, 0.19889964163303375: 1, -0.12463472783565521: 1, -1.0407896041870117: 1, -1.1999518871307373: 1, 0.06941241025924683: 1, 1.9090871810913086: 1, 1.5657020807266235: 1, -1.1800106763839722: 1, 0.006390336435288191: 1, -1.1358855962753296: 1, -1.0411947965621948: 1, 1.6431117057800293: 1, -1.1897622346878052: 1, 0.5211433172225952: 1, 0.6570013761520386: 1, -0.08619289845228195: 1, -0.9803085327148438: 1, 0.39953649044036865: 1, 0.4876076281070709: 1, 1.0163378715515137: 1, -0.233070969581604: 1, -1.1343045234680176: 1, -0.8108161091804504: 1, 1.4876383543014526: 1, -0.9951556921005249: 1, -0.8936908841133118: 1, -0.90742427110672: 1, -1.024053692817688: 1, 0.6749281883239746: 1, -0.3465023636817932: 1, 1.3890480995178223: 1, -1.0539400577545166: 1, -0.8580590486526489: 1, -0.7244925498962402: 1, -1.1898142099380493: 1, -0.8890491724014282: 1, -1.0776159763336182: 1, 0.638069212436676: 1, 1.6304298639297485: 1, 0.46070119738578796: 1, 0.7275790572166443: 1, 0.3124358654022217: 1, 1.556864857673645: 1, -0.20395949482917786: 1, 0.9760450720787048: 1, -0.13872799277305603: 1, 0.3074534237384796: 1, 0.8999701142311096: 1, -1.1716605424880981: 1, -1.1999285221099854: 1, 1.0112850666046143: 1, 0.49092090129852295: 1, -0.9134752750396729: 1, -0.49170398712158203: 1, -0.8270519375801086: 1, -0.04211708903312683: 1, -0.974824070930481: 1, -0.9460977911949158: 1, -0.7166287899017334: 1, -0.67027348279953: 1, 1.8432141542434692: 1, -0.8671839237213135: 1, 0.6293842196464539: 1, -0.8917420506477356: 1, 1.1045739650726318: 1, -0.9299888610839844: 1, -0.608140766620636: 1, -1.0076677799224854: 1, 1.0116826295852661: 1, -0.9982122778892517: 1, -1.2001420259475708: 1, -1.1845873594284058: 1, -1.2009141445159912: 1, -1.0507465600967407: 1, -1.1325860023498535: 1, -0.36594024300575256: 1, -1.1659823656082153: 1, 0.04404761642217636: 1, -0.4354245066642761: 1, -1.1712636947631836: 1, -1.1646332740783691: 1, -1.194373369216919: 1, -0.9642779231071472: 1, -1.2012841701507568: 1, -0.19042661786079407: 1, -0.2147151529788971: 1, -0.9940837025642395: 1, -1.0393953323364258: 1, 0.30057549476623535: 1, -1.199107050895691: 1, -1.1946264505386353: 1, 3.259727716445923: 1, -1.1212905645370483: 1, 0.0663938894867897: 1, 0.17207567393779755: 1, 1.4842547178268433: 1, -0.981871485710144: 1, -1.1791499853134155: 1, 0.2662027180194855: 1, -1.078048825263977: 1, 1.536348581314087: 1, 1.8480533361434937: 1, 1.0025136470794678: 1, 1.6904795169830322: 1, -1.0221163034439087: 1, -0.1391475349664688: 1, -1.094030499458313: 1, 0.5474697947502136: 1, -0.7628646492958069: 1, 1.114488959312439: 1, -0.8923998475074768: 1, 1.0554637908935547: 1, 0.3200169503688812: 1, 0.5357815027236938: 1, -0.5850008726119995: 1, -1.1196062564849854: 1, 1.0170906782150269: 1, -1.107023000717163: 1, -0.16252407431602478: 1, 0.27488669753074646: 1, 1.2950940132141113: 1, 0.7110782265663147: 1, -0.16512484848499298: 1, 0.1011820137500763: 1, 0.6780659556388855: 1, 1.495969295501709: 1, 1.2984728813171387: 1, 0.6696186065673828: 1, 0.11351441591978073: 1, -0.6162902116775513: 1, 1.3002121448516846: 1, 1.4811328649520874: 1, -0.8986467123031616: 1, -1.1914112567901611: 1, 1.5811641216278076: 1, -0.25767362117767334: 1, -0.027011625468730927: 1, 1.3250715732574463: 1, 1.9331234693527222: 1, 1.0355088710784912: 1, 1.2882025241851807: 1, -1.1970343589782715: 1, 1.7417840957641602: 1, 1.3437533378601074: 1, -0.26631277799606323: 1, 0.9666041731834412: 1, 1.4656307697296143: 1, 0.16298139095306396: 1, 0.9499619007110596: 1, 0.6718612909317017: 1, -0.9881011247634888: 1, -1.2016340494155884: 1, 0.8268551230430603: 1, -0.7180438041687012: 1, 0.9129876494407654: 1, -0.6611031889915466: 1, -0.7170498371124268: 1, 0.6606081128120422: 1, 1.5755736827850342: 1, 0.8417104482650757: 1, 0.8741052150726318: 1, 0.8892757296562195: 1, 0.4746771454811096: 1, 1.0541952848434448: 1, 0.5222756862640381: 1, -0.5288078188896179: 1, 1.224327802658081: 1, -0.9218448400497437: 1, -0.004194003064185381: 1, 1.573736548423767: 1, 0.7825468182563782: 1, 1.4694101810455322: 1, 0.3604428768157959: 1, 0.4633817672729492: 1, 1.427628755569458: 1, 1.4536134004592896: 1, 0.699573814868927: 1, 0.3411409258842468: 1, 0.394859254360199: 1, -0.030014334246516228: 1, 0.2308363914489746: 1, -0.4297850430011749: 1, -0.059458885341882706: 1, -0.22176611423492432: 1, 0.13578376173973083: 1, 0.2726168930530548: 1, 0.03207547590136528: 1, 1.5805106163024902: 1, 0.4048600494861603: 1, 1.4308977127075195: 1, 0.637361466884613: 1, 1.2712597846984863: 1, -1.113705039024353: 1, 1.2730098962783813: 1, 1.3784377574920654: 1, 1.1008855104446411: 1, 1.4938230514526367: 1, 1.1383615732192993: 1, 1.2588475942611694: 1, 0.4723914861679077: 1, -0.24819114804267883: 1, -0.08030800521373749: 1, -0.06996402144432068: 1, 1.427193522453308: 1, -0.39828142523765564: 1, 0.7533062696456909: 1, 1.3024239540100098: 1, 0.9470359086990356: 1, 1.2427196502685547: 1, 0.71575528383255: 1, 1.6178103685379028: 1, -1.2016338109970093: 1, 0.15925046801567078: 1, -0.37734130024909973: 1, 1.0202414989471436: 1, 0.22981515526771545: 1, 0.2808535397052765: 1, 0.003300704760476947: 1, 0.16384904086589813: 1, 0.7951827049255371: 1, 1.2659807205200195: 1, -0.8956496715545654: 1, -1.0024443864822388: 1, -0.38419657945632935: 1, -0.6876721978187561: 1, -0.27164560556411743: 1, 1.6284458637237549: 1, -0.0960833728313446: 1, -0.48030614852905273: 1, 1.9067574739456177: 1, 0.07571198046207428: 1, -1.1507015228271484: 1, -0.8877851366996765: 1, -0.9158876538276672: 1, 0.342952162027359: 1, -0.9859256744384766: 1, 1.425097107887268: 1, -1.1913617849349976: 1, 1.287703037261963: 1, 0.35307395458221436: 1, 1.4413039684295654: 1, -0.8234555125236511: 1, -0.8622487187385559: 1, 1.7851945161819458: 1, -0.9035985469818115: 1, 0.40835040807724: 1, -0.03840658441185951: 1, -0.9793020486831665: 1, 1.324471354484558: 1, -1.2006902694702148: 1, -1.0523524284362793: 1, -0.5724524259567261: 1, -0.7811231017112732: 1, 0.4252239763736725: 1, 0.784543514251709: 1, -1.199570655822754: 1, -0.5242934823036194: 1, 0.9346560835838318: 1, -1.1873440742492676: 1, -0.8747767210006714: 1, 1.4774726629257202: 1, -0.731924295425415: 1, -0.05734042823314667: 1, -0.45086827874183655: 1, -0.9131625890731812: 1, 1.1575602293014526: 1, -1.1544640064239502: 1, -1.1940946578979492: 1, -0.9339156150817871: 1, 0.1881372481584549: 1, -0.32526895403862: 1, -0.16177639365196228: 1, 0.3255096673965454: 1, 0.9005318284034729: 1, -0.18478137254714966: 1, 0.3698660731315613: 1, -0.539240837097168: 1, 0.2775695323944092: 1, 0.28860634565353394: 1, 0.2634084224700928: 1, -0.589444637298584: 1, 1.4644434452056885: 1, -1.1607003211975098: 1, 1.053705096244812: 1, 1.001185417175293: 1, -1.031872034072876: 1, 0.1467430293560028: 1, 0.3286954462528229: 1, 0.46373531222343445: 1, -0.016411839053034782: 1, 0.10751932114362717: 1, -0.5022063255310059: 1, 0.31799715757369995: 1, 1.7614071369171143: 1, 0.36046868562698364: 1, 0.9138351678848267: 1, -0.514695405960083: 1, 1.1217374801635742: 1, -0.13197508454322815: 1, -1.07969331741333: 1, -0.14438967406749725: 1, -0.4185396134853363: 1, 0.7527873516082764: 1, 0.739153265953064: 1, 0.002877143444493413: 1, -0.047685928642749786: 1, 0.7724436521530151: 1, 1.3262649774551392: 1, -0.11414719372987747: 1, 0.3607413172721863: 1, -0.18094922602176666: 1, 1.366266131401062: 1, 0.9568199515342712: 1, -0.1799832135438919: 1, 0.1897212415933609: 1, 0.32160520553588867: 1, -0.8486149311065674: 1, 1.0683897733688354: 1, 1.5324621200561523: 1, -0.6546655297279358: 1, -0.5717604756355286: 1, -1.2016263008117676: 1, 0.42848649621009827: 1, -0.41689032316207886: 1, 1.8881990909576416: 1, 1.7208062410354614: 1, -1.201619267463684: 1, 0.11243647336959839: 1, -1.2014974355697632: 1, -0.4105451703071594: 1, -0.807515025138855: 1, 0.28526046872138977: 1, 1.3301595449447632: 1, 0.454349547624588: 1, 0.4860861599445343: 1, 1.6493014097213745: 1, -1.1416743993759155: 1, -0.9635469913482666: 1, -0.7447215914726257: 1, -0.35984915494918823: 1, 1.1998889446258545: 1, -0.9704957008361816: 1, -1.102870225906372: 1, 1.5992790460586548: 1, 1.5793328285217285: 1, -1.2016189098358154: 1, 1.8360271453857422: 1, -1.1663979291915894: 1, 0.19680047035217285: 1, 0.8425688147544861: 1, 0.5497986674308777: 1, 0.08808748424053192: 1, 1.6464146375656128: 1, -1.0590986013412476: 1, 1.6939563751220703: 1, 0.523788332939148: 1, 0.2675780653953552: 1, 1.6441566944122314: 1, -0.7435175180435181: 1, -0.962668240070343: 1, -1.2016352415084839: 1, -0.2495342493057251: 1, -0.26848453283309937: 1, 0.3117649555206299: 1, 1.76934015750885: 1, -1.2016271352767944: 1, 0.9834553003311157: 1, -1.2006548643112183: 1, -1.0099024772644043: 1, -0.9806917309761047: 1, 1.2604477405548096: 1, -0.9877029061317444: 1, 0.5713223814964294: 1, 1.8529928922653198: 1, -0.10222127288579941: 1, 1.7350994348526: 1, -0.44384631514549255: 1, 1.7166484594345093: 1, 1.5665374994277954: 1, 0.5419895052909851: 1, -1.0994056463241577: 1, 1.0374422073364258: 1, 1.901644229888916: 1, 1.5698747634887695: 1, 1.2889325618743896: 1, 1.9196945428848267: 1, -1.0791629552841187: 1, -0.5369133949279785: 1, 1.1523829698562622: 1, 1.6013163328170776: 1, -0.94952791929245: 1, 0.3571586012840271: 1, 1.8800233602523804: 1, -0.7931265234947205: 1, -0.5870760679244995: 1, 0.14208731055259705: 1, -0.253825306892395: 1, 0.9758270978927612: 1, 0.6003371477127075: 1, 0.35712626576423645: 1, -0.864342451095581: 1, 1.9024626016616821: 1, 1.6134487390518188: 1, 1.5396013259887695: 1, -1.1658029556274414: 1, -1.2016136646270752: 1, -1.0555497407913208: 1, 1.633592128753662: 1, 1.3466922044754028: 1, 1.5722275972366333: 1, -0.8662946820259094: 1, 0.5880253314971924: 1, -1.088794231414795: 1, -0.7490406036376953: 1, -1.042400598526001: 1, 0.10260368138551712: 1, 1.4280599355697632: 1, -0.2810556888580322: 1, 1.2709132432937622: 1, 0.804813027381897: 1, 0.014584558084607124: 1, 0.9832457304000854: 1, 1.0101338624954224: 1, 0.5755087733268738: 1, 0.2889866232872009: 1, 0.766596257686615: 1, -0.22210338711738586: 1, 0.9173278212547302: 1, 0.14515815675258636: 1, 0.7045637369155884: 1, 0.03225273638963699: 1, 1.545008897781372: 1, 1.590468406677246: 1, 1.3033519983291626: 1, 0.19334031641483307: 1, -0.2232077419757843: 1, 1.8893579244613647: 1, 0.39559683203697205: 1, -0.1754150390625: 1, -0.4791189730167389: 1, 0.9761800765991211: 1, 0.29674577713012695: 1, 0.7860816717147827: 1, -0.2709258496761322: 1, 1.0254307985305786: 1, -0.12864308059215546: 1, 1.4337563514709473: 1, -0.08275699615478516: 1, 0.04922454059123993: 1, 0.7481110692024231: 1, 1.5299123525619507: 1, 1.9158005714416504: 1, 1.7392624616622925: 1, 1.6278821229934692: 1, 1.6179100275039673: 1, 0.11675674468278885: 1, 0.3740043342113495: 1, -0.22299611568450928: 1, 0.056893277913331985: 1, 1.6735926866531372: 1, 0.0023630079813301563: 1, 0.8739101886749268: 1, 0.24924464523792267: 1, 0.7833142876625061: 1, -0.7948459982872009: 1, -0.7293344736099243: 1, 1.1236602067947388: 1, 0.26557838916778564: 1, 0.43645578622817993: 1, -0.2772659659385681: 1, 1.6143009662628174: 1, -0.180640310049057: 1, -0.6814844608306885: 1, 0.4076603651046753: 1, 0.9554869532585144: 1, 1.6500415802001953: 1, 0.5059344172477722: 1, 1.8263726234436035: 1, -0.04758370667695999: 1, 1.7850080728530884: 1, -0.2531147599220276: 1, 0.9454357028007507: 1, 0.9327585697174072: 1, 0.8505056500434875: 1, 0.521833598613739: 1, 0.26564425230026245: 1, 0.18436919152736664: 1, 0.9952530264854431: 1, 1.5315228700637817: 1, 0.27008119225502014: 1, 0.24442099034786224: 1, 0.7021965980529785: 1, 0.27857378125190735: 1, -0.025178229436278343: 1, 1.7298698425292969: 1, 1.4182209968566895: 1, 0.7741647958755493: 1, 0.723430871963501: 1, 1.8846700191497803: 1, -0.3399538993835449: 1, 0.6905592679977417: 1, 1.1628241539001465: 1, -0.22097758948802948: 1, 1.4221618175506592: 1, -1.054260492324829: 1, -0.10395042598247528: 1, -0.0515512116253376: 1, 1.3716888427734375: 1, -0.5665901303291321: 1, 0.949316143989563: 1, 1.6153100728988647: 1, 1.865704894065857: 1, 1.7069745063781738: 1, 1.7395148277282715: 1, -0.31705647706985474: 1, 0.9889004230499268: 1, 0.8423707485198975: 1, 0.5330069065093994: 1, -0.19816404581069946: 1, -0.2079317569732666: 1, 1.612230896949768: 1, 0.5326334834098816: 1, 0.5422061085700989: 1, -0.07945768535137177: 1, 1.4854387044906616: 1, 0.261216938495636: 1, -0.43114355206489563: 1, 1.271135687828064: 1, -0.0861414223909378: 1, 0.562964141368866: 1, 0.8071705102920532: 1, -1.2015928030014038: 1, -1.2015763521194458: 1, 0.6071187853813171: 1, 1.3225599527359009: 1, 0.9784976243972778: 1, 0.963599681854248: 1, 1.7376213073730469: 1, 1.0590068101882935: 1, 1.2539737224578857: 1, 1.610649824142456: 1, -1.0813920497894287: 1, -1.0571757555007935: 1, 0.8005363941192627: 1, 0.10180643945932388: 1, 1.2595564126968384: 1, -1.012891411781311: 1, 0.5681759119033813: 1, -1.201594352722168: 1, 1.2727433443069458: 1, 0.2591017186641693: 1, 1.8303353786468506: 1, 0.46223318576812744: 1, 1.127722144126892: 1, -1.201613187789917: 1, 0.3816104531288147: 1, -0.20698606967926025: 1, 0.20394694805145264: 1, 0.780617356300354: 1, 1.8291547298431396: 1, 1.6759966611862183: 1, 1.0316096544265747: 1, -1.2015867233276367: 1, -0.538144052028656: 1, 1.7389863729476929: 1, 1.896134853363037: 1, 1.519827961921692: 1, 1.3174397945404053: 1, 1.16111421585083: 1, -1.085170865058899: 1, 0.8646785616874695: 1, -0.7696746587753296: 1, -1.2016215324401855: 1, 0.6376197934150696: 1, 1.5719680786132812: 1, -0.6896651983261108: 1, 0.4620521366596222: 1, -1.0611162185668945: 1, -0.34224218130111694: 1, -0.06900987029075623: 1, 1.8261455297470093: 1, -0.40272125601768494: 1, -1.130736231803894: 1, -0.24564200639724731: 1, -1.2007230520248413: 1, 0.9158507585525513: 1, 1.3129916191101074: 1, 0.6801310777664185: 1, 0.8895251154899597: 1, 1.5496872663497925: 1, -1.2016278505325317: 1, 0.4616207182407379: 1, -0.7448302507400513: 1, -1.2016236782073975: 1, 0.8969712257385254: 1, -0.8653848767280579: 1, 1.8480027914047241: 1, -0.7542659640312195: 1, 0.5805153250694275: 1, -0.9954119920730591: 1, 1.2369016408920288: 1, 1.7776927947998047: 1, 0.7080846428871155: 1, -1.2015223503112793: 1, 0.9156394600868225: 1, -1.2016057968139648: 1, -0.2255825698375702: 1, 1.3490053415298462: 1, 1.0934252738952637: 1, -0.9789384603500366: 1, -1.14544677734375: 1, 0.9750880599021912: 1, 1.3510494232177734: 1, -0.6586742401123047: 1, -1.2011404037475586: 1, 1.339892864227295: 1, -0.7379482984542847: 1, -0.5476839542388916: 1, 1.1129239797592163: 1, -1.1302224397659302: 1, 1.3335411548614502: 1, 1.3847272396087646: 1, 1.110692024230957: 1, 1.7461694478988647: 1, -0.655949056148529: 1, -0.24759358167648315: 1, 1.446770191192627: 1, -1.2014168500900269: 1, 0.9160022139549255: 1, -0.9175146222114563: 1, 0.10331395268440247: 1, 1.0329307317733765: 1, 0.05316608399152756: 1, 1.4475048780441284: 1, 1.6221997737884521: 1, 1.8285820484161377: 1, -0.22918304800987244: 1, -0.2059621661901474: 1, 0.12914451956748962: 1, 1.7338974475860596: 1, -1.1562846899032593: 1, 1.8416829109191895: 1, 1.7511584758758545: 1, 1.788615107536316: 1, 0.8217967748641968: 1, 0.63859623670578: 1, 0.8861773610115051: 1, -0.010684509761631489: 1, 1.887890100479126: 1, 1.6676998138427734: 1, 1.636014699935913: 1, -0.4143541157245636: 1, 0.8007993698120117: 1, -1.2016080617904663: 1, 0.23050570487976074: 1, -0.7923315763473511: 1, 0.8521577715873718: 1, -0.009660118259489536: 1, -0.7990305423736572: 1, -1.174880027770996: 1, -1.201361894607544: 1, 1.7429335117340088: 1, 1.92928946018219: 1, 1.7397531270980835: 1, -0.8557604551315308: 1, -0.08804576843976974: 1, 1.2302463054656982: 1, 0.8335886001586914: 1, -0.8916471600532532: 1, 1.4848576784133911: 1, 1.8896540403366089: 1, -1.2014458179473877: 1, 0.6508381366729736: 1, 0.9216548800468445: 1, 0.09368380159139633: 1, 1.4813575744628906: 1, -0.907404899597168: 1, 0.13984030485153198: 1, -0.11489463597536087: 1, 1.0331618785858154: 1, -1.156775951385498: 1, -0.4194781482219696: 1, 1.5742621421813965: 1, 0.1478143036365509: 1, 0.35647323727607727: 1, 1.2849032878875732: 1, 1.5142070055007935: 1, -0.420527845621109: 1, -0.5466570258140564: 1, 1.6787821054458618: 1, 1.0356202125549316: 1, 1.8354456424713135: 1, 1.8180814981460571: 1, 1.4667671918869019: 1, 1.455495834350586: 1, -0.21755054593086243: 1, -1.1579643487930298: 1, 0.8198267817497253: 1, 1.8067305088043213: 1, 1.9223994016647339: 1, -0.7876200675964355: 1, 1.2843722105026245: 1, 1.7499927282333374: 1, -0.796614408493042: 1, -0.8602240085601807: 1, 1.473099708557129: 1, 1.0176738500595093: 1, 1.703546404838562: 1, 1.746630072593689: 1, 0.9968640804290771: 1, 1.5016316175460815: 1, 1.6772441864013672: 1, 5.270293235778809: 1, 0.7037346363067627: 1, -0.5458275079727173: 1, 1.0276689529418945: 1, 1.0119178295135498: 1, 1.8225407600402832: 1, -0.1335328370332718: 1, 1.7602899074554443: 1, 1.8266348838806152: 1, 1.058468222618103: 1, 0.32922476530075073: 1, 1.7661612033843994: 1, -0.8965012431144714: 1, -0.15924076735973358: 1, 1.2042120695114136: 1, 0.017721591517329216: 1, -0.3422335684299469: 1, -0.9143604040145874: 1, -0.29665860533714294: 1, -0.8232591152191162: 1, -0.032225385308265686: 1, 1.2115764617919922: 1, -0.558110237121582: 1, -0.47733497619628906: 1, -0.014680324122309685: 1, -0.24040797352790833: 1, -1.1957098245620728: 1, -0.6786049008369446: 1, 0.3244344890117645: 1, 1.1735796928405762: 1, -0.4622204601764679: 1, -0.7031784653663635: 1, -0.37577909231185913: 1, -0.03591597080230713: 1, 0.2143784463405609: 1, 0.5354000329971313: 1, -0.34326422214508057: 1, 1.9232004880905151: 1, -1.0387167930603027: 1, -0.4592617452144623: 1, 1.2142442464828491: 1, -0.5345551371574402: 1, 0.2551988363265991: 1, 1.483720302581787: 1, 1.4788439273834229: 1, 1.052090048789978: 1, 1.2964091300964355: 1, 1.5233625173568726: 1, -0.466978520154953: 1, -0.9003047943115234: 1, 1.569981575012207: 1, -0.92970210313797: 1, 0.9121710062026978: 1, -0.23732632398605347: 1, -1.093284010887146: 1, 1.8227369785308838: 1, -0.16766004264354706: 1, 0.7785534858703613: 1, 0.006228437647223473: 1, 1.456301212310791: 1, 1.542358636856079: 1, -0.6440175771713257: 1, -0.7915438413619995: 1, -0.3439752459526062: 1, -0.07709828019142151: 1, -0.22727444767951965: 1, -0.06596078723669052: 1, 1.3363116979599: 1, -0.8456059098243713: 1, -0.8443267941474915: 1, -1.0356733798980713: 1, -1.1333893537521362: 1, 0.43118155002593994: 1, 1.6555308103561401: 1, -0.2761753499507904: 1, -0.5378462672233582: 1, -0.21501778066158295: 1, 1.6570682525634766: 1, -0.4369107186794281: 1, 1.8792171478271484: 1, -0.42767956852912903: 1, -0.0695866122841835: 1, 0.16830426454544067: 1, 0.3824501037597656: 1, 0.1589841991662979: 1, 0.7185215353965759: 1, -0.608808159828186: 1, 1.8601784706115723: 1, -0.3237372636795044: 1, -0.3169466555118561: 1, 0.6444322466850281: 1, 1.5919677019119263: 1, -0.46113380789756775: 1, -0.48933231830596924: 1, -0.7822737693786621: 1, -0.28641819953918457: 1, -0.9299617409706116: 1, -1.1991149187088013: 1, 1.8707987070083618: 1, 0.29917436838150024: 1, 0.6026607155799866: 1, -1.185569167137146: 1, 1.6845048666000366: 1, -1.0817426443099976: 1, -1.2014148235321045: 1, -0.31910526752471924: 1, -1.193153738975525: 1, -1.1891201734542847: 1, 1.3399251699447632: 1, 0.15484686195850372: 1, -0.5629591941833496: 1, -0.3858093321323395: 1, 1.3953920602798462: 1, 1.878305196762085: 1, -0.06991042196750641: 1, 0.19413244724273682: 1, -0.0211151335388422: 1, 3.323413610458374: 1, 0.47734835743904114: 1, 0.16428887844085693: 1, -0.35491982102394104: 1, -0.5378177762031555: 1, -0.960394024848938: 1, -0.29989245533943176: 1, -1.1109116077423096: 1, -0.4605187475681305: 1, 0.06790906190872192: 1, -0.2670007050037384: 1, 0.3248298168182373: 1, 1.5722923278808594: 1, 1.9010276794433594: 1, -0.879592776298523: 1, -0.288830429315567: 1, -0.8833669424057007: 1, -0.007546336855739355: 1, 0.6150393486022949: 1, 1.187366247177124: 1, -0.766767144203186: 1, -0.238590806722641: 1, -0.5713714957237244: 1, 0.8612715601921082: 1, 0.9746946096420288: 1, -0.5843047499656677: 1, -0.46731990575790405: 1, -0.8472578525543213: 1, 1.5701237916946411: 1, 1.3883335590362549: 1, -0.47762712836265564: 1, -0.9396678805351257: 1, 1.0024393796920776: 1, -0.1308683305978775: 1, 0.31239357590675354: 1, 1.6493046283721924: 1, -0.3424704372882843: 1, -0.2937408983707428: 1, -0.5880576372146606: 1, -0.7417653203010559: 1, 0.4303920269012451: 1, -0.13842415809631348: 1, -0.3907565474510193: 1, 1.491416096687317: 1, -1.1868388652801514: 1, -0.9987406134605408: 1, 1.4534815549850464: 1, -0.30685290694236755: 1, -1.0160380601882935: 1, -0.4835919141769409: 1, -0.13618627190589905: 1, 0.37810415029525757: 1, -0.49563685059547424: 1, -0.2992294430732727: 1, -0.8521113991737366: 1, 1.3037792444229126: 1, 0.17652635276317596: 1, 1.6146701574325562: 1, -0.36514076590538025: 1, 0.31867286562919617: 1, 1.7650138139724731: 1, -0.29448574781417847: 1, -0.8708242177963257: 1, -0.023513898253440857: 1, -1.0031712055206299: 1, -0.6234576106071472: 1, -0.014453819021582603: 1, -0.3702247440814972: 1, -0.8769359588623047: 1, 0.7159720659255981: 1, -0.0832226425409317: 1, 0.012689988128840923: 1, -0.3753896951675415: 1, -1.0713788270950317: 1, 1.2261123657226562: 1, 0.9851498603820801: 1, 0.6790488958358765: 1, -0.5030357241630554: 1, 0.672914445400238: 1, 0.273947536945343: 1, -1.1766016483306885: 1, 1.5933094024658203: 1, 1.8017815351486206: 1, -0.4548023045063019: 1, 1.6648616790771484: 1, 0.9130324721336365: 1, 0.7212937474250793: 1, -0.5381679534912109: 1, 0.5300026535987854: 1, -0.6655375957489014: 1, 0.3571058511734009: 1, 0.3796524703502655: 1, 0.5128249526023865: 1, 0.04851381108164787: 1, -0.8232764601707458: 1, -0.3349834084510803: 1, -0.21868036687374115: 1, 1.6888245344161987: 1, 0.13262629508972168: 1, 1.629822015762329: 1, -0.8671026229858398: 1, 0.12007596343755722: 1, 1.5428107976913452: 1, 0.3213244378566742: 1, -0.5802597403526306: 1, -0.9834045767784119: 1, -0.1501469612121582: 1, -0.3135724663734436: 1, -1.1350090503692627: 1, 1.7555813789367676: 1, -1.1569743156433105: 1, -0.28438881039619446: 1, 0.5530001521110535: 1, -0.2970311641693115: 1, 1.6552691459655762: 1, 1.0318180322647095: 1, -0.25207433104515076: 1, 0.6704513430595398: 1, 0.2092854529619217: 1, 0.15644948184490204: 1, 1.8505216836929321: 1, -0.24634599685668945: 1, 0.3915187418460846: 1, 1.5747997760772705: 1, -0.18857857584953308: 1, -0.9720814824104309: 1, 0.3640252351760864: 1, 1.6034691333770752: 1, -1.0625981092453003: 1, -0.6904935240745544: 1, -1.1205850839614868: 1, -0.13741447031497955: 1, -0.9971945881843567: 1, 0.2097160518169403: 1, 1.2531977891921997: 1, -0.46826034784317017: 1, -0.9147067666053772: 1, 0.8109627962112427: 1, 2.0270323753356934: 1, 0.04068145155906677: 1, -0.39972129464149475: 1, 1.7796648740768433: 1, -0.5899453163146973: 1, 0.2126206010580063: 1, -1.0319366455078125: 1, -1.1006834506988525: 1, 0.2422785758972168: 1, -0.21730461716651917: 1, -0.03495830297470093: 1, 0.49887171387672424: 1, -0.01187801081687212: 1, 0.10666077584028244: 1, 0.611980676651001: 1, 0.11937177926301956: 1, -0.27935805916786194: 1, -1.2016310691833496: 1, -0.2874172031879425: 1, -0.9310538172721863: 1, 1.4624748229980469: 1, 0.8171444535255432: 1, 1.8530430793762207: 1, 0.46222802996635437: 1, 1.2401331663131714: 1, -1.2016234397888184: 1, 0.7334970831871033: 1, -0.2089931219816208: 1, -0.2558027505874634: 1, -0.024080123752355576: 1, -0.4433523118495941: 1, -0.6431723237037659: 1, -0.6276612877845764: 1, -0.23398016393184662: 1, -0.4275071322917938: 1, 1.231010913848877: 1, 0.5500550270080566: 1, -0.14373785257339478: 1, 1.4984081983566284: 1, 1.905008316040039: 1, -1.2015986442565918: 1, 0.956155002117157: 1, -0.9726563692092896: 1, -1.0813374519348145: 1, 1.525660514831543: 1, 1.6697852611541748: 1, -0.6562255024909973: 1, 0.21643884479999542: 1, -0.3255411982536316: 1, -0.061833277344703674: 1, 0.21555371582508087: 1, 0.3141234815120697: 1, -0.22874142229557037: 1, -1.2016154527664185: 1, -1.2016232013702393: 1, 0.04079057276248932: 1, -0.3426608741283417: 1, 1.7097703218460083: 1, 0.6405824422836304: 1, -0.09393322467803955: 1, -0.8678878545761108: 1, -0.7790790796279907: 1, -1.1760764122009277: 1, 1.8596769571304321: 1, -0.9465891718864441: 1, 0.4937743842601776: 1, -0.364163339138031: 1, -0.3189155161380768: 1, -0.539626955986023: 1, -0.7983001470565796: 1, -0.7234905958175659: 1, -0.25122812390327454: 1, -0.9730278849601746: 1, 1.571593999862671: 1, -0.26545509696006775: 1, -0.6922621726989746: 1, -1.188301682472229: 1, -0.26783594489097595: 1, -0.8816695213317871: 1, -0.7939702272415161: 1, -0.5884281992912292: 1, -1.1422733068466187: 1, 0.4647309482097626: 1, -0.29477736353874207: 1, -0.6015652418136597: 1, 0.012895430438220501: 1, -0.36108481884002686: 1, -0.27475300431251526: 1, -0.30786851048469543: 1, -0.9930511116981506: 1, -0.8952922821044922: 1, 1.2151012420654297: 1, -0.43086937069892883: 1, 1.7544053792953491: 1, -0.5209143161773682: 1, -1.1991721391677856: 1, -0.4168057143688202: 1, -0.906280517578125: 1, -0.7995052933692932: 1, -0.22242674231529236: 1, 1.7963730096817017: 1, -1.1617506742477417: 1, -1.061142921447754: 1, 1.5116616487503052: 1, -0.24450848996639252: 1, -0.3431845009326935: 1, -0.563433825969696: 1, -1.201271653175354: 1, 0.18035785853862762: 1, -0.4653182625770569: 1, -1.1008306741714478: 1, -0.37186357378959656: 1, -0.7296833395957947: 1, 0.7230984568595886: 1, -1.2016171216964722: 1, -0.9751223921775818: 1, -0.7289202809333801: 1, -1.1515345573425293: 1, -1.1694631576538086: 1, -0.13113076984882355: 1, -1.1207038164138794: 1, 3.341240167617798: 1, -1.1624175310134888: 1, -1.2015869617462158: 1, -0.2223142683506012: 1, -1.1236215829849243: 1, -0.8602992296218872: 1, -1.201583981513977: 1, -0.3992172181606293: 1, -1.1680830717086792: 1, -1.2016146183013916: 1, -1.1971925497055054: 1, 0.5123543739318848: 1, -1.155177116394043: 1, 0.5790113210678101: 1, -1.1232612133026123: 1, -0.8350648880004883: 1, -1.199577808380127: 1, -1.201555848121643: 1, -1.1925454139709473: 1, 0.8056516647338867: 1, -1.0483030080795288: 1, -1.2002716064453125: 1, -1.201569676399231: 1, -1.1498054265975952: 1, -1.1406184434890747: 1, -0.9465845823287964: 1, 4.282702445983887: 1, -1.1945738792419434: 1, -1.2015953063964844: 1, -1.1635701656341553: 1, -1.1953339576721191: 1, -1.1108695268630981: 1, -1.0460647344589233: 1, -1.1475187540054321: 1, 0.9462845921516418: 1, 4.112330913543701: 1, -1.2015608549118042: 1, -1.1969398260116577: 1, -1.201536774635315: 1, -1.1448140144348145: 1, 4.5118513107299805: 1, -0.7269378900527954: 1, 0.2692132890224457: 1, 0.6463801264762878: 1, -1.1981743574142456: 1, 0.02189079485833645: 1, -1.169080138206482: 1, -0.9202075004577637: 1, 0.264765202999115: 1, -0.8984062075614929: 1, 0.7242860198020935: 1, -1.2016023397445679: 1, 0.5111210942268372: 1, 0.7875488996505737: 1, 0.8666924238204956: 1, -0.16389766335487366: 1, -0.32589226961135864: 1, 1.488932728767395: 1, 0.8550933599472046: 1, -1.2011059522628784: 1, -0.05180063098669052: 1, -0.8982016444206238: 1, -0.35893186926841736: 1, -1.2001534700393677: 1, -0.12521179020404816: 1, -0.5657321810722351: 1, -1.051085352897644: 1, -1.1992239952087402: 1, -1.186597228050232: 1, -1.193078875541687: 1, 1.2778337001800537: 1, -0.29541367292404175: 1, -1.1735186576843262: 1, 0.3790889084339142: 1, 0.6585915088653564: 1, -0.16215069591999054: 1, -1.192414402961731: 1, 0.19746625423431396: 1, 1.1413462162017822: 1, -0.6031686067581177: 1, 1.25115966796875: 1, 0.3299405574798584: 1, 0.9907589554786682: 1, -0.861803412437439: 1, -1.187011957168579: 1, -0.671938955783844: 1, -0.7864221334457397: 1, -1.1379677057266235: 1, -0.17077305912971497: 1, -0.7683218121528625: 1, 0.8018452525138855: 1, 0.1461368203163147: 1, -0.5369265675544739: 1, -0.39080610871315: 1, -0.03033183142542839: 1, 0.8944936394691467: 1, -0.012192374095320702: 1, 0.6273993253707886: 1, -0.40984249114990234: 1, -0.04764068126678467: 1, -1.1882494688034058: 1, -0.6981508731842041: 1, -0.01552529539912939: 1, -0.9249427914619446: 1, -0.17132940888404846: 1, -0.0996609777212143: 1, -1.20045006275177: 1, -1.1139642000198364: 1, 0.944926917552948: 1, 0.5761882066726685: 1, -0.03975825384259224: 1, -1.2010724544525146: 1, 0.013616573065519333: 1, 0.00948107335716486: 1, -1.1908975839614868: 1, 0.4138732850551605: 1, -0.5374748706817627: 1, -1.1906999349594116: 1, -1.1091188192367554: 1, -0.7482910752296448: 1, 1.2888545989990234: 1, -0.07614605128765106: 1, -1.15325927734375: 1, -1.0079307556152344: 1, -1.1992580890655518: 1, 0.9220188856124878: 1, 1.3031054735183716: 1, -0.4707425832748413: 1, -1.0164505243301392: 1, -0.1598413586616516: 1, -1.1756606101989746: 1, -1.1632294654846191: 1, -1.1747305393218994: 1, -1.1993433237075806: 1, -0.20172715187072754: 1, 0.4001394212245941: 1, -0.7597726583480835: 1, -0.4824763238430023: 1, -0.4976504147052765: 1, -1.1774768829345703: 1, 0.3949566185474396: 1, -1.1074978113174438: 1, -1.0825824737548828: 1, -0.5272284746170044: 1, -1.086830973625183: 1, -0.3503449857234955: 1, -1.2016373872756958: 1, -1.195853590965271: 1, 0.821479320526123: 1, -1.1690752506256104: 1, -0.5976389050483704: 1, 1.2671806812286377: 1, 1.0670119524002075: 1, 0.6907184720039368: 1, -0.4774172008037567: 1, 1.282943606376648: 1, -1.2002339363098145: 1, -1.083876609802246: 1, -0.2299073189496994: 1, 0.9924389719963074: 1, 0.9692907333374023: 1, -1.0224525928497314: 1, -0.21156159043312073: 1, -1.144519329071045: 1, -0.39197561144828796: 1, -1.1276121139526367: 1, 0.004596139770001173: 1, -1.0416687726974487: 1, -1.0180860757827759: 1, -0.4656817615032196: 1, 0.7670885920524597: 1, -1.2016358375549316: 1, 0.9127581715583801: 1, 0.4960012137889862: 1, 1.0147548913955688: 1, -0.44190311431884766: 1, -1.1707801818847656: 1, 1.2256038188934326: 1, 1.229008674621582: 1, 0.5329916477203369: 1, -0.3316475749015808: 1, -1.084214448928833: 1, 0.21256738901138306: 1, -0.8594576716423035: 1, -0.35862404108047485: 1, -1.1689379215240479: 1, -0.8211961388587952: 1, 0.369517058134079: 1, 1.2405850887298584: 1, -0.9499993324279785: 1, 0.5858985185623169: 1, 0.48456287384033203: 1, -0.30494359135627747: 1, -0.042717207223176956: 1, -0.6867349743843079: 1, -1.2015101909637451: 1, 1.1708914041519165: 1, -1.20154869556427: 1, 1.1807068586349487: 1, -0.15606260299682617: 1, -0.7601802349090576: 1, -1.1549781560897827: 1, -0.5920382142066956: 1, 0.8677871227264404: 1, 0.669349730014801: 1, 0.5067287683486938: 1, 1.0160198211669922: 1, 0.5890273451805115: 1, 0.8621184229850769: 1, -0.8395327925682068: 1, 0.050834186375141144: 1, -1.192929744720459: 1, -0.7811260223388672: 1, -0.7184330821037292: 1, -0.7514510154724121: 1, -1.09720778465271: 1, -0.06760650873184204: 1, -1.0178923606872559: 1, 1.177069067955017: 1, 0.3778545558452606: 1, -0.39157962799072266: 1, 1.1708778142929077: 1, -0.09844925999641418: 1, -1.1992988586425781: 1, -1.1941806077957153: 1, -1.1760457754135132: 1, 1.2313082218170166: 1, -0.8366453051567078: 1, 0.8209275603294373: 1, -0.41031748056411743: 1, -0.3642917275428772: 1, -0.5142630338668823: 1, -0.005674127489328384: 1, 0.7907249331474304: 1, -0.13443662226200104: 1, 0.03998654708266258: 1, -0.002722974168136716: 1, -1.1580485105514526: 1, 0.8628989458084106: 1, -1.1866058111190796: 1, -0.5910298824310303: 1, -0.5807573795318604: 1, 1.0215860605239868: 1, -0.171332448720932: 1, -0.8429122567176819: 1, 0.040903303772211075: 1, -0.003650385420769453: 1, -0.42906203866004944: 1, -0.024461327120661736: 1, -0.32544630765914917: 1, -0.2519146203994751: 1, 1.30207359790802: 1, 0.6744810342788696: 1, 0.9348665475845337: 1, 0.6152291297912598: 1, -1.0131398439407349: 1, 0.01674770377576351: 1, -0.3348834216594696: 1, 1.257509708404541: 1, 1.182131290435791: 1, -0.06836508214473724: 1, -0.15388239920139313: 1, 0.05971863865852356: 1, -0.12709279358386993: 1, 1.1462621688842773: 1, -1.1966403722763062: 1, -0.8245752453804016: 1, -0.5774872899055481: 1, -1.198868989944458: 1, -0.06410756707191467: 1, -0.048064157366752625: 1, -0.24131079018115997: 1, -0.6155209541320801: 1, -0.9567206501960754: 1, -0.6019781231880188: 1, -0.016240552067756653: 1, 0.7346874475479126: 1, -0.01721060648560524: 1, -0.2265346348285675: 1, -0.9494366645812988: 1, -0.18796367943286896: 1, 1.043671727180481: 1, -0.15634003281593323: 1, -0.3034926950931549: 1, -0.34825557470321655: 1, -0.06601618975400925: 1, 0.7746310830116272: 1, -1.2016302347183228: 1, -1.1586220264434814: 1, -0.3917173445224762: 1, 0.2524677515029907: 1, 0.2549906373023987: 1, 0.6609118580818176: 1, 1.00320303440094: 1, 1.1278204917907715: 1, 0.2748369872570038: 1, -0.21517714858055115: 1, 0.15262554585933685: 1, 0.07662157714366913: 1, -0.09314227104187012: 1, 1.274375557899475: 1, 1.0399528741836548: 1, -0.09158548712730408: 1, -0.4809620976448059: 1, 0.2788977324962616: 1, -0.1383906453847885: 1, 1.2882169485092163: 1, 0.7786849141120911: 1, -0.19042982161045074: 1, -0.257107138633728: 1, -0.5744653940200806: 1, 0.028185075148940086: 1, -0.27063482999801636: 1, 0.5745441317558289: 1, 0.2699461281299591: 1, -0.024909501895308495: 1, 0.5615567564964294: 1, 0.09289468824863434: 1, 1.2464758157730103: 1, -1.1068792343139648: 1, 0.41225412487983704: 1, 0.9726781249046326: 1, 0.9105262160301208: 1, -0.12876513600349426: 1, -1.1697360277175903: 1, 0.008013189770281315: 1, -0.2588912844657898: 1, 1.3108209371566772: 1, -0.1045512706041336: 1, -0.01294313371181488: 1, 1.0150052309036255: 1, -0.7380070686340332: 1, -0.0404001921415329: 1, 0.6649335622787476: 1, -0.028692159801721573: 1, -0.11206144839525223: 1, -0.7266088128089905: 1, 0.022439440712332726: 1, -0.3780987560749054: 1, 1.1997345685958862: 1, -0.36745402216911316: 1, -0.26567548513412476: 1, -0.19212104380130768: 1, -1.2016348838806152: 1, 0.048983871936798096: 1, -1.1260875463485718: 1, 1.1558300256729126: 1, 1.0084635019302368: 1, 0.6161129474639893: 1, -1.006115436553955: 1, 1.0866621732711792: 1, -0.27813780307769775: 1, -0.3076569437980652: 1, 0.0014178809942677617: 1, -0.9430715441703796: 1, -0.7650251388549805: 1, -1.1914066076278687: 1, -1.0579359531402588: 1, 1.1363762617111206: 1, -1.2016372680664062: 1, -1.1513574123382568: 1, 0.8519235253334045: 1, -0.03992554917931557: 1, -0.07969757169485092: 1, -1.0076838731765747: 1, -0.2553582787513733: 1, -0.2878003716468811: 1, -1.155191421508789: 1, -1.1991511583328247: 1, -0.0674244612455368: 1, -0.25587567687034607: 1, 1.023429274559021: 1, -1.192280888557434: 1, 0.9156388640403748: 1, -0.7673166990280151: 1, -0.15097910165786743: 1, -0.14771254360675812: 1, -0.7642948627471924: 1, -0.9743238091468811: 1, -0.10418325662612915: 1, 1.1114397048950195: 1, 0.68455970287323: 1, -0.05808640271425247: 1, 0.03431294485926628: 1, -0.881252646446228: 1, -0.5050346255302429: 1, -1.1873303651809692: 1, -1.200080394744873: 1, -0.04230939969420433: 1, -0.35624369978904724: 1, -0.03235474228858948: 1, -0.3301823139190674: 1, -1.1322532892227173: 1, -0.8901136517524719: 1, -0.16514527797698975: 1, -0.7420557737350464: 1, -0.056251220405101776: 1, 1.2829649448394775: 1, 0.03461417928338051: 1, 0.7100340127944946: 1, 1.030333161354065: 1, 0.9881972074508667: 1, 0.5070648193359375: 1, -0.5908447504043579: 1, -0.29025569558143616: 1, -0.15591250360012054: 1, -0.07907160371541977: 1, -1.0697368383407593: 1, 0.12906195223331451: 1, -0.08915898948907852: 1, 0.021945519372820854: 1, 0.004652172327041626: 1, -0.32367783784866333: 1, -0.23161835968494415: 1, 0.9278587102890015: 1, -0.19999556243419647: 1, 0.26051849126815796: 1, 0.053155358880758286: 1, 1.0157313346862793: 1, -0.23146884143352509: 1, -0.22990889847278595: 1, -0.3634156584739685: 1, 0.9011586904525757: 1, 1.1214849948883057: 1, -1.0054869651794434: 1, -0.002401667181402445: 1, -0.11618001013994217: 1, -1.1654343605041504: 1, -0.26975223422050476: 1, 1.2993144989013672: 1, 0.05442709103226662: 1, -1.1338447332382202: 1, -0.6266202926635742: 1, -0.5088394284248352: 1, 1.2035483121871948: 1, 1.0808086395263672: 1, -1.2008949518203735: 1, -0.455705851316452: 1, -0.9566242694854736: 1, 0.033837273716926575: 1, -0.999508798122406: 1, -0.05626079440116882: 1, -0.5931430459022522: 1, 1.0766181945800781: 1, 0.6975425481796265: 1, -0.8736492395401001: 1, 1.0577486753463745: 1, -1.17979097366333: 1, -1.0467379093170166: 1, -1.0309724807739258: 1, -0.8834558129310608: 1, -0.6588079929351807: 1, -1.1501390933990479: 1, -1.201562523841858: 1, -0.6635865569114685: 1, -0.5772318840026855: 1, -0.2236318439245224: 1, -1.200749397277832: 1, -1.1744723320007324: 1, -0.6185922622680664: 1, -1.1722731590270996: 1, -0.4475323557853699: 1, -1.0135008096694946: 1, -1.1582452058792114: 1, -1.191677212715149: 1, -0.63347989320755: 1, -1.0160771608352661: 1, -0.5631559491157532: 1, -1.1520825624465942: 1, -1.167817234992981: 1, -1.010536789894104: 1, -1.1729844808578491: 1, -1.194840431213379: 1, -0.6325321793556213: 1, -0.7736660242080688: 1, -0.41631850600242615: 1, -1.0399388074874878: 1, -1.1062737703323364: 1, -0.8366922736167908: 1, -0.7228063344955444: 1, -0.9656466245651245: 1, -1.0281414985656738: 1, -0.694696843624115: 1, -1.05448579788208: 1, -0.6411266922950745: 1, -0.4774998426437378: 1, -1.2015254497528076: 1, -1.1344302892684937: 1, -0.2650972902774811: 1, -1.026742935180664: 1, -0.4338090121746063: 1, -1.1509727239608765: 1, -0.5115338563919067: 1, -1.193282127380371: 1, -0.7443411946296692: 1, -0.9714952111244202: 1, -1.0810678005218506: 1, -0.6395750641822815: 1, -1.2008585929870605: 1, -0.9933805465698242: 1, -1.2016000747680664: 1, -1.1563661098480225: 1, -0.9732658267021179: 1, -1.1349726915359497: 1, -1.1859160661697388: 1, -1.1969208717346191: 1, -1.1816785335540771: 1, -1.1568188667297363: 1, -0.17838822305202484: 1, -0.5632508397102356: 1, -1.1857632398605347: 1, -1.198327660560608: 1, -0.40758535265922546: 1, -1.091170310974121: 1, -1.175873041152954: 1, -1.1691778898239136: 1, -1.0323843955993652: 1, -1.2014809846878052: 1, -1.1855055093765259: 1, -1.1832531690597534: 1, -0.5804309248924255: 1, -1.201621413230896: 1, -1.1800310611724854: 1, -1.180148720741272: 1, -1.194927453994751: 1, -1.1923733949661255: 1, -1.1721937656402588: 1, -1.0433918237686157: 1, -1.162119746208191: 1, -1.2006478309631348: 1, -1.1858601570129395: 1, -0.8829452991485596: 1, -1.1954984664916992: 1, -0.6137503981590271: 1, -0.788809597492218: 1, -1.1685314178466797: 1, -1.0831377506256104: 1, -1.1470420360565186: 1, -1.161582589149475: 1, -1.0068711042404175: 1, -0.7897263169288635: 1, -1.1350377798080444: 1, -1.126016616821289: 1, -1.1558103561401367: 1, -0.862193763256073: 1, -1.0223268270492554: 1, -0.9129625558853149: 1, -0.5520062446594238: 1, -1.1754673719406128: 1, 1.7253838777542114: 1, -0.4444347023963928: 1, -0.7420345544815063: 1, -0.21827441453933716: 1, -0.7001152634620667: 1, -0.9021695256233215: 1, -1.0589720010757446: 1, -1.1486388444900513: 1, -1.1036909818649292: 1, -0.25539615750312805: 1, -1.1933567523956299: 1, -0.86516273021698: 1, -1.0501618385314941: 1, -1.1981457471847534: 1, -0.9225814342498779: 1, -0.9485399723052979: 1, -1.087907314300537: 1, -0.3156169652938843: 1, -1.1891672611236572: 1, -1.1977088451385498: 1, -1.1703253984451294: 1, 5.037554740905762: 1, 5.012721538543701: 1, -0.9449849724769592: 1, -0.6635236740112305: 1, 0.057195477187633514: 1, 4.863577365875244: 1, -0.9046985507011414: 1, -0.622269868850708: 1, 5.001932144165039: 1, 4.900933742523193: 1, 4.722275257110596: 1, 0.939198911190033: 1, -1.1445417404174805: 1, -0.7939543128013611: 1, 4.58308219909668: 1, -1.1364892721176147: 1, -0.07361169159412384: 1, -1.1480247974395752: 1, 5.066896438598633: 1, 5.05051851272583: 1, 3.830434560775757: 1, -1.1785725355148315: 1, -1.1136521100997925: 1, 5.006033897399902: 1, -0.4824954569339752: 1, -0.9726890325546265: 1, -1.1986464262008667: 1, -0.5385211110115051: 1, -0.38882899284362793: 1, -0.9539603590965271: 1, -0.5497206449508667: 1, -1.1790684461593628: 1, -1.167614459991455: 1, -0.6818874478340149: 1, -1.0620733499526978: 1, -1.1940333843231201: 1, -0.8758900761604309: 1, -1.1150031089782715: 1, 0.18425099551677704: 1, -0.75212162733078: 1, 0.29684698581695557: 1, -0.7968862652778625: 1, -1.1212965250015259: 1, -1.2005667686462402: 1, -0.9272821545600891: 1, -1.02364981174469: 1, -1.0699774026870728: 1, -0.5409148931503296: 1, -1.0538722276687622: 1, -0.712040364742279: 1, -1.1946135759353638: 1, 0.216363787651062: 1, -1.0933367013931274: 1, -1.0416630506515503: 1, -1.0186684131622314: 1, -1.1181161403656006: 1, -0.4794164001941681: 1, -1.1434556245803833: 1, -1.1468044519424438: 1, -0.4437340795993805: 1, -0.967963457107544: 1, -1.0246316194534302: 1, -0.1397843062877655: 1, -0.8599510192871094: 1, -0.9453350901603699: 1, -0.8791208267211914: 1, -1.0074002742767334: 1, -1.1503796577453613: 1, -0.9824236631393433: 1, -1.189503788948059: 1, -0.5638068318367004: 1, -0.4648326635360718: 1, -1.0434380769729614: 1, -1.2014120817184448: 1, -0.6967374682426453: 1, -1.2012261152267456: 1, -0.8899372816085815: 1, -0.9352316856384277: 1, 1.1564749479293823: 1, 0.8697875142097473: 1, 0.7108749151229858: 1, 0.5599294900894165: 1, -0.6900844573974609: 1, -0.726171612739563: 1, -1.1882411241531372: 1, 1.5164122581481934: 1, -1.2010202407836914: 1, -0.23251420259475708: 1, -0.15451399981975555: 1, 1.094826102256775: 1, -0.9966712594032288: 1, -0.1682627946138382: 1, 0.7936035394668579: 1, -0.9092623591423035: 1, 1.0043728351593018: 1, 0.6101611256599426: 1, 0.9798484444618225: 1, -0.6111128330230713: 1, -0.433001846075058: 1, -1.0877141952514648: 1, 0.4647965729236603: 1, -0.16204535961151123: 1, 0.8136993050575256: 1, 1.6066374778747559: 1, 0.5490202307701111: 1, -1.1971783638000488: 1, -0.17054051160812378: 1, -1.1964715719223022: 1, 0.3594037592411041: 1, -1.2015913724899292: 1, -0.4477367699146271: 1, -1.016434669494629: 1, -0.31994664669036865: 1, -0.7167530059814453: 1, 0.18603742122650146: 1, -0.9458178281784058: 1, 0.6880602836608887: 1, 0.6052579283714294: 1, 0.8645955324172974: 1, 0.7324214577674866: 1, 1.6113003492355347: 1, 0.29118791222572327: 1, -0.15851348638534546: 1, -1.1721446514129639: 1, -0.23552395403385162: 1, -0.7970835566520691: 1, 0.9053057432174683: 1, 1.6532078981399536: 1, 1.5002496242523193: 1, -1.171905517578125: 1, 1.123262643814087: 1, -0.45371508598327637: 1, -0.411021888256073: 1, 0.6509128212928772: 1, -1.2016305923461914: 1, -1.2016229629516602: 1, 0.5780434608459473: 1, -0.7776852250099182: 1, -0.32108673453330994: 1, -1.1028145551681519: 1, -0.4003660976886749: 1, 0.6591589450836182: 1, -0.1892606019973755: 1, -0.8111313581466675: 1, -0.8244988322257996: 1, -0.5816406607627869: 1, -0.4005826711654663: 1, -0.6496835947036743: 1, -0.7252834439277649: 1, -0.7019102573394775: 1, -1.1959316730499268: 1, 0.009732205420732498: 1, -1.199397087097168: 1, -1.1626108884811401: 1, -1.201583743095398: 1, -0.8635501265525818: 1, 0.5944019556045532: 1, 1.8255465030670166: 1, -0.44575440883636475: 1, 1.2443398237228394: 1, -0.750208854675293: 1, 1.7609484195709229: 1, -1.201631784439087: 1, -0.1808653175830841: 1, -0.17489729821681976: 1, 0.05797763168811798: 1, 1.0823159217834473: 1, 0.5601547360420227: 1, 1.0962333679199219: 1, 0.6412140727043152: 1, 1.3093386888504028: 1, -0.3561877906322479: 1, -0.8054187893867493: 1, 0.5863446593284607: 1, 1.1463862657546997: 1, -1.154268741607666: 1, -1.1990872621536255: 1, -1.201563835144043: 1, -1.1948130130767822: 1, -1.1595861911773682: 1, -1.076475739479065: 1, -1.2009553909301758: 1, -1.0944702625274658: 1, -1.1019858121871948: 1, -1.1784441471099854: 1, -1.1624016761779785: 1, -0.7622874975204468: 1, -1.1929867267608643: 1, -1.18364417552948: 1, -1.1482324600219727: 1, -0.5079992413520813: 1, -1.0951330661773682: 1, -1.1485586166381836: 1, -1.0865159034729004: 1, -1.1970044374465942: 1, -1.1610828638076782: 1, -0.6870049834251404: 1, -1.1676386594772339: 1, -0.6259949207305908: 1, -1.1963162422180176: 1, -1.201351284980774: 1, -1.1821529865264893: 1, -0.7456731200218201: 1, -1.1594713926315308: 1, -1.1066040992736816: 1, -0.7500604391098022: 1, -1.2013036012649536: 1, -0.6249680519104004: 1, -1.1552096605300903: 1, -1.1936330795288086: 1, -1.170555830001831: 1, -0.957834780216217: 1, -1.0740134716033936: 1, -1.1981604099273682: 1, -1.1905823945999146: 1, -1.2015275955200195: 1, -1.1300313472747803: 1, -1.2016135454177856: 1, -1.1405699253082275: 1, -1.0521938800811768: 1, -1.1961623430252075: 1, -1.1871360540390015: 1, -0.6205415725708008: 1, -1.0014375448226929: 1, -1.2015451192855835: 1, -1.0986745357513428: 1, -1.0963668823242188: 1, -1.1365838050842285: 1, -0.04312499612569809: 1, -1.1986582279205322: 1, -1.1966667175292969: 1, -1.198758602142334: 1, -1.196737289428711: 1, -1.1819733381271362: 1, -1.1612766981124878: 1, -1.1862393617630005: 1, -1.1950762271881104: 1, -0.9986592531204224: 1, -1.1970906257629395: 1, 1.7719837427139282: 1, -1.1995155811309814: 1, -1.2008846998214722: 1, -1.0349972248077393: 1, -1.0542218685150146: 1, -1.1924408674240112: 1, -0.6460915207862854: 1, -1.1948115825653076: 1, -0.638097882270813: 1, -0.6645460724830627: 1, 0.05473056063055992: 1, -1.0081939697265625: 1, -1.2010711431503296: 1, -0.40274444222450256: 1, -1.196357011795044: 1, -0.6273359060287476: 1, 5.645717620849609: 1, -0.7359880805015564: 1, -0.459964781999588: 1, -1.1054575443267822: 1, 1.8886953592300415: 1, 0.19575154781341553: 1, -1.2002415657043457: 1, -1.164048194885254: 1, -1.2014538049697876: 1, -1.194710373878479: 1, 0.6086026430130005: 1, -1.175083041191101: 1, -0.23024950921535492: 1, -1.171040415763855: 1, -1.1539435386657715: 1, -1.1912822723388672: 1, -1.1973918676376343: 1, -0.4641222357749939: 1, -1.1811505556106567: 1, 1.6846264600753784: 1, -0.7029581665992737: 1, 0.8297379016876221: 1, -0.27802959084510803: 1, 0.05186900869011879: 1, 0.05425111949443817: 1, 0.2561040222644806: 1, 0.6983891129493713: 1, 1.431997537612915: 1, 0.06706182658672333: 1, 0.18826737999916077: 1, 0.30889958143234253: 1, -0.10326965153217316: 1, -0.11514480412006378: 1, 1.6200270652770996: 1, 0.12041562795639038: 1, 0.3626178801059723: 1, 0.44933900237083435: 1, 0.4515918791294098: 1, 0.20913533866405487: 1, 1.622856616973877: 1, -0.20442236959934235: 1, -0.10369612276554108: 1, 1.4727312326431274: 1, 0.7984791398048401: 1, 1.481839656829834: 1, 1.3110328912734985: 1, 0.5713070034980774: 1, 1.914624810218811: 1, 0.3338538706302643: 1, 0.06557659059762955: 1, 0.11681299656629562: 1, 0.753506600856781: 1, 0.2539007067680359: 1, 0.002965901279821992: 1, 1.4364150762557983: 1, 1.4786081314086914: 1, 1.3110779523849487: 1, 1.55558443069458: 1, -0.16230207681655884: 1, 1.4751214981079102: 1, 0.3319496512413025: 1, 0.18870316445827484: 1, 0.22794750332832336: 1, 1.1887938976287842: 1, -0.1031346470117569: 1, -0.22873257100582123: 1, 1.379611611366272: 1, 0.22853031754493713: 1, -0.6209532618522644: 1, -0.0526600144803524: 1, 1.5148382186889648: 1, 0.33931559324264526: 1, 1.5503476858139038: 1, 0.24058645963668823: 1, 0.3425867557525635: 1, -0.6031805276870728: 1, -0.9130086302757263: 1, -0.10372047126293182: 1, 1.1509000062942505: 1, 1.1246896982192993: 1, 0.7825908064842224: 1, 1.375723958015442: 1, -0.2681446373462677: 1, 0.3024345636367798: 1, 0.276736855506897: 1, 0.35236239433288574: 1, 1.6643083095550537: 1, 1.4752575159072876: 1, -0.10448039323091507: 1, -0.2946179509162903: 1, 0.07517638802528381: 1, 0.2801852524280548: 1, 1.4599251747131348: 1, 1.5357881784439087: 1, 0.31122344732284546: 1, -0.05793027952313423: 1, 1.3920340538024902: 1, 0.6647680401802063: 1, 1.556430697441101: 1, 0.35902467370033264: 1, 0.592692494392395: 1, 1.561331033706665: 1, 0.1928955614566803: 1, 0.23876602947711945: 1, -0.13019442558288574: 1, 0.14948004484176636: 1, -0.1196289211511612: 1, 0.11391282081604004: 1, 1.2999117374420166: 1, -0.44012218713760376: 1, -0.1709771454334259: 1, -0.812441349029541: 1, 0.3157300651073456: 1, 0.6854439377784729: 1, 1.5044559240341187: 1, 1.1449819803237915: 1, 0.7352478504180908: 1, 1.4972656965255737: 1, 1.0554699897766113: 1, -0.31073465943336487: 1, 0.21012525260448456: 1, 1.3231751918792725: 1, -0.6454114317893982: 1, 0.8502970337867737: 1, 0.35881760716438293: 1, 0.31476086378097534: 1, 0.4845307767391205: 1, 0.021630888804793358: 1, 1.0225938558578491: 1, -0.16844011843204498: 1, -0.22581757605075836: 1, -0.4890022277832031: 1, 0.8458276391029358: 1, 0.29691436886787415: 1, 1.0407052040100098: 1, -1.1736985445022583: 1, 0.8794386982917786: 1, 0.26258811354637146: 1, 1.244690179824829: 1, -0.22985504567623138: 1, 1.5059622526168823: 1, -0.4757806956768036: 1, 0.021963730454444885: 1, 0.3591007590293884: 1, -0.06609977036714554: 1, 1.5724915266036987: 1, 0.7346283793449402: 1, -1.140577793121338: 1, 0.9252344369888306: 1, 0.11165464669466019: 1, -0.8935246467590332: 1, -1.2016048431396484: 1, -1.2015061378479004: 1, -0.41403406858444214: 1, -1.2015372514724731: 1, -1.180970549583435: 1, 0.20136801898479462: 1, -0.416104793548584: 1, -0.11705746501684189: 1, -1.201634168624878: 1, -1.150158166885376: 1, -1.1623584032058716: 1, -1.201610803604126: 1, -1.1100589036941528: 1, -0.6426911950111389: 1, -0.9999420642852783: 1, -0.8209242224693298: 1, -1.2014681100845337: 1, 0.34270647168159485: 1, -0.5093275308609009: 1, -0.0922759622335434: 1, 0.3153885304927826: 1, 1.466652274131775: 1, 1.111115574836731: 1, -1.0057076215744019: 1, 0.36868804693222046: 1, 0.21266861259937286: 1, -0.03138621151447296: 1, 1.466456651687622: 1, 0.9224774241447449: 1, 1.256608247756958: 1, 1.533963680267334: 1, 1.3359390497207642: 1, 1.4527193307876587: 1, 1.5988372564315796: 1, 1.3192821741104126: 1, 1.533202886581421: 1, 0.19674475491046906: 1, -0.3147757053375244: 1, -0.1758507341146469: 1, -0.11516252905130386: 1, 0.7623692750930786: 1, -1.1871042251586914: 1, 0.7778939008712769: 1, 1.5112932920455933: 1, -1.1865193843841553: 1, 0.9693878293037415: 1, -0.5517370104789734: 1, 0.19260334968566895: 1, 1.5909359455108643: 1, -0.11230547726154327: 1, 0.017385760322213173: 1, 1.3268651962280273: 1, 0.853833794593811: 1, 0.8655160665512085: 1, 0.8642333745956421: 1, 0.3231067657470703: 1, -0.5888111591339111: 1, 1.7802213430404663: 1, 0.2615222632884979: 1, 0.5367603302001953: 1, -0.06798223406076431: 1, 1.3918044567108154: 1, 1.3780083656311035: 1, 1.1870133876800537: 1, 1.126368761062622: 1, 1.2197668552398682: 1, 0.7675086855888367: 1, 1.0209075212478638: 1, 0.11682117730379105: 1, 1.5651975870132446: 1, 1.4282907247543335: 1, 1.0482161045074463: 1, 0.5573470592498779: 1, -1.1383882761001587: 1, 1.5033998489379883: 1, 1.547389030456543: 1, 1.471611738204956: 1, 1.4821670055389404: 1, 0.11520501971244812: 1, 1.2778698205947876: 1, -0.6971253156661987: 1, 0.24872112274169922: 1, 1.0432312488555908: 1, 1.4069277048110962: 1, -1.1837621927261353: 1, 0.2389289289712906: 1, -0.5775644183158875: 1, 1.1072840690612793: 1, 0.22126778960227966: 1, 1.5512200593948364: 1, -0.19494549930095673: 1, -0.16245944797992706: 1, 0.29415011405944824: 1, 1.6108869314193726: 1, 0.8618729114532471: 1, 0.9195832014083862: 1, 1.5557059049606323: 1, 1.3578754663467407: 1, 0.8847638368606567: 1, 1.133412480354309: 1, 0.8533394932746887: 1, 1.4529069662094116: 1, 1.4793431758880615: 1, 0.8744557499885559: 1, 0.5000193119049072: 1, 0.9953116178512573: 1, 1.5040947198867798: 1, 1.0842758417129517: 1, 0.9064841866493225: 1, 1.3597513437271118: 1, 1.368375539779663: 1, 0.0863155722618103: 1, -0.11868320405483246: 1, 0.3126457929611206: 1, 1.5239653587341309: 1, 0.5069756507873535: 1, 1.5916389226913452: 1, 0.19180983304977417: 1, 0.8997297883033752: 1, 0.9983642101287842: 1, 0.6865427494049072: 1, 1.5009726285934448: 1, 0.2642950117588043: 1, 0.9053120613098145: 1, -0.16789886355400085: 1, 0.27865079045295715: 1, 1.4580349922180176: 1, 1.3759030103683472: 1, 0.8865175843238831: 1, 0.7280606031417847: 1, 0.7922017574310303: 1, 1.5027039051055908: 1, 0.6448225975036621: 1, -1.2015975713729858: 1, 1.4735376834869385: 1, -0.870884895324707: 1, 1.5217971801757812: 1, 0.008224710822105408: 1, 0.48444247245788574: 1, -0.30212122201919556: 1, -0.19083698093891144: 1, -0.091583751142025: 1, 0.2824403941631317: 1, -0.009973025880753994: 1, 0.3697844445705414: 1, -1.1851680278778076: 1, 1.2598285675048828: 1, 0.7332795858383179: 1, 0.0384686179459095: 1, 0.6183062195777893: 1, 0.7092652916908264: 1, 1.3463716506958008: 1, -0.9639919996261597: 1, -0.025239035487174988: 1, 0.041503969579935074: 1, 1.4127790927886963: 1, 0.8287729024887085: 1, -1.1504056453704834: 1, 1.5179330110549927: 1, -0.20943275094032288: 1, 0.3431006968021393: 1, 0.35639217495918274: 1, 1.405086636543274: 1, 0.7253832817077637: 1, 0.80833899974823: 1, 0.9344565272331238: 1, -0.2038322389125824: 1, -0.004799619782716036: 1, 1.1847658157348633: 1, -0.56696617603302: 1, 1.1921144723892212: 1, 0.7161386013031006: 1, 0.8943637013435364: 1, -1.024586796760559: 1, 1.0762102603912354: 1, -0.10053509473800659: 1, 0.11328306794166565: 1, 0.5472216606140137: 1, 0.2568971812725067: 1, 0.5611000061035156: 1, 0.8098611235618591: 1, 0.4243623912334442: 1, 1.5731940269470215: 1, 0.6104775071144104: 1, 0.39806419610977173: 1, 1.473071813583374: 1, 1.069445013999939: 1, 1.5127235651016235: 1, -0.30856987833976746: 1, 1.2513844966888428: 1, 0.1911333203315735: 1, 0.39437854290008545: 1, 0.2846507132053375: 1, 1.3862555027008057: 1, 1.5425565242767334: 1, -0.41700541973114014: 1, 1.5978120565414429: 1, 0.339995801448822: 1, -0.15476621687412262: 1, 1.2155990600585938: 1, 0.2816910445690155: 1, 0.81052166223526: 1, 1.233654499053955: 1, 1.3900312185287476: 1, 0.3554361164569855: 1, 1.3589857816696167: 1, 0.2826254069805145: 1, -0.3440307080745697: 1, 0.3602719008922577: 1, 0.04298178479075432: 1, -0.11455459892749786: 1, 0.30982303619384766: 1, 0.340713769197464: 1, 0.822748601436615: 1, 1.4639532566070557: 1, 1.1571227312088013: 1, -0.30866673588752747: 1, 1.540098786354065: 1, 1.4051384925842285: 1, 1.0418422222137451: 1, 0.1144905686378479: 1, 1.2591054439544678: 1, 0.881543755531311: 1, 0.3661552369594574: 1, 0.6668532490730286: 1, 1.7404301166534424: 1, 0.2925977110862732: 1, 0.31707215309143066: 1, -0.3739321231842041: 1, 0.7008569240570068: 1, 0.5241292119026184: 1, 0.8512904644012451: 1, 0.5769325494766235: 1, 1.916335940361023: 1, 1.5063830614089966: 1, -0.4644731879234314: 1, 1.4151798486709595: 1, -0.11862857639789581: 1, -0.1884830892086029: 1, 0.35628634691238403: 1, -1.0716415643692017: 1, -0.2168547362089157: 1, 0.6513680219650269: 1, -0.040548212826251984: 1, -0.4737872779369354: 1, -0.2417261004447937: 1, -0.20986565947532654: 1, -0.08143828064203262: 1, 1.5686708688735962: 1, 1.5401815176010132: 1, -1.0314160585403442: 1, 0.8500742316246033: 1, 0.02118554152548313: 1, 1.622040867805481: 1, 0.8342987895011902: 1, 0.98076331615448: 1, 1.007346510887146: 1, -0.6390724778175354: 1, 0.23464715480804443: 1, -1.1742432117462158: 1, 0.9422100782394409: 1, -0.6223658919334412: 1, 1.4056357145309448: 1, 2.4050354957580566: 1, -0.09898030012845993: 1, 0.29443448781967163: 1, -0.20047886669635773: 1, 0.3499395549297333: 1, -0.1565595418214798: 1, 0.9952307939529419: 1, 0.8222253322601318: 1, 1.5599995851516724: 1, -1.2016196250915527: 1, -1.2016316652297974: 1, -0.5160067081451416: 1, 0.5979693531990051: 1, 0.7264453172683716: 1, 0.23339834809303284: 1, 1.2736730575561523: 1, -0.16393840312957764: 1, 0.840314507484436: 1, -0.24846623837947845: 1, -1.2004797458648682: 1, -1.1211071014404297: 1, -0.42487016320228577: 1, -0.002975589595735073: 1, 0.8318881988525391: 1, -0.1517976075410843: 1, 1.0863690376281738: 1, 0.5795131325721741: 1, 0.7288377285003662: 1, 1.3004077672958374: 1, -0.012521528638899326: 1, -0.4453357756137848: 1, 1.150854468345642: 1, -0.8759819269180298: 1, 1.009254813194275: 1, -0.24570585787296295: 1, 0.654328465461731: 1, -0.41674312949180603: 1, 0.838370680809021: 1, -0.18254651129245758: 1, -0.20062661170959473: 1, -1.1656997203826904: 1, -0.1855221837759018: 1, 1.0187321901321411: 1, 1.1683013439178467: 1, -1.1928194761276245: 1, 1.1942393779754639: 1, 1.179612159729004: 1, 1.2479115724563599: 1, -0.22866979241371155: 1, -1.1798354387283325: 1, 0.6535213589668274: 1, 1.2529374361038208: 1, -0.37873539328575134: 1, 1.2188843488693237: 1, 0.9235488772392273: 1, -0.5479841232299805: 1, 0.4542813301086426: 1, 0.8476697206497192: 1, -0.22304323315620422: 1, -0.12660875916481018: 1, -0.674019455909729: 1, -1.1972460746765137: 1, 0.055386364459991455: 1, 0.9840258359909058: 1, -0.2164342701435089: 1, -0.39580973982810974: 1, 1.2822530269622803: 1, -1.1879688501358032: 1, 1.2413678169250488: 1, 0.008748067542910576: 1, 0.6657525897026062: 1, -0.9053927063941956: 1, 0.8952587246894836: 1, -0.37779542803764343: 1, -0.5749701261520386: 1, 1.150696039199829: 1, 0.9305616617202759: 1, 0.6777730584144592: 1, 1.0972230434417725: 1, -0.25068432092666626: 1, 0.8119110465049744: 1, 0.4251300096511841: 1, -0.2664187550544739: 1, -0.33581042289733887: 1, 0.4259852468967438: 1, 0.008470889180898666: 1, 0.938378632068634: 1, -0.28629931807518005: 1, -0.1916339248418808: 1, -0.2671576142311096: 1, 0.009196557104587555: 1, 1.256977915763855: 1, 0.24701066315174103: 1, 1.1922990083694458: 1, -0.8705235719680786: 1, -1.1247143745422363: 1, 0.8834699392318726: 1, -0.007039392367005348: 1, -0.621107280254364: 1, 1.0945255756378174: 1, -0.9168434739112854: 1, -0.8882886171340942: 1, -0.09275590628385544: 1, -0.06864805519580841: 1, 0.8631362915039062: 1, 0.008864369243383408: 1, 0.8532567620277405: 1, 0.23268936574459076: 1, 0.6013088822364807: 1, 1.185150146484375: 1, -0.9900954961776733: 1, 0.033690646290779114: 1, 0.4205377995967865: 1, -0.34817975759506226: 1, 0.029728559777140617: 1, -0.35746997594833374: 1, -0.025104349479079247: 1, -0.17582924664020538: 1, 0.32448264956474304: 1, -0.7624772787094116: 1, 0.6605957746505737: 1, -0.6921688914299011: 1, -0.019019143655896187: 1, -1.1852235794067383: 1, -0.16533638536930084: 1, 1.104429006576538: 1, 0.525627076625824: 1, 0.31131237745285034: 1, 1.2513697147369385: 1, -0.6482374668121338: 1, -1.1878859996795654: 1, -0.8494775891304016: 1, -0.12005668878555298: 1, 1.1650327444076538: 1, 1.1985303163528442: 1, 1.0467203855514526: 1, 0.12959904968738556: 1, -0.11709770560264587: 1, -1.194725751876831: 1, -1.0983095169067383: 1, 1.2668349742889404: 1, 0.21761490404605865: 1, 0.43160757422447205: 1, -0.809281051158905: 1, -1.1104997396469116: 1, 0.34432777762413025: 1, 1.1265980005264282: 1, 0.02654222585260868: 1, 0.017293494194746017: 1, -0.32669803500175476: 1, -0.253052294254303: 1, -0.8852211833000183: 1, -0.12913857400417328: 1, 0.5908839702606201: 1, 1.0087279081344604: 1, 1.2238743305206299: 1, -1.194933295249939: 1, 1.1819933652877808: 1, -0.5622422695159912: 1, 1.2926610708236694: 1, -0.13838951289653778: 1, 0.0796559751033783: 1, -0.816495954990387: 1, 1.1279875040054321: 1, -0.11954380571842194: 1, -1.1981785297393799: 1, 0.1316474825143814: 1, 0.29174771904945374: 1, 1.039072036743164: 1, 1.0578463077545166: 1, -0.5372467637062073: 1, -0.0449078269302845: 1, -0.24153171479701996: 1, -0.8713244199752808: 1, -1.2016297578811646: 1, 0.02958042360842228: 1, -0.04175446555018425: 1, -0.2098180055618286: 1, 1.2327803373336792: 1, -0.1759326308965683: 1, -0.19223442673683167: 1, 0.7244752645492554: 1, -0.29299479722976685: 1, 0.4987215995788574: 1, -0.6826592683792114: 1, 0.7561059594154358: 1, 0.15707539021968842: 1, -0.20500345528125763: 1, 0.042822714895009995: 1, 0.004906347021460533: 1, -0.23531334102153778: 1, -0.15267345309257507: 1, -0.3641962707042694: 1, -0.7914682626724243: 1, -0.23942992091178894: 1, -0.11078709363937378: 1, -1.095828652381897: 1, 1.2373515367507935: 1, -0.3953478932380676: 1, 0.7884150743484497: 1, -0.4121406674385071: 1, -1.0349785089492798: 1, 0.02054119110107422: 1, -0.9308528304100037: 1, -0.9307324290275574: 1, -0.35524412989616394: 1, -0.39703771471977234: 1, -0.1827705055475235: 1, -1.2004859447479248: 1, -1.0583271980285645: 1, 0.6872115731239319: 1, -0.16826894879341125: 1, -0.009843221865594387: 1, 0.36895880103111267: 1, -1.1948002576828003: 1, -0.5937166213989258: 1, -1.2016299962997437: 1, 4.363720893859863: 1, -1.1943039894104004: 1, -1.2015916109085083: 1, -0.11716806888580322: 1, -0.14289136230945587: 1, 0.9705496430397034: 1, 0.469992071390152: 1, -1.2015831470489502: 1, -0.9032037854194641: 1, -1.2016255855560303: 1, -1.2015341520309448: 1, -1.1318936347961426: 1, -1.2013877630233765: 1, -1.1967262029647827: 1, 0.1497194468975067: 1, -1.2014774084091187: 1, -1.2016361951828003: 1, -1.201634407043457: 1, -1.201493740081787: 1, -0.3404213488101959: 1, -0.848010778427124: 1, 2.26233172416687: 1, 0.19668835401535034: 1, -0.1688508540391922: 1, 1.4819786548614502: 1, -0.2126571536064148: 1, 1.018097996711731: 1, -0.3213060796260834: 1, -1.1992485523223877: 1, 0.2079305797815323: 1, -1.1586899757385254: 1, -1.0591267347335815: 1, -1.2015695571899414: 1, -1.2016273736953735: 1, -1.2015702724456787: 1, -1.201588749885559: 1, -1.201620101928711: 1, 0.1331777125597: 1, -1.2016350030899048: 1, 0.0891273021697998: 1, -1.201523780822754: 1, -1.2015215158462524: 1, -1.2015316486358643: 1, -1.201473593711853: 1, -1.2016335725784302: 1, -0.7989376187324524: 1, -0.8910970091819763: 1, -1.2013384103775024: 1, -1.1490250825881958: 1, -0.11703558266162872: 1, -1.201635718345642: 1, 0.9015107750892639: 1, -0.28742867708206177: 1, -0.518195390701294: 1, -0.5834662318229675: 1, -1.2015115022659302: 1, -1.1257261037826538: 1, -0.006573406048119068: 1, -0.5612114071846008: 1, 0.07849415391683578: 1, 0.86496901512146: 1, -0.21048426628112793: 1, 0.8498935103416443: 1, -1.201079249382019: 1, -1.2016206979751587: 1, -1.2003253698349: 1, 0.5647678971290588: 1, -1.1779894828796387: 1, 0.15111742913722992: 1, -0.045158740133047104: 1, -1.2016379833221436: 1, -1.193373441696167: 1, -1.1954847574234009: 1, 0.3015405535697937: 1, -1.1923046112060547: 1, -1.1979888677597046: 1, -1.1363192796707153: 1, -1.0203382968902588: 1, 0.2464127391576767: 1, -1.2016191482543945: 1, -1.2016382217407227: 1, -1.201630711555481: 1, -0.4132004976272583: 1, -1.2015154361724854: 1, -0.4687884449958801: 1, -1.1849600076675415: 1, -0.4057319462299347: 1, -1.2015478610992432: 1, -0.08719541132450104: 1, 0.1322988122701645: 1, -1.2016332149505615: 1, -1.0628424882888794: 1, -0.6676098704338074: 1, -0.6758065819740295: 1, -0.16829612851142883: 1, -0.2904500663280487: 1, -0.040736664086580276: 1, -1.1812138557434082: 1, 0.8218473196029663: 1, -0.5884501934051514: 1, 0.9394524693489075: 1, -0.1427413374185562: 1, -0.8740671277046204: 1, -0.0332360677421093: 1, 1.1960792541503906: 1, 0.08744630217552185: 1, 0.01664043217897415: 1, -0.6421114802360535: 1, -0.3469093143939972: 1, -1.1963841915130615: 1, -0.14633353054523468: 1, -1.1634924411773682: 1, 0.27843114733695984: 1, 0.6842910647392273: 1, 0.5252094268798828: 1, -0.3879204988479614: 1, -0.14631414413452148: 1, -0.4360010027885437: 1, -0.0580214224755764: 1, -0.28555259108543396: 1, -1.2005233764648438: 1, 1.2064505815505981: 1, 0.637002170085907: 1, -0.09905305504798889: 1, 1.0347987413406372: 1, 0.6233600974082947: 1, 1.2749443054199219: 1, 1.113309621810913: 1, 1.2229245901107788: 1, -1.2015588283538818: 1, -1.0537559986114502: 1, -1.1755220890045166: 1, -1.0965174436569214: 1, 0.015011060051620007: 1, -1.1905653476715088: 1, -0.6103007793426514: 1, 0.029840881004929543: 1, 0.09383262693881989: 1, 0.36468705534935: 1, 0.5362502932548523: 1, 0.5997416377067566: 1, 0.6232529878616333: 1, -0.5104274749755859: 1, 0.5468651056289673: 1, 0.8548043966293335: 1, 0.18457916378974915: 1, -0.10290281474590302: 1, 0.29349300265312195: 1, 0.8965467214584351: 1, -1.0558182001113892: 1, 0.047311048954725266: 1, -0.34107181429862976: 1, 1.295539140701294: 1, -0.33075013756752014: 1, 1.2726079225540161: 1, 0.30879271030426025: 1, -0.24580752849578857: 1, -1.0237786769866943: 1, 0.9712859988212585: 1, 1.176633596420288: 1, -0.04390183836221695: 1, 0.005373222753405571: 1, 1.1995047330856323: 1, 1.0674824714660645: 1, 0.8222996592521667: 1, -0.47594985365867615: 1, -0.21433545649051666: 1, 0.5694330334663391: 1, 1.0520529747009277: 1, -0.11945565789937973: 1, 1.28579580783844: 1, -0.84548020362854: 1, 1.0769490003585815: 1, 1.200035810470581: 1, 0.01943967677652836: 1, 0.04932570457458496: 1, -0.9972583055496216: 1, -0.30726683139801025: 1, 1.2879470586776733: 1, -0.8751402497291565: 1, -1.1107620000839233: 1, 0.9265954494476318: 1, 1.2916063070297241: 1, 0.9066123366355896: 1, -1.193730115890503: 1, -0.015705665573477745: 1, -0.011501064524054527: 1, -0.3390061855316162: 1, 0.3886134922504425: 1, -1.1947468519210815: 1, 3.9672465324401855: 1, -0.463926762342453: 1, -0.37206733226776123: 1, 0.8610304594039917: 1, -1.2003904581069946: 1} test data: {-1.2016366720199585: 4, -1.2016339302062988: 2, -1.2016353607177734: 2, -1.2016288042068481: 2, -1.193596363067627: 1, 1.415509581565857: 1, 1.5035943984985352: 1, -0.10739652067422867: 1, 1.4158756732940674: 1, -1.0201867818832397: 1, 0.2609155476093292: 1, 0.36003923416137695: 1, -0.5706648826599121: 1, 0.22961212694644928: 1, 0.5601941347122192: 1, 0.22109845280647278: 1, 1.168498158454895: 1, 0.10525540262460709: 1, 0.3664189279079437: 1, -0.4459679424762726: 1, 1.4836735725402832: 1, 0.831580638885498: 1, -0.6966411471366882: 1, 0.6829988956451416: 1, -0.425843745470047: 1, 0.7279888391494751: 1, 1.9054079055786133: 1, 0.2500914931297302: 1, 0.49536043405532837: 1, 1.0551327466964722: 1, -0.21738800406455994: 1, 0.912930965423584: 1, 0.7471779584884644: 1, 1.1508636474609375: 1, 1.0995265245437622: 1, 0.3443826735019684: 1, 1.5370275974273682: 1, 0.9285130500793457: 1, -0.13643299043178558: 1, 0.23716896772384644: 1, 0.695344090461731: 1, -1.189130187034607: 1, 1.4424492120742798: 1, 0.31334978342056274: 1, -0.20045150816440582: 1, -0.2393776774406433: 1, 0.7402693033218384: 1, 0.540539026260376: 1, 1.4923255443572998: 1, -0.9669303297996521: 1, 1.1683435440063477: 1, -0.31709083914756775: 1, -1.201627492904663: 1, -0.02133699133992195: 1, -1.201625108718872: 1, -0.512914776802063: 1, -1.2007057666778564: 1, -1.1964974403381348: 1, -1.1945018768310547: 1, -1.1664562225341797: 1, 0.005114047322422266: 1, -1.2016290426254272: 1, -1.1980621814727783: 1, -1.198028802871704: 1, -1.1962871551513672: 1, -0.7543148398399353: 1, 0.28807583451271057: 1, -0.7196366786956787: 1, -0.5718616843223572: 1, -0.26343750953674316: 1, 0.9686956405639648: 1, -0.5639210939407349: 1, -0.39423879981040955: 1, 0.5479292273521423: 1, -0.32787415385246277: 1, 0.2115481197834015: 1, -0.8235211968421936: 1, -1.1987498998641968: 1, -1.201596975326538: 1, -0.06465810537338257: 1, -1.2002341747283936: 1, 0.3488617241382599: 1, 0.25783771276474: 1, -0.9223102331161499: 1, -0.09881063550710678: 1, -0.5620038509368896: 1, 0.12603989243507385: 1, 1.1153340339660645: 1, 0.44327667355537415: 1, -1.2012096643447876: 1, 0.37413451075553894: 1, 0.16982176899909973: 1, -1.2016302347183228: 1, -1.2016162872314453: 1, -0.3419588804244995: 1, 0.8243502378463745: 1, -1.201509714126587: 1, 0.812082827091217: 1, -1.201629400253296: 1, -1.201606273651123: 1, -1.201348900794983: 1, -0.8027163147926331: 1, 1.3996703624725342: 1, -0.4253336787223816: 1, -0.30772721767425537: 1, 0.20812031626701355: 1, -1.2013144493103027: 1, -0.8241826295852661: 1, -1.0799649953842163: 1, -1.1919200420379639: 1, -0.9751180410385132: 1, 1.7476170063018799: 1, -0.3605387806892395: 1, 0.292474627494812: 1, -0.5120261311531067: 1, 0.6492028832435608: 1, 1.281778335571289: 1, -0.24214474856853485: 1, -0.4367877244949341: 1, -1.1983946561813354: 1, -0.7376867532730103: 1, -1.17500901222229: 1, -0.748826265335083: 1, 0.22341269254684448: 1, -1.0084080696105957: 1, -0.9050359129905701: 1, -0.5784834027290344: 1, -0.8754668831825256: 1, -1.1312958002090454: 1, -1.098894715309143: 1, -0.09434226900339127: 1, -1.2003082036972046: 1, -0.09378552436828613: 1, -0.8962797522544861: 1, 0.9532740116119385: 1, 0.8790757060050964: 1, -0.9627416729927063: 1, 1.8212419748306274: 1, -1.0714704990386963: 1, -1.1041064262390137: 1, 1.3862724304199219: 1, -1.197619080543518: 1, -0.4400210678577423: 1, -1.1573199033737183: 1, -1.1370965242385864: 1, -0.8680421113967896: 1, -0.45158419013023376: 1, -1.188598394393921: 1, -1.1560314893722534: 1, 1.4887964725494385: 1, 1.788353681564331: 1, 0.43905025720596313: 1, -1.0395952463150024: 1, -0.3497014045715332: 1, -0.2101057469844818: 1, 1.1112544536590576: 1, 0.7950616478919983: 1, -0.16857680678367615: 1, 1.210314393043518: 1, 0.5456136465072632: 1, 1.3494455814361572: 1, -0.15746326744556427: 1, 0.12876805663108826: 1, 0.465168297290802: 1, 0.2403424084186554: 1, 0.8504006266593933: 1, 0.36215391755104065: 1, -0.5825971961021423: 1, 1.5823100805282593: 1, 1.5870535373687744: 1, -0.5881722569465637: 1, -0.8510425090789795: 1, -0.930620014667511: 1, -1.2016324996948242: 1, 1.1998642683029175: 1, 0.7611046433448792: 1, 0.35480692982673645: 1, 0.23328566551208496: 1, 1.6003168821334839: 1, -0.20539666712284088: 1, 0.9657272696495056: 1, 1.0303751230239868: 1, 1.6006011962890625: 1, -0.19720852375030518: 1, -1.1895103454589844: 1, -1.0719594955444336: 1, 1.9238320589065552: 1, -1.1969212293624878: 1, 0.1708119511604309: 1, -0.23679472506046295: 1, 0.2964378595352173: 1, 0.03332117572426796: 1, 1.4062000513076782: 1, -0.2782626748085022: 1, 0.3313300311565399: 1, -0.01062939316034317: 1, -0.10225572437047958: 1, -0.16261765360832214: 1, 0.900846004486084: 1, 0.12815426290035248: 1, -0.021469445899128914: 1, 0.6039040088653564: 1, 0.2640135586261749: 1, 0.02830575592815876: 1, 1.3615977764129639: 1, 1.0402084589004517: 1, 0.2787120044231415: 1, 1.0926192998886108: 1, 1.4690353870391846: 1, -1.2013123035430908: 1, 1.7558945417404175: 1, -0.4567924439907074: 1, 0.5131713151931763: 1, 1.5100682973861694: 1, -1.1264077425003052: 1, 1.0540943145751953: 1, -0.2686515748500824: 1, -1.061113953590393: 1, 1.2202951908111572: 1, 1.280470848083496: 1, -1.2016212940216064: 1, 1.9098440408706665: 1, -0.5300003886222839: 1, -1.2002416849136353: 1, -0.5811882019042969: 1, -1.2016383409500122: 1, 1.3496158123016357: 1, 1.9020731449127197: 1, -1.2016111612319946: 1, -0.6532332301139832: 1, 0.9807740449905396: 1, 1.796111822128296: 1, 1.4320223331451416: 1, 1.891126036643982: 1, -0.4007585644721985: 1, 1.7223035097122192: 1, 0.81379234790802: 1, -0.03429622948169708: 1, -1.0097246170043945: 1, -0.03317539766430855: 1, 1.7539278268814087: 1, 1.0263571739196777: 1, -0.4220041036605835: 1, 0.48093563318252563: 1, 1.6906383037567139: 1, 1.5815212726593018: 1, 0.34220972657203674: 1, 1.1086541414260864: 1, 0.6320856809616089: 1, 1.9327584505081177: 1, 0.6905925273895264: 1, 1.4504951238632202: 1, 1.0135881900787354: 1, 1.0885628461837769: 1, 0.1083393469452858: 1, 0.6886505484580994: 1, 0.5438616275787354: 1, 0.591416597366333: 1, -0.4422439932823181: 1, 1.114802360534668: 1, 0.04224063828587532: 1, 1.7425659894943237: 1, -0.8136187195777893: 1, 1.0987391471862793: 1, -0.7838892936706543: 1, 1.6581584215164185: 1, 0.38025134801864624: 1, 1.9389169216156006: 1, -0.9450681209564209: 1, -0.13542917370796204: 1, 0.4253986179828644: 1, 1.5920872688293457: 1, -1.2016384601593018: 1, 0.25442907214164734: 1, -0.6306192278862: 1, 1.6034055948257446: 1, 0.1718989461660385: 1, -1.200726866722107: 1, -0.13886263966560364: 1, -0.24342182278633118: 1, -0.8793452382087708: 1, -0.3587249517440796: 1, -0.20700129866600037: 1, -1.1936933994293213: 1, -1.040601134300232: 1, -1.002498984336853: 1, 1.8749902248382568: 1, 1.5291143655776978: 1, 0.9006003737449646: 1, 1.0343732833862305: 1, -1.2016191482543945: 1, 1.9172451496124268: 1, 0.43482455611228943: 1, 1.8706549406051636: 1, 1.760259985923767: 1, -0.11584310233592987: 1, -0.9069592952728271: 1, 0.08071509003639221: 1, 0.8788592219352722: 1, 1.5984208583831787: 1, 1.7614209651947021: 1, 1.7114263772964478: 1, 0.9352988600730896: 1, 1.597861409187317: 1, 0.9059203267097473: 1, 0.9797016382217407: 1, 1.7267848253250122: 1, -0.7692103385925293: 1, 0.019096076488494873: 1, -0.03557446971535683: 1, -0.7589280009269714: 1, 0.16001862287521362: 1, 0.6785101294517517: 1, -0.9877877235412598: 1, -0.4729682505130768: 1, 0.9276106357574463: 1, -0.6007823944091797: 1, -0.7505127787590027: 1, 1.3759360313415527: 1, -0.14395083487033844: 1, 0.171223446726799: 1, 1.4956955909729004: 1, -0.33291712403297424: 1, -0.44544142484664917: 1, 1.93668532371521: 1, -0.5857658982276917: 1, -1.1479105949401855: 1, -0.24207068979740143: 1, 1.7606215476989746: 1, -0.9762966632843018: 1, 0.22667939960956573: 1, 0.6444940567016602: 1, -0.0690179392695427: 1, 1.058951735496521: 1, 1.1691573858261108: 1, -0.46590861678123474: 1, -0.8751227855682373: 1, -0.3081098794937134: 1, -0.47152507305145264: 1, -0.6538054943084717: 1, -0.46823152899742126: 1, -1.1767117977142334: 1, 0.6789939403533936: 1, -0.5413272380828857: 1, 0.15043634176254272: 1, -0.17455770075321198: 1, -0.1967451572418213: 1, -0.5582360029220581: 1, -0.200442373752594: 1, 1.7616217136383057: 1, 1.4998488426208496: 1, -0.5338919758796692: 1, 1.376474142074585: 1, 0.48826223611831665: 1, -0.5174428224563599: 1, -0.5210259556770325: 1, -1.179208755493164: 1, 0.025362450629472733: 1, 1.3913298845291138: 1, -0.34284013509750366: 1, 1.4193427562713623: 1, -0.2183121144771576: 1, 0.4917255938053131: 1, 1.852098822593689: 1, 0.3162490129470825: 1, 1.552185297012329: 1, -1.1448076963424683: 1, -0.04817575961351395: 1, 0.5874748826026917: 1, -1.0406304597854614: 1, -0.6055639982223511: 1, -0.4845651388168335: 1, 1.0491001605987549: 1, 0.3744315505027771: 1, 0.7721536755561829: 1, -0.9981749057769775: 1, 0.7304840087890625: 1, -0.26246213912963867: 1, 1.6683679819107056: 1, -0.5307965278625488: 1, 0.1769435554742813: 1, -0.4171464443206787: 1, 0.7566526532173157: 1, -0.3100615441799164: 1, 1.692647099494934: 1, 1.8799982070922852: 1, 1.9273109436035156: 1, -0.8279891610145569: 1, -1.103760838508606: 1, -0.392406165599823: 1, 1.896323323249817: 1, -0.3123464286327362: 1, 1.5011951923370361: 1, -1.1689732074737549: 1, -0.44949105381965637: 1, -1.1831696033477783: 1, -0.3383774757385254: 1, -0.8523722290992737: 1, 0.5143935680389404: 1, -1.110012173652649: 1, -0.6403390765190125: 1, -1.1494790315628052: 1, -1.201560139656067: 1, -1.2015472650527954: 1, -1.1829675436019897: 1, -1.1994960308074951: 1, 0.7078472375869751: 1, -0.5361114740371704: 1, -1.2013746500015259: 1, -0.5809049010276794: 1, -1.1336802244186401: 1, 0.025435535237193108: 1, -0.3573164939880371: 1, -1.1605626344680786: 1, 0.49005118012428284: 1, 0.032617583870887756: 1, -1.0657732486724854: 1, -0.3311309218406677: 1, -1.2016228437423706: 1, -0.14265145361423492: 1, -1.1809542179107666: 1, -0.8764730095863342: 1, -0.24605818092823029: 1, 1.0700626373291016: 1, 0.17733901739120483: 1, -0.7893494963645935: 1, 0.5583640336990356: 1, -1.2016328573226929: 1, 0.5382747054100037: 1, 0.810942530632019: 1, -1.2016351222991943: 1, -0.16474246978759766: 1, 1.3037681579589844: 1, 1.0595874786376953: 1, -1.1934514045715332: 1, -1.2016363143920898: 1, -0.3548189401626587: 1, -1.150406837463379: 1, -0.10903797298669815: 1, -0.6006320714950562: 1, -0.6715388298034668: 1, -0.08644621819257736: 1, -1.201612949371338: 1, -1.1375747919082642: 1, -0.19534148275852203: 1, -1.201613426208496: 1, -0.3075839579105377: 1, 0.7587617635726929: 1, -1.2009185552597046: 1, 0.043348729610443115: 1, -0.4258005619049072: 1, 0.7787987589836121: 1, 0.7848809361457825: 1, 1.143233299255371: 1, 0.8696494698524475: 1, -0.136034294962883: 1, -1.1420694589614868: 1, -0.3552163541316986: 1, -1.2008600234985352: 1, -0.11124473065137863: 1, 1.2057219743728638: 1, -0.7557051777839661: 1, 0.762050449848175: 1, -1.109034538269043: 1, -0.07435453683137894: 1, -1.182140827178955: 1, 0.5923774242401123: 1, 0.9495259523391724: 1, 0.6296795010566711: 1, -0.6869497895240784: 1, 1.196900725364685: 1, -0.17342792451381683: 1, -1.1826090812683105: 1, -1.200330376625061: 1, 1.0692790746688843: 1, 0.606712281703949: 1, 0.666642963886261: 1, -0.019765598699450493: 1, -0.4923703670501709: 1, 0.167218878865242: 1, -1.1653438806533813: 1, 1.0611008405685425: 1, 1.2197721004486084: 1, -1.1774789094924927: 1, -0.19495585560798645: 1, -1.2016282081604004: 1, 1.0062413215637207: 1, -0.02250954695045948: 1, 1.0350605249404907: 1, -0.618209183216095: 1, 1.1690500974655151: 1, -0.11562295258045197: 1, -0.0026253368705511093: 1, -0.5897085666656494: 1, -1.0963338613510132: 1, -0.07029284536838531: 1, 1.1092147827148438: 1, 0.6174435615539551: 1, -0.7010860443115234: 1, -0.21185417473316193: 1, -1.2016310691833496: 1, -0.02432066947221756: 1, 0.7206960916519165: 1, -0.6903147101402283: 1, -1.193916916847229: 1, -0.15946216881275177: 1, -0.7992537021636963: 1, -0.13060888648033142: 1, 1.1562855243682861: 1, -0.5649014711380005: 1, -0.21699510514736176: 1, -0.1227283924818039: 1, 0.8076177835464478: 1, -0.5897277593612671: 1, 1.258391261100769: 1, 1.0134633779525757: 1, -1.1910632848739624: 1, -0.832706868648529: 1, -1.1333073377609253: 1, -1.1857080459594727: 1, -1.1643073558807373: 1, -0.40432149171829224: 1, -1.188031554222107: 1, -1.186979055404663: 1, -0.9010990262031555: 1, -0.5026354193687439: 1, -1.1999870538711548: 1, -1.195172905921936: 1, -1.1728081703186035: 1, 0.741217315196991: 1, -1.1993547677993774: 1, -1.1929975748062134: 1, 0.3444092869758606: 1, 0.7184975147247314: 1, -1.157366394996643: 1, 0.275499552488327: 1, -1.1865227222442627: 1, -1.0972120761871338: 1, -0.9958106875419617: 1, -1.1339654922485352: 1, -1.0668615102767944: 1, -0.19914157688617706: 1, -1.194190263748169: 1, 4.875144004821777: 1, 5.072229385375977: 1, 0.4937030076980591: 1, -1.0461647510528564: 1, -0.8267379403114319: 1, -1.200010895729065: 1, -1.2006280422210693: 1, -1.144382119178772: 1, -0.47614291310310364: 1, -0.8666650652885437: 1, -1.00275719165802: 1, -0.6867276430130005: 1, -0.669135332107544: 1, -1.145255446434021: 1, -0.9141108393669128: 1, -0.8772833347320557: 1, -1.1463063955307007: 1, -0.9300684928894043: 1, -1.157931923866272: 1, -0.8867827653884888: 1, -1.1989647150039673: 1, 0.4647902548313141: 1, -1.1685289144515991: 1, 1.0952398777008057: 1, -0.5342384576797485: 1, -0.35478705167770386: 1, 0.9517895579338074: 1, 0.880368709564209: 1, 0.605282723903656: 1, 0.842113196849823: 1, -0.5739816427230835: 1, -0.576421856880188: 1, -0.7073397040367126: 1, 0.6389100551605225: 1, -1.0666824579238892: 1, -0.5935716032981873: 1, -1.1994209289550781: 1, 1.0410280227661133: 1, -0.3055903911590576: 1, -0.3122004270553589: 1, -1.1028809547424316: 1, -0.24963954091072083: 1, 0.9452794194221497: 1, 0.4334481656551361: 1, -0.13237591087818146: 1, 0.6366953253746033: 1, 1.1094199419021606: 1, 1.2156920433044434: 1, -0.5885310769081116: 1, -0.5334532856941223: 1, 0.8030492067337036: 1, -1.173497200012207: 1, -1.2015430927276611: 1, 0.23683039844036102: 1, -1.188680648803711: 1, -0.39169713854789734: 1, -1.201418161392212: 1, -0.2950673997402191: 1, 0.7067909836769104: 1, 0.8852934837341309: 1, 1.593440055847168: 1, 1.6096405982971191: 1, 0.2436400204896927: 1, 1.036679744720459: 1, 0.2817704975605011: 1, 1.2748090028762817: 1, -0.7198786735534668: 1, 1.5706232786178589: 1, -0.34684932231903076: 1, 1.585684895515442: 1, -1.2015637159347534: 1, -1.1621276140213013: 1, -0.586733877658844: 1, 0.44458866119384766: 1, 4.302996635437012: 1, 0.2222539335489273: 1, -0.40100592374801636: 1, 0.23767612874507904: 1, 1.4899855852127075: 1, -0.6113037467002869: 1, -0.9236016869544983: 1, 1.0590577125549316: 1, 1.260263442993164: 1, 1.4145740270614624: 1, 0.7438257932662964: 1, 0.18295887112617493: 1, -0.19414900243282318: 1, 1.0041277408599854: 1, -1.1556631326675415: 1, -0.48821160197257996: 1, 1.215183138847351: 1, 0.9935461282730103: 1, 0.2596873939037323: 1, -0.20360040664672852: 1, 1.4644227027893066: 1, -1.1900941133499146: 1, 1.4411144256591797: 1, 1.1854524612426758: 1, 0.7718502283096313: 1, 0.3140702247619629: 1, -0.632728099822998: 1, -0.21601253747940063: 1, 0.9222752451896667: 1, -0.20083148777484894: 1, 1.059990644454956: 1, -0.0920228511095047: 1, -0.014552570879459381: 1, -0.3219013214111328: 1, 0.004675476811826229: 1, 0.7520656585693359: 1, 0.7781831622123718: 1, 0.01178812701255083: 1, -0.5199218392372131: 1, 0.5634517669677734: 1, -0.03743843734264374: 1, 0.31844547390937805: 1, -0.1960129290819168: 1, 0.858607292175293: 1, 1.282376766204834: 1, 0.7782679200172424: 1, 1.0659617185592651: 1, 0.987504780292511: 1, 0.9434877038002014: 1, 0.968934953212738: 1, 0.06307864934206009: 1, 1.189407229423523: 1, -0.6757361888885498: 1, 0.7684015035629272: 1, 0.22540217638015747: 1, -0.11932859569787979: 1, 1.5550216436386108: 1, 2.0702569484710693: 1, -1.1040070056915283: 1, -0.5194405913352966: 1, 0.009009350091218948: 1, 1.4103199243545532: 1, 0.3276282548904419: 1, 1.4938876628875732: 1, -0.28925973176956177: 1, -0.04237562045454979: 1, -1.1579349040985107: 1, 1.4680920839309692: 1, -1.19014310836792: 1, 1.4807751178741455: 1, 0.5575955510139465: 1, -0.9919153451919556: 1, -1.0858170986175537: 1, -0.7701697945594788: 1, -0.5982488989830017: 1, 1.3844947814941406: 1, 0.263934850692749: 1, -0.17545948922634125: 1, 0.8805737495422363: 1, -0.2210041582584381: 1, 0.9266675710678101: 1, 0.8433452844619751: 1, -0.11317183822393417: 1, 0.3683989942073822: 1, 0.1563034951686859: 1, 0.30258285999298096: 1, 1.1731380224227905: 1, 1.302850365638733: 1, 0.7159395813941956: 1, 0.8933335542678833: 1, 1.3772739171981812: 1, -0.43688738346099854: 1, -0.07051629573106766: 1, 1.3494865894317627: 1, -1.1632437705993652: 1, 1.5597952604293823: 1, 0.09744428098201752: 1, -0.8828660249710083: 1, 0.05755604803562164: 1, -0.7541334629058838: 1, -1.0867854356765747: 1, 0.05546194687485695: 1, 1.0013794898986816: 1, 1.3061707019805908: 1, 0.8562594652175903: 1, -0.4684484004974365: 1, 0.7325264811515808: 1, 1.2978951930999756: 1, 0.729964017868042: 1, -0.8322789072990417: 1, -0.2597183287143707: 1, 1.035197138786316: 1, 1.196738839149475: 1, -0.7430113554000854: 1, -0.08886344730854034: 1, -0.9601404666900635: 1, 1.310013771057129: 1, -0.14243346452713013: 1, -0.05695538595318794: 1, 0.2588636577129364: 1, -1.201636552810669: 1, -0.34523066878318787: 1, -0.029267514124512672: 1, -0.21881107985973358: 1, 0.33690160512924194: 1, 0.04192548990249634: 1, 0.9413108229637146: 1, -0.2299586683511734: 1, 1.2350752353668213: 1, -0.27776581048965454: 1, 0.41254743933677673: 1, -0.14272816479206085: 1, 1.1865397691726685: 1, -0.42109400033950806: 1, 0.8838387131690979: 1, -0.24156887829303741: 1, 1.1694233417510986: 1, -1.2016277313232422: 1, -0.0411478653550148: 1, -1.0489486455917358: 1, -0.12074612081050873: 1, 1.2192449569702148: 1, 0.9730016589164734: 1, 0.889022946357727: 1, 0.23997803032398224: 1, 0.23272329568862915: 1, -0.20541705191135406: 1, -0.35842111706733704: 1, -1.126828908920288: 1, 0.9848727583885193: 1, -1.201277732849121: 1, 0.9529565572738647: 1, -0.9734629392623901: 1, 0.10832516103982925: 1, -0.9857455492019653: 1, -1.2015197277069092: 1, -1.201407790184021: 1, -0.053435858339071274: 1, -1.2011888027191162: 1, -1.194821834564209: 1, 0.1581522524356842: 1, -0.9676279425621033: 1, -1.2014601230621338: 1, -0.9951181411743164: 1, -1.201635479927063: 1, -0.5475073456764221: 1, -0.19123347103595734: 1, -1.2014092206954956: 1, -1.2012838125228882: 1, -1.201635718345642: 1, -1.1903178691864014: 1, -1.171350359916687: 1, 0.3900337219238281: 1, -1.201622486114502: 1, -0.06736226379871368: 1, -0.5888482928276062: 1, -1.2015849351882935: 1, -0.07099064439535141: 1, -0.6771114468574524: 1, -1.1987295150756836: 1, -1.2016342878341675: 1, 0.8673886060714722: 1, -1.175829291343689: 1, 0.7298784255981445: 1, 1.2526917457580566: 1, 0.3951069414615631: 1, 1.0069524049758911: 1, 1.1649004220962524: 1, 1.1717408895492554: 1, -0.10037212073802948: 1, 0.09893729537725449: 1, 1.0808240175247192: 1, 0.9786769151687622: 1, 0.6481048464775085: 1, -0.10643515735864639: 1, -0.16572129726409912: 1, -1.1484540700912476: 1, -1.201595425605774: 1, -1.2015864849090576: 1, -1.179785132408142: 1, -1.2016286849975586: 1, -1.2016292810440063: 1, -1.1320221424102783: 1}
INFO     Network type: REG
INFO     Model: "sequential_1"
INFO     _________________________________________________________________
INFO     Layer (type)                 Output Shape              Param #
INFO     =================================================================
INFO     dense_5 (Dense)              (None, 1024)              2098176
INFO     _________________________________________________________________
INFO     dropout_4 (Dropout)          (None, 1024)              0
INFO     _________________________________________________________________
INFO     dense_6 (Dense)              (None, 512)               524800
INFO     _________________________________________________________________
INFO     dropout_5 (Dropout)          (None, 512)               0
INFO     _________________________________________________________________
INFO     dense_7 (Dense)              (None, 256)               131328
INFO     _________________________________________________________________
INFO     dropout_6 (Dropout)          (None, 256)               0
INFO     _________________________________________________________________
INFO     dense_8 (Dense)              (None, 128)               32896
INFO     _________________________________________________________________
INFO     dropout_7 (Dropout)          (None, 128)               0
INFO     _________________________________________________________________
INFO     dense_9 (Dense)              (None, 1)                 129
INFO     =================================================================
INFO     Total params: 2,787,329
INFO     Trainable params: 2,787,329
INFO     Non-trainable params: 0
INFO     _________________________________________________________________
WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.
WARNING  `period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.
2024-07-15 20:02:45.263619: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0
2024-07-15 20:02:45.263790: I tensorflow/core/grappler/clusters/single_machine.cc:357] Starting new session
2024-07-15 20:02:45.266502: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:1137] Optimization results for grappler item: graph_to_optimize
  function_optimizer: function_optimizer did nothing. time = 0.01ms.
  function_optimizer: function_optimizer did nothing. time = 0.002ms.
26/26 - 2s - loss: 4.6639 - val_loss: 4.6302
Epoch 2/5000
26/26 - 1s - loss: 4.6267 - val_loss: 4.6072
Epoch 3/5000
26/26 - 1s - loss: 4.6022 - val_loss: 4.5870
Epoch 4/5000
26/26 - 1s - loss: 4.5783 - val_loss: 4.5667
Epoch 5/5000
26/26 - 1s - loss: 4.5527 - val_loss: 4.5469
Epoch 6/5000
26/26 - 1s - loss: 4.5267 - val_loss: 4.5272
Epoch 7/5000
26/26 - 1s - loss: 4.5040 - val_loss: 4.5092
Epoch 8/5000
26/26 - 1s - loss: 4.4817 - val_loss: 4.4916
Epoch 9/5000
26/26 - 1s - loss: 4.4617 - val_loss: 4.4754
Epoch 10/5000
26/26 - 1s - loss: 4.4434 - val_loss: 4.4609
Epoch 00010: val_loss improved from inf to 4.46092, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 11/5000
26/26 - 1s - loss: 4.4235 - val_loss: 4.4464
Epoch 12/5000
26/26 - 1s - loss: 4.4072 - val_loss: 4.4330
Epoch 13/5000
26/26 - 1s - loss: 4.3897 - val_loss: 4.4190
Epoch 14/5000
26/26 - 1s - loss: 4.3717 - val_loss: 4.4062
Epoch 15/5000
26/26 - 1s - loss: 4.3569 - val_loss: 4.3934
Epoch 16/5000
26/26 - 1s - loss: 4.3399 - val_loss: 4.3803
Epoch 17/5000
26/26 - 1s - loss: 4.3217 - val_loss: 4.3681
Epoch 18/5000
26/26 - 1s - loss: 4.3069 - val_loss: 4.3561
Epoch 19/5000
26/26 - 1s - loss: 4.2927 - val_loss: 4.3443
Epoch 20/5000
26/26 - 1s - loss: 4.2763 - val_loss: 4.3319
Epoch 00020: val_loss improved from 4.46092 to 4.33190, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 21/5000
26/26 - 1s - loss: 4.2593 - val_loss: 4.3207
Epoch 22/5000
26/26 - 1s - loss: 4.2448 - val_loss: 4.3095
Epoch 23/5000
26/26 - 1s - loss: 4.2303 - val_loss: 4.2979
Epoch 24/5000
26/26 - 1s - loss: 4.2148 - val_loss: 4.2870
Epoch 25/5000
26/26 - 2s - loss: 4.2014 - val_loss: 4.2763
Epoch 26/5000
26/26 - 1s - loss: 4.1853 - val_loss: 4.2650
Epoch 27/5000
26/26 - 1s - loss: 4.1713 - val_loss: 4.2541
Epoch 28/5000
26/26 - 1s - loss: 4.1577 - val_loss: 4.2439
Epoch 29/5000
26/26 - 1s - loss: 4.1415 - val_loss: 4.2340
Epoch 30/5000
26/26 - 2s - loss: 4.1286 - val_loss: 4.2241
Epoch 00030: val_loss improved from 4.33190 to 4.22406, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 31/5000
26/26 - 1s - loss: 4.1134 - val_loss: 4.2132
Epoch 32/5000
26/26 - 1s - loss: 4.0980 - val_loss: 4.2032
Epoch 33/5000
26/26 - 1s - loss: 4.0856 - val_loss: 4.1935
Epoch 34/5000
26/26 - 1s - loss: 4.0696 - val_loss: 4.1841
Epoch 35/5000
26/26 - 1s - loss: 4.0573 - val_loss: 4.1741
Epoch 36/5000
26/26 - 1s - loss: 4.0432 - val_loss: 4.1649
Epoch 37/5000
26/26 - 1s - loss: 4.0309 - val_loss: 4.1564
Epoch 38/5000
26/26 - 1s - loss: 4.0186 - val_loss: 4.1465
Epoch 39/5000
26/26 - 1s - loss: 4.0036 - val_loss: 4.1382
Epoch 40/5000
26/26 - 1s - loss: 3.9897 - val_loss: 4.1293
Epoch 00040: val_loss improved from 4.22406 to 4.12928, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 41/5000
26/26 - 1s - loss: 3.9767 - val_loss: 4.1206
Epoch 42/5000
26/26 - 1s - loss: 3.9651 - val_loss: 4.1118
Epoch 43/5000
26/26 - 1s - loss: 3.9519 - val_loss: 4.1028
Epoch 44/5000
26/26 - 1s - loss: 3.9409 - val_loss: 4.0952
Epoch 45/5000
26/26 - 1s - loss: 3.9274 - val_loss: 4.0873
Epoch 46/5000
26/26 - 1s - loss: 3.9161 - val_loss: 4.0790
Epoch 47/5000
26/26 - 1s - loss: 3.9026 - val_loss: 4.0709
Epoch 48/5000
26/26 - 1s - loss: 3.8908 - val_loss: 4.0628
Epoch 49/5000
26/26 - 1s - loss: 3.8780 - val_loss: 4.0549
Epoch 50/5000
26/26 - 1s - loss: 3.8693 - val_loss: 4.0468
Epoch 00050: val_loss improved from 4.12928 to 4.04676, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 51/5000
26/26 - 1s - loss: 3.8565 - val_loss: 4.0392
Epoch 52/5000
26/26 - 1s - loss: 3.8432 - val_loss: 4.0315
Epoch 53/5000
26/26 - 1s - loss: 3.8336 - val_loss: 4.0241
Epoch 54/5000
26/26 - 1s - loss: 3.8224 - val_loss: 4.0163
Epoch 55/5000
26/26 - 1s - loss: 3.8128 - val_loss: 4.0086
Epoch 56/5000
26/26 - 1s - loss: 3.8023 - val_loss: 4.0007
Epoch 57/5000
26/26 - 1s - loss: 3.7895 - val_loss: 3.9940
Epoch 58/5000
26/26 - 1s - loss: 3.7816 - val_loss: 3.9874
Epoch 59/5000
26/26 - 1s - loss: 3.7717 - val_loss: 3.9813
Epoch 60/5000
26/26 - 1s - loss: 3.7613 - val_loss: 3.9731
Epoch 00060: val_loss improved from 4.04676 to 3.97306, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 61/5000
26/26 - 1s - loss: 3.7495 - val_loss: 3.9658
Epoch 62/5000
26/26 - 1s - loss: 3.7415 - val_loss: 3.9591
Epoch 63/5000
26/26 - 1s - loss: 3.7342 - val_loss: 3.9522
Epoch 64/5000
26/26 - 1s - loss: 3.7214 - val_loss: 3.9458
Epoch 65/5000
26/26 - 1s - loss: 3.7123 - val_loss: 3.9385
Epoch 66/5000
26/26 - 1s - loss: 3.7044 - val_loss: 3.9315
Epoch 67/5000
26/26 - 1s - loss: 3.6945 - val_loss: 3.9256
Epoch 68/5000
26/26 - 1s - loss: 3.6873 - val_loss: 3.9194
Epoch 69/5000
26/26 - 1s - loss: 3.6800 - val_loss: 3.9139
Epoch 70/5000
26/26 - 1s - loss: 3.6686 - val_loss: 3.9086
Epoch 00070: val_loss improved from 3.97306 to 3.90860, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 71/5000
26/26 - 1s - loss: 3.6603 - val_loss: 3.9006
Epoch 72/5000
26/26 - 1s - loss: 3.6507 - val_loss: 3.8943
Epoch 73/5000
26/26 - 1s - loss: 3.6438 - val_loss: 3.8878
Epoch 74/5000
26/26 - 1s - loss: 3.6329 - val_loss: 3.8826
Epoch 75/5000
26/26 - 1s - loss: 3.6250 - val_loss: 3.8772
Epoch 76/5000
26/26 - 1s - loss: 3.6176 - val_loss: 3.8710
Epoch 77/5000
26/26 - 1s - loss: 3.6079 - val_loss: 3.8648
Epoch 78/5000
26/26 - 1s - loss: 3.6002 - val_loss: 3.8585
Epoch 79/5000
26/26 - 1s - loss: 3.5907 - val_loss: 3.8519
Epoch 80/5000
26/26 - 1s - loss: 3.5828 - val_loss: 3.8469
Epoch 00080: val_loss improved from 3.90860 to 3.84688, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 81/5000
26/26 - 1s - loss: 3.5771 - val_loss: 3.8420
Epoch 82/5000
26/26 - 1s - loss: 3.5705 - val_loss: 3.8354
Epoch 83/5000
26/26 - 1s - loss: 3.5630 - val_loss: 3.8298
Epoch 84/5000
26/26 - 1s - loss: 3.5540 - val_loss: 3.8254
Epoch 85/5000
26/26 - 1s - loss: 3.5444 - val_loss: 3.8196
Epoch 86/5000
26/26 - 1s - loss: 3.5406 - val_loss: 3.8141
Epoch 87/5000
26/26 - 1s - loss: 3.5284 - val_loss: 3.8070
Epoch 88/5000
26/26 - 1s - loss: 3.5240 - val_loss: 3.8019
Epoch 89/5000
26/26 - 1s - loss: 3.5161 - val_loss: 3.7968
Epoch 90/5000
26/26 - 1s - loss: 3.5083 - val_loss: 3.7909
Epoch 00090: val_loss improved from 3.84688 to 3.79092, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 91/5000
26/26 - 1s - loss: 3.5020 - val_loss: 3.7848
Epoch 92/5000
26/26 - 1s - loss: 3.4934 - val_loss: 3.7804
Epoch 93/5000
26/26 - 1s - loss: 3.4868 - val_loss: 3.7750
Epoch 94/5000
26/26 - 1s - loss: 3.4785 - val_loss: 3.7704
Epoch 95/5000
26/26 - 1s - loss: 3.4714 - val_loss: 3.7637
Epoch 96/5000
26/26 - 1s - loss: 3.4655 - val_loss: 3.7591
Epoch 97/5000
26/26 - 1s - loss: 3.4608 - val_loss: 3.7538
Epoch 98/5000
26/26 - 1s - loss: 3.4540 - val_loss: 3.7479
Epoch 99/5000
26/26 - 1s - loss: 3.4410 - val_loss: 3.7428
Epoch 100/5000
26/26 - 1s - loss: 3.4390 - val_loss: 3.7384
Epoch 00100: val_loss improved from 3.79092 to 3.73841, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 101/5000
26/26 - 1s - loss: 3.4342 - val_loss: 3.7329
Epoch 102/5000
26/26 - 1s - loss: 3.4256 - val_loss: 3.7278
Epoch 103/5000
26/26 - 1s - loss: 3.4199 - val_loss: 3.7236
Epoch 104/5000
26/26 - 2s - loss: 3.4124 - val_loss: 3.7185
Epoch 105/5000
26/26 - 1s - loss: 3.4082 - val_loss: 3.7121
Epoch 106/5000
26/26 - 1s - loss: 3.3990 - val_loss: 3.7069
Epoch 107/5000
26/26 - 1s - loss: 3.3929 - val_loss: 3.7028
Epoch 108/5000
26/26 - 1s - loss: 3.3896 - val_loss: 3.6990
Epoch 109/5000
26/26 - 1s - loss: 3.3826 - val_loss: 3.6936
Epoch 110/5000
26/26 - 1s - loss: 3.3777 - val_loss: 3.6885
Epoch 00110: val_loss improved from 3.73841 to 3.68849, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 111/5000
26/26 - 1s - loss: 3.3711 - val_loss: 3.6864
Epoch 112/5000
26/26 - 1s - loss: 3.3644 - val_loss: 3.6820
Epoch 113/5000
26/26 - 1s - loss: 3.3602 - val_loss: 3.6760
Epoch 114/5000
26/26 - 1s - loss: 3.3518 - val_loss: 3.6698
Epoch 115/5000
26/26 - 1s - loss: 3.3491 - val_loss: 3.6661
Epoch 116/5000
26/26 - 1s - loss: 3.3399 - val_loss: 3.6603
Epoch 117/5000
26/26 - 1s - loss: 3.3367 - val_loss: 3.6556
Epoch 118/5000
26/26 - 1s - loss: 3.3279 - val_loss: 3.6519
Epoch 119/5000
26/26 - 1s - loss: 3.3241 - val_loss: 3.6460
Epoch 120/5000
26/26 - 1s - loss: 3.3202 - val_loss: 3.6417
Epoch 00120: val_loss improved from 3.68849 to 3.64171, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 121/5000
26/26 - 1s - loss: 3.3129 - val_loss: 3.6377
Epoch 122/5000
26/26 - 1s - loss: 3.3084 - val_loss: 3.6342
Epoch 123/5000
26/26 - 1s - loss: 3.3026 - val_loss: 3.6294
Epoch 124/5000
26/26 - 1s - loss: 3.2950 - val_loss: 3.6240
Epoch 125/5000
26/26 - 1s - loss: 3.2906 - val_loss: 3.6209
Epoch 126/5000
26/26 - 1s - loss: 3.2851 - val_loss: 3.6171
Epoch 127/5000
26/26 - 1s - loss: 3.2799 - val_loss: 3.6120
Epoch 128/5000
26/26 - 1s - loss: 3.2733 - val_loss: 3.6061
Epoch 129/5000
26/26 - 1s - loss: 3.2680 - val_loss: 3.6005
Epoch 130/5000
26/26 - 1s - loss: 3.2612 - val_loss: 3.5963
Epoch 00130: val_loss improved from 3.64171 to 3.59634, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 131/5000
26/26 - 1s - loss: 3.2583 - val_loss: 3.5923
Epoch 132/5000
26/26 - 1s - loss: 3.2534 - val_loss: 3.5887
Epoch 133/5000
26/26 - 1s - loss: 3.2471 - val_loss: 3.5859
Epoch 134/5000
26/26 - 1s - loss: 3.2456 - val_loss: 3.5804
Epoch 135/5000
26/26 - 1s - loss: 3.2365 - val_loss: 3.5757
Epoch 136/5000
26/26 - 1s - loss: 3.2339 - val_loss: 3.5722
Epoch 137/5000
26/26 - 1s - loss: 3.2292 - val_loss: 3.5665
Epoch 138/5000
26/26 - 1s - loss: 3.2214 - val_loss: 3.5625
Epoch 139/5000
26/26 - 1s - loss: 3.2190 - val_loss: 3.5568
Epoch 140/5000
26/26 - 1s - loss: 3.2100 - val_loss: 3.5525
Epoch 00140: val_loss improved from 3.59634 to 3.55248, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 141/5000
26/26 - 1s - loss: 3.2071 - val_loss: 3.5491
Epoch 142/5000
26/26 - 1s - loss: 3.2022 - val_loss: 3.5452
Epoch 143/5000
26/26 - 1s - loss: 3.1990 - val_loss: 3.5418
Epoch 144/5000
26/26 - 1s - loss: 3.1936 - val_loss: 3.5366
Epoch 145/5000
26/26 - 1s - loss: 3.1872 - val_loss: 3.5319
Epoch 146/5000
26/26 - 1s - loss: 3.1828 - val_loss: 3.5270
Epoch 147/5000
26/26 - 1s - loss: 3.1763 - val_loss: 3.5221
Epoch 148/5000
26/26 - 1s - loss: 3.1743 - val_loss: 3.5183
Epoch 149/5000
26/26 - 1s - loss: 3.1665 - val_loss: 3.5146
Epoch 150/5000
26/26 - 1s - loss: 3.1644 - val_loss: 3.5104
Epoch 00150: val_loss improved from 3.55248 to 3.51044, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 151/5000
26/26 - 1s - loss: 3.1587 - val_loss: 3.5067
Epoch 152/5000
26/26 - 1s - loss: 3.1559 - val_loss: 3.5022
Epoch 153/5000
26/26 - 1s - loss: 3.1509 - val_loss: 3.4980
Epoch 154/5000
26/26 - 1s - loss: 3.1428 - val_loss: 3.4934
Epoch 155/5000
26/26 - 1s - loss: 3.1416 - val_loss: 3.4886
Epoch 156/5000
26/26 - 1s - loss: 3.1366 - val_loss: 3.4834
Epoch 157/5000
26/26 - 2s - loss: 3.1308 - val_loss: 3.4802
Epoch 158/5000
26/26 - 1s - loss: 3.1259 - val_loss: 3.4759
Epoch 159/5000
26/26 - 1s - loss: 3.1202 - val_loss: 3.4736
Epoch 160/5000
26/26 - 1s - loss: 3.1186 - val_loss: 3.4717
Epoch 00160: val_loss improved from 3.51044 to 3.47168, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 161/5000
26/26 - 1s - loss: 3.1148 - val_loss: 3.4672
Epoch 162/5000
26/26 - 1s - loss: 3.1106 - val_loss: 3.4655
Epoch 163/5000
26/26 - 1s - loss: 3.1027 - val_loss: 3.4580
Epoch 164/5000
26/26 - 1s - loss: 3.1012 - val_loss: 3.4553
Epoch 165/5000
26/26 - 1s - loss: 3.0961 - val_loss: 3.4515
Epoch 166/5000
26/26 - 1s - loss: 3.0919 - val_loss: 3.4466
Epoch 167/5000
26/26 - 1s - loss: 3.0859 - val_loss: 3.4422
Epoch 168/5000
26/26 - 1s - loss: 3.0810 - val_loss: 3.4384
Epoch 169/5000
26/26 - 1s - loss: 3.0780 - val_loss: 3.4353
Epoch 170/5000
26/26 - 1s - loss: 3.0735 - val_loss: 3.4316
Epoch 00170: val_loss improved from 3.47168 to 3.43158, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 171/5000
26/26 - 1s - loss: 3.0684 - val_loss: 3.4277
Epoch 172/5000
26/26 - 1s - loss: 3.0660 - val_loss: 3.4228
Epoch 173/5000
26/26 - 1s - loss: 3.0649 - val_loss: 3.4192
Epoch 174/5000
26/26 - 1s - loss: 3.0580 - val_loss: 3.4164
Epoch 175/5000
26/26 - 1s - loss: 3.0540 - val_loss: 3.4127
Epoch 176/5000
26/26 - 1s - loss: 3.0499 - val_loss: 3.4084
Epoch 177/5000
26/26 - 1s - loss: 3.0445 - val_loss: 3.4038
Epoch 178/5000
26/26 - 1s - loss: 3.0418 - val_loss: 3.4000
Epoch 179/5000
26/26 - 1s - loss: 3.0372 - val_loss: 3.3976
Epoch 180/5000
26/26 - 1s - loss: 3.0304 - val_loss: 3.3935
Epoch 00180: val_loss improved from 3.43158 to 3.39348, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 181/5000
26/26 - 1s - loss: 3.0287 - val_loss: 3.3903
Epoch 182/5000
26/26 - 1s - loss: 3.0243 - val_loss: 3.3865
Epoch 183/5000
26/26 - 1s - loss: 3.0191 - val_loss: 3.3823
Epoch 184/5000
26/26 - 1s - loss: 3.0140 - val_loss: 3.3793
Epoch 185/5000
26/26 - 1s - loss: 3.0100 - val_loss: 3.3757
Epoch 186/5000
26/26 - 1s - loss: 3.0077 - val_loss: 3.3720
Epoch 187/5000
26/26 - 1s - loss: 3.0031 - val_loss: 3.3683
Epoch 188/5000
26/26 - 1s - loss: 3.0014 - val_loss: 3.3647
Epoch 189/5000
26/26 - 1s - loss: 2.9953 - val_loss: 3.3616
Epoch 190/5000
26/26 - 1s - loss: 2.9889 - val_loss: 3.3581
Epoch 00190: val_loss improved from 3.39348 to 3.35815, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 191/5000
26/26 - 1s - loss: 2.9853 - val_loss: 3.3546
Epoch 192/5000
26/26 - 1s - loss: 2.9856 - val_loss: 3.3504
Epoch 193/5000
26/26 - 1s - loss: 2.9770 - val_loss: 3.3473
Epoch 194/5000
26/26 - 1s - loss: 2.9767 - val_loss: 3.3428
Epoch 195/5000
26/26 - 1s - loss: 2.9711 - val_loss: 3.3405
Epoch 196/5000
26/26 - 1s - loss: 2.9686 - val_loss: 3.3356
Epoch 197/5000
26/26 - 1s - loss: 2.9607 - val_loss: 3.3324
Epoch 198/5000
26/26 - 1s - loss: 2.9590 - val_loss: 3.3288
Epoch 199/5000
26/26 - 1s - loss: 2.9555 - val_loss: 3.3262
Epoch 200/5000
26/26 - 1s - loss: 2.9553 - val_loss: 3.3220
Epoch 00200: val_loss improved from 3.35815 to 3.32203, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 201/5000
26/26 - 1s - loss: 2.9464 - val_loss: 3.3196
Epoch 202/5000
26/26 - 1s - loss: 2.9442 - val_loss: 3.3156
Epoch 203/5000
26/26 - 1s - loss: 2.9398 - val_loss: 3.3113
Epoch 204/5000
26/26 - 1s - loss: 2.9378 - val_loss: 3.3074
Epoch 205/5000
26/26 - 1s - loss: 2.9339 - val_loss: 3.3052
Epoch 206/5000
26/26 - 1s - loss: 2.9280 - val_loss: 3.3006
Epoch 207/5000
26/26 - 1s - loss: 2.9241 - val_loss: 3.2965
Epoch 208/5000
26/26 - 1s - loss: 2.9185 - val_loss: 3.2941
Epoch 209/5000
26/26 - 1s - loss: 2.9194 - val_loss: 3.2914
Epoch 210/5000
26/26 - 1s - loss: 2.9128 - val_loss: 3.2889
Epoch 00210: val_loss improved from 3.32203 to 3.28890, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 211/5000
26/26 - 1s - loss: 2.9113 - val_loss: 3.2857
Epoch 212/5000
26/26 - 1s - loss: 2.9040 - val_loss: 3.2835
Epoch 213/5000
26/26 - 1s - loss: 2.9040 - val_loss: 3.2792
Epoch 214/5000
26/26 - 1s - loss: 2.8972 - val_loss: 3.2754
Epoch 215/5000
26/26 - 1s - loss: 2.8942 - val_loss: 3.2726
Epoch 216/5000
26/26 - 1s - loss: 2.8907 - val_loss: 3.2702
Epoch 217/5000
26/26 - 1s - loss: 2.8874 - val_loss: 3.2670
Epoch 218/5000
26/26 - 1s - loss: 2.8843 - val_loss: 3.2621
Epoch 219/5000
26/26 - 1s - loss: 2.8797 - val_loss: 3.2572
Epoch 220/5000
26/26 - 1s - loss: 2.8796 - val_loss: 3.2547
Epoch 00220: val_loss improved from 3.28890 to 3.25468, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 221/5000
26/26 - 1s - loss: 2.8742 - val_loss: 3.2535
Epoch 222/5000
26/26 - 1s - loss: 2.8685 - val_loss: 3.2482
Epoch 223/5000
26/26 - 1s - loss: 2.8640 - val_loss: 3.2448
Epoch 224/5000
26/26 - 1s - loss: 2.8624 - val_loss: 3.2409
Epoch 225/5000
26/26 - 1s - loss: 2.8594 - val_loss: 3.2382
Epoch 226/5000
26/26 - 1s - loss: 2.8561 - val_loss: 3.2360
Epoch 227/5000
26/26 - 1s - loss: 2.8504 - val_loss: 3.2338
Epoch 228/5000
26/26 - 1s - loss: 2.8476 - val_loss: 3.2306
Epoch 229/5000
26/26 - 1s - loss: 2.8440 - val_loss: 3.2263
Epoch 230/5000
26/26 - 1s - loss: 2.8406 - val_loss: 3.2232
Epoch 00230: val_loss improved from 3.25468 to 3.22320, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 231/5000
26/26 - 1s - loss: 2.8402 - val_loss: 3.2210
Epoch 232/5000
26/26 - 1s - loss: 2.8381 - val_loss: 3.2183
Epoch 233/5000
26/26 - 1s - loss: 2.8318 - val_loss: 3.2135
Epoch 234/5000
26/26 - 1s - loss: 2.8274 - val_loss: 3.2096
Epoch 235/5000
26/26 - 1s - loss: 2.8228 - val_loss: 3.2072
Epoch 236/5000
26/26 - 1s - loss: 2.8224 - val_loss: 3.2043
Epoch 237/5000
26/26 - 1s - loss: 2.8207 - val_loss: 3.2013
Epoch 238/5000
26/26 - 1s - loss: 2.8139 - val_loss: 3.1963
Epoch 239/5000
26/26 - 1s - loss: 2.8103 - val_loss: 3.1927
Epoch 240/5000
26/26 - 1s - loss: 2.8096 - val_loss: 3.1900
Epoch 00240: val_loss improved from 3.22320 to 3.19002, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 241/5000
26/26 - 1s - loss: 2.8022 - val_loss: 3.1883
Epoch 242/5000
26/26 - 1s - loss: 2.7997 - val_loss: 3.1859
Epoch 243/5000
26/26 - 1s - loss: 2.7953 - val_loss: 3.1837
Epoch 244/5000
26/26 - 1s - loss: 2.7934 - val_loss: 3.1817
Epoch 245/5000
26/26 - 1s - loss: 2.7896 - val_loss: 3.1778
Epoch 246/5000
26/26 - 1s - loss: 2.7894 - val_loss: 3.1764
Epoch 247/5000
26/26 - 1s - loss: 2.7834 - val_loss: 3.1721
Epoch 248/5000
26/26 - 1s - loss: 2.7834 - val_loss: 3.1681
Epoch 249/5000
26/26 - 1s - loss: 2.7780 - val_loss: 3.1633
Epoch 250/5000
26/26 - 1s - loss: 2.7730 - val_loss: 3.1608
Epoch 00250: val_loss improved from 3.19002 to 3.16076, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 251/5000
26/26 - 1s - loss: 2.7686 - val_loss: 3.1585
Epoch 252/5000
26/26 - 1s - loss: 2.7670 - val_loss: 3.1575
Epoch 253/5000
26/26 - 1s - loss: 2.7623 - val_loss: 3.1544
Epoch 254/5000
26/26 - 1s - loss: 2.7620 - val_loss: 3.1517
Epoch 255/5000
26/26 - 1s - loss: 2.7585 - val_loss: 3.1491
Epoch 256/5000
26/26 - 1s - loss: 2.7523 - val_loss: 3.1464
Epoch 257/5000
26/26 - 1s - loss: 2.7487 - val_loss: 3.1436
Epoch 258/5000
26/26 - 1s - loss: 2.7443 - val_loss: 3.1415
Epoch 259/5000
26/26 - 1s - loss: 2.7442 - val_loss: 3.1366
Epoch 260/5000
26/26 - 1s - loss: 2.7414 - val_loss: 3.1324
Epoch 00260: val_loss improved from 3.16076 to 3.13244, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 261/5000
26/26 - 1s - loss: 2.7392 - val_loss: 3.1294
Epoch 262/5000
26/26 - 1s - loss: 2.7348 - val_loss: 3.1255
Epoch 263/5000
26/26 - 1s - loss: 2.7311 - val_loss: 3.1218
Epoch 264/5000
26/26 - 1s - loss: 2.7294 - val_loss: 3.1202
Epoch 265/5000
26/26 - 1s - loss: 2.7232 - val_loss: 3.1164
Epoch 266/5000
26/26 - 1s - loss: 2.7225 - val_loss: 3.1130
Epoch 267/5000
26/26 - 1s - loss: 2.7200 - val_loss: 3.1093
Epoch 268/5000
26/26 - 1s - loss: 2.7153 - val_loss: 3.1071
Epoch 269/5000
26/26 - 1s - loss: 2.7122 - val_loss: 3.1036
Epoch 270/5000
26/26 - 1s - loss: 2.7111 - val_loss: 3.1011
Epoch 00270: val_loss improved from 3.13244 to 3.10111, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 271/5000
26/26 - 1s - loss: 2.7048 - val_loss: 3.0982
Epoch 272/5000
26/26 - 1s - loss: 2.7042 - val_loss: 3.0942
Epoch 273/5000
26/26 - 1s - loss: 2.7016 - val_loss: 3.0924
Epoch 274/5000
26/26 - 1s - loss: 2.6948 - val_loss: 3.0897
Epoch 275/5000
26/26 - 1s - loss: 2.6941 - val_loss: 3.0878
Epoch 276/5000
26/26 - 1s - loss: 2.6938 - val_loss: 3.0834
Epoch 277/5000
26/26 - 1s - loss: 2.6877 - val_loss: 3.0797
Epoch 278/5000
26/26 - 1s - loss: 2.6842 - val_loss: 3.0770
Epoch 279/5000
26/26 - 1s - loss: 2.6821 - val_loss: 3.0747
Epoch 280/5000
26/26 - 1s - loss: 2.6787 - val_loss: 3.0722
Epoch 00280: val_loss improved from 3.10111 to 3.07221, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 281/5000
26/26 - 1s - loss: 2.6757 - val_loss: 3.0684
Epoch 282/5000
26/26 - 1s - loss: 2.6746 - val_loss: 3.0650
Epoch 283/5000
26/26 - 1s - loss: 2.6681 - val_loss: 3.0628
Epoch 284/5000
26/26 - 1s - loss: 2.6675 - val_loss: 3.0617
Epoch 285/5000
26/26 - 1s - loss: 2.6629 - val_loss: 3.0592
Epoch 286/5000
26/26 - 1s - loss: 2.6593 - val_loss: 3.0579
Epoch 287/5000
26/26 - 1s - loss: 2.6567 - val_loss: 3.0536
Epoch 288/5000
26/26 - 1s - loss: 2.6536 - val_loss: 3.0496
Epoch 289/5000
26/26 - 2s - loss: 2.6516 - val_loss: 3.0463
Epoch 290/5000
26/26 - 1s - loss: 2.6458 - val_loss: 3.0445
Epoch 00290: val_loss improved from 3.07221 to 3.04447, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 291/5000
26/26 - 1s - loss: 2.6447 - val_loss: 3.0420
Epoch 292/5000
26/26 - 1s - loss: 2.6424 - val_loss: 3.0393
Epoch 293/5000
26/26 - 1s - loss: 2.6394 - val_loss: 3.0372
Epoch 294/5000
26/26 - 1s - loss: 2.6391 - val_loss: 3.0335
Epoch 295/5000
26/26 - 1s - loss: 2.6315 - val_loss: 3.0312
Epoch 296/5000
26/26 - 1s - loss: 2.6303 - val_loss: 3.0274
Epoch 297/5000
26/26 - 1s - loss: 2.6281 - val_loss: 3.0239
Epoch 298/5000
26/26 - 1s - loss: 2.6241 - val_loss: 3.0219
Epoch 299/5000
26/26 - 1s - loss: 2.6190 - val_loss: 3.0202
Epoch 300/5000
26/26 - 1s - loss: 2.6192 - val_loss: 3.0176
Epoch 00300: val_loss improved from 3.04447 to 3.01757, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 301/5000
26/26 - 1s - loss: 2.6156 - val_loss: 3.0163
Epoch 302/5000
26/26 - 1s - loss: 2.6147 - val_loss: 3.0130
Epoch 303/5000
26/26 - 1s - loss: 2.6119 - val_loss: 3.0083
Epoch 304/5000
26/26 - 1s - loss: 2.6088 - val_loss: 3.0063
Epoch 305/5000
26/26 - 1s - loss: 2.6032 - val_loss: 3.0031
Epoch 306/5000
26/26 - 1s - loss: 2.5999 - val_loss: 3.0004
Epoch 307/5000
26/26 - 1s - loss: 2.5960 - val_loss: 2.9986
Epoch 308/5000
26/26 - 1s - loss: 2.5946 - val_loss: 2.9973
Epoch 309/5000
26/26 - 1s - loss: 2.5911 - val_loss: 2.9935
Epoch 310/5000
26/26 - 1s - loss: 2.5898 - val_loss: 2.9898
Epoch 00310: val_loss improved from 3.01757 to 2.98976, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 311/5000
26/26 - 1s - loss: 2.5858 - val_loss: 2.9863
Epoch 312/5000
26/26 - 1s - loss: 2.5838 - val_loss: 2.9830
Epoch 313/5000
26/26 - 1s - loss: 2.5813 - val_loss: 2.9821
Epoch 314/5000
26/26 - 1s - loss: 2.5804 - val_loss: 2.9790
Epoch 315/5000
26/26 - 1s - loss: 2.5738 - val_loss: 2.9758
Epoch 316/5000
26/26 - 1s - loss: 2.5730 - val_loss: 2.9735
Epoch 317/5000
26/26 - 1s - loss: 2.5711 - val_loss: 2.9707
Epoch 318/5000
26/26 - 1s - loss: 2.5683 - val_loss: 2.9675
Epoch 319/5000
26/26 - 1s - loss: 2.5626 - val_loss: 2.9671
Epoch 320/5000
26/26 - 1s - loss: 2.5631 - val_loss: 2.9637
Epoch 00320: val_loss improved from 2.98976 to 2.96369, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 321/5000
26/26 - 1s - loss: 2.5591 - val_loss: 2.9613
Epoch 322/5000
26/26 - 1s - loss: 2.5564 - val_loss: 2.9585
Epoch 323/5000
26/26 - 1s - loss: 2.5538 - val_loss: 2.9558
Epoch 324/5000
26/26 - 1s - loss: 2.5519 - val_loss: 2.9517
Epoch 325/5000
26/26 - 1s - loss: 2.5483 - val_loss: 2.9496
Epoch 326/5000
26/26 - 2s - loss: 2.5457 - val_loss: 2.9474
Epoch 327/5000
26/26 - 1s - loss: 2.5425 - val_loss: 2.9457
Epoch 328/5000
26/26 - 1s - loss: 2.5416 - val_loss: 2.9423
Epoch 329/5000
26/26 - 1s - loss: 2.5353 - val_loss: 2.9394
Epoch 330/5000
26/26 - 1s - loss: 2.5333 - val_loss: 2.9361
Epoch 00330: val_loss improved from 2.96369 to 2.93611, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 331/5000
26/26 - 1s - loss: 2.5329 - val_loss: 2.9344
Epoch 332/5000
26/26 - 1s - loss: 2.5283 - val_loss: 2.9327
Epoch 333/5000
26/26 - 1s - loss: 2.5260 - val_loss: 2.9302
Epoch 334/5000
26/26 - 1s - loss: 2.5221 - val_loss: 2.9273
Epoch 335/5000
26/26 - 1s - loss: 2.5182 - val_loss: 2.9240
Epoch 336/5000
26/26 - 1s - loss: 2.5184 - val_loss: 2.9217
Epoch 337/5000
26/26 - 1s - loss: 2.5150 - val_loss: 2.9196
Epoch 338/5000
26/26 - 1s - loss: 2.5131 - val_loss: 2.9163
Epoch 339/5000
26/26 - 1s - loss: 2.5083 - val_loss: 2.9144
Epoch 340/5000
26/26 - 2s - loss: 2.5061 - val_loss: 2.9123
Epoch 00340: val_loss improved from 2.93611 to 2.91229, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 341/5000
26/26 - 1s - loss: 2.5049 - val_loss: 2.9078
Epoch 342/5000
26/26 - 1s - loss: 2.5028 - val_loss: 2.9054
Epoch 343/5000
26/26 - 1s - loss: 2.4983 - val_loss: 2.9032
Epoch 344/5000
26/26 - 2s - loss: 2.4940 - val_loss: 2.9005
Epoch 345/5000
26/26 - 2s - loss: 2.4928 - val_loss: 2.8974
Epoch 346/5000
26/26 - 2s - loss: 2.4919 - val_loss: 2.8962
Epoch 347/5000
26/26 - 2s - loss: 2.4875 - val_loss: 2.8935
Epoch 348/5000
26/26 - 2s - loss: 2.4856 - val_loss: 2.8901
Epoch 349/5000
26/26 - 1s - loss: 2.4799 - val_loss: 2.8873
Epoch 350/5000
26/26 - 1s - loss: 2.4767 - val_loss: 2.8860
Epoch 00350: val_loss improved from 2.91229 to 2.88605, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 351/5000
26/26 - 1s - loss: 2.4786 - val_loss: 2.8849
Epoch 352/5000
26/26 - 1s - loss: 2.4750 - val_loss: 2.8824
Epoch 353/5000
26/26 - 2s - loss: 2.4719 - val_loss: 2.8804
Epoch 354/5000
26/26 - 1s - loss: 2.4697 - val_loss: 2.8777
Epoch 355/5000
26/26 - 1s - loss: 2.4692 - val_loss: 2.8744
Epoch 356/5000
26/26 - 2s - loss: 2.4659 - val_loss: 2.8721
Epoch 357/5000
26/26 - 2s - loss: 2.4615 - val_loss: 2.8702
Epoch 358/5000
26/26 - 1s - loss: 2.4582 - val_loss: 2.8672
Epoch 359/5000
26/26 - 2s - loss: 2.4552 - val_loss: 2.8647
Epoch 360/5000
26/26 - 1s - loss: 2.4527 - val_loss: 2.8617
Epoch 00360: val_loss improved from 2.88605 to 2.86170, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 361/5000
26/26 - 2s - loss: 2.4536 - val_loss: 2.8590
Epoch 362/5000
26/26 - 2s - loss: 2.4491 - val_loss: 2.8571
Epoch 363/5000
26/26 - 2s - loss: 2.4473 - val_loss: 2.8549
Epoch 364/5000
26/26 - 2s - loss: 2.4445 - val_loss: 2.8524
Epoch 365/5000
26/26 - 2s - loss: 2.4399 - val_loss: 2.8494
Epoch 366/5000
26/26 - 1s - loss: 2.4386 - val_loss: 2.8481
Epoch 367/5000
26/26 - 1s - loss: 2.4357 - val_loss: 2.8459
Epoch 368/5000
26/26 - 1s - loss: 2.4333 - val_loss: 2.8417
Epoch 369/5000
26/26 - 2s - loss: 2.4302 - val_loss: 2.8391
Epoch 370/5000
26/26 - 1s - loss: 2.4269 - val_loss: 2.8363
Epoch 00370: val_loss improved from 2.86170 to 2.83634, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 371/5000
26/26 - 1s - loss: 2.4253 - val_loss: 2.8340
Epoch 372/5000
26/26 - 1s - loss: 2.4233 - val_loss: 2.8317
Epoch 373/5000
26/26 - 1s - loss: 2.4184 - val_loss: 2.8300
Epoch 374/5000
26/26 - 1s - loss: 2.4195 - val_loss: 2.8270
Epoch 375/5000
26/26 - 1s - loss: 2.4156 - val_loss: 2.8247
Epoch 376/5000
26/26 - 1s - loss: 2.4135 - val_loss: 2.8235
Epoch 377/5000
26/26 - 1s - loss: 2.4106 - val_loss: 2.8216
Epoch 378/5000
26/26 - 1s - loss: 2.4079 - val_loss: 2.8196
Epoch 379/5000
26/26 - 1s - loss: 2.4047 - val_loss: 2.8162
Epoch 380/5000
26/26 - 1s - loss: 2.4027 - val_loss: 2.8135
Epoch 00380: val_loss improved from 2.83634 to 2.81354, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 381/5000
26/26 - 1s - loss: 2.3996 - val_loss: 2.8114
Epoch 382/5000
26/26 - 1s - loss: 2.3971 - val_loss: 2.8093
Epoch 383/5000
26/26 - 1s - loss: 2.3960 - val_loss: 2.8062
Epoch 384/5000
26/26 - 1s - loss: 2.3921 - val_loss: 2.8046
Epoch 385/5000
26/26 - 1s - loss: 2.3925 - val_loss: 2.8022
Epoch 386/5000
26/26 - 1s - loss: 2.3872 - val_loss: 2.8000
Epoch 387/5000
26/26 - 1s - loss: 2.3864 - val_loss: 2.7966
Epoch 388/5000
26/26 - 2s - loss: 2.3834 - val_loss: 2.7947
Epoch 389/5000
26/26 - 1s - loss: 2.3813 - val_loss: 2.7924
Epoch 390/5000
26/26 - 1s - loss: 2.3780 - val_loss: 2.7891
Epoch 00390: val_loss improved from 2.81354 to 2.78907, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 391/5000
26/26 - 1s - loss: 2.3776 - val_loss: 2.7875
Epoch 392/5000
26/26 - 1s - loss: 2.3730 - val_loss: 2.7854
Epoch 393/5000
26/26 - 1s - loss: 2.3726 - val_loss: 2.7833
Epoch 394/5000
26/26 - 1s - loss: 2.3678 - val_loss: 2.7812
Epoch 395/5000
26/26 - 1s - loss: 2.3681 - val_loss: 2.7787
Epoch 396/5000
26/26 - 1s - loss: 2.3645 - val_loss: 2.7761
Epoch 397/5000
26/26 - 1s - loss: 2.3625 - val_loss: 2.7736
Epoch 398/5000
26/26 - 1s - loss: 2.3595 - val_loss: 2.7724
Epoch 399/5000
26/26 - 1s - loss: 2.3579 - val_loss: 2.7698
Epoch 400/5000
26/26 - 1s - loss: 2.3528 - val_loss: 2.7675
Epoch 00400: val_loss improved from 2.78907 to 2.76753, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 401/5000
26/26 - 1s - loss: 2.3493 - val_loss: 2.7651
Epoch 402/5000
26/26 - 1s - loss: 2.3497 - val_loss: 2.7622
Epoch 403/5000
26/26 - 1s - loss: 2.3454 - val_loss: 2.7603
Epoch 404/5000
26/26 - 1s - loss: 2.3428 - val_loss: 2.7568
Epoch 405/5000
26/26 - 1s - loss: 2.3427 - val_loss: 2.7554
Epoch 406/5000
26/26 - 1s - loss: 2.3403 - val_loss: 2.7535
Epoch 407/5000
26/26 - 1s - loss: 2.3363 - val_loss: 2.7515
Epoch 408/5000
26/26 - 1s - loss: 2.3352 - val_loss: 2.7472
Epoch 409/5000
26/26 - 1s - loss: 2.3343 - val_loss: 2.7456
Epoch 410/5000
26/26 - 1s - loss: 2.3300 - val_loss: 2.7434
Epoch 00410: val_loss improved from 2.76753 to 2.74338, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 411/5000
26/26 - 1s - loss: 2.3290 - val_loss: 2.7415
Epoch 412/5000
26/26 - 1s - loss: 2.3277 - val_loss: 2.7383
Epoch 413/5000
26/26 - 1s - loss: 2.3208 - val_loss: 2.7370
Epoch 414/5000
26/26 - 1s - loss: 2.3235 - val_loss: 2.7347
Epoch 415/5000
26/26 - 1s - loss: 2.3209 - val_loss: 2.7323
Epoch 416/5000
26/26 - 1s - loss: 2.3188 - val_loss: 2.7299
Epoch 417/5000
26/26 - 1s - loss: 2.3142 - val_loss: 2.7279
Epoch 418/5000
26/26 - 1s - loss: 2.3103 - val_loss: 2.7258
Epoch 419/5000
26/26 - 1s - loss: 2.3090 - val_loss: 2.7236
Epoch 420/5000
26/26 - 1s - loss: 2.3066 - val_loss: 2.7211
Epoch 00420: val_loss improved from 2.74338 to 2.72113, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 421/5000
26/26 - 1s - loss: 2.3064 - val_loss: 2.7175
Epoch 422/5000
26/26 - 1s - loss: 2.3009 - val_loss: 2.7159
Epoch 423/5000
26/26 - 1s - loss: 2.3013 - val_loss: 2.7142
Epoch 424/5000
26/26 - 1s - loss: 2.2996 - val_loss: 2.7124
Epoch 425/5000
26/26 - 1s - loss: 2.2955 - val_loss: 2.7114
Epoch 426/5000
26/26 - 1s - loss: 2.2934 - val_loss: 2.7079
Epoch 427/5000
26/26 - 1s - loss: 2.2899 - val_loss: 2.7056
Epoch 428/5000
26/26 - 1s - loss: 2.2864 - val_loss: 2.7042
Epoch 429/5000
26/26 - 1s - loss: 2.2868 - val_loss: 2.7019
Epoch 430/5000
26/26 - 1s - loss: 2.2836 - val_loss: 2.7004
Epoch 00430: val_loss improved from 2.72113 to 2.70040, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 431/5000
26/26 - 1s - loss: 2.2834 - val_loss: 2.6978
Epoch 432/5000
26/26 - 1s - loss: 2.2805 - val_loss: 2.6942
Epoch 433/5000
26/26 - 1s - loss: 2.2798 - val_loss: 2.6916
Epoch 434/5000
26/26 - 1s - loss: 2.2728 - val_loss: 2.6906
Epoch 435/5000
26/26 - 1s - loss: 2.2702 - val_loss: 2.6891
Epoch 436/5000
26/26 - 1s - loss: 2.2704 - val_loss: 2.6862
Epoch 437/5000
26/26 - 1s - loss: 2.2708 - val_loss: 2.6836
Epoch 438/5000
26/26 - 1s - loss: 2.2639 - val_loss: 2.6820
Epoch 439/5000
26/26 - 1s - loss: 2.2634 - val_loss: 2.6798
Epoch 440/5000
26/26 - 1s - loss: 2.2622 - val_loss: 2.6790
Epoch 00440: val_loss improved from 2.70040 to 2.67903, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 441/5000
26/26 - 1s - loss: 2.2599 - val_loss: 2.6760
Epoch 442/5000
26/26 - 1s - loss: 2.2578 - val_loss: 2.6750
Epoch 443/5000
26/26 - 1s - loss: 2.2550 - val_loss: 2.6721
Epoch 444/5000
26/26 - 1s - loss: 2.2519 - val_loss: 2.6698
Epoch 445/5000
26/26 - 1s - loss: 2.2518 - val_loss: 2.6662
Epoch 446/5000
26/26 - 1s - loss: 2.2504 - val_loss: 2.6648
Epoch 447/5000
26/26 - 1s - loss: 2.2482 - val_loss: 2.6619
Epoch 448/5000
26/26 - 2s - loss: 2.2449 - val_loss: 2.6600
Epoch 449/5000
26/26 - 1s - loss: 2.2423 - val_loss: 2.6589
Epoch 450/5000
26/26 - 1s - loss: 2.2399 - val_loss: 2.6575
Epoch 00450: val_loss improved from 2.67903 to 2.65746, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 451/5000
26/26 - 1s - loss: 2.2363 - val_loss: 2.6544
Epoch 452/5000
26/26 - 1s - loss: 2.2366 - val_loss: 2.6519
Epoch 453/5000
26/26 - 1s - loss: 2.2333 - val_loss: 2.6486
Epoch 454/5000
26/26 - 1s - loss: 2.2300 - val_loss: 2.6481
Epoch 455/5000
26/26 - 1s - loss: 2.2289 - val_loss: 2.6460
Epoch 456/5000
26/26 - 1s - loss: 2.2264 - val_loss: 2.6440
Epoch 457/5000
26/26 - 1s - loss: 2.2255 - val_loss: 2.6428
Epoch 458/5000
26/26 - 1s - loss: 2.2214 - val_loss: 2.6398
Epoch 459/5000
26/26 - 1s - loss: 2.2201 - val_loss: 2.6362
Epoch 460/5000
26/26 - 1s - loss: 2.2201 - val_loss: 2.6354
Epoch 00460: val_loss improved from 2.65746 to 2.63541, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 461/5000
26/26 - 1s - loss: 2.2150 - val_loss: 2.6325
Epoch 462/5000
26/26 - 1s - loss: 2.2137 - val_loss: 2.6299
Epoch 463/5000
26/26 - 1s - loss: 2.2098 - val_loss: 2.6288
Epoch 464/5000
26/26 - 1s - loss: 2.2096 - val_loss: 2.6258
Epoch 465/5000
26/26 - 1s - loss: 2.2068 - val_loss: 2.6239
Epoch 466/5000
26/26 - 1s - loss: 2.2036 - val_loss: 2.6225
Epoch 467/5000
26/26 - 1s - loss: 2.2037 - val_loss: 2.6203
Epoch 468/5000
26/26 - 1s - loss: 2.2005 - val_loss: 2.6169
Epoch 469/5000
26/26 - 1s - loss: 2.1983 - val_loss: 2.6156
Epoch 470/5000
26/26 - 1s - loss: 2.1948 - val_loss: 2.6133
Epoch 00470: val_loss improved from 2.63541 to 2.61325, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 471/5000
26/26 - 1s - loss: 2.1950 - val_loss: 2.6105
Epoch 472/5000
26/26 - 1s - loss: 2.1936 - val_loss: 2.6094
Epoch 473/5000
26/26 - 1s - loss: 2.1885 - val_loss: 2.6074
Epoch 474/5000
26/26 - 1s - loss: 2.1879 - val_loss: 2.6068
Epoch 475/5000
26/26 - 1s - loss: 2.1850 - val_loss: 2.6038
Epoch 476/5000
26/26 - 1s - loss: 2.1832 - val_loss: 2.6026
Epoch 477/5000
26/26 - 1s - loss: 2.1805 - val_loss: 2.6002
Epoch 478/5000
26/26 - 1s - loss: 2.1807 - val_loss: 2.5981
Epoch 479/5000
26/26 - 1s - loss: 2.1717 - val_loss: 2.5954
Epoch 480/5000
26/26 - 1s - loss: 2.1757 - val_loss: 2.5933
Epoch 00480: val_loss improved from 2.61325 to 2.59334, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 481/5000
26/26 - 1s - loss: 2.1739 - val_loss: 2.5916
Epoch 482/5000
26/26 - 1s - loss: 2.1698 - val_loss: 2.5908
Epoch 483/5000
26/26 - 1s - loss: 2.1691 - val_loss: 2.5888
Epoch 484/5000
26/26 - 1s - loss: 2.1647 - val_loss: 2.5846
Epoch 485/5000
26/26 - 1s - loss: 2.1627 - val_loss: 2.5837
Epoch 486/5000
26/26 - 1s - loss: 2.1633 - val_loss: 2.5809
Epoch 487/5000
26/26 - 1s - loss: 2.1592 - val_loss: 2.5798
Epoch 488/5000
26/26 - 1s - loss: 2.1590 - val_loss: 2.5759
Epoch 489/5000
26/26 - 1s - loss: 2.1567 - val_loss: 2.5753
Epoch 490/5000
26/26 - 2s - loss: 2.1522 - val_loss: 2.5731
Epoch 00490: val_loss improved from 2.59334 to 2.57309, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 491/5000
26/26 - 1s - loss: 2.1494 - val_loss: 2.5712
Epoch 492/5000
26/26 - 1s - loss: 2.1510 - val_loss: 2.5681
Epoch 493/5000
26/26 - 1s - loss: 2.1467 - val_loss: 2.5674
Epoch 494/5000
26/26 - 1s - loss: 2.1452 - val_loss: 2.5649
Epoch 495/5000
26/26 - 1s - loss: 2.1439 - val_loss: 2.5628
Epoch 496/5000
26/26 - 1s - loss: 2.1432 - val_loss: 2.5605
Epoch 497/5000
26/26 - 1s - loss: 2.1411 - val_loss: 2.5590
Epoch 498/5000
26/26 - 1s - loss: 2.1366 - val_loss: 2.5569
Epoch 499/5000
26/26 - 1s - loss: 2.1359 - val_loss: 2.5555
Epoch 500/5000
26/26 - 1s - loss: 2.1332 - val_loss: 2.5517
Epoch 00500: val_loss improved from 2.57309 to 2.55170, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 501/5000
26/26 - 1s - loss: 2.1305 - val_loss: 2.5499
Epoch 502/5000
26/26 - 1s - loss: 2.1279 - val_loss: 2.5483
Epoch 503/5000
26/26 - 1s - loss: 2.1279 - val_loss: 2.5475
Epoch 504/5000
26/26 - 1s - loss: 2.1231 - val_loss: 2.5452
Epoch 505/5000
26/26 - 1s - loss: 2.1236 - val_loss: 2.5424
Epoch 506/5000
26/26 - 1s - loss: 2.1185 - val_loss: 2.5409
Epoch 507/5000
26/26 - 1s - loss: 2.1193 - val_loss: 2.5396
Epoch 508/5000
26/26 - 1s - loss: 2.1144 - val_loss: 2.5382
Epoch 509/5000
26/26 - 1s - loss: 2.1140 - val_loss: 2.5340
Epoch 510/5000
26/26 - 1s - loss: 2.1127 - val_loss: 2.5328
Epoch 00510: val_loss improved from 2.55170 to 2.53275, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 511/5000
26/26 - 1s - loss: 2.1110 - val_loss: 2.5320
Epoch 512/5000
26/26 - 1s - loss: 2.1101 - val_loss: 2.5307
Epoch 513/5000
26/26 - 1s - loss: 2.1070 - val_loss: 2.5283
Epoch 514/5000
26/26 - 1s - loss: 2.1048 - val_loss: 2.5270
Epoch 515/5000
26/26 - 1s - loss: 2.1018 - val_loss: 2.5247
Epoch 516/5000
26/26 - 1s - loss: 2.1010 - val_loss: 2.5208
Epoch 517/5000
26/26 - 1s - loss: 2.0989 - val_loss: 2.5202
Epoch 518/5000
26/26 - 1s - loss: 2.0969 - val_loss: 2.5159
Epoch 519/5000
26/26 - 1s - loss: 2.0945 - val_loss: 2.5152
Epoch 520/5000
26/26 - 1s - loss: 2.0913 - val_loss: 2.5124
Epoch 00520: val_loss improved from 2.53275 to 2.51243, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 521/5000
26/26 - 1s - loss: 2.0908 - val_loss: 2.5104
Epoch 522/5000
26/26 - 1s - loss: 2.0879 - val_loss: 2.5085
Epoch 523/5000
26/26 - 1s - loss: 2.0883 - val_loss: 2.5078
Epoch 524/5000
26/26 - 1s - loss: 2.0858 - val_loss: 2.5037
Epoch 525/5000
26/26 - 1s - loss: 2.0814 - val_loss: 2.5026
Epoch 526/5000
26/26 - 1s - loss: 2.0806 - val_loss: 2.5012
Epoch 527/5000
26/26 - 1s - loss: 2.0806 - val_loss: 2.5010
Epoch 528/5000
26/26 - 1s - loss: 2.0778 - val_loss: 2.4993
Epoch 529/5000
26/26 - 1s - loss: 2.0724 - val_loss: 2.4957
Epoch 530/5000
26/26 - 1s - loss: 2.0748 - val_loss: 2.4930
Epoch 00530: val_loss improved from 2.51243 to 2.49295, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 531/5000
26/26 - 1s - loss: 2.0727 - val_loss: 2.4922
Epoch 532/5000
26/26 - 1s - loss: 2.0685 - val_loss: 2.4907
Epoch 533/5000
26/26 - 1s - loss: 2.0688 - val_loss: 2.4889
Epoch 534/5000
26/26 - 1s - loss: 2.0647 - val_loss: 2.4879
Epoch 535/5000
26/26 - 1s - loss: 2.0637 - val_loss: 2.4849
Epoch 536/5000
26/26 - 1s - loss: 2.0607 - val_loss: 2.4830
Epoch 537/5000
26/26 - 1s - loss: 2.0605 - val_loss: 2.4816
Epoch 538/5000
26/26 - 1s - loss: 2.0602 - val_loss: 2.4803
Epoch 539/5000
26/26 - 1s - loss: 2.0558 - val_loss: 2.4784
Epoch 540/5000
26/26 - 1s - loss: 2.0539 - val_loss: 2.4773
Epoch 00540: val_loss improved from 2.49295 to 2.47727, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 541/5000
26/26 - 1s - loss: 2.0532 - val_loss: 2.4737
Epoch 542/5000
26/26 - 1s - loss: 2.0526 - val_loss: 2.4714
Epoch 543/5000
26/26 - 1s - loss: 2.0492 - val_loss: 2.4702
Epoch 544/5000
26/26 - 2s - loss: 2.0462 - val_loss: 2.4671
Epoch 545/5000
26/26 - 1s - loss: 2.0436 - val_loss: 2.4659
Epoch 546/5000
26/26 - 1s - loss: 2.0454 - val_loss: 2.4636
Epoch 547/5000
26/26 - 2s - loss: 2.0404 - val_loss: 2.4620
Epoch 548/5000
26/26 - 1s - loss: 2.0380 - val_loss: 2.4614
Epoch 549/5000
26/26 - 1s - loss: 2.0379 - val_loss: 2.4593
Epoch 550/5000
26/26 - 1s - loss: 2.0346 - val_loss: 2.4569
Epoch 00550: val_loss improved from 2.47727 to 2.45690, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 551/5000
26/26 - 1s - loss: 2.0340 - val_loss: 2.4552
Epoch 552/5000
26/26 - 1s - loss: 2.0343 - val_loss: 2.4531
Epoch 553/5000
26/26 - 1s - loss: 2.0288 - val_loss: 2.4517
Epoch 554/5000
26/26 - 1s - loss: 2.0257 - val_loss: 2.4497
Epoch 555/5000
26/26 - 1s - loss: 2.0256 - val_loss: 2.4476
Epoch 556/5000
26/26 - 1s - loss: 2.0240 - val_loss: 2.4461
Epoch 557/5000
26/26 - 1s - loss: 2.0233 - val_loss: 2.4441
Epoch 558/5000
26/26 - 1s - loss: 2.0185 - val_loss: 2.4427
Epoch 559/5000
26/26 - 1s - loss: 2.0163 - val_loss: 2.4408
Epoch 560/5000
26/26 - 1s - loss: 2.0161 - val_loss: 2.4392
Epoch 00560: val_loss improved from 2.45690 to 2.43917, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 561/5000
26/26 - 1s - loss: 2.0154 - val_loss: 2.4358
Epoch 562/5000
26/26 - 1s - loss: 2.0144 - val_loss: 2.4350
Epoch 563/5000
26/26 - 1s - loss: 2.0125 - val_loss: 2.4344
Epoch 564/5000
26/26 - 1s - loss: 2.0090 - val_loss: 2.4312
Epoch 565/5000
26/26 - 1s - loss: 2.0079 - val_loss: 2.4294
Epoch 566/5000
26/26 - 1s - loss: 2.0062 - val_loss: 2.4279
Epoch 567/5000
26/26 - 1s - loss: 2.0037 - val_loss: 2.4249
Epoch 568/5000
26/26 - 1s - loss: 1.9992 - val_loss: 2.4230
Epoch 569/5000
26/26 - 1s - loss: 2.0015 - val_loss: 2.4224
Epoch 570/5000
26/26 - 1s - loss: 1.9980 - val_loss: 2.4183
Epoch 00570: val_loss improved from 2.43917 to 2.41827, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 571/5000
26/26 - 1s - loss: 1.9955 - val_loss: 2.4188
Epoch 572/5000
26/26 - 1s - loss: 1.9944 - val_loss: 2.4156
Epoch 573/5000
26/26 - 2s - loss: 1.9934 - val_loss: 2.4134
Epoch 574/5000
26/26 - 1s - loss: 1.9907 - val_loss: 2.4121
Epoch 575/5000
26/26 - 1s - loss: 1.9892 - val_loss: 2.4120
Epoch 576/5000
26/26 - 1s - loss: 1.9897 - val_loss: 2.4107
Epoch 577/5000
26/26 - 1s - loss: 1.9854 - val_loss: 2.4088
Epoch 578/5000
26/26 - 1s - loss: 1.9843 - val_loss: 2.4072
Epoch 579/5000
26/26 - 1s - loss: 1.9825 - val_loss: 2.4053
Epoch 580/5000
26/26 - 1s - loss: 1.9799 - val_loss: 2.4041
Epoch 00580: val_loss improved from 2.41827 to 2.40412, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 581/5000
26/26 - 1s - loss: 1.9791 - val_loss: 2.4019
Epoch 582/5000
26/26 - 1s - loss: 1.9769 - val_loss: 2.4001
Epoch 583/5000
26/26 - 1s - loss: 1.9751 - val_loss: 2.3990
Epoch 584/5000
26/26 - 1s - loss: 1.9724 - val_loss: 2.3965
Epoch 585/5000
26/26 - 1s - loss: 1.9711 - val_loss: 2.3944
Epoch 586/5000
26/26 - 1s - loss: 1.9683 - val_loss: 2.3932
Epoch 587/5000
26/26 - 1s - loss: 1.9664 - val_loss: 2.3909
Epoch 588/5000
26/26 - 1s - loss: 1.9640 - val_loss: 2.3885
Epoch 589/5000
26/26 - 1s - loss: 1.9639 - val_loss: 2.3864
Epoch 590/5000
26/26 - 1s - loss: 1.9626 - val_loss: 2.3841
Epoch 00590: val_loss improved from 2.40412 to 2.38410, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 591/5000
26/26 - 1s - loss: 1.9585 - val_loss: 2.3826
Epoch 592/5000
26/26 - 1s - loss: 1.9579 - val_loss: 2.3808
Epoch 593/5000
26/26 - 1s - loss: 1.9574 - val_loss: 2.3794
Epoch 594/5000
26/26 - 1s - loss: 1.9567 - val_loss: 2.3786
Epoch 595/5000
26/26 - 1s - loss: 1.9525 - val_loss: 2.3744
Epoch 596/5000
26/26 - 1s - loss: 1.9517 - val_loss: 2.3734
Epoch 597/5000
26/26 - 1s - loss: 1.9505 - val_loss: 2.3714
Epoch 598/5000
26/26 - 1s - loss: 1.9469 - val_loss: 2.3709
Epoch 599/5000
26/26 - 1s - loss: 1.9436 - val_loss: 2.3701
Epoch 600/5000
26/26 - 1s - loss: 1.9424 - val_loss: 2.3673
Epoch 00600: val_loss improved from 2.38410 to 2.36730, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 601/5000
26/26 - 1s - loss: 1.9439 - val_loss: 2.3655
Epoch 602/5000
26/26 - 1s - loss: 1.9407 - val_loss: 2.3645
Epoch 603/5000
26/26 - 1s - loss: 1.9389 - val_loss: 2.3626
Epoch 604/5000
26/26 - 1s - loss: 1.9358 - val_loss: 2.3601
Epoch 605/5000
26/26 - 2s - loss: 1.9366 - val_loss: 2.3583
Epoch 606/5000
26/26 - 1s - loss: 1.9335 - val_loss: 2.3562
Epoch 607/5000
26/26 - 1s - loss: 1.9298 - val_loss: 2.3554
Epoch 608/5000
26/26 - 1s - loss: 1.9316 - val_loss: 2.3536
Epoch 609/5000
26/26 - 1s - loss: 1.9280 - val_loss: 2.3514
Epoch 610/5000
26/26 - 1s - loss: 1.9276 - val_loss: 2.3499
Epoch 00610: val_loss improved from 2.36730 to 2.34988, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 611/5000
26/26 - 1s - loss: 1.9266 - val_loss: 2.3474
Epoch 612/5000
26/26 - 1s - loss: 1.9241 - val_loss: 2.3450
Epoch 613/5000
26/26 - 1s - loss: 1.9226 - val_loss: 2.3441
Epoch 614/5000
26/26 - 2s - loss: 1.9187 - val_loss: 2.3421
Epoch 615/5000
26/26 - 1s - loss: 1.9208 - val_loss: 2.3398
Epoch 616/5000
26/26 - 1s - loss: 1.9140 - val_loss: 2.3402
Epoch 617/5000
26/26 - 1s - loss: 1.9114 - val_loss: 2.3374
Epoch 618/5000
26/26 - 1s - loss: 1.9135 - val_loss: 2.3366
Epoch 619/5000
26/26 - 1s - loss: 1.9108 - val_loss: 2.3351
Epoch 620/5000
26/26 - 1s - loss: 1.9082 - val_loss: 2.3327
Epoch 00620: val_loss improved from 2.34988 to 2.33268, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 621/5000
26/26 - 1s - loss: 1.9053 - val_loss: 2.3325
Epoch 622/5000
26/26 - 1s - loss: 1.9065 - val_loss: 2.3302
Epoch 623/5000
26/26 - 1s - loss: 1.9054 - val_loss: 2.3288
Epoch 624/5000
26/26 - 1s - loss: 1.9006 - val_loss: 2.3270
Epoch 625/5000
26/26 - 1s - loss: 1.8986 - val_loss: 2.3251
Epoch 626/5000
26/26 - 1s - loss: 1.8975 - val_loss: 2.3259
Epoch 627/5000
26/26 - 1s - loss: 1.8999 - val_loss: 2.3219
Epoch 628/5000
26/26 - 1s - loss: 1.8951 - val_loss: 2.3184
Epoch 629/5000
26/26 - 1s - loss: 1.8943 - val_loss: 2.3190
Epoch 630/5000
26/26 - 1s - loss: 1.8930 - val_loss: 2.3163
Epoch 00630: val_loss improved from 2.33268 to 2.31633, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 631/5000
26/26 - 1s - loss: 1.8898 - val_loss: 2.3147
Epoch 632/5000
26/26 - 1s - loss: 1.8884 - val_loss: 2.3130
Epoch 633/5000
26/26 - 1s - loss: 1.8856 - val_loss: 2.3128
Epoch 634/5000
26/26 - 1s - loss: 1.8846 - val_loss: 2.3108
Epoch 635/5000
26/26 - 1s - loss: 1.8848 - val_loss: 2.3078
Epoch 636/5000
26/26 - 1s - loss: 1.8823 - val_loss: 2.3082
Epoch 637/5000
26/26 - 1s - loss: 1.8799 - val_loss: 2.3075
Epoch 638/5000
26/26 - 1s - loss: 1.8798 - val_loss: 2.3038
Epoch 639/5000
26/26 - 1s - loss: 1.8778 - val_loss: 2.3028
Epoch 640/5000
26/26 - 1s - loss: 1.8759 - val_loss: 2.3008
Epoch 00640: val_loss improved from 2.31633 to 2.30080, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 641/5000
26/26 - 1s - loss: 1.8744 - val_loss: 2.2996
Epoch 642/5000
26/26 - 1s - loss: 1.8701 - val_loss: 2.2985
Epoch 643/5000
26/26 - 1s - loss: 1.8699 - val_loss: 2.2966
Epoch 644/5000
26/26 - 1s - loss: 1.8684 - val_loss: 2.2948
Epoch 645/5000
26/26 - 1s - loss: 1.8659 - val_loss: 2.2919
Epoch 646/5000
26/26 - 1s - loss: 1.8649 - val_loss: 2.2908
Epoch 647/5000
26/26 - 1s - loss: 1.8631 - val_loss: 2.2905
Epoch 648/5000
26/26 - 1s - loss: 1.8621 - val_loss: 2.2880
Epoch 649/5000
26/26 - 1s - loss: 1.8602 - val_loss: 2.2838
Epoch 650/5000
26/26 - 1s - loss: 1.8592 - val_loss: 2.2853
Epoch 00650: val_loss improved from 2.30080 to 2.28526, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 651/5000
26/26 - 1s - loss: 1.8572 - val_loss: 2.2813
Epoch 652/5000
26/26 - 1s - loss: 1.8556 - val_loss: 2.2814
Epoch 653/5000
26/26 - 1s - loss: 1.8524 - val_loss: 2.2796
Epoch 654/5000
26/26 - 1s - loss: 1.8537 - val_loss: 2.2780
Epoch 655/5000
26/26 - 1s - loss: 1.8497 - val_loss: 2.2765
Epoch 656/5000
26/26 - 1s - loss: 1.8489 - val_loss: 2.2745
Epoch 657/5000
26/26 - 1s - loss: 1.8465 - val_loss: 2.2736
Epoch 658/5000
26/26 - 1s - loss: 1.8457 - val_loss: 2.2730
Epoch 659/5000
26/26 - 1s - loss: 1.8430 - val_loss: 2.2713
Epoch 660/5000
26/26 - 1s - loss: 1.8429 - val_loss: 2.2696
Epoch 00660: val_loss improved from 2.28526 to 2.26957, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 661/5000
26/26 - 1s - loss: 1.8391 - val_loss: 2.2680
Epoch 662/5000
26/26 - 1s - loss: 1.8385 - val_loss: 2.2644
Epoch 663/5000
26/26 - 1s - loss: 1.8374 - val_loss: 2.2643
Epoch 664/5000
26/26 - 1s - loss: 1.8363 - val_loss: 2.2631
Epoch 665/5000
26/26 - 1s - loss: 1.8339 - val_loss: 2.2616
Epoch 666/5000
26/26 - 1s - loss: 1.8315 - val_loss: 2.2597
Epoch 667/5000
26/26 - 1s - loss: 1.8316 - val_loss: 2.2588
Epoch 668/5000
26/26 - 1s - loss: 1.8285 - val_loss: 2.2558
Epoch 669/5000
26/26 - 1s - loss: 1.8288 - val_loss: 2.2551
Epoch 670/5000
26/26 - 1s - loss: 1.8246 - val_loss: 2.2520
Epoch 00670: val_loss improved from 2.26957 to 2.25204, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 671/5000
26/26 - 1s - loss: 1.8248 - val_loss: 2.2520
Epoch 672/5000
26/26 - 1s - loss: 1.8225 - val_loss: 2.2507
Epoch 673/5000
26/26 - 1s - loss: 1.8203 - val_loss: 2.2500
Epoch 674/5000
26/26 - 1s - loss: 1.8187 - val_loss: 2.2471
Epoch 675/5000
26/26 - 1s - loss: 1.8176 - val_loss: 2.2456
Epoch 676/5000
26/26 - 1s - loss: 1.8193 - val_loss: 2.2460
Epoch 677/5000
26/26 - 1s - loss: 1.8165 - val_loss: 2.2428
Epoch 678/5000
26/26 - 1s - loss: 1.8129 - val_loss: 2.2398
Epoch 679/5000
26/26 - 1s - loss: 1.8113 - val_loss: 2.2379
Epoch 680/5000
26/26 - 1s - loss: 1.8118 - val_loss: 2.2363
Epoch 00680: val_loss improved from 2.25204 to 2.23634, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 681/5000
26/26 - 1s - loss: 1.8107 - val_loss: 2.2357
Epoch 682/5000
26/26 - 1s - loss: 1.8075 - val_loss: 2.2345
Epoch 683/5000
26/26 - 1s - loss: 1.8049 - val_loss: 2.2339
Epoch 684/5000
26/26 - 1s - loss: 1.8047 - val_loss: 2.2304
Epoch 685/5000
26/26 - 1s - loss: 1.8030 - val_loss: 2.2273
Epoch 686/5000
26/26 - 1s - loss: 1.8023 - val_loss: 2.2261
Epoch 687/5000
26/26 - 1s - loss: 1.7996 - val_loss: 2.2262
Epoch 688/5000
26/26 - 1s - loss: 1.7980 - val_loss: 2.2244
Epoch 689/5000
26/26 - 1s - loss: 1.7964 - val_loss: 2.2230
Epoch 690/5000
26/26 - 1s - loss: 1.7955 - val_loss: 2.2222
Epoch 00690: val_loss improved from 2.23634 to 2.22225, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 691/5000
26/26 - 1s - loss: 1.7956 - val_loss: 2.2198
Epoch 692/5000
26/26 - 1s - loss: 1.7925 - val_loss: 2.2198
Epoch 693/5000
26/26 - 1s - loss: 1.7900 - val_loss: 2.2170
Epoch 694/5000
26/26 - 1s - loss: 1.7878 - val_loss: 2.2160
Epoch 695/5000
26/26 - 1s - loss: 1.7867 - val_loss: 2.2138
Epoch 696/5000
26/26 - 1s - loss: 1.7850 - val_loss: 2.2129
Epoch 697/5000
26/26 - 2s - loss: 1.7859 - val_loss: 2.2115
Epoch 698/5000
26/26 - 1s - loss: 1.7840 - val_loss: 2.2087
Epoch 699/5000
26/26 - 1s - loss: 1.7808 - val_loss: 2.2078
Epoch 700/5000
26/26 - 1s - loss: 1.7782 - val_loss: 2.2076
Epoch 00700: val_loss improved from 2.22225 to 2.20760, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 701/5000
26/26 - 1s - loss: 1.7794 - val_loss: 2.2061
Epoch 702/5000
26/26 - 1s - loss: 1.7770 - val_loss: 2.2037
Epoch 703/5000
26/26 - 1s - loss: 1.7756 - val_loss: 2.2017
Epoch 704/5000
26/26 - 1s - loss: 1.7753 - val_loss: 2.1997
Epoch 705/5000
26/26 - 1s - loss: 1.7724 - val_loss: 2.2009
Epoch 706/5000
26/26 - 1s - loss: 1.7702 - val_loss: 2.1995
Epoch 707/5000
26/26 - 1s - loss: 1.7690 - val_loss: 2.1975
Epoch 708/5000
26/26 - 1s - loss: 1.7664 - val_loss: 2.1969
Epoch 709/5000
26/26 - 1s - loss: 1.7651 - val_loss: 2.1948
Epoch 710/5000
26/26 - 1s - loss: 1.7661 - val_loss: 2.1929
Epoch 00710: val_loss improved from 2.20760 to 2.19286, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 711/5000
26/26 - 1s - loss: 1.7630 - val_loss: 2.1915
Epoch 712/5000
26/26 - 1s - loss: 1.7620 - val_loss: 2.1902
Epoch 713/5000
26/26 - 1s - loss: 1.7614 - val_loss: 2.1875
Epoch 714/5000
26/26 - 1s - loss: 1.7568 - val_loss: 2.1851
Epoch 715/5000
26/26 - 1s - loss: 1.7575 - val_loss: 2.1833
Epoch 716/5000
26/26 - 1s - loss: 1.7559 - val_loss: 2.1829
Epoch 717/5000
26/26 - 1s - loss: 1.7547 - val_loss: 2.1816
Epoch 718/5000
26/26 - 1s - loss: 1.7523 - val_loss: 2.1791
Epoch 719/5000
26/26 - 1s - loss: 1.7510 - val_loss: 2.1780
Epoch 720/5000
26/26 - 1s - loss: 1.7490 - val_loss: 2.1765
Epoch 00720: val_loss improved from 2.19286 to 2.17647, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 721/5000
26/26 - 1s - loss: 1.7482 - val_loss: 2.1762
Epoch 722/5000
26/26 - 1s - loss: 1.7468 - val_loss: 2.1747
Epoch 723/5000
26/26 - 1s - loss: 1.7447 - val_loss: 2.1715
Epoch 724/5000
26/26 - 1s - loss: 1.7443 - val_loss: 2.1694
Epoch 725/5000
26/26 - 1s - loss: 1.7427 - val_loss: 2.1691
Epoch 726/5000
26/26 - 1s - loss: 1.7418 - val_loss: 2.1683
Epoch 727/5000
26/26 - 2s - loss: 1.7385 - val_loss: 2.1648
Epoch 728/5000
26/26 - 1s - loss: 1.7377 - val_loss: 2.1646
Epoch 729/5000
26/26 - 1s - loss: 1.7344 - val_loss: 2.1641
Epoch 730/5000
26/26 - 1s - loss: 1.7345 - val_loss: 2.1615
Epoch 00730: val_loss improved from 2.17647 to 2.16152, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 731/5000
26/26 - 1s - loss: 1.7339 - val_loss: 2.1607
Epoch 732/5000
26/26 - 1s - loss: 1.7299 - val_loss: 2.1600
Epoch 733/5000
26/26 - 1s - loss: 1.7310 - val_loss: 2.1591
Epoch 734/5000
26/26 - 1s - loss: 1.7298 - val_loss: 2.1590
Epoch 735/5000
26/26 - 1s - loss: 1.7267 - val_loss: 2.1583
Epoch 736/5000
26/26 - 1s - loss: 1.7255 - val_loss: 2.1553
Epoch 737/5000
26/26 - 1s - loss: 1.7246 - val_loss: 2.1529
Epoch 738/5000
26/26 - 1s - loss: 1.7242 - val_loss: 2.1511
Epoch 739/5000
26/26 - 2s - loss: 1.7243 - val_loss: 2.1493
Epoch 740/5000
26/26 - 1s - loss: 1.7192 - val_loss: 2.1493
Epoch 00740: val_loss improved from 2.16152 to 2.14934, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 741/5000
26/26 - 1s - loss: 1.7183 - val_loss: 2.1471
Epoch 742/5000
26/26 - 1s - loss: 1.7178 - val_loss: 2.1467
Epoch 743/5000
26/26 - 1s - loss: 1.7140 - val_loss: 2.1433
Epoch 744/5000
26/26 - 1s - loss: 1.7136 - val_loss: 2.1419
Epoch 745/5000
26/26 - 1s - loss: 1.7121 - val_loss: 2.1407
Epoch 746/5000
26/26 - 1s - loss: 1.7108 - val_loss: 2.1395
Epoch 747/5000
26/26 - 1s - loss: 1.7097 - val_loss: 2.1395
Epoch 748/5000
26/26 - 1s - loss: 1.7082 - val_loss: 2.1353
Epoch 749/5000
26/26 - 1s - loss: 1.7069 - val_loss: 2.1346
Epoch 750/5000
26/26 - 1s - loss: 1.7059 - val_loss: 2.1338
Epoch 00750: val_loss improved from 2.14934 to 2.13383, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 751/5000
26/26 - 1s - loss: 1.7031 - val_loss: 2.1330
Epoch 752/5000
26/26 - 1s - loss: 1.7031 - val_loss: 2.1301
Epoch 753/5000
26/26 - 1s - loss: 1.7005 - val_loss: 2.1294
Epoch 754/5000
26/26 - 1s - loss: 1.6989 - val_loss: 2.1295
Epoch 755/5000
26/26 - 1s - loss: 1.6983 - val_loss: 2.1289
Epoch 756/5000
26/26 - 1s - loss: 1.6958 - val_loss: 2.1269
Epoch 757/5000
26/26 - 1s - loss: 1.6960 - val_loss: 2.1230
Epoch 758/5000
26/26 - 1s - loss: 1.6940 - val_loss: 2.1209
Epoch 759/5000
26/26 - 1s - loss: 1.6904 - val_loss: 2.1195
Epoch 760/5000
26/26 - 1s - loss: 1.6911 - val_loss: 2.1196
Epoch 00760: val_loss improved from 2.13383 to 2.11960, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 761/5000
26/26 - 1s - loss: 1.6884 - val_loss: 2.1190
Epoch 762/5000
26/26 - 1s - loss: 1.6876 - val_loss: 2.1168
Epoch 763/5000
26/26 - 1s - loss: 1.6860 - val_loss: 2.1152
Epoch 764/5000
26/26 - 1s - loss: 1.6867 - val_loss: 2.1136
Epoch 765/5000
26/26 - 1s - loss: 1.6835 - val_loss: 2.1135
Epoch 766/5000
26/26 - 1s - loss: 1.6812 - val_loss: 2.1122
Epoch 767/5000
26/26 - 1s - loss: 1.6815 - val_loss: 2.1117
Epoch 768/5000
26/26 - 1s - loss: 1.6821 - val_loss: 2.1089
Epoch 769/5000
26/26 - 1s - loss: 1.6787 - val_loss: 2.1079
Epoch 770/5000
26/26 - 1s - loss: 1.6783 - val_loss: 2.1064
Epoch 00770: val_loss improved from 2.11960 to 2.10639, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 771/5000
26/26 - 1s - loss: 1.6753 - val_loss: 2.1049
Epoch 772/5000
26/26 - 1s - loss: 1.6737 - val_loss: 2.1014
Epoch 773/5000
26/26 - 1s - loss: 1.6734 - val_loss: 2.1025
Epoch 774/5000
26/26 - 1s - loss: 1.6711 - val_loss: 2.1021
Epoch 775/5000
26/26 - 1s - loss: 1.6697 - val_loss: 2.0994
Epoch 776/5000
26/26 - 1s - loss: 1.6685 - val_loss: 2.0994
Epoch 777/5000
26/26 - 1s - loss: 1.6657 - val_loss: 2.0973
Epoch 778/5000
26/26 - 1s - loss: 1.6653 - val_loss: 2.0969
Epoch 779/5000
26/26 - 1s - loss: 1.6622 - val_loss: 2.0934
Epoch 780/5000
26/26 - 1s - loss: 1.6617 - val_loss: 2.0938
Epoch 00780: val_loss improved from 2.10639 to 2.09378, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 781/5000
26/26 - 1s - loss: 1.6618 - val_loss: 2.0930
Epoch 782/5000
26/26 - 1s - loss: 1.6617 - val_loss: 2.0900
Epoch 783/5000
26/26 - 1s - loss: 1.6591 - val_loss: 2.0869
Epoch 784/5000
26/26 - 1s - loss: 1.6572 - val_loss: 2.0872
Epoch 785/5000
26/26 - 1s - loss: 1.6583 - val_loss: 2.0845
Epoch 786/5000
26/26 - 1s - loss: 1.6544 - val_loss: 2.0849
Epoch 787/5000
26/26 - 1s - loss: 1.6545 - val_loss: 2.0837
Epoch 788/5000
26/26 - 1s - loss: 1.6514 - val_loss: 2.0807
Epoch 789/5000
26/26 - 1s - loss: 1.6499 - val_loss: 2.0809
Epoch 790/5000
26/26 - 1s - loss: 1.6498 - val_loss: 2.0790
Epoch 00790: val_loss improved from 2.09378 to 2.07898, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 791/5000
26/26 - 1s - loss: 1.6474 - val_loss: 2.0776
Epoch 792/5000
26/26 - 1s - loss: 1.6476 - val_loss: 2.0754
Epoch 793/5000
26/26 - 1s - loss: 1.6449 - val_loss: 2.0745
Epoch 794/5000
26/26 - 1s - loss: 1.6423 - val_loss: 2.0735
Epoch 795/5000
26/26 - 1s - loss: 1.6421 - val_loss: 2.0730
Epoch 796/5000
26/26 - 1s - loss: 1.6417 - val_loss: 2.0706
Epoch 797/5000
26/26 - 1s - loss: 1.6419 - val_loss: 2.0697
Epoch 798/5000
26/26 - 1s - loss: 1.6398 - val_loss: 2.0680
Epoch 799/5000
26/26 - 1s - loss: 1.6361 - val_loss: 2.0682
Epoch 800/5000
26/26 - 1s - loss: 1.6358 - val_loss: 2.0659
Epoch 00800: val_loss improved from 2.07898 to 2.06592, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 801/5000
26/26 - 1s - loss: 1.6353 - val_loss: 2.0660
Epoch 802/5000
26/26 - 1s - loss: 1.6317 - val_loss: 2.0646
Epoch 803/5000
26/26 - 1s - loss: 1.6320 - val_loss: 2.0604
Epoch 804/5000
26/26 - 1s - loss: 1.6302 - val_loss: 2.0608
Epoch 805/5000
26/26 - 1s - loss: 1.6290 - val_loss: 2.0596
Epoch 806/5000
26/26 - 1s - loss: 1.6279 - val_loss: 2.0579
Epoch 807/5000
26/26 - 1s - loss: 1.6279 - val_loss: 2.0561
Epoch 808/5000
26/26 - 1s - loss: 1.6216 - val_loss: 2.0557
Epoch 809/5000
26/26 - 1s - loss: 1.6246 - val_loss: 2.0558
Epoch 810/5000
26/26 - 1s - loss: 1.6244 - val_loss: 2.0531
Epoch 00810: val_loss improved from 2.06592 to 2.05314, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 811/5000
26/26 - 1s - loss: 1.6220 - val_loss: 2.0523
Epoch 812/5000
26/26 - 1s - loss: 1.6201 - val_loss: 2.0498
Epoch 813/5000
26/26 - 1s - loss: 1.6207 - val_loss: 2.0498
Epoch 814/5000
26/26 - 1s - loss: 1.6176 - val_loss: 2.0480
Epoch 815/5000
26/26 - 1s - loss: 1.6144 - val_loss: 2.0466
Epoch 816/5000
26/26 - 1s - loss: 1.6143 - val_loss: 2.0456
Epoch 817/5000
26/26 - 1s - loss: 1.6138 - val_loss: 2.0448
Epoch 818/5000
26/26 - 1s - loss: 1.6116 - val_loss: 2.0428
Epoch 819/5000
26/26 - 1s - loss: 1.6093 - val_loss: 2.0415
Epoch 820/5000
26/26 - 1s - loss: 1.6102 - val_loss: 2.0422
Epoch 00820: val_loss improved from 2.05314 to 2.04218, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 821/5000
26/26 - 1s - loss: 1.6110 - val_loss: 2.0397
Epoch 822/5000
26/26 - 1s - loss: 1.6086 - val_loss: 2.0387
Epoch 823/5000
26/26 - 1s - loss: 1.6072 - val_loss: 2.0357
Epoch 824/5000
26/26 - 1s - loss: 1.6040 - val_loss: 2.0358
Epoch 825/5000
26/26 - 1s - loss: 1.6035 - val_loss: 2.0327
Epoch 826/5000
26/26 - 1s - loss: 1.6028 - val_loss: 2.0305
Epoch 827/5000
26/26 - 1s - loss: 1.6016 - val_loss: 2.0321
Epoch 828/5000
26/26 - 1s - loss: 1.5959 - val_loss: 2.0295
Epoch 829/5000
26/26 - 1s - loss: 1.5972 - val_loss: 2.0285
Epoch 830/5000
26/26 - 1s - loss: 1.5984 - val_loss: 2.0273
Epoch 00830: val_loss improved from 2.04218 to 2.02729, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 831/5000
26/26 - 1s - loss: 1.5942 - val_loss: 2.0267
Epoch 832/5000
26/26 - 1s - loss: 1.5942 - val_loss: 2.0244
Epoch 833/5000
26/26 - 1s - loss: 1.5941 - val_loss: 2.0232
Epoch 834/5000
26/26 - 1s - loss: 1.5903 - val_loss: 2.0221
Epoch 835/5000
26/26 - 1s - loss: 1.5893 - val_loss: 2.0214
Epoch 836/5000
26/26 - 1s - loss: 1.5906 - val_loss: 2.0206
Epoch 837/5000
26/26 - 1s - loss: 1.5889 - val_loss: 2.0186
Epoch 838/5000
26/26 - 1s - loss: 1.5858 - val_loss: 2.0166
Epoch 839/5000
26/26 - 1s - loss: 1.5857 - val_loss: 2.0166
Epoch 840/5000
26/26 - 1s - loss: 1.5835 - val_loss: 2.0141
Epoch 00840: val_loss improved from 2.02729 to 2.01412, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 841/5000
26/26 - 1s - loss: 1.5838 - val_loss: 2.0132
Epoch 842/5000
26/26 - 1s - loss: 1.5801 - val_loss: 2.0134
Epoch 843/5000
26/26 - 1s - loss: 1.5786 - val_loss: 2.0126
Epoch 844/5000
26/26 - 1s - loss: 1.5815 - val_loss: 2.0105
Epoch 845/5000
26/26 - 1s - loss: 1.5774 - val_loss: 2.0097
Epoch 846/5000
26/26 - 1s - loss: 1.5765 - val_loss: 2.0076
Epoch 847/5000
26/26 - 1s - loss: 1.5756 - val_loss: 2.0074
Epoch 848/5000
26/26 - 1s - loss: 1.5745 - val_loss: 2.0052
Epoch 849/5000
26/26 - 1s - loss: 1.5740 - val_loss: 2.0042
Epoch 850/5000
26/26 - 1s - loss: 1.5721 - val_loss: 2.0027
Epoch 00850: val_loss improved from 2.01412 to 2.00274, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 851/5000
26/26 - 1s - loss: 1.5687 - val_loss: 2.0032
Epoch 852/5000
26/26 - 1s - loss: 1.5701 - val_loss: 2.0010
Epoch 853/5000
26/26 - 1s - loss: 1.5691 - val_loss: 2.0000
Epoch 854/5000
26/26 - 1s - loss: 1.5649 - val_loss: 1.9978
Epoch 855/5000
26/26 - 1s - loss: 1.5669 - val_loss: 1.9947
Epoch 856/5000
26/26 - 1s - loss: 1.5626 - val_loss: 1.9958
Epoch 857/5000
26/26 - 1s - loss: 1.5629 - val_loss: 1.9958
Epoch 858/5000
26/26 - 1s - loss: 1.5620 - val_loss: 1.9928
Epoch 859/5000
26/26 - 1s - loss: 1.5613 - val_loss: 1.9929
Epoch 860/5000
26/26 - 1s - loss: 1.5572 - val_loss: 1.9904
Epoch 00860: val_loss improved from 2.00274 to 1.99037, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 861/5000
26/26 - 1s - loss: 1.5583 - val_loss: 1.9892
Epoch 862/5000
26/26 - 1s - loss: 1.5554 - val_loss: 1.9892
Epoch 863/5000
26/26 - 2s - loss: 1.5550 - val_loss: 1.9869
Epoch 864/5000
26/26 - 1s - loss: 1.5530 - val_loss: 1.9868
Epoch 865/5000
26/26 - 1s - loss: 1.5532 - val_loss: 1.9855
Epoch 866/5000
26/26 - 1s - loss: 1.5526 - val_loss: 1.9817
Epoch 867/5000
26/26 - 1s - loss: 1.5487 - val_loss: 1.9810
Epoch 868/5000
26/26 - 1s - loss: 1.5503 - val_loss: 1.9781
Epoch 869/5000
26/26 - 1s - loss: 1.5475 - val_loss: 1.9785
Epoch 870/5000
26/26 - 1s - loss: 1.5464 - val_loss: 1.9758
Epoch 00870: val_loss improved from 1.99037 to 1.97579, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 871/5000
26/26 - 1s - loss: 1.5448 - val_loss: 1.9746
Epoch 872/5000
26/26 - 1s - loss: 1.5440 - val_loss: 1.9738
Epoch 873/5000
26/26 - 1s - loss: 1.5427 - val_loss: 1.9738
Epoch 874/5000
26/26 - 1s - loss: 1.5435 - val_loss: 1.9719
Epoch 875/5000
26/26 - 1s - loss: 1.5427 - val_loss: 1.9702
Epoch 876/5000
26/26 - 1s - loss: 1.5394 - val_loss: 1.9686
Epoch 877/5000
26/26 - 1s - loss: 1.5395 - val_loss: 1.9691
Epoch 878/5000
26/26 - 1s - loss: 1.5359 - val_loss: 1.9691
Epoch 879/5000
26/26 - 1s - loss: 1.5356 - val_loss: 1.9671
Epoch 880/5000
26/26 - 1s - loss: 1.5338 - val_loss: 1.9657
Epoch 00880: val_loss improved from 1.97579 to 1.96575, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 881/5000
26/26 - 1s - loss: 1.5333 - val_loss: 1.9642
Epoch 882/5000
26/26 - 1s - loss: 1.5316 - val_loss: 1.9626
Epoch 883/5000
26/26 - 2s - loss: 1.5324 - val_loss: 1.9622
Epoch 884/5000
26/26 - 1s - loss: 1.5292 - val_loss: 1.9589
Epoch 885/5000
26/26 - 1s - loss: 1.5286 - val_loss: 1.9607
Epoch 886/5000
26/26 - 1s - loss: 1.5274 - val_loss: 1.9610
Epoch 887/5000
26/26 - 1s - loss: 1.5261 - val_loss: 1.9585
Epoch 888/5000
26/26 - 1s - loss: 1.5238 - val_loss: 1.9573
Epoch 889/5000
26/26 - 1s - loss: 1.5234 - val_loss: 1.9548
Epoch 890/5000
26/26 - 1s - loss: 1.5222 - val_loss: 1.9556
Epoch 00890: val_loss improved from 1.96575 to 1.95556, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 891/5000
26/26 - 1s - loss: 1.5205 - val_loss: 1.9535
Epoch 892/5000
26/26 - 1s - loss: 1.5184 - val_loss: 1.9516
Epoch 893/5000
26/26 - 1s - loss: 1.5185 - val_loss: 1.9491
Epoch 894/5000
26/26 - 1s - loss: 1.5172 - val_loss: 1.9492
Epoch 895/5000
26/26 - 1s - loss: 1.5174 - val_loss: 1.9469
Epoch 896/5000
26/26 - 1s - loss: 1.5142 - val_loss: 1.9469
Epoch 897/5000
26/26 - 1s - loss: 1.5143 - val_loss: 1.9459
Epoch 898/5000
26/26 - 1s - loss: 1.5119 - val_loss: 1.9442
Epoch 899/5000
26/26 - 1s - loss: 1.5114 - val_loss: 1.9421
Epoch 900/5000
26/26 - 1s - loss: 1.5114 - val_loss: 1.9411
Epoch 00900: val_loss improved from 1.95556 to 1.94115, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 901/5000
26/26 - 1s - loss: 1.5089 - val_loss: 1.9404
Epoch 902/5000
26/26 - 1s - loss: 1.5076 - val_loss: 1.9405
Epoch 903/5000
26/26 - 1s - loss: 1.5077 - val_loss: 1.9397
Epoch 904/5000
26/26 - 1s - loss: 1.5055 - val_loss: 1.9380
Epoch 905/5000
26/26 - 1s - loss: 1.5030 - val_loss: 1.9362
Epoch 906/5000
26/26 - 1s - loss: 1.5031 - val_loss: 1.9342
Epoch 907/5000
26/26 - 1s - loss: 1.5023 - val_loss: 1.9345
Epoch 908/5000
26/26 - 1s - loss: 1.5019 - val_loss: 1.9329
Epoch 909/5000
26/26 - 1s - loss: 1.5008 - val_loss: 1.9308
Epoch 910/5000
26/26 - 1s - loss: 1.4984 - val_loss: 1.9304
Epoch 00910: val_loss improved from 1.94115 to 1.93036, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 911/5000
26/26 - 1s - loss: 1.4964 - val_loss: 1.9298
Epoch 912/5000
26/26 - 1s - loss: 1.4968 - val_loss: 1.9286
Epoch 913/5000
26/26 - 1s - loss: 1.4959 - val_loss: 1.9269
Epoch 914/5000
26/26 - 1s - loss: 1.4937 - val_loss: 1.9268
Epoch 915/5000
26/26 - 2s - loss: 1.4915 - val_loss: 1.9260
Epoch 916/5000
26/26 - 1s - loss: 1.4920 - val_loss: 1.9240
Epoch 917/5000
26/26 - 1s - loss: 1.4896 - val_loss: 1.9211
Epoch 918/5000
26/26 - 1s - loss: 1.4895 - val_loss: 1.9212
Epoch 919/5000
26/26 - 1s - loss: 1.4884 - val_loss: 1.9196
Epoch 920/5000
26/26 - 1s - loss: 1.4872 - val_loss: 1.9178
Epoch 00920: val_loss improved from 1.93036 to 1.91776, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 921/5000
26/26 - 1s - loss: 1.4851 - val_loss: 1.9169
Epoch 922/5000
26/26 - 1s - loss: 1.4856 - val_loss: 1.9164
Epoch 923/5000
26/26 - 1s - loss: 1.4846 - val_loss: 1.9167
Epoch 924/5000
26/26 - 1s - loss: 1.4830 - val_loss: 1.9150
Epoch 925/5000
26/26 - 1s - loss: 1.4807 - val_loss: 1.9141
Epoch 926/5000
26/26 - 1s - loss: 1.4797 - val_loss: 1.9142
Epoch 927/5000
26/26 - 1s - loss: 1.4806 - val_loss: 1.9132
Epoch 928/5000
26/26 - 1s - loss: 1.4773 - val_loss: 1.9094
Epoch 929/5000
26/26 - 1s - loss: 1.4765 - val_loss: 1.9078
Epoch 930/5000
26/26 - 1s - loss: 1.4753 - val_loss: 1.9082
Epoch 00930: val_loss improved from 1.91776 to 1.90815, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 931/5000
26/26 - 1s - loss: 1.4757 - val_loss: 1.9050
Epoch 932/5000
26/26 - 1s - loss: 1.4740 - val_loss: 1.9064
Epoch 933/5000
26/26 - 1s - loss: 1.4717 - val_loss: 1.9036
Epoch 934/5000
26/26 - 1s - loss: 1.4708 - val_loss: 1.9026
Epoch 935/5000
26/26 - 2s - loss: 1.4695 - val_loss: 1.9041
Epoch 936/5000
26/26 - 1s - loss: 1.4692 - val_loss: 1.9028
Epoch 937/5000
26/26 - 1s - loss: 1.4674 - val_loss: 1.9020
Epoch 938/5000
26/26 - 1s - loss: 1.4668 - val_loss: 1.8993
Epoch 939/5000
26/26 - 1s - loss: 1.4667 - val_loss: 1.8985
Epoch 940/5000
26/26 - 1s - loss: 1.4642 - val_loss: 1.8976
Epoch 00940: val_loss improved from 1.90815 to 1.89758, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 941/5000
26/26 - 1s - loss: 1.4636 - val_loss: 1.8954
Epoch 942/5000
26/26 - 1s - loss: 1.4615 - val_loss: 1.8953
Epoch 943/5000
26/26 - 1s - loss: 1.4615 - val_loss: 1.8943
Epoch 944/5000
26/26 - 1s - loss: 1.4594 - val_loss: 1.8923
Epoch 945/5000
26/26 - 1s - loss: 1.4602 - val_loss: 1.8892
Epoch 946/5000
26/26 - 2s - loss: 1.4579 - val_loss: 1.8900
Epoch 947/5000
26/26 - 1s - loss: 1.4552 - val_loss: 1.8892
Epoch 948/5000
26/26 - 1s - loss: 1.4550 - val_loss: 1.8881
Epoch 949/5000
26/26 - 1s - loss: 1.4542 - val_loss: 1.8850
Epoch 950/5000
26/26 - 2s - loss: 1.4534 - val_loss: 1.8849
Epoch 00950: val_loss improved from 1.89758 to 1.88494, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 951/5000
26/26 - 1s - loss: 1.4533 - val_loss: 1.8845
Epoch 952/5000
26/26 - 2s - loss: 1.4516 - val_loss: 1.8839
Epoch 953/5000
26/26 - 1s - loss: 1.4501 - val_loss: 1.8823
Epoch 954/5000
26/26 - 1s - loss: 1.4476 - val_loss: 1.8812
Epoch 955/5000
26/26 - 1s - loss: 1.4486 - val_loss: 1.8795
Epoch 956/5000
26/26 - 1s - loss: 1.4453 - val_loss: 1.8786
Epoch 957/5000
26/26 - 1s - loss: 1.4449 - val_loss: 1.8805
Epoch 958/5000
26/26 - 1s - loss: 1.4447 - val_loss: 1.8762
Epoch 959/5000
26/26 - 1s - loss: 1.4441 - val_loss: 1.8744
Epoch 960/5000
26/26 - 1s - loss: 1.4428 - val_loss: 1.8746
Epoch 00960: val_loss improved from 1.88494 to 1.87459, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 961/5000
26/26 - 1s - loss: 1.4400 - val_loss: 1.8734
Epoch 962/5000
26/26 - 1s - loss: 1.4403 - val_loss: 1.8724
Epoch 963/5000
26/26 - 1s - loss: 1.4398 - val_loss: 1.8713
Epoch 964/5000
26/26 - 1s - loss: 1.4369 - val_loss: 1.8713
Epoch 965/5000
26/26 - 1s - loss: 1.4372 - val_loss: 1.8703
Epoch 966/5000
26/26 - 1s - loss: 1.4367 - val_loss: 1.8691
Epoch 967/5000
26/26 - 2s - loss: 1.4352 - val_loss: 1.8672
Epoch 968/5000
26/26 - 1s - loss: 1.4322 - val_loss: 1.8668
Epoch 969/5000
26/26 - 1s - loss: 1.4329 - val_loss: 1.8659
Epoch 970/5000
26/26 - 1s - loss: 1.4304 - val_loss: 1.8649
Epoch 00970: val_loss improved from 1.87459 to 1.86493, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 971/5000
26/26 - 1s - loss: 1.4299 - val_loss: 1.8639
Epoch 972/5000
26/26 - 1s - loss: 1.4286 - val_loss: 1.8625
Epoch 973/5000
26/26 - 1s - loss: 1.4289 - val_loss: 1.8610
Epoch 974/5000
26/26 - 1s - loss: 1.4265 - val_loss: 1.8597
Epoch 975/5000
26/26 - 1s - loss: 1.4263 - val_loss: 1.8604
Epoch 976/5000
26/26 - 1s - loss: 1.4246 - val_loss: 1.8584
Epoch 977/5000
26/26 - 1s - loss: 1.4234 - val_loss: 1.8579
Epoch 978/5000
26/26 - 1s - loss: 1.4215 - val_loss: 1.8557
Epoch 979/5000
26/26 - 1s - loss: 1.4219 - val_loss: 1.8557
Epoch 980/5000
26/26 - 1s - loss: 1.4201 - val_loss: 1.8540
Epoch 00980: val_loss improved from 1.86493 to 1.85397, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 981/5000
26/26 - 1s - loss: 1.4170 - val_loss: 1.8537
Epoch 982/5000
26/26 - 1s - loss: 1.4187 - val_loss: 1.8520
Epoch 983/5000
26/26 - 1s - loss: 1.4180 - val_loss: 1.8516
Epoch 984/5000
26/26 - 1s - loss: 1.4149 - val_loss: 1.8497
Epoch 985/5000
26/26 - 1s - loss: 1.4173 - val_loss: 1.8484
Epoch 986/5000
26/26 - 1s - loss: 1.4143 - val_loss: 1.8475
Epoch 987/5000
26/26 - 1s - loss: 1.4121 - val_loss: 1.8456
Epoch 988/5000
26/26 - 1s - loss: 1.4114 - val_loss: 1.8465
Epoch 989/5000
26/26 - 2s - loss: 1.4109 - val_loss: 1.8448
Epoch 990/5000
26/26 - 1s - loss: 1.4105 - val_loss: 1.8432
Epoch 00990: val_loss improved from 1.85397 to 1.84325, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 991/5000
26/26 - 1s - loss: 1.4077 - val_loss: 1.8428
Epoch 992/5000
26/26 - 1s - loss: 1.4064 - val_loss: 1.8418
Epoch 993/5000
26/26 - 1s - loss: 1.4073 - val_loss: 1.8407
Epoch 994/5000
26/26 - 1s - loss: 1.4045 - val_loss: 1.8400
Epoch 995/5000
26/26 - 1s - loss: 1.4032 - val_loss: 1.8400
Epoch 996/5000
26/26 - 1s - loss: 1.4039 - val_loss: 1.8393
Epoch 997/5000
26/26 - 1s - loss: 1.4028 - val_loss: 1.8372
Epoch 998/5000
26/26 - 1s - loss: 1.4010 - val_loss: 1.8359
Epoch 999/5000
26/26 - 1s - loss: 1.4000 - val_loss: 1.8355
Epoch 1000/5000
26/26 - 1s - loss: 1.3981 - val_loss: 1.8361
Epoch 01000: val_loss improved from 1.84325 to 1.83605, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1001/5000
26/26 - 1s - loss: 1.3965 - val_loss: 1.8330
Epoch 1002/5000
26/26 - 1s - loss: 1.3978 - val_loss: 1.8308
Epoch 1003/5000
26/26 - 1s - loss: 1.3967 - val_loss: 1.8307
Epoch 1004/5000
26/26 - 1s - loss: 1.3954 - val_loss: 1.8297
Epoch 1005/5000
26/26 - 1s - loss: 1.3939 - val_loss: 1.8293
Epoch 1006/5000
26/26 - 1s - loss: 1.3935 - val_loss: 1.8273
Epoch 1007/5000
26/26 - 1s - loss: 1.3911 - val_loss: 1.8266
Epoch 1008/5000
26/26 - 1s - loss: 1.3910 - val_loss: 1.8246
Epoch 1009/5000
26/26 - 1s - loss: 1.3896 - val_loss: 1.8247
Epoch 1010/5000
26/26 - 1s - loss: 1.3896 - val_loss: 1.8235
Epoch 01010: val_loss improved from 1.83605 to 1.82352, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1011/5000
26/26 - 1s - loss: 1.3873 - val_loss: 1.8214
Epoch 1012/5000
26/26 - 1s - loss: 1.3857 - val_loss: 1.8218
Epoch 1013/5000
26/26 - 1s - loss: 1.3857 - val_loss: 1.8216
Epoch 1014/5000
26/26 - 1s - loss: 1.3839 - val_loss: 1.8210
Epoch 1015/5000
26/26 - 1s - loss: 1.3834 - val_loss: 1.8196
Epoch 1016/5000
26/26 - 1s - loss: 1.3833 - val_loss: 1.8186
Epoch 1017/5000
26/26 - 1s - loss: 1.3819 - val_loss: 1.8179
Epoch 1018/5000
26/26 - 1s - loss: 1.3813 - val_loss: 1.8159
Epoch 1019/5000
26/26 - 1s - loss: 1.3800 - val_loss: 1.8138
Epoch 1020/5000
26/26 - 1s - loss: 1.3779 - val_loss: 1.8124
Epoch 01020: val_loss improved from 1.82352 to 1.81237, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1021/5000
26/26 - 1s - loss: 1.3780 - val_loss: 1.8117
Epoch 1022/5000
26/26 - 1s - loss: 1.3770 - val_loss: 1.8102
Epoch 1023/5000
26/26 - 1s - loss: 1.3758 - val_loss: 1.8110
Epoch 1024/5000
26/26 - 1s - loss: 1.3743 - val_loss: 1.8111
Epoch 1025/5000
26/26 - 1s - loss: 1.3740 - val_loss: 1.8085
Epoch 1026/5000
26/26 - 1s - loss: 1.3710 - val_loss: 1.8075
Epoch 1027/5000
26/26 - 1s - loss: 1.3707 - val_loss: 1.8078
Epoch 1028/5000
26/26 - 1s - loss: 1.3715 - val_loss: 1.8054
Epoch 1029/5000
26/26 - 2s - loss: 1.3710 - val_loss: 1.8046
Epoch 1030/5000
26/26 - 1s - loss: 1.3686 - val_loss: 1.8032
Epoch 01030: val_loss improved from 1.81237 to 1.80322, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1031/5000
26/26 - 1s - loss: 1.3663 - val_loss: 1.8020
Epoch 1032/5000
26/26 - 1s - loss: 1.3659 - val_loss: 1.7997
Epoch 1033/5000
26/26 - 1s - loss: 1.3645 - val_loss: 1.8007
Epoch 1034/5000
26/26 - 1s - loss: 1.3647 - val_loss: 1.7991
Epoch 1035/5000
26/26 - 1s - loss: 1.3623 - val_loss: 1.7971
Epoch 1036/5000
26/26 - 1s - loss: 1.3634 - val_loss: 1.7968
Epoch 1037/5000
26/26 - 1s - loss: 1.3602 - val_loss: 1.7965
Epoch 1038/5000
26/26 - 1s - loss: 1.3613 - val_loss: 1.7968
Epoch 1039/5000
26/26 - 1s - loss: 1.3595 - val_loss: 1.7947
Epoch 1040/5000
26/26 - 1s - loss: 1.3571 - val_loss: 1.7938
Epoch 01040: val_loss improved from 1.80322 to 1.79384, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1041/5000
26/26 - 1s - loss: 1.3573 - val_loss: 1.7937
Epoch 1042/5000
26/26 - 1s - loss: 1.3567 - val_loss: 1.7920
Epoch 1043/5000
26/26 - 1s - loss: 1.3547 - val_loss: 1.7891
Epoch 1044/5000
26/26 - 1s - loss: 1.3543 - val_loss: 1.7889
Epoch 1045/5000
26/26 - 1s - loss: 1.3528 - val_loss: 1.7891
Epoch 1046/5000
26/26 - 1s - loss: 1.3532 - val_loss: 1.7892
Epoch 1047/5000
26/26 - 1s - loss: 1.3531 - val_loss: 1.7873
Epoch 1048/5000
26/26 - 1s - loss: 1.3490 - val_loss: 1.7860
Epoch 1049/5000
26/26 - 1s - loss: 1.3499 - val_loss: 1.7851
Epoch 1050/5000
26/26 - 1s - loss: 1.3474 - val_loss: 1.7850
Epoch 01050: val_loss improved from 1.79384 to 1.78497, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1051/5000
26/26 - 1s - loss: 1.3468 - val_loss: 1.7839
Epoch 1052/5000
26/26 - 1s - loss: 1.3470 - val_loss: 1.7828
Epoch 1053/5000
26/26 - 1s - loss: 1.3464 - val_loss: 1.7816
Epoch 1054/5000
26/26 - 1s - loss: 1.3433 - val_loss: 1.7801
Epoch 1055/5000
26/26 - 1s - loss: 1.3425 - val_loss: 1.7816
Epoch 1056/5000
26/26 - 1s - loss: 1.3435 - val_loss: 1.7798
Epoch 1057/5000
26/26 - 1s - loss: 1.3415 - val_loss: 1.7769
Epoch 1058/5000
26/26 - 1s - loss: 1.3397 - val_loss: 1.7772
Epoch 1059/5000
26/26 - 1s - loss: 1.3391 - val_loss: 1.7747
Epoch 1060/5000
26/26 - 1s - loss: 1.3382 - val_loss: 1.7751
Epoch 01060: val_loss improved from 1.78497 to 1.77506, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1061/5000
26/26 - 1s - loss: 1.3359 - val_loss: 1.7755
Epoch 1062/5000
26/26 - 1s - loss: 1.3369 - val_loss: 1.7734
Epoch 1063/5000
26/26 - 1s - loss: 1.3351 - val_loss: 1.7725
Epoch 1064/5000
26/26 - 1s - loss: 1.3361 - val_loss: 1.7714
Epoch 1065/5000
26/26 - 1s - loss: 1.3337 - val_loss: 1.7708
Epoch 1066/5000
26/26 - 1s - loss: 1.3346 - val_loss: 1.7694
Epoch 1067/5000
26/26 - 1s - loss: 1.3321 - val_loss: 1.7688
Epoch 1068/5000
26/26 - 1s - loss: 1.3311 - val_loss: 1.7657
Epoch 1069/5000
26/26 - 1s - loss: 1.3294 - val_loss: 1.7648
Epoch 1070/5000
26/26 - 1s - loss: 1.3288 - val_loss: 1.7653
Epoch 01070: val_loss improved from 1.77506 to 1.76527, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1071/5000
26/26 - 1s - loss: 1.3293 - val_loss: 1.7640
Epoch 1072/5000
26/26 - 1s - loss: 1.3267 - val_loss: 1.7626
Epoch 1073/5000
26/26 - 1s - loss: 1.3272 - val_loss: 1.7618
Epoch 1074/5000
26/26 - 1s - loss: 1.3268 - val_loss: 1.7621
Epoch 1075/5000
26/26 - 1s - loss: 1.3237 - val_loss: 1.7611
Epoch 1076/5000
26/26 - 1s - loss: 1.3248 - val_loss: 1.7601
Epoch 1077/5000
26/26 - 1s - loss: 1.3219 - val_loss: 1.7594
Epoch 1078/5000
26/26 - 1s - loss: 1.3225 - val_loss: 1.7573
Epoch 1079/5000
26/26 - 1s - loss: 1.3207 - val_loss: 1.7570
Epoch 1080/5000
26/26 - 1s - loss: 1.3201 - val_loss: 1.7548
Epoch 01080: val_loss improved from 1.76527 to 1.75482, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1081/5000
26/26 - 1s - loss: 1.3193 - val_loss: 1.7543
Epoch 1082/5000
26/26 - 1s - loss: 1.3177 - val_loss: 1.7547
Epoch 1083/5000
26/26 - 1s - loss: 1.3162 - val_loss: 1.7524
Epoch 1084/5000
26/26 - 1s - loss: 1.3163 - val_loss: 1.7511
Epoch 1085/5000
26/26 - 1s - loss: 1.3151 - val_loss: 1.7500
Epoch 1086/5000
26/26 - 1s - loss: 1.3139 - val_loss: 1.7488
Epoch 1087/5000
26/26 - 1s - loss: 1.3137 - val_loss: 1.7491
Epoch 1088/5000
26/26 - 1s - loss: 1.3146 - val_loss: 1.7497
Epoch 1089/5000
26/26 - 1s - loss: 1.3111 - val_loss: 1.7492
Epoch 1090/5000
26/26 - 1s - loss: 1.3080 - val_loss: 1.7467
Epoch 01090: val_loss improved from 1.75482 to 1.74667, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1091/5000
26/26 - 1s - loss: 1.3076 - val_loss: 1.7444
Epoch 1092/5000
26/26 - 1s - loss: 1.3082 - val_loss: 1.7442
Epoch 1093/5000
26/26 - 1s - loss: 1.3070 - val_loss: 1.7444
Epoch 1094/5000
26/26 - 1s - loss: 1.3050 - val_loss: 1.7437
Epoch 1095/5000
26/26 - 1s - loss: 1.3040 - val_loss: 1.7418
Epoch 1096/5000
26/26 - 1s - loss: 1.3037 - val_loss: 1.7417
Epoch 1097/5000
26/26 - 1s - loss: 1.3035 - val_loss: 1.7411
Epoch 1098/5000
26/26 - 1s - loss: 1.3026 - val_loss: 1.7398
Epoch 1099/5000
26/26 - 1s - loss: 1.3018 - val_loss: 1.7391
Epoch 1100/5000
26/26 - 1s - loss: 1.3004 - val_loss: 1.7374
Epoch 01100: val_loss improved from 1.74667 to 1.73737, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1101/5000
26/26 - 1s - loss: 1.3012 - val_loss: 1.7362
Epoch 1102/5000
26/26 - 1s - loss: 1.2980 - val_loss: 1.7361
Epoch 1103/5000
26/26 - 1s - loss: 1.2974 - val_loss: 1.7356
Epoch 1104/5000
26/26 - 1s - loss: 1.2961 - val_loss: 1.7332
Epoch 1105/5000
26/26 - 1s - loss: 1.2958 - val_loss: 1.7329
Epoch 1106/5000
26/26 - 1s - loss: 1.2948 - val_loss: 1.7323
Epoch 1107/5000
26/26 - 1s - loss: 1.2937 - val_loss: 1.7315
Epoch 1108/5000
26/26 - 1s - loss: 1.2918 - val_loss: 1.7310
Epoch 1109/5000
26/26 - 1s - loss: 1.2923 - val_loss: 1.7301
Epoch 1110/5000
26/26 - 1s - loss: 1.2924 - val_loss: 1.7292
Epoch 01110: val_loss improved from 1.73737 to 1.72923, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1111/5000
26/26 - 1s - loss: 1.2910 - val_loss: 1.7276
Epoch 1112/5000
26/26 - 2s - loss: 1.2891 - val_loss: 1.7274
Epoch 1113/5000
26/26 - 1s - loss: 1.2890 - val_loss: 1.7248
Epoch 1114/5000
26/26 - 1s - loss: 1.2883 - val_loss: 1.7243
Epoch 1115/5000
26/26 - 1s - loss: 1.2861 - val_loss: 1.7237
Epoch 1116/5000
26/26 - 1s - loss: 1.2867 - val_loss: 1.7216
Epoch 1117/5000
26/26 - 1s - loss: 1.2855 - val_loss: 1.7193
Epoch 1118/5000
26/26 - 1s - loss: 1.2853 - val_loss: 1.7192
Epoch 1119/5000
26/26 - 1s - loss: 1.2825 - val_loss: 1.7192
Epoch 1120/5000
26/26 - 1s - loss: 1.2815 - val_loss: 1.7188
Epoch 01120: val_loss improved from 1.72923 to 1.71877, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1121/5000
26/26 - 1s - loss: 1.2817 - val_loss: 1.7179
Epoch 1122/5000
26/26 - 1s - loss: 1.2797 - val_loss: 1.7178
Epoch 1123/5000
26/26 - 1s - loss: 1.2785 - val_loss: 1.7156
Epoch 1124/5000
26/26 - 1s - loss: 1.2788 - val_loss: 1.7155
Epoch 1125/5000
26/26 - 2s - loss: 1.2789 - val_loss: 1.7152
Epoch 1126/5000
26/26 - 1s - loss: 1.2764 - val_loss: 1.7122
Epoch 1127/5000
26/26 - 1s - loss: 1.2750 - val_loss: 1.7124
Epoch 1128/5000
26/26 - 1s - loss: 1.2734 - val_loss: 1.7124
Epoch 1129/5000
26/26 - 1s - loss: 1.2716 - val_loss: 1.7112
Epoch 1130/5000
26/26 - 1s - loss: 1.2734 - val_loss: 1.7108
Epoch 01130: val_loss improved from 1.71877 to 1.71077, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1131/5000
26/26 - 1s - loss: 1.2715 - val_loss: 1.7105
Epoch 1132/5000
26/26 - 1s - loss: 1.2709 - val_loss: 1.7082
Epoch 1133/5000
26/26 - 1s - loss: 1.2704 - val_loss: 1.7076
Epoch 1134/5000
26/26 - 1s - loss: 1.2699 - val_loss: 1.7068
Epoch 1135/5000
26/26 - 1s - loss: 1.2675 - val_loss: 1.7056
Epoch 1136/5000
26/26 - 1s - loss: 1.2674 - val_loss: 1.7050
Epoch 1137/5000
26/26 - 1s - loss: 1.2668 - val_loss: 1.7051
Epoch 1138/5000
26/26 - 1s - loss: 1.2656 - val_loss: 1.7019
Epoch 1139/5000
26/26 - 1s - loss: 1.2654 - val_loss: 1.7018
Epoch 1140/5000
26/26 - 1s - loss: 1.2660 - val_loss: 1.7001
Epoch 01140: val_loss improved from 1.71077 to 1.70009, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1141/5000
26/26 - 1s - loss: 1.2621 - val_loss: 1.6998
Epoch 1142/5000
26/26 - 1s - loss: 1.2638 - val_loss: 1.6976
Epoch 1143/5000
26/26 - 2s - loss: 1.2612 - val_loss: 1.7000
Epoch 1144/5000
26/26 - 1s - loss: 1.2623 - val_loss: 1.6984
Epoch 1145/5000
26/26 - 1s - loss: 1.2615 - val_loss: 1.6965
Epoch 1146/5000
26/26 - 1s - loss: 1.2593 - val_loss: 1.6959
Epoch 1147/5000
26/26 - 1s - loss: 1.2567 - val_loss: 1.6945
Epoch 1148/5000
26/26 - 1s - loss: 1.2576 - val_loss: 1.6937
Epoch 1149/5000
26/26 - 1s - loss: 1.2564 - val_loss: 1.6933
Epoch 1150/5000
26/26 - 1s - loss: 1.2548 - val_loss: 1.6940
Epoch 01150: val_loss improved from 1.70009 to 1.69403, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1151/5000
26/26 - 1s - loss: 1.2561 - val_loss: 1.6898
Epoch 1152/5000
26/26 - 1s - loss: 1.2553 - val_loss: 1.6916
Epoch 1153/5000
26/26 - 1s - loss: 1.2526 - val_loss: 1.6914
Epoch 1154/5000
26/26 - 1s - loss: 1.2520 - val_loss: 1.6901
Epoch 1155/5000
26/26 - 1s - loss: 1.2524 - val_loss: 1.6897
Epoch 1156/5000
26/26 - 1s - loss: 1.2491 - val_loss: 1.6882
Epoch 1157/5000
26/26 - 1s - loss: 1.2490 - val_loss: 1.6873
Epoch 1158/5000
26/26 - 1s - loss: 1.2488 - val_loss: 1.6858
Epoch 1159/5000
26/26 - 1s - loss: 1.2482 - val_loss: 1.6841
Epoch 1160/5000
26/26 - 2s - loss: 1.2451 - val_loss: 1.6833
Epoch 01160: val_loss improved from 1.69403 to 1.68326, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1161/5000
26/26 - 1s - loss: 1.2457 - val_loss: 1.6820
Epoch 1162/5000
26/26 - 1s - loss: 1.2457 - val_loss: 1.6804
Epoch 1163/5000
26/26 - 1s - loss: 1.2430 - val_loss: 1.6799
Epoch 1164/5000
26/26 - 1s - loss: 1.2452 - val_loss: 1.6803
Epoch 1165/5000
26/26 - 1s - loss: 1.2412 - val_loss: 1.6797
Epoch 1166/5000
26/26 - 1s - loss: 1.2417 - val_loss: 1.6788
Epoch 1167/5000
26/26 - 1s - loss: 1.2407 - val_loss: 1.6770
Epoch 1168/5000
26/26 - 1s - loss: 1.2395 - val_loss: 1.6763
Epoch 1169/5000
26/26 - 1s - loss: 1.2379 - val_loss: 1.6758
Epoch 1170/5000
26/26 - 1s - loss: 1.2389 - val_loss: 1.6754
Epoch 01170: val_loss improved from 1.68326 to 1.67542, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1171/5000
26/26 - 1s - loss: 1.2368 - val_loss: 1.6749
Epoch 1172/5000
26/26 - 1s - loss: 1.2347 - val_loss: 1.6741
Epoch 1173/5000
26/26 - 1s - loss: 1.2350 - val_loss: 1.6742
Epoch 1174/5000
26/26 - 1s - loss: 1.2336 - val_loss: 1.6714
Epoch 1175/5000
26/26 - 1s - loss: 1.2325 - val_loss: 1.6725
Epoch 1176/5000
26/26 - 1s - loss: 1.2330 - val_loss: 1.6714
Epoch 1177/5000
26/26 - 1s - loss: 1.2326 - val_loss: 1.6703
Epoch 1178/5000
26/26 - 1s - loss: 1.2316 - val_loss: 1.6689
Epoch 1179/5000
26/26 - 1s - loss: 1.2296 - val_loss: 1.6673
Epoch 1180/5000
26/26 - 1s - loss: 1.2295 - val_loss: 1.6668
Epoch 01180: val_loss improved from 1.67542 to 1.66684, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1181/5000
26/26 - 1s - loss: 1.2291 - val_loss: 1.6665
Epoch 1182/5000
26/26 - 1s - loss: 1.2274 - val_loss: 1.6666
Epoch 1183/5000
26/26 - 1s - loss: 1.2262 - val_loss: 1.6653
Epoch 1184/5000
26/26 - 1s - loss: 1.2256 - val_loss: 1.6641
Epoch 1185/5000
26/26 - 1s - loss: 1.2258 - val_loss: 1.6647
Epoch 1186/5000
26/26 - 1s - loss: 1.2259 - val_loss: 1.6627
Epoch 1187/5000
26/26 - 1s - loss: 1.2232 - val_loss: 1.6614
Epoch 1188/5000
26/26 - 1s - loss: 1.2230 - val_loss: 1.6607
Epoch 1189/5000
26/26 - 1s - loss: 1.2217 - val_loss: 1.6609
Epoch 1190/5000
26/26 - 1s - loss: 1.2237 - val_loss: 1.6606
Epoch 01190: val_loss improved from 1.66684 to 1.66065, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1191/5000
26/26 - 1s - loss: 1.2192 - val_loss: 1.6586
Epoch 1192/5000
26/26 - 1s - loss: 1.2187 - val_loss: 1.6579
Epoch 1193/5000
26/26 - 1s - loss: 1.2186 - val_loss: 1.6574
Epoch 1194/5000
26/26 - 1s - loss: 1.2170 - val_loss: 1.6553
Epoch 1195/5000
26/26 - 2s - loss: 1.2170 - val_loss: 1.6549
Epoch 1196/5000
26/26 - 1s - loss: 1.2161 - val_loss: 1.6543
Epoch 1197/5000
26/26 - 1s - loss: 1.2147 - val_loss: 1.6536
Epoch 1198/5000
26/26 - 1s - loss: 1.2130 - val_loss: 1.6524
Epoch 1199/5000
26/26 - 1s - loss: 1.2137 - val_loss: 1.6511
Epoch 1200/5000
26/26 - 1s - loss: 1.2130 - val_loss: 1.6500
Epoch 01200: val_loss improved from 1.66065 to 1.64999, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1201/5000
26/26 - 1s - loss: 1.2110 - val_loss: 1.6492
Epoch 1202/5000
26/26 - 1s - loss: 1.2119 - val_loss: 1.6490
Epoch 1203/5000
26/26 - 1s - loss: 1.2095 - val_loss: 1.6482
Epoch 1204/5000
26/26 - 1s - loss: 1.2081 - val_loss: 1.6481
Epoch 1205/5000
26/26 - 1s - loss: 1.2091 - val_loss: 1.6460
Epoch 1206/5000
26/26 - 1s - loss: 1.2093 - val_loss: 1.6443
Epoch 1207/5000
26/26 - 1s - loss: 1.2071 - val_loss: 1.6462
Epoch 1208/5000
26/26 - 1s - loss: 1.2062 - val_loss: 1.6441
Epoch 1209/5000
26/26 - 1s - loss: 1.2064 - val_loss: 1.6433
Epoch 1210/5000
26/26 - 1s - loss: 1.2049 - val_loss: 1.6435
Epoch 01210: val_loss improved from 1.64999 to 1.64352, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1211/5000
26/26 - 1s - loss: 1.2030 - val_loss: 1.6419
Epoch 1212/5000
26/26 - 1s - loss: 1.2027 - val_loss: 1.6421
Epoch 1213/5000
26/26 - 1s - loss: 1.2018 - val_loss: 1.6405
Epoch 1214/5000
26/26 - 1s - loss: 1.1990 - val_loss: 1.6391
Epoch 1215/5000
26/26 - 1s - loss: 1.1991 - val_loss: 1.6378
Epoch 1216/5000
26/26 - 2s - loss: 1.1992 - val_loss: 1.6380
Epoch 1217/5000
26/26 - 1s - loss: 1.1970 - val_loss: 1.6377
Epoch 1218/5000
26/26 - 1s - loss: 1.1977 - val_loss: 1.6366
Epoch 1219/5000
26/26 - 1s - loss: 1.1979 - val_loss: 1.6375
Epoch 1220/5000
26/26 - 1s - loss: 1.1961 - val_loss: 1.6348
Epoch 01220: val_loss improved from 1.64352 to 1.63480, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1221/5000
26/26 - 1s - loss: 1.1958 - val_loss: 1.6344
Epoch 1222/5000
26/26 - 1s - loss: 1.1945 - val_loss: 1.6326
Epoch 1223/5000
26/26 - 1s - loss: 1.1928 - val_loss: 1.6319
Epoch 1224/5000
26/26 - 1s - loss: 1.1936 - val_loss: 1.6315
Epoch 1225/5000
26/26 - 1s - loss: 1.1927 - val_loss: 1.6318
Epoch 1226/5000
26/26 - 2s - loss: 1.1904 - val_loss: 1.6314
Epoch 1227/5000
26/26 - 1s - loss: 1.1915 - val_loss: 1.6295
Epoch 1228/5000
26/26 - 1s - loss: 1.1901 - val_loss: 1.6274
Epoch 1229/5000
26/26 - 1s - loss: 1.1888 - val_loss: 1.6272
Epoch 1230/5000
26/26 - 1s - loss: 1.1874 - val_loss: 1.6265
Epoch 01230: val_loss improved from 1.63480 to 1.62653, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1231/5000
26/26 - 1s - loss: 1.1857 - val_loss: 1.6267
Epoch 1232/5000
26/26 - 1s - loss: 1.1860 - val_loss: 1.6269
Epoch 1233/5000
26/26 - 1s - loss: 1.1863 - val_loss: 1.6247
Epoch 1234/5000
26/26 - 1s - loss: 1.1838 - val_loss: 1.6256
Epoch 1235/5000
26/26 - 1s - loss: 1.1825 - val_loss: 1.6225
Epoch 1236/5000
26/26 - 1s - loss: 1.1838 - val_loss: 1.6219
Epoch 1237/5000
26/26 - 1s - loss: 1.1834 - val_loss: 1.6214
Epoch 1238/5000
26/26 - 1s - loss: 1.1828 - val_loss: 1.6211
Epoch 1239/5000
26/26 - 1s - loss: 1.1801 - val_loss: 1.6219
Epoch 1240/5000
26/26 - 1s - loss: 1.1802 - val_loss: 1.6186
Epoch 01240: val_loss improved from 1.62653 to 1.61865, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1241/5000
26/26 - 1s - loss: 1.1777 - val_loss: 1.6180
Epoch 1242/5000
26/26 - 1s - loss: 1.1800 - val_loss: 1.6173
Epoch 1243/5000
26/26 - 1s - loss: 1.1769 - val_loss: 1.6170
Epoch 1244/5000
26/26 - 1s - loss: 1.1767 - val_loss: 1.6172
Epoch 1245/5000
26/26 - 1s - loss: 1.1762 - val_loss: 1.6156
Epoch 1246/5000
26/26 - 1s - loss: 1.1764 - val_loss: 1.6134
Epoch 1247/5000
26/26 - 1s - loss: 1.1746 - val_loss: 1.6134
Epoch 1248/5000
26/26 - 1s - loss: 1.1739 - val_loss: 1.6122
Epoch 1249/5000
26/26 - 1s - loss: 1.1726 - val_loss: 1.6126
Epoch 1250/5000
26/26 - 1s - loss: 1.1718 - val_loss: 1.6102
Epoch 01250: val_loss improved from 1.61865 to 1.61017, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1251/5000
26/26 - 1s - loss: 1.1705 - val_loss: 1.6114
Epoch 1252/5000
26/26 - 1s - loss: 1.1721 - val_loss: 1.6128
Epoch 1253/5000
26/26 - 1s - loss: 1.1705 - val_loss: 1.6102
Epoch 1254/5000
26/26 - 1s - loss: 1.1691 - val_loss: 1.6085
Epoch 1255/5000
26/26 - 1s - loss: 1.1670 - val_loss: 1.6077
Epoch 1256/5000
26/26 - 1s - loss: 1.1686 - val_loss: 1.6082
Epoch 1257/5000
26/26 - 1s - loss: 1.1661 - val_loss: 1.6065
Epoch 1258/5000
26/26 - 1s - loss: 1.1658 - val_loss: 1.6071
Epoch 1259/5000
26/26 - 1s - loss: 1.1662 - val_loss: 1.6036
Epoch 1260/5000
26/26 - 1s - loss: 1.1648 - val_loss: 1.6028
Epoch 01260: val_loss improved from 1.61017 to 1.60280, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1261/5000
26/26 - 1s - loss: 1.1643 - val_loss: 1.6027
Epoch 1262/5000
26/26 - 1s - loss: 1.1643 - val_loss: 1.6013
Epoch 1263/5000
26/26 - 1s - loss: 1.1629 - val_loss: 1.6007
Epoch 1264/5000
26/26 - 1s - loss: 1.1599 - val_loss: 1.6002
Epoch 1265/5000
26/26 - 1s - loss: 1.1596 - val_loss: 1.5993
Epoch 1266/5000
26/26 - 1s - loss: 1.1601 - val_loss: 1.5995
Epoch 1267/5000
26/26 - 1s - loss: 1.1594 - val_loss: 1.5998
Epoch 1268/5000
26/26 - 1s - loss: 1.1585 - val_loss: 1.5982
Epoch 1269/5000
26/26 - 1s - loss: 1.1585 - val_loss: 1.5966
Epoch 1270/5000
26/26 - 1s - loss: 1.1574 - val_loss: 1.5958
Epoch 01270: val_loss improved from 1.60280 to 1.59580, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1271/5000
26/26 - 1s - loss: 1.1551 - val_loss: 1.5962
Epoch 1272/5000
26/26 - 1s - loss: 1.1553 - val_loss: 1.5942
Epoch 1273/5000
26/26 - 1s - loss: 1.1552 - val_loss: 1.5944
Epoch 1274/5000
26/26 - 1s - loss: 1.1535 - val_loss: 1.5930
Epoch 1275/5000
26/26 - 1s - loss: 1.1527 - val_loss: 1.5920
Epoch 1276/5000
26/26 - 1s - loss: 1.1525 - val_loss: 1.5919
Epoch 1277/5000
26/26 - 1s - loss: 1.1526 - val_loss: 1.5911
Epoch 1278/5000
26/26 - 2s - loss: 1.1512 - val_loss: 1.5887
Epoch 1279/5000
26/26 - 1s - loss: 1.1500 - val_loss: 1.5897
Epoch 1280/5000
26/26 - 1s - loss: 1.1498 - val_loss: 1.5885
Epoch 01280: val_loss improved from 1.59580 to 1.58854, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1281/5000
26/26 - 1s - loss: 1.1480 - val_loss: 1.5891
Epoch 1282/5000
26/26 - 1s - loss: 1.1469 - val_loss: 1.5872
Epoch 1283/5000
26/26 - 1s - loss: 1.1468 - val_loss: 1.5866
Epoch 1284/5000
26/26 - 1s - loss: 1.1457 - val_loss: 1.5858
Epoch 1285/5000
26/26 - 1s - loss: 1.1458 - val_loss: 1.5847
Epoch 1286/5000
26/26 - 1s - loss: 1.1440 - val_loss: 1.5845
Epoch 1287/5000
26/26 - 1s - loss: 1.1436 - val_loss: 1.5823
Epoch 1288/5000
26/26 - 1s - loss: 1.1426 - val_loss: 1.5822
Epoch 1289/5000
26/26 - 1s - loss: 1.1427 - val_loss: 1.5809
Epoch 1290/5000
26/26 - 1s - loss: 1.1416 - val_loss: 1.5805
Epoch 01290: val_loss improved from 1.58854 to 1.58049, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1291/5000
26/26 - 1s - loss: 1.1402 - val_loss: 1.5798
Epoch 1292/5000
26/26 - 1s - loss: 1.1411 - val_loss: 1.5780
Epoch 1293/5000
26/26 - 1s - loss: 1.1402 - val_loss: 1.5801
Epoch 1294/5000
26/26 - 1s - loss: 1.1397 - val_loss: 1.5774
Epoch 1295/5000
26/26 - 1s - loss: 1.1368 - val_loss: 1.5781
Epoch 1296/5000
26/26 - 1s - loss: 1.1380 - val_loss: 1.5770
Epoch 1297/5000
26/26 - 1s - loss: 1.1356 - val_loss: 1.5764
Epoch 1298/5000
26/26 - 1s - loss: 1.1354 - val_loss: 1.5749
Epoch 1299/5000
26/26 - 1s - loss: 1.1353 - val_loss: 1.5758
Epoch 1300/5000
26/26 - 1s - loss: 1.1347 - val_loss: 1.5761
Epoch 01300: val_loss improved from 1.58049 to 1.57606, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1301/5000
26/26 - 1s - loss: 1.1335 - val_loss: 1.5743
Epoch 1302/5000
26/26 - 1s - loss: 1.1320 - val_loss: 1.5745
Epoch 1303/5000
26/26 - 1s - loss: 1.1318 - val_loss: 1.5730
Epoch 1304/5000
26/26 - 1s - loss: 1.1337 - val_loss: 1.5726
Epoch 1305/5000
26/26 - 1s - loss: 1.1302 - val_loss: 1.5727
Epoch 1306/5000
26/26 - 1s - loss: 1.1304 - val_loss: 1.5706
Epoch 1307/5000
26/26 - 1s - loss: 1.1285 - val_loss: 1.5691
Epoch 1308/5000
26/26 - 1s - loss: 1.1288 - val_loss: 1.5675
Epoch 1309/5000
26/26 - 1s - loss: 1.1285 - val_loss: 1.5694
Epoch 1310/5000
26/26 - 1s - loss: 1.1258 - val_loss: 1.5660
Epoch 01310: val_loss improved from 1.57606 to 1.56596, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1311/5000
26/26 - 1s - loss: 1.1271 - val_loss: 1.5651
Epoch 1312/5000
26/26 - 1s - loss: 1.1269 - val_loss: 1.5659
Epoch 1313/5000
26/26 - 1s - loss: 1.1258 - val_loss: 1.5666
Epoch 1314/5000
26/26 - 1s - loss: 1.1241 - val_loss: 1.5646
Epoch 1315/5000
26/26 - 1s - loss: 1.1236 - val_loss: 1.5624
Epoch 1316/5000
26/26 - 1s - loss: 1.1228 - val_loss: 1.5624
Epoch 1317/5000
26/26 - 1s - loss: 1.1217 - val_loss: 1.5616
Epoch 1318/5000
26/26 - 1s - loss: 1.1196 - val_loss: 1.5613
Epoch 1319/5000
26/26 - 1s - loss: 1.1207 - val_loss: 1.5613
Epoch 1320/5000
26/26 - 2s - loss: 1.1189 - val_loss: 1.5599
Epoch 01320: val_loss improved from 1.56596 to 1.55988, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1321/5000
26/26 - 1s - loss: 1.1192 - val_loss: 1.5593
Epoch 1322/5000
26/26 - 1s - loss: 1.1178 - val_loss: 1.5605
Epoch 1323/5000
26/26 - 1s - loss: 1.1185 - val_loss: 1.5590
Epoch 1324/5000
26/26 - 1s - loss: 1.1180 - val_loss: 1.5566
Epoch 1325/5000
26/26 - 1s - loss: 1.1162 - val_loss: 1.5571
Epoch 1326/5000
26/26 - 1s - loss: 1.1153 - val_loss: 1.5578
Epoch 1327/5000
26/26 - 1s - loss: 1.1146 - val_loss: 1.5538
Epoch 1328/5000
26/26 - 1s - loss: 1.1136 - val_loss: 1.5548
Epoch 1329/5000
26/26 - 1s - loss: 1.1126 - val_loss: 1.5533
Epoch 1330/5000
26/26 - 1s - loss: 1.1120 - val_loss: 1.5533
Epoch 01330: val_loss improved from 1.55988 to 1.55328, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1331/5000
26/26 - 1s - loss: 1.1136 - val_loss: 1.5526
Epoch 1332/5000
26/26 - 1s - loss: 1.1116 - val_loss: 1.5509
Epoch 1333/5000
26/26 - 1s - loss: 1.1105 - val_loss: 1.5506
Epoch 1334/5000
26/26 - 1s - loss: 1.1092 - val_loss: 1.5492
Epoch 1335/5000
26/26 - 1s - loss: 1.1093 - val_loss: 1.5491
Epoch 1336/5000
26/26 - 1s - loss: 1.1085 - val_loss: 1.5471
Epoch 1337/5000
26/26 - 1s - loss: 1.1092 - val_loss: 1.5459
Epoch 1338/5000
26/26 - 1s - loss: 1.1075 - val_loss: 1.5468
Epoch 1339/5000
26/26 - 1s - loss: 1.1064 - val_loss: 1.5460
Epoch 1340/5000
26/26 - 1s - loss: 1.1069 - val_loss: 1.5458
Epoch 01340: val_loss improved from 1.55328 to 1.54580, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1341/5000
26/26 - 1s - loss: 1.1049 - val_loss: 1.5466
Epoch 1342/5000
26/26 - 1s - loss: 1.1061 - val_loss: 1.5445
Epoch 1343/5000
26/26 - 1s - loss: 1.1016 - val_loss: 1.5429
Epoch 1344/5000
26/26 - 1s - loss: 1.1038 - val_loss: 1.5429
Epoch 1345/5000
26/26 - 1s - loss: 1.1015 - val_loss: 1.5424
Epoch 1346/5000
26/26 - 1s - loss: 1.1019 - val_loss: 1.5427
Epoch 1347/5000
26/26 - 1s - loss: 1.1013 - val_loss: 1.5414
Epoch 1348/5000
26/26 - 1s - loss: 1.1004 - val_loss: 1.5395
Epoch 1349/5000
26/26 - 1s - loss: 1.0999 - val_loss: 1.5393
Epoch 1350/5000
26/26 - 1s - loss: 1.0996 - val_loss: 1.5391
Epoch 01350: val_loss improved from 1.54580 to 1.53909, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1351/5000
26/26 - 1s - loss: 1.0991 - val_loss: 1.5388
Epoch 1352/5000
26/26 - 1s - loss: 1.0969 - val_loss: 1.5380
Epoch 1353/5000
26/26 - 1s - loss: 1.0978 - val_loss: 1.5364
Epoch 1354/5000
26/26 - 1s - loss: 1.0978 - val_loss: 1.5370
Epoch 1355/5000
26/26 - 1s - loss: 1.0962 - val_loss: 1.5363
Epoch 1356/5000
26/26 - 1s - loss: 1.0928 - val_loss: 1.5351
Epoch 1357/5000
26/26 - 1s - loss: 1.0941 - val_loss: 1.5344
Epoch 1358/5000
26/26 - 1s - loss: 1.0928 - val_loss: 1.5325
Epoch 1359/5000
26/26 - 1s - loss: 1.0916 - val_loss: 1.5329
Epoch 1360/5000
26/26 - 1s - loss: 1.0931 - val_loss: 1.5330
Epoch 01360: val_loss improved from 1.53909 to 1.53300, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1361/5000
26/26 - 1s - loss: 1.0908 - val_loss: 1.5314
Epoch 1362/5000
26/26 - 1s - loss: 1.0910 - val_loss: 1.5305
Epoch 1363/5000
26/26 - 1s - loss: 1.0891 - val_loss: 1.5295
Epoch 1364/5000
26/26 - 1s - loss: 1.0881 - val_loss: 1.5296
Epoch 1365/5000
26/26 - 1s - loss: 1.0888 - val_loss: 1.5293
Epoch 1366/5000
26/26 - 1s - loss: 1.0876 - val_loss: 1.5291
Epoch 1367/5000
26/26 - 1s - loss: 1.0870 - val_loss: 1.5280
Epoch 1368/5000
26/26 - 1s - loss: 1.0857 - val_loss: 1.5275
Epoch 1369/5000
26/26 - 1s - loss: 1.0869 - val_loss: 1.5252
Epoch 1370/5000
26/26 - 1s - loss: 1.0838 - val_loss: 1.5257
Epoch 01370: val_loss improved from 1.53300 to 1.52574, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1371/5000
26/26 - 1s - loss: 1.0841 - val_loss: 1.5268
Epoch 1372/5000
26/26 - 1s - loss: 1.0839 - val_loss: 1.5253
Epoch 1373/5000
26/26 - 1s - loss: 1.0845 - val_loss: 1.5230
Epoch 1374/5000
26/26 - 1s - loss: 1.0836 - val_loss: 1.5241
Epoch 1375/5000
26/26 - 1s - loss: 1.0818 - val_loss: 1.5223
Epoch 1376/5000
26/26 - 1s - loss: 1.0802 - val_loss: 1.5200
Epoch 1377/5000
26/26 - 1s - loss: 1.0797 - val_loss: 1.5199
Epoch 1378/5000
26/26 - 1s - loss: 1.0790 - val_loss: 1.5203
Epoch 1379/5000
26/26 - 1s - loss: 1.0793 - val_loss: 1.5198
Epoch 1380/5000
26/26 - 1s - loss: 1.0792 - val_loss: 1.5191
Epoch 01380: val_loss improved from 1.52574 to 1.51911, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1381/5000
26/26 - 1s - loss: 1.0769 - val_loss: 1.5167
Epoch 1382/5000
26/26 - 1s - loss: 1.0742 - val_loss: 1.5171
Epoch 1383/5000
26/26 - 1s - loss: 1.0770 - val_loss: 1.5175
Epoch 1384/5000
26/26 - 1s - loss: 1.0751 - val_loss: 1.5153
Epoch 1385/5000
26/26 - 2s - loss: 1.0737 - val_loss: 1.5160
Epoch 1386/5000
26/26 - 1s - loss: 1.0734 - val_loss: 1.5142
Epoch 1387/5000
26/26 - 1s - loss: 1.0723 - val_loss: 1.5134
Epoch 1388/5000
26/26 - 1s - loss: 1.0738 - val_loss: 1.5125
Epoch 1389/5000
26/26 - 1s - loss: 1.0706 - val_loss: 1.5116
Epoch 1390/5000
26/26 - 1s - loss: 1.0698 - val_loss: 1.5119
Epoch 01390: val_loss improved from 1.51911 to 1.51188, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1391/5000
26/26 - 1s - loss: 1.0698 - val_loss: 1.5121
Epoch 1392/5000
26/26 - 1s - loss: 1.0711 - val_loss: 1.5091
Epoch 1393/5000
26/26 - 1s - loss: 1.0708 - val_loss: 1.5104
Epoch 1394/5000
26/26 - 1s - loss: 1.0683 - val_loss: 1.5092
Epoch 1395/5000
26/26 - 1s - loss: 1.0678 - val_loss: 1.5094
Epoch 1396/5000
26/26 - 1s - loss: 1.0658 - val_loss: 1.5098
Epoch 1397/5000
26/26 - 1s - loss: 1.0667 - val_loss: 1.5087
Epoch 1398/5000
26/26 - 1s - loss: 1.0672 - val_loss: 1.5085
Epoch 1399/5000
26/26 - 1s - loss: 1.0652 - val_loss: 1.5071
Epoch 1400/5000
26/26 - 1s - loss: 1.0635 - val_loss: 1.5055
Epoch 01400: val_loss improved from 1.51188 to 1.50549, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1401/5000
26/26 - 1s - loss: 1.0628 - val_loss: 1.5048
Epoch 1402/5000
26/26 - 1s - loss: 1.0638 - val_loss: 1.5043
Epoch 1403/5000
26/26 - 1s - loss: 1.0627 - val_loss: 1.5022
Epoch 1404/5000
26/26 - 1s - loss: 1.0624 - val_loss: 1.5030
Epoch 1405/5000
26/26 - 1s - loss: 1.0613 - val_loss: 1.5007
Epoch 1406/5000
26/26 - 1s - loss: 1.0595 - val_loss: 1.5019
Epoch 1407/5000
26/26 - 1s - loss: 1.0608 - val_loss: 1.4991
Epoch 1408/5000
26/26 - 1s - loss: 1.0588 - val_loss: 1.5007
Epoch 1409/5000
26/26 - 1s - loss: 1.0586 - val_loss: 1.4987
Epoch 1410/5000
26/26 - 1s - loss: 1.0583 - val_loss: 1.4995
Epoch 01410: val_loss improved from 1.50549 to 1.49947, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1411/5000
26/26 - 1s - loss: 1.0578 - val_loss: 1.4973
Epoch 1412/5000
26/26 - 1s - loss: 1.0555 - val_loss: 1.4986
Epoch 1413/5000
26/26 - 1s - loss: 1.0566 - val_loss: 1.4969
Epoch 1414/5000
26/26 - 1s - loss: 1.0551 - val_loss: 1.4959
Epoch 1415/5000
26/26 - 1s - loss: 1.0550 - val_loss: 1.4949
Epoch 1416/5000
26/26 - 1s - loss: 1.0537 - val_loss: 1.4949
Epoch 1417/5000
26/26 - 1s - loss: 1.0516 - val_loss: 1.4931
Epoch 1418/5000
26/26 - 1s - loss: 1.0512 - val_loss: 1.4942
Epoch 1419/5000
26/26 - 1s - loss: 1.0531 - val_loss: 1.4944
Epoch 1420/5000
26/26 - 1s - loss: 1.0520 - val_loss: 1.4940
Epoch 01420: val_loss improved from 1.49947 to 1.49398, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1421/5000
26/26 - 1s - loss: 1.0513 - val_loss: 1.4898
Epoch 1422/5000
26/26 - 1s - loss: 1.0515 - val_loss: 1.4911
Epoch 1423/5000
26/26 - 1s - loss: 1.0493 - val_loss: 1.4903
Epoch 1424/5000
26/26 - 1s - loss: 1.0489 - val_loss: 1.4900
Epoch 1425/5000
26/26 - 1s - loss: 1.0482 - val_loss: 1.4906
Epoch 1426/5000
26/26 - 1s - loss: 1.0467 - val_loss: 1.4898
Epoch 1427/5000
26/26 - 1s - loss: 1.0494 - val_loss: 1.4894
Epoch 1428/5000
26/26 - 1s - loss: 1.0460 - val_loss: 1.4888
Epoch 1429/5000
26/26 - 1s - loss: 1.0447 - val_loss: 1.4870
Epoch 1430/5000
26/26 - 1s - loss: 1.0451 - val_loss: 1.4878
Epoch 01430: val_loss improved from 1.49398 to 1.48777, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1431/5000
26/26 - 1s - loss: 1.0443 - val_loss: 1.4860
Epoch 1432/5000
26/26 - 1s - loss: 1.0430 - val_loss: 1.4864
Epoch 1433/5000
26/26 - 1s - loss: 1.0437 - val_loss: 1.4851
Epoch 1434/5000
26/26 - 1s - loss: 1.0414 - val_loss: 1.4849
Epoch 1435/5000
26/26 - 1s - loss: 1.0422 - val_loss: 1.4847
Epoch 1436/5000
26/26 - 1s - loss: 1.0415 - val_loss: 1.4843
Epoch 1437/5000
26/26 - 1s - loss: 1.0406 - val_loss: 1.4827
Epoch 1438/5000
26/26 - 1s - loss: 1.0404 - val_loss: 1.4824
Epoch 1439/5000
26/26 - 1s - loss: 1.0395 - val_loss: 1.4829
Epoch 1440/5000
26/26 - 1s - loss: 1.0388 - val_loss: 1.4804
Epoch 01440: val_loss improved from 1.48777 to 1.48042, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1441/5000
26/26 - 1s - loss: 1.0382 - val_loss: 1.4804
Epoch 1442/5000
26/26 - 1s - loss: 1.0376 - val_loss: 1.4791
Epoch 1443/5000
26/26 - 1s - loss: 1.0367 - val_loss: 1.4791
Epoch 1444/5000
26/26 - 1s - loss: 1.0369 - val_loss: 1.4780
Epoch 1445/5000
26/26 - 1s - loss: 1.0361 - val_loss: 1.4778
Epoch 1446/5000
26/26 - 2s - loss: 1.0335 - val_loss: 1.4756
Epoch 1447/5000
26/26 - 1s - loss: 1.0346 - val_loss: 1.4787
Epoch 1448/5000
26/26 - 1s - loss: 1.0339 - val_loss: 1.4761
Epoch 1449/5000
26/26 - 1s - loss: 1.0347 - val_loss: 1.4737
Epoch 1450/5000
26/26 - 1s - loss: 1.0323 - val_loss: 1.4727
Epoch 01450: val_loss improved from 1.48042 to 1.47267, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1451/5000
26/26 - 1s - loss: 1.0309 - val_loss: 1.4751
Epoch 1452/5000
26/26 - 1s - loss: 1.0313 - val_loss: 1.4732
Epoch 1453/5000
26/26 - 1s - loss: 1.0295 - val_loss: 1.4723
Epoch 1454/5000
26/26 - 1s - loss: 1.0304 - val_loss: 1.4725
Epoch 1455/5000
26/26 - 1s - loss: 1.0302 - val_loss: 1.4711
Epoch 1456/5000
26/26 - 1s - loss: 1.0300 - val_loss: 1.4698
Epoch 1457/5000
26/26 - 1s - loss: 1.0280 - val_loss: 1.4700
Epoch 1458/5000
26/26 - 1s - loss: 1.0268 - val_loss: 1.4685
Epoch 1459/5000
26/26 - 1s - loss: 1.0286 - val_loss: 1.4697
Epoch 1460/5000
26/26 - 2s - loss: 1.0266 - val_loss: 1.4693
Epoch 01460: val_loss improved from 1.47267 to 1.46927, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1461/5000
26/26 - 1s - loss: 1.0254 - val_loss: 1.4658
Epoch 1462/5000
26/26 - 1s - loss: 1.0250 - val_loss: 1.4675
Epoch 1463/5000
26/26 - 1s - loss: 1.0245 - val_loss: 1.4657
Epoch 1464/5000
26/26 - 1s - loss: 1.0234 - val_loss: 1.4666
Epoch 1465/5000
26/26 - 1s - loss: 1.0247 - val_loss: 1.4653
Epoch 1466/5000
26/26 - 1s - loss: 1.0224 - val_loss: 1.4646
Epoch 1467/5000
26/26 - 1s - loss: 1.0232 - val_loss: 1.4646
Epoch 1468/5000
26/26 - 1s - loss: 1.0192 - val_loss: 1.4637
Epoch 1469/5000
26/26 - 1s - loss: 1.0214 - val_loss: 1.4622
Epoch 1470/5000
26/26 - 1s - loss: 1.0190 - val_loss: 1.4618
Epoch 01470: val_loss improved from 1.46927 to 1.46184, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1471/5000
26/26 - 1s - loss: 1.0195 - val_loss: 1.4613
Epoch 1472/5000
26/26 - 1s - loss: 1.0202 - val_loss: 1.4601
Epoch 1473/5000
26/26 - 1s - loss: 1.0191 - val_loss: 1.4592
Epoch 1474/5000
26/26 - 1s - loss: 1.0179 - val_loss: 1.4600
Epoch 1475/5000
26/26 - 1s - loss: 1.0179 - val_loss: 1.4576
Epoch 1476/5000
26/26 - 1s - loss: 1.0158 - val_loss: 1.4577
Epoch 1477/5000
26/26 - 2s - loss: 1.0154 - val_loss: 1.4583
Epoch 1478/5000
26/26 - 1s - loss: 1.0140 - val_loss: 1.4570
Epoch 1479/5000
26/26 - 1s - loss: 1.0149 - val_loss: 1.4570
Epoch 1480/5000
26/26 - 1s - loss: 1.0152 - val_loss: 1.4547
Epoch 01480: val_loss improved from 1.46184 to 1.45469, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1481/5000
26/26 - 1s - loss: 1.0137 - val_loss: 1.4541
Epoch 1482/5000
26/26 - 1s - loss: 1.0123 - val_loss: 1.4539
Epoch 1483/5000
26/26 - 1s - loss: 1.0123 - val_loss: 1.4525
Epoch 1484/5000
26/26 - 1s - loss: 1.0127 - val_loss: 1.4514
Epoch 1485/5000
26/26 - 1s - loss: 1.0115 - val_loss: 1.4521
Epoch 1486/5000
26/26 - 1s - loss: 1.0086 - val_loss: 1.4512
Epoch 1487/5000
26/26 - 1s - loss: 1.0106 - val_loss: 1.4523
Epoch 1488/5000
26/26 - 1s - loss: 1.0097 - val_loss: 1.4508
Epoch 1489/5000
26/26 - 1s - loss: 1.0065 - val_loss: 1.4517
Epoch 1490/5000
26/26 - 1s - loss: 1.0080 - val_loss: 1.4506
Epoch 01490: val_loss improved from 1.45469 to 1.45064, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1491/5000
26/26 - 1s - loss: 1.0066 - val_loss: 1.4488
Epoch 1492/5000
26/26 - 1s - loss: 1.0060 - val_loss: 1.4477
Epoch 1493/5000
26/26 - 1s - loss: 1.0060 - val_loss: 1.4460
Epoch 1494/5000
26/26 - 1s - loss: 1.0073 - val_loss: 1.4445
Epoch 1495/5000
26/26 - 1s - loss: 1.0069 - val_loss: 1.4451
Epoch 1496/5000
26/26 - 1s - loss: 1.0032 - val_loss: 1.4462
Epoch 1497/5000
26/26 - 1s - loss: 1.0038 - val_loss: 1.4457
Epoch 1498/5000
26/26 - 1s - loss: 1.0033 - val_loss: 1.4437
Epoch 1499/5000
26/26 - 1s - loss: 1.0022 - val_loss: 1.4433
Epoch 1500/5000
26/26 - 1s - loss: 1.0037 - val_loss: 1.4424
Epoch 01500: val_loss improved from 1.45064 to 1.44236, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1501/5000
26/26 - 1s - loss: 1.0002 - val_loss: 1.4437
Epoch 1502/5000
26/26 - 1s - loss: 1.0012 - val_loss: 1.4414
Epoch 1503/5000
26/26 - 1s - loss: 0.9993 - val_loss: 1.4438
Epoch 1504/5000
26/26 - 1s - loss: 1.0002 - val_loss: 1.4421
Epoch 1505/5000
26/26 - 1s - loss: 0.9995 - val_loss: 1.4402
Epoch 1506/5000
26/26 - 1s - loss: 0.9992 - val_loss: 1.4392
Epoch 1507/5000
26/26 - 1s - loss: 0.9988 - val_loss: 1.4402
Epoch 1508/5000
26/26 - 1s - loss: 0.9967 - val_loss: 1.4397
Epoch 1509/5000
26/26 - 1s - loss: 0.9979 - val_loss: 1.4396
Epoch 1510/5000
26/26 - 1s - loss: 0.9965 - val_loss: 1.4390
Epoch 01510: val_loss improved from 1.44236 to 1.43902, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1511/5000
26/26 - 1s - loss: 0.9975 - val_loss: 1.4389
Epoch 1512/5000
26/26 - 2s - loss: 0.9956 - val_loss: 1.4360
Epoch 1513/5000
26/26 - 1s - loss: 0.9945 - val_loss: 1.4364
Epoch 1514/5000
26/26 - 1s - loss: 0.9932 - val_loss: 1.4362
Epoch 1515/5000
26/26 - 1s - loss: 0.9923 - val_loss: 1.4348
Epoch 1516/5000
26/26 - 2s - loss: 0.9921 - val_loss: 1.4345
Epoch 1517/5000
26/26 - 1s - loss: 0.9941 - val_loss: 1.4345
Epoch 1518/5000
26/26 - 1s - loss: 0.9916 - val_loss: 1.4332
Epoch 1519/5000
26/26 - 1s - loss: 0.9910 - val_loss: 1.4331
Epoch 1520/5000
26/26 - 1s - loss: 0.9907 - val_loss: 1.4319
Epoch 01520: val_loss improved from 1.43902 to 1.43186, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1521/5000
26/26 - 1s - loss: 0.9889 - val_loss: 1.4326
Epoch 1522/5000
26/26 - 1s - loss: 0.9891 - val_loss: 1.4319
Epoch 1523/5000
26/26 - 1s - loss: 0.9893 - val_loss: 1.4303
Epoch 1524/5000
26/26 - 1s - loss: 0.9878 - val_loss: 1.4300
Epoch 1525/5000
26/26 - 1s - loss: 0.9887 - val_loss: 1.4296
Epoch 1526/5000
26/26 - 1s - loss: 0.9874 - val_loss: 1.4295
Epoch 1527/5000
26/26 - 1s - loss: 0.9865 - val_loss: 1.4292
Epoch 1528/5000
26/26 - 1s - loss: 0.9849 - val_loss: 1.4286
Epoch 1529/5000
26/26 - 1s - loss: 0.9851 - val_loss: 1.4284
Epoch 1530/5000
26/26 - 1s - loss: 0.9827 - val_loss: 1.4273
Epoch 01530: val_loss improved from 1.43186 to 1.42734, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1531/5000
26/26 - 1s - loss: 0.9839 - val_loss: 1.4256
Epoch 1532/5000
26/26 - 1s - loss: 0.9835 - val_loss: 1.4251
Epoch 1533/5000
26/26 - 1s - loss: 0.9823 - val_loss: 1.4245
Epoch 1534/5000
26/26 - 1s - loss: 0.9819 - val_loss: 1.4243
Epoch 1535/5000
26/26 - 1s - loss: 0.9837 - val_loss: 1.4241
Epoch 1536/5000
26/26 - 1s - loss: 0.9820 - val_loss: 1.4235
Epoch 1537/5000
26/26 - 1s - loss: 0.9814 - val_loss: 1.4242
Epoch 1538/5000
26/26 - 2s - loss: 0.9799 - val_loss: 1.4222
Epoch 1539/5000
26/26 - 1s - loss: 0.9794 - val_loss: 1.4231
Epoch 1540/5000
26/26 - 1s - loss: 0.9784 - val_loss: 1.4211
Epoch 01540: val_loss improved from 1.42734 to 1.42111, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1541/5000
26/26 - 1s - loss: 0.9791 - val_loss: 1.4202
Epoch 1542/5000
26/26 - 1s - loss: 0.9796 - val_loss: 1.4203
Epoch 1543/5000
26/26 - 1s - loss: 0.9767 - val_loss: 1.4197
Epoch 1544/5000
26/26 - 2s - loss: 0.9771 - val_loss: 1.4186
Epoch 1545/5000
26/26 - 1s - loss: 0.9772 - val_loss: 1.4188
Epoch 1546/5000
26/26 - 1s - loss: 0.9755 - val_loss: 1.4192
Epoch 1547/5000
26/26 - 1s - loss: 0.9728 - val_loss: 1.4179
Epoch 1548/5000
26/26 - 1s - loss: 0.9749 - val_loss: 1.4163
Epoch 1549/5000
26/26 - 1s - loss: 0.9741 - val_loss: 1.4163
Epoch 1550/5000
26/26 - 1s - loss: 0.9712 - val_loss: 1.4152
Epoch 01550: val_loss improved from 1.42111 to 1.41517, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1551/5000
26/26 - 1s - loss: 0.9749 - val_loss: 1.4164
Epoch 1552/5000
26/26 - 1s - loss: 0.9727 - val_loss: 1.4155
Epoch 1553/5000
26/26 - 2s - loss: 0.9713 - val_loss: 1.4143
Epoch 1554/5000
26/26 - 1s - loss: 0.9716 - val_loss: 1.4118
Epoch 1555/5000
26/26 - 1s - loss: 0.9704 - val_loss: 1.4133
Epoch 1556/5000
26/26 - 1s - loss: 0.9689 - val_loss: 1.4126
Epoch 1557/5000
26/26 - 1s - loss: 0.9699 - val_loss: 1.4128
Epoch 1558/5000
26/26 - 1s - loss: 0.9676 - val_loss: 1.4112
Epoch 1559/5000
26/26 - 1s - loss: 0.9683 - val_loss: 1.4086
Epoch 1560/5000
26/26 - 1s - loss: 0.9674 - val_loss: 1.4095
Epoch 01560: val_loss improved from 1.41517 to 1.40952, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1561/5000
26/26 - 1s - loss: 0.9681 - val_loss: 1.4092
Epoch 1562/5000
26/26 - 1s - loss: 0.9676 - val_loss: 1.4085
Epoch 1563/5000
26/26 - 1s - loss: 0.9651 - val_loss: 1.4081
Epoch 1564/5000
26/26 - 1s - loss: 0.9652 - val_loss: 1.4072
Epoch 1565/5000
26/26 - 1s - loss: 0.9641 - val_loss: 1.4069
Epoch 1566/5000
26/26 - 1s - loss: 0.9649 - val_loss: 1.4054
Epoch 1567/5000
26/26 - 1s - loss: 0.9634 - val_loss: 1.4059
Epoch 1568/5000
26/26 - 1s - loss: 0.9641 - val_loss: 1.4058
Epoch 1569/5000
26/26 - 1s - loss: 0.9616 - val_loss: 1.4042
Epoch 1570/5000
26/26 - 1s - loss: 0.9613 - val_loss: 1.4041
Epoch 01570: val_loss improved from 1.40952 to 1.40410, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1571/5000
26/26 - 1s - loss: 0.9614 - val_loss: 1.4029
Epoch 1572/5000
26/26 - 1s - loss: 0.9625 - val_loss: 1.4012
Epoch 1573/5000
26/26 - 1s - loss: 0.9610 - val_loss: 1.4021
Epoch 1574/5000
26/26 - 1s - loss: 0.9597 - val_loss: 1.4007
Epoch 1575/5000
26/26 - 1s - loss: 0.9597 - val_loss: 1.4019
Epoch 1576/5000
26/26 - 1s - loss: 0.9599 - val_loss: 1.4006
Epoch 1577/5000
26/26 - 1s - loss: 0.9577 - val_loss: 1.4009
Epoch 1578/5000
26/26 - 1s - loss: 0.9575 - val_loss: 1.3994
Epoch 1579/5000
26/26 - 1s - loss: 0.9591 - val_loss: 1.4008
Epoch 1580/5000
26/26 - 1s - loss: 0.9560 - val_loss: 1.4001
Epoch 01580: val_loss improved from 1.40410 to 1.40012, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1581/5000
26/26 - 1s - loss: 0.9578 - val_loss: 1.3994
Epoch 1582/5000
26/26 - 1s - loss: 0.9553 - val_loss: 1.3988
Epoch 1583/5000
26/26 - 1s - loss: 0.9552 - val_loss: 1.3964
Epoch 1584/5000
26/26 - 1s - loss: 0.9553 - val_loss: 1.3974
Epoch 1585/5000
26/26 - 1s - loss: 0.9537 - val_loss: 1.3972
Epoch 1586/5000
26/26 - 1s - loss: 0.9532 - val_loss: 1.3970
Epoch 1587/5000
26/26 - 1s - loss: 0.9515 - val_loss: 1.3962
Epoch 1588/5000
26/26 - 1s - loss: 0.9541 - val_loss: 1.3957
Epoch 1589/5000
26/26 - 1s - loss: 0.9514 - val_loss: 1.3936
Epoch 1590/5000
26/26 - 1s - loss: 0.9504 - val_loss: 1.3946
Epoch 01590: val_loss improved from 1.40012 to 1.39464, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1591/5000
26/26 - 1s - loss: 0.9506 - val_loss: 1.3932
Epoch 1592/5000
26/26 - 1s - loss: 0.9499 - val_loss: 1.3915
Epoch 1593/5000
26/26 - 1s - loss: 0.9492 - val_loss: 1.3940
Epoch 1594/5000
26/26 - 1s - loss: 0.9500 - val_loss: 1.3913
Epoch 1595/5000
26/26 - 1s - loss: 0.9485 - val_loss: 1.3911
Epoch 1596/5000
26/26 - 1s - loss: 0.9481 - val_loss: 1.3909
Epoch 1597/5000
26/26 - 1s - loss: 0.9474 - val_loss: 1.3904
Epoch 1598/5000
26/26 - 1s - loss: 0.9477 - val_loss: 1.3913
Epoch 1599/5000
26/26 - 1s - loss: 0.9463 - val_loss: 1.3904
Epoch 1600/5000
26/26 - 1s - loss: 0.9455 - val_loss: 1.3901
Epoch 01600: val_loss improved from 1.39464 to 1.39007, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1601/5000
26/26 - 1s - loss: 0.9440 - val_loss: 1.3889
Epoch 1602/5000
26/26 - 1s - loss: 0.9461 - val_loss: 1.3895
Epoch 1603/5000
26/26 - 1s - loss: 0.9461 - val_loss: 1.3882
Epoch 1604/5000
26/26 - 1s - loss: 0.9447 - val_loss: 1.3870
Epoch 1605/5000
26/26 - 1s - loss: 0.9443 - val_loss: 1.3861
Epoch 1606/5000
26/26 - 1s - loss: 0.9427 - val_loss: 1.3857
Epoch 1607/5000
26/26 - 1s - loss: 0.9413 - val_loss: 1.3853
Epoch 1608/5000
26/26 - 1s - loss: 0.9402 - val_loss: 1.3850
Epoch 1609/5000
26/26 - 1s - loss: 0.9412 - val_loss: 1.3848
Epoch 1610/5000
26/26 - 1s - loss: 0.9406 - val_loss: 1.3839
Epoch 01610: val_loss improved from 1.39007 to 1.38390, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1611/5000
26/26 - 1s - loss: 0.9394 - val_loss: 1.3835
Epoch 1612/5000
26/26 - 1s - loss: 0.9395 - val_loss: 1.3848
Epoch 1613/5000
26/26 - 1s - loss: 0.9398 - val_loss: 1.3813
Epoch 1614/5000
26/26 - 1s - loss: 0.9385 - val_loss: 1.3818
Epoch 1615/5000
26/26 - 2s - loss: 0.9367 - val_loss: 1.3816
Epoch 1616/5000
26/26 - 1s - loss: 0.9371 - val_loss: 1.3814
Epoch 1617/5000
26/26 - 1s - loss: 0.9367 - val_loss: 1.3801
Epoch 1618/5000
26/26 - 1s - loss: 0.9368 - val_loss: 1.3822
Epoch 1619/5000
26/26 - 1s - loss: 0.9359 - val_loss: 1.3793
Epoch 1620/5000
26/26 - 1s - loss: 0.9350 - val_loss: 1.3805
Epoch 01620: val_loss improved from 1.38390 to 1.38053, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1621/5000
26/26 - 1s - loss: 0.9351 - val_loss: 1.3790
Epoch 1622/5000
26/26 - 1s - loss: 0.9320 - val_loss: 1.3774
Epoch 1623/5000
26/26 - 1s - loss: 0.9337 - val_loss: 1.3769
Epoch 1624/5000
26/26 - 1s - loss: 0.9350 - val_loss: 1.3770
Epoch 1625/5000
26/26 - 1s - loss: 0.9339 - val_loss: 1.3767
Epoch 1626/5000
26/26 - 1s - loss: 0.9333 - val_loss: 1.3768
Epoch 1627/5000
26/26 - 1s - loss: 0.9308 - val_loss: 1.3755
Epoch 1628/5000
26/26 - 1s - loss: 0.9310 - val_loss: 1.3741
Epoch 1629/5000
26/26 - 1s - loss: 0.9296 - val_loss: 1.3741
Epoch 1630/5000
26/26 - 1s - loss: 0.9300 - val_loss: 1.3726
Epoch 01630: val_loss improved from 1.38053 to 1.37261, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1631/5000
26/26 - 1s - loss: 0.9292 - val_loss: 1.3736
Epoch 1632/5000
26/26 - 1s - loss: 0.9306 - val_loss: 1.3732
Epoch 1633/5000
26/26 - 1s - loss: 0.9275 - val_loss: 1.3723
Epoch 1634/5000
26/26 - 1s - loss: 0.9293 - val_loss: 1.3712
Epoch 1635/5000
26/26 - 1s - loss: 0.9281 - val_loss: 1.3697
Epoch 1636/5000
26/26 - 1s - loss: 0.9276 - val_loss: 1.3688
Epoch 1637/5000
26/26 - 1s - loss: 0.9267 - val_loss: 1.3711
Epoch 1638/5000
26/26 - 1s - loss: 0.9255 - val_loss: 1.3713
Epoch 1639/5000
26/26 - 1s - loss: 0.9264 - val_loss: 1.3710
Epoch 1640/5000
26/26 - 1s - loss: 0.9249 - val_loss: 1.3699
Epoch 01640: val_loss improved from 1.37261 to 1.36992, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1641/5000
26/26 - 1s - loss: 0.9248 - val_loss: 1.3689
Epoch 1642/5000
26/26 - 1s - loss: 0.9248 - val_loss: 1.3696
Epoch 1643/5000
26/26 - 1s - loss: 0.9224 - val_loss: 1.3664
Epoch 1644/5000
26/26 - 1s - loss: 0.9233 - val_loss: 1.3666
Epoch 1645/5000
26/26 - 1s - loss: 0.9232 - val_loss: 1.3665
Epoch 1646/5000
26/26 - 1s - loss: 0.9212 - val_loss: 1.3657
Epoch 1647/5000
26/26 - 1s - loss: 0.9206 - val_loss: 1.3652
Epoch 1648/5000
26/26 - 1s - loss: 0.9209 - val_loss: 1.3647
Epoch 1649/5000
26/26 - 1s - loss: 0.9196 - val_loss: 1.3651
Epoch 1650/5000
26/26 - 1s - loss: 0.9207 - val_loss: 1.3638
Epoch 01650: val_loss improved from 1.36992 to 1.36382, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1651/5000
26/26 - 1s - loss: 0.9193 - val_loss: 1.3642
Epoch 1652/5000
26/26 - 1s - loss: 0.9187 - val_loss: 1.3629
Epoch 1653/5000
26/26 - 1s - loss: 0.9190 - val_loss: 1.3632
Epoch 1654/5000
26/26 - 1s - loss: 0.9173 - val_loss: 1.3625
Epoch 1655/5000
26/26 - 1s - loss: 0.9190 - val_loss: 1.3601
Epoch 1656/5000
26/26 - 1s - loss: 0.9153 - val_loss: 1.3596
Epoch 1657/5000
26/26 - 1s - loss: 0.9167 - val_loss: 1.3615
Epoch 1658/5000
26/26 - 1s - loss: 0.9175 - val_loss: 1.3601
Epoch 1659/5000
26/26 - 1s - loss: 0.9165 - val_loss: 1.3595
Epoch 1660/5000
26/26 - 1s - loss: 0.9157 - val_loss: 1.3582
Epoch 01660: val_loss improved from 1.36382 to 1.35824, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1661/5000
26/26 - 1s - loss: 0.9154 - val_loss: 1.3587
Epoch 1662/5000
26/26 - 1s - loss: 0.9127 - val_loss: 1.3582
Epoch 1663/5000
26/26 - 1s - loss: 0.9147 - val_loss: 1.3552
Epoch 1664/5000
26/26 - 1s - loss: 0.9115 - val_loss: 1.3567
Epoch 1665/5000
26/26 - 1s - loss: 0.9145 - val_loss: 1.3548
Epoch 1666/5000
26/26 - 1s - loss: 0.9115 - val_loss: 1.3561
Epoch 1667/5000
26/26 - 1s - loss: 0.9112 - val_loss: 1.3554
Epoch 1668/5000
26/26 - 1s - loss: 0.9132 - val_loss: 1.3558
Epoch 1669/5000
26/26 - 1s - loss: 0.9113 - val_loss: 1.3535
Epoch 1670/5000
26/26 - 1s - loss: 0.9103 - val_loss: 1.3537
Epoch 01670: val_loss improved from 1.35824 to 1.35366, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1671/5000
26/26 - 1s - loss: 0.9104 - val_loss: 1.3538
Epoch 1672/5000
26/26 - 1s - loss: 0.9092 - val_loss: 1.3530
Epoch 1673/5000
26/26 - 1s - loss: 0.9095 - val_loss: 1.3533
Epoch 1674/5000
26/26 - 1s - loss: 0.9097 - val_loss: 1.3508
Epoch 1675/5000
26/26 - 1s - loss: 0.9076 - val_loss: 1.3508
Epoch 1676/5000
26/26 - 2s - loss: 0.9073 - val_loss: 1.3489
Epoch 1677/5000
26/26 - 1s - loss: 0.9087 - val_loss: 1.3503
Epoch 1678/5000
26/26 - 1s - loss: 0.9064 - val_loss: 1.3504
Epoch 1679/5000
26/26 - 1s - loss: 0.9045 - val_loss: 1.3505
Epoch 1680/5000
26/26 - 1s - loss: 0.9048 - val_loss: 1.3481
Epoch 01680: val_loss improved from 1.35366 to 1.34811, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1681/5000
26/26 - 1s - loss: 0.9029 - val_loss: 1.3498
Epoch 1682/5000
26/26 - 1s - loss: 0.9042 - val_loss: 1.3502
Epoch 1683/5000
26/26 - 1s - loss: 0.9026 - val_loss: 1.3485
Epoch 1684/5000
26/26 - 1s - loss: 0.9048 - val_loss: 1.3469
Epoch 1685/5000
26/26 - 1s - loss: 0.9033 - val_loss: 1.3461
Epoch 1686/5000
26/26 - 1s - loss: 0.9018 - val_loss: 1.3457
Epoch 1687/5000
26/26 - 1s - loss: 0.9024 - val_loss: 1.3467
Epoch 1688/5000
26/26 - 1s - loss: 0.9025 - val_loss: 1.3463
Epoch 1689/5000
26/26 - 1s - loss: 0.9006 - val_loss: 1.3440
Epoch 1690/5000
26/26 - 1s - loss: 0.9005 - val_loss: 1.3438
Epoch 01690: val_loss improved from 1.34811 to 1.34381, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1691/5000
26/26 - 1s - loss: 0.8994 - val_loss: 1.3428
Epoch 1692/5000
26/26 - 1s - loss: 0.9006 - val_loss: 1.3426
Epoch 1693/5000
26/26 - 1s - loss: 0.8993 - val_loss: 1.3431
Epoch 1694/5000
26/26 - 1s - loss: 0.8983 - val_loss: 1.3438
Epoch 1695/5000
26/26 - 1s - loss: 0.8977 - val_loss: 1.3411
Epoch 1696/5000
26/26 - 1s - loss: 0.8970 - val_loss: 1.3415
Epoch 1697/5000
26/26 - 1s - loss: 0.8972 - val_loss: 1.3397
Epoch 1698/5000
26/26 - 1s - loss: 0.8968 - val_loss: 1.3411
Epoch 1699/5000
26/26 - 1s - loss: 0.8970 - val_loss: 1.3395
Epoch 1700/5000
26/26 - 1s - loss: 0.8941 - val_loss: 1.3408
Epoch 01700: val_loss improved from 1.34381 to 1.34075, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1701/5000
26/26 - 1s - loss: 0.8946 - val_loss: 1.3393
Epoch 1702/5000
26/26 - 1s - loss: 0.8963 - val_loss: 1.3381
Epoch 1703/5000
26/26 - 1s - loss: 0.8945 - val_loss: 1.3363
Epoch 1704/5000
26/26 - 1s - loss: 0.8925 - val_loss: 1.3378
Epoch 1705/5000
26/26 - 1s - loss: 0.8930 - val_loss: 1.3350
Epoch 1706/5000
26/26 - 1s - loss: 0.8931 - val_loss: 1.3360
Epoch 1707/5000
26/26 - 1s - loss: 0.8918 - val_loss: 1.3361
Epoch 1708/5000
26/26 - 1s - loss: 0.8915 - val_loss: 1.3357
Epoch 1709/5000
26/26 - 1s - loss: 0.8922 - val_loss: 1.3341
Epoch 1710/5000
26/26 - 1s - loss: 0.8906 - val_loss: 1.3359
Epoch 01710: val_loss improved from 1.34075 to 1.33593, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1711/5000
26/26 - 1s - loss: 0.8913 - val_loss: 1.3338
Epoch 1712/5000
26/26 - 1s - loss: 0.8903 - val_loss: 1.3329
Epoch 1713/5000
26/26 - 1s - loss: 0.8903 - val_loss: 1.3320
Epoch 1714/5000
26/26 - 1s - loss: 0.8887 - val_loss: 1.3330
Epoch 1715/5000
26/26 - 1s - loss: 0.8882 - val_loss: 1.3337
Epoch 1716/5000
26/26 - 1s - loss: 0.8862 - val_loss: 1.3298
Epoch 1717/5000
26/26 - 1s - loss: 0.8882 - val_loss: 1.3308
Epoch 1718/5000
26/26 - 1s - loss: 0.8861 - val_loss: 1.3297
Epoch 1719/5000
26/26 - 1s - loss: 0.8853 - val_loss: 1.3302
Epoch 1720/5000
26/26 - 1s - loss: 0.8857 - val_loss: 1.3301
Epoch 01720: val_loss improved from 1.33593 to 1.33009, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1721/5000
26/26 - 1s - loss: 0.8857 - val_loss: 1.3295
Epoch 1722/5000
26/26 - 1s - loss: 0.8865 - val_loss: 1.3306
Epoch 1723/5000
26/26 - 1s - loss: 0.8858 - val_loss: 1.3294
Epoch 1724/5000
26/26 - 1s - loss: 0.8850 - val_loss: 1.3282
Epoch 1725/5000
26/26 - 1s - loss: 0.8829 - val_loss: 1.3274
Epoch 1726/5000
26/26 - 1s - loss: 0.8838 - val_loss: 1.3285
Epoch 1727/5000
26/26 - 1s - loss: 0.8831 - val_loss: 1.3280
Epoch 1728/5000
26/26 - 1s - loss: 0.8826 - val_loss: 1.3263
Epoch 1729/5000
26/26 - 1s - loss: 0.8816 - val_loss: 1.3275
Epoch 1730/5000
26/26 - 1s - loss: 0.8822 - val_loss: 1.3279
Epoch 01730: val_loss improved from 1.33009 to 1.32787, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1731/5000
26/26 - 1s - loss: 0.8798 - val_loss: 1.3258
Epoch 1732/5000
26/26 - 1s - loss: 0.8809 - val_loss: 1.3237
Epoch 1733/5000
26/26 - 1s - loss: 0.8794 - val_loss: 1.3244
Epoch 1734/5000
26/26 - 2s - loss: 0.8799 - val_loss: 1.3239
Epoch 1735/5000
26/26 - 1s - loss: 0.8777 - val_loss: 1.3241
Epoch 1736/5000
26/26 - 1s - loss: 0.8802 - val_loss: 1.3219
Epoch 1737/5000
26/26 - 1s - loss: 0.8776 - val_loss: 1.3215
Epoch 1738/5000
26/26 - 1s - loss: 0.8775 - val_loss: 1.3214
Epoch 1739/5000
26/26 - 1s - loss: 0.8764 - val_loss: 1.3203
Epoch 1740/5000
26/26 - 1s - loss: 0.8770 - val_loss: 1.3195
Epoch 01740: val_loss improved from 1.32787 to 1.31953, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1741/5000
26/26 - 1s - loss: 0.8765 - val_loss: 1.3196
Epoch 1742/5000
26/26 - 1s - loss: 0.8765 - val_loss: 1.3201
Epoch 1743/5000
26/26 - 1s - loss: 0.8735 - val_loss: 1.3180
Epoch 1744/5000
26/26 - 1s - loss: 0.8756 - val_loss: 1.3195
Epoch 1745/5000
26/26 - 1s - loss: 0.8751 - val_loss: 1.3187
Epoch 1746/5000
26/26 - 1s - loss: 0.8742 - val_loss: 1.3174
Epoch 1747/5000
26/26 - 1s - loss: 0.8723 - val_loss: 1.3197
Epoch 1748/5000
26/26 - 1s - loss: 0.8724 - val_loss: 1.3171
Epoch 1749/5000
26/26 - 1s - loss: 0.8724 - val_loss: 1.3174
Epoch 1750/5000
26/26 - 1s - loss: 0.8727 - val_loss: 1.3173
Epoch 01750: val_loss improved from 1.31953 to 1.31728, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1751/5000
26/26 - 1s - loss: 0.8713 - val_loss: 1.3156
Epoch 1752/5000
26/26 - 1s - loss: 0.8713 - val_loss: 1.3160
Epoch 1753/5000
26/26 - 1s - loss: 0.8707 - val_loss: 1.3147
Epoch 1754/5000
26/26 - 1s - loss: 0.8717 - val_loss: 1.3132
Epoch 1755/5000
26/26 - 1s - loss: 0.8708 - val_loss: 1.3145
Epoch 1756/5000
26/26 - 1s - loss: 0.8700 - val_loss: 1.3146
Epoch 1757/5000
26/26 - 1s - loss: 0.8680 - val_loss: 1.3133
Epoch 1758/5000
26/26 - 1s - loss: 0.8687 - val_loss: 1.3107
Epoch 1759/5000
26/26 - 1s - loss: 0.8704 - val_loss: 1.3114
Epoch 1760/5000
26/26 - 1s - loss: 0.8681 - val_loss: 1.3122
Epoch 01760: val_loss improved from 1.31728 to 1.31223, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1761/5000
26/26 - 1s - loss: 0.8684 - val_loss: 1.3122
Epoch 1762/5000
26/26 - 1s - loss: 0.8654 - val_loss: 1.3122
Epoch 1763/5000
26/26 - 1s - loss: 0.8668 - val_loss: 1.3123
Epoch 1764/5000
26/26 - 1s - loss: 0.8657 - val_loss: 1.3110
Epoch 1765/5000
26/26 - 1s - loss: 0.8658 - val_loss: 1.3124
Epoch 1766/5000
26/26 - 1s - loss: 0.8648 - val_loss: 1.3099
Epoch 1767/5000
26/26 - 1s - loss: 0.8646 - val_loss: 1.3100
Epoch 1768/5000
26/26 - 1s - loss: 0.8650 - val_loss: 1.3097
Epoch 1769/5000
26/26 - 1s - loss: 0.8638 - val_loss: 1.3070
Epoch 1770/5000
26/26 - 1s - loss: 0.8635 - val_loss: 1.3082
Epoch 01770: val_loss improved from 1.31223 to 1.30824, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1771/5000
26/26 - 1s - loss: 0.8616 - val_loss: 1.3090
Epoch 1772/5000
26/26 - 1s - loss: 0.8615 - val_loss: 1.3059
Epoch 1773/5000
26/26 - 1s - loss: 0.8630 - val_loss: 1.3063
Epoch 1774/5000
26/26 - 1s - loss: 0.8616 - val_loss: 1.3074
Epoch 1775/5000
26/26 - 1s - loss: 0.8596 - val_loss: 1.3057
Epoch 1776/5000
26/26 - 1s - loss: 0.8600 - val_loss: 1.3045
Epoch 1777/5000
26/26 - 1s - loss: 0.8594 - val_loss: 1.3064
Epoch 1778/5000
26/26 - 1s - loss: 0.8592 - val_loss: 1.3054
Epoch 1779/5000
26/26 - 1s - loss: 0.8578 - val_loss: 1.3035
Epoch 1780/5000
26/26 - 1s - loss: 0.8600 - val_loss: 1.3044
Epoch 01780: val_loss improved from 1.30824 to 1.30442, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1781/5000
26/26 - 1s - loss: 0.8591 - val_loss: 1.3038
Epoch 1782/5000
26/26 - 1s - loss: 0.8575 - val_loss: 1.3036
Epoch 1783/5000
26/26 - 1s - loss: 0.8569 - val_loss: 1.3025
Epoch 1784/5000
26/26 - 1s - loss: 0.8586 - val_loss: 1.3019
Epoch 1785/5000
26/26 - 1s - loss: 0.8552 - val_loss: 1.3006
Epoch 1786/5000
26/26 - 1s - loss: 0.8557 - val_loss: 1.3016
Epoch 1787/5000
26/26 - 1s - loss: 0.8551 - val_loss: 1.2993
Epoch 1788/5000
26/26 - 1s - loss: 0.8552 - val_loss: 1.2987
Epoch 1789/5000
26/26 - 1s - loss: 0.8551 - val_loss: 1.3005
Epoch 1790/5000
26/26 - 1s - loss: 0.8550 - val_loss: 1.2992
Epoch 01790: val_loss improved from 1.30442 to 1.29916, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1791/5000
26/26 - 1s - loss: 0.8544 - val_loss: 1.2986
Epoch 1792/5000
26/26 - 1s - loss: 0.8529 - val_loss: 1.2977
Epoch 1793/5000
26/26 - 1s - loss: 0.8537 - val_loss: 1.2971
Epoch 1794/5000
26/26 - 1s - loss: 0.8519 - val_loss: 1.2973
Epoch 1795/5000
26/26 - 1s - loss: 0.8532 - val_loss: 1.2967
Epoch 1796/5000
26/26 - 1s - loss: 0.8507 - val_loss: 1.2960
Epoch 1797/5000
26/26 - 1s - loss: 0.8512 - val_loss: 1.2955
Epoch 1798/5000
26/26 - 1s - loss: 0.8510 - val_loss: 1.2962
Epoch 1799/5000
26/26 - 1s - loss: 0.8500 - val_loss: 1.2960
Epoch 1800/5000
26/26 - 1s - loss: 0.8513 - val_loss: 1.2948
Epoch 01800: val_loss improved from 1.29916 to 1.29482, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1801/5000
26/26 - 1s - loss: 0.8489 - val_loss: 1.2944
Epoch 1802/5000
26/26 - 1s - loss: 0.8490 - val_loss: 1.2951
Epoch 1803/5000
26/26 - 1s - loss: 0.8513 - val_loss: 1.2926
Epoch 1804/5000
26/26 - 2s - loss: 0.8475 - val_loss: 1.2946
Epoch 1805/5000
26/26 - 1s - loss: 0.8480 - val_loss: 1.2942
Epoch 1806/5000
26/26 - 1s - loss: 0.8491 - val_loss: 1.2922
Epoch 1807/5000
26/26 - 1s - loss: 0.8484 - val_loss: 1.2921
Epoch 1808/5000
26/26 - 1s - loss: 0.8465 - val_loss: 1.2928
Epoch 1809/5000
26/26 - 1s - loss: 0.8464 - val_loss: 1.2889
Epoch 1810/5000
26/26 - 1s - loss: 0.8446 - val_loss: 1.2901
Epoch 01810: val_loss improved from 1.29482 to 1.29014, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1811/5000
26/26 - 1s - loss: 0.8436 - val_loss: 1.2895
Epoch 1812/5000
26/26 - 1s - loss: 0.8456 - val_loss: 1.2913
Epoch 1813/5000
26/26 - 1s - loss: 0.8446 - val_loss: 1.2880
Epoch 1814/5000
26/26 - 1s - loss: 0.8441 - val_loss: 1.2884
Epoch 1815/5000
26/26 - 1s - loss: 0.8445 - val_loss: 1.2882
Epoch 1816/5000
26/26 - 1s - loss: 0.8417 - val_loss: 1.2879
Epoch 1817/5000
26/26 - 1s - loss: 0.8424 - val_loss: 1.2887
Epoch 1818/5000
26/26 - 1s - loss: 0.8415 - val_loss: 1.2870
Epoch 1819/5000
26/26 - 1s - loss: 0.8424 - val_loss: 1.2877
Epoch 1820/5000
26/26 - 1s - loss: 0.8412 - val_loss: 1.2872
Epoch 01820: val_loss improved from 1.29014 to 1.28716, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1821/5000
26/26 - 1s - loss: 0.8410 - val_loss: 1.2845
Epoch 1822/5000
26/26 - 1s - loss: 0.8400 - val_loss: 1.2858
Epoch 1823/5000
26/26 - 1s - loss: 0.8400 - val_loss: 1.2854
Epoch 1824/5000
26/26 - 1s - loss: 0.8390 - val_loss: 1.2865
Epoch 1825/5000
26/26 - 1s - loss: 0.8404 - val_loss: 1.2852
Epoch 1826/5000
26/26 - 1s - loss: 0.8399 - val_loss: 1.2835
Epoch 1827/5000
26/26 - 1s - loss: 0.8384 - val_loss: 1.2843
Epoch 1828/5000
26/26 - 2s - loss: 0.8374 - val_loss: 1.2821
Epoch 1829/5000
26/26 - 1s - loss: 0.8376 - val_loss: 1.2823
Epoch 1830/5000
26/26 - 1s - loss: 0.8373 - val_loss: 1.2822
Epoch 01830: val_loss improved from 1.28716 to 1.28224, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1831/5000
26/26 - 1s - loss: 0.8370 - val_loss: 1.2812
Epoch 1832/5000
26/26 - 1s - loss: 0.8366 - val_loss: 1.2806
Epoch 1833/5000
26/26 - 1s - loss: 0.8370 - val_loss: 1.2816
Epoch 1834/5000
26/26 - 1s - loss: 0.8358 - val_loss: 1.2809
Epoch 1835/5000
26/26 - 1s - loss: 0.8366 - val_loss: 1.2808
Epoch 1836/5000
26/26 - 1s - loss: 0.8352 - val_loss: 1.2797
Epoch 1837/5000
26/26 - 1s - loss: 0.8347 - val_loss: 1.2799
Epoch 1838/5000
26/26 - 2s - loss: 0.8347 - val_loss: 1.2797
Epoch 1839/5000
26/26 - 1s - loss: 0.8322 - val_loss: 1.2787
Epoch 1840/5000
26/26 - 1s - loss: 0.8339 - val_loss: 1.2759
Epoch 01840: val_loss improved from 1.28224 to 1.27586, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1841/5000
26/26 - 1s - loss: 0.8321 - val_loss: 1.2761
Epoch 1842/5000
26/26 - 1s - loss: 0.8323 - val_loss: 1.2778
Epoch 1843/5000
26/26 - 1s - loss: 0.8315 - val_loss: 1.2765
Epoch 1844/5000
26/26 - 1s - loss: 0.8310 - val_loss: 1.2759
Epoch 1845/5000
26/26 - 1s - loss: 0.8314 - val_loss: 1.2767
Epoch 1846/5000
26/26 - 1s - loss: 0.8312 - val_loss: 1.2747
Epoch 1847/5000
26/26 - 1s - loss: 0.8297 - val_loss: 1.2754
Epoch 1848/5000
26/26 - 1s - loss: 0.8299 - val_loss: 1.2740
Epoch 1849/5000
26/26 - 1s - loss: 0.8299 - val_loss: 1.2743
Epoch 1850/5000
26/26 - 1s - loss: 0.8301 - val_loss: 1.2744
Epoch 01850: val_loss improved from 1.27586 to 1.27437, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1851/5000
26/26 - 1s - loss: 0.8287 - val_loss: 1.2734
Epoch 1852/5000
26/26 - 1s - loss: 0.8284 - val_loss: 1.2729
Epoch 1853/5000
26/26 - 1s - loss: 0.8270 - val_loss: 1.2732
Epoch 1854/5000
26/26 - 1s - loss: 0.8261 - val_loss: 1.2720
Epoch 1855/5000
26/26 - 1s - loss: 0.8271 - val_loss: 1.2720
Epoch 1856/5000
26/26 - 1s - loss: 0.8277 - val_loss: 1.2724
Epoch 1857/5000
26/26 - 1s - loss: 0.8271 - val_loss: 1.2719
Epoch 1858/5000
26/26 - 1s - loss: 0.8257 - val_loss: 1.2710
Epoch 1859/5000
26/26 - 1s - loss: 0.8247 - val_loss: 1.2697
Epoch 1860/5000
26/26 - 1s - loss: 0.8251 - val_loss: 1.2716
Epoch 01860: val_loss improved from 1.27437 to 1.27163, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1861/5000
26/26 - 1s - loss: 0.8242 - val_loss: 1.2716
Epoch 1862/5000
26/26 - 1s - loss: 0.8248 - val_loss: 1.2704
Epoch 1863/5000
26/26 - 1s - loss: 0.8234 - val_loss: 1.2689
Epoch 1864/5000
26/26 - 1s - loss: 0.8231 - val_loss: 1.2686
Epoch 1865/5000
26/26 - 1s - loss: 0.8222 - val_loss: 1.2682
Epoch 1866/5000
26/26 - 1s - loss: 0.8230 - val_loss: 1.2694
Epoch 1867/5000
26/26 - 1s - loss: 0.8208 - val_loss: 1.2662
Epoch 1868/5000
26/26 - 1s - loss: 0.8213 - val_loss: 1.2667
Epoch 1869/5000
26/26 - 1s - loss: 0.8222 - val_loss: 1.2658
Epoch 1870/5000
26/26 - 1s - loss: 0.8224 - val_loss: 1.2660
Epoch 01870: val_loss improved from 1.27163 to 1.26596, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1871/5000
26/26 - 1s - loss: 0.8198 - val_loss: 1.2654
Epoch 1872/5000
26/26 - 1s - loss: 0.8184 - val_loss: 1.2662
Epoch 1873/5000
26/26 - 1s - loss: 0.8201 - val_loss: 1.2624
Epoch 1874/5000
26/26 - 1s - loss: 0.8194 - val_loss: 1.2650
Epoch 1875/5000
26/26 - 1s - loss: 0.8188 - val_loss: 1.2632
Epoch 1876/5000
26/26 - 1s - loss: 0.8177 - val_loss: 1.2628
Epoch 1877/5000
26/26 - 1s - loss: 0.8189 - val_loss: 1.2634
Epoch 1878/5000
26/26 - 1s - loss: 0.8167 - val_loss: 1.2626
Epoch 1879/5000
26/26 - 1s - loss: 0.8187 - val_loss: 1.2619
Epoch 1880/5000
26/26 - 1s - loss: 0.8164 - val_loss: 1.2605
Epoch 01880: val_loss improved from 1.26596 to 1.26055, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1881/5000
26/26 - 1s - loss: 0.8177 - val_loss: 1.2627
Epoch 1882/5000
26/26 - 1s - loss: 0.8156 - val_loss: 1.2598
Epoch 1883/5000
26/26 - 1s - loss: 0.8157 - val_loss: 1.2600
Epoch 1884/5000
26/26 - 1s - loss: 0.8144 - val_loss: 1.2608
Epoch 1885/5000
26/26 - 1s - loss: 0.8158 - val_loss: 1.2589
Epoch 1886/5000
26/26 - 1s - loss: 0.8155 - val_loss: 1.2587
Epoch 1887/5000
26/26 - 1s - loss: 0.8157 - val_loss: 1.2595
Epoch 1888/5000
26/26 - 1s - loss: 0.8136 - val_loss: 1.2592
Epoch 1889/5000
26/26 - 1s - loss: 0.8134 - val_loss: 1.2594
Epoch 1890/5000
26/26 - 1s - loss: 0.8150 - val_loss: 1.2584
Epoch 01890: val_loss improved from 1.26055 to 1.25842, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1891/5000
26/26 - 1s - loss: 0.8121 - val_loss: 1.2578
Epoch 1892/5000
26/26 - 1s - loss: 0.8104 - val_loss: 1.2569
Epoch 1893/5000
26/26 - 1s - loss: 0.8123 - val_loss: 1.2559
Epoch 1894/5000
26/26 - 1s - loss: 0.8122 - val_loss: 1.2550
Epoch 1895/5000
26/26 - 1s - loss: 0.8105 - val_loss: 1.2549
Epoch 1896/5000
26/26 - 1s - loss: 0.8105 - val_loss: 1.2561
Epoch 1897/5000
26/26 - 1s - loss: 0.8116 - val_loss: 1.2541
Epoch 1898/5000
26/26 - 1s - loss: 0.8099 - val_loss: 1.2536
Epoch 1899/5000
26/26 - 2s - loss: 0.8082 - val_loss: 1.2550
Epoch 1900/5000
26/26 - 2s - loss: 0.8091 - val_loss: 1.2546
Epoch 01900: val_loss improved from 1.25842 to 1.25456, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1901/5000
26/26 - 1s - loss: 0.8082 - val_loss: 1.2533
Epoch 1902/5000
26/26 - 1s - loss: 0.8076 - val_loss: 1.2525
Epoch 1903/5000
26/26 - 1s - loss: 0.8078 - val_loss: 1.2524
Epoch 1904/5000
26/26 - 1s - loss: 0.8063 - val_loss: 1.2523
Epoch 1905/5000
26/26 - 1s - loss: 0.8062 - val_loss: 1.2507
Epoch 1906/5000
26/26 - 1s - loss: 0.8059 - val_loss: 1.2517
Epoch 1907/5000
26/26 - 1s - loss: 0.8061 - val_loss: 1.2530
Epoch 1908/5000
26/26 - 1s - loss: 0.8056 - val_loss: 1.2497
Epoch 1909/5000
26/26 - 1s - loss: 0.8071 - val_loss: 1.2517
Epoch 1910/5000
26/26 - 1s - loss: 0.8060 - val_loss: 1.2497
Epoch 01910: val_loss improved from 1.25456 to 1.24967, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1911/5000
26/26 - 1s - loss: 0.8050 - val_loss: 1.2502
Epoch 1912/5000
26/26 - 1s - loss: 0.8039 - val_loss: 1.2485
Epoch 1913/5000
26/26 - 1s - loss: 0.8036 - val_loss: 1.2506
Epoch 1914/5000
26/26 - 1s - loss: 0.8030 - val_loss: 1.2489
Epoch 1915/5000
26/26 - 1s - loss: 0.8043 - val_loss: 1.2488
Epoch 1916/5000
26/26 - 1s - loss: 0.8023 - val_loss: 1.2491
Epoch 1917/5000
26/26 - 1s - loss: 0.8034 - val_loss: 1.2475
Epoch 1918/5000
26/26 - 1s - loss: 0.8022 - val_loss: 1.2470
Epoch 1919/5000
26/26 - 1s - loss: 0.8011 - val_loss: 1.2474
Epoch 1920/5000
26/26 - 1s - loss: 0.8011 - val_loss: 1.2479
Epoch 01920: val_loss improved from 1.24967 to 1.24795, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1921/5000
26/26 - 1s - loss: 0.8000 - val_loss: 1.2455
Epoch 1922/5000
26/26 - 1s - loss: 0.8009 - val_loss: 1.2459
Epoch 1923/5000
26/26 - 1s - loss: 0.7999 - val_loss: 1.2456
Epoch 1924/5000
26/26 - 1s - loss: 0.7997 - val_loss: 1.2439
Epoch 1925/5000
26/26 - 1s - loss: 0.7997 - val_loss: 1.2456
Epoch 1926/5000
26/26 - 1s - loss: 0.7990 - val_loss: 1.2446
Epoch 1927/5000
26/26 - 1s - loss: 0.7991 - val_loss: 1.2445
Epoch 1928/5000
26/26 - 1s - loss: 0.7991 - val_loss: 1.2451
Epoch 1929/5000
26/26 - 1s - loss: 0.7983 - val_loss: 1.2440
Epoch 1930/5000
26/26 - 1s - loss: 0.7966 - val_loss: 1.2440
Epoch 01930: val_loss improved from 1.24795 to 1.24400, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1931/5000
26/26 - 1s - loss: 0.7951 - val_loss: 1.2421
Epoch 1932/5000
26/26 - 1s - loss: 0.7967 - val_loss: 1.2420
Epoch 1933/5000
26/26 - 1s - loss: 0.7966 - val_loss: 1.2418
Epoch 1934/5000
26/26 - 1s - loss: 0.7968 - val_loss: 1.2424
Epoch 1935/5000
26/26 - 1s - loss: 0.7951 - val_loss: 1.2417
Epoch 1936/5000
26/26 - 1s - loss: 0.7956 - val_loss: 1.2430
Epoch 1937/5000
26/26 - 1s - loss: 0.7956 - val_loss: 1.2412
Epoch 1938/5000
26/26 - 1s - loss: 0.7948 - val_loss: 1.2418
Epoch 1939/5000
26/26 - 1s - loss: 0.7934 - val_loss: 1.2395
Epoch 1940/5000
26/26 - 1s - loss: 0.7922 - val_loss: 1.2406
Epoch 01940: val_loss improved from 1.24400 to 1.24055, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1941/5000
26/26 - 2s - loss: 0.7939 - val_loss: 1.2401
Epoch 1942/5000
26/26 - 1s - loss: 0.7926 - val_loss: 1.2383
Epoch 1943/5000
26/26 - 1s - loss: 0.7922 - val_loss: 1.2387
Epoch 1944/5000
26/26 - 1s - loss: 0.7927 - val_loss: 1.2382
Epoch 1945/5000
26/26 - 1s - loss: 0.7927 - val_loss: 1.2373
Epoch 1946/5000
26/26 - 1s - loss: 0.7909 - val_loss: 1.2368
Epoch 1947/5000
26/26 - 1s - loss: 0.7900 - val_loss: 1.2357
Epoch 1948/5000
26/26 - 1s - loss: 0.7910 - val_loss: 1.2347
Epoch 1949/5000
26/26 - 1s - loss: 0.7909 - val_loss: 1.2366
Epoch 1950/5000
26/26 - 1s - loss: 0.7910 - val_loss: 1.2359
Epoch 01950: val_loss improved from 1.24055 to 1.23589, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1951/5000
26/26 - 1s - loss: 0.7897 - val_loss: 1.2365
Epoch 1952/5000
26/26 - 1s - loss: 0.7892 - val_loss: 1.2364
Epoch 1953/5000
26/26 - 1s - loss: 0.7875 - val_loss: 1.2346
Epoch 1954/5000
26/26 - 1s - loss: 0.7882 - val_loss: 1.2342
Epoch 1955/5000
26/26 - 1s - loss: 0.7884 - val_loss: 1.2333
Epoch 1956/5000
26/26 - 1s - loss: 0.7872 - val_loss: 1.2319
Epoch 1957/5000
26/26 - 1s - loss: 0.7871 - val_loss: 1.2323
Epoch 1958/5000
26/26 - 1s - loss: 0.7882 - val_loss: 1.2327
Epoch 1959/5000
26/26 - 1s - loss: 0.7871 - val_loss: 1.2331
Epoch 1960/5000
26/26 - 1s - loss: 0.7864 - val_loss: 1.2305
Epoch 01960: val_loss improved from 1.23589 to 1.23055, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1961/5000
26/26 - 1s - loss: 0.7868 - val_loss: 1.2305
Epoch 1962/5000
26/26 - 1s - loss: 0.7854 - val_loss: 1.2293
Epoch 1963/5000
26/26 - 1s - loss: 0.7861 - val_loss: 1.2304
Epoch 1964/5000
26/26 - 1s - loss: 0.7837 - val_loss: 1.2319
Epoch 1965/5000
26/26 - 1s - loss: 0.7839 - val_loss: 1.2304
Epoch 1966/5000
26/26 - 1s - loss: 0.7848 - val_loss: 1.2299
Epoch 1967/5000
26/26 - 1s - loss: 0.7841 - val_loss: 1.2279
Epoch 1968/5000
26/26 - 1s - loss: 0.7832 - val_loss: 1.2296
Epoch 1969/5000
26/26 - 1s - loss: 0.7842 - val_loss: 1.2276
Epoch 1970/5000
26/26 - 1s - loss: 0.7826 - val_loss: 1.2263
Epoch 01970: val_loss improved from 1.23055 to 1.22631, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1971/5000
26/26 - 1s - loss: 0.7807 - val_loss: 1.2280
Epoch 1972/5000
26/26 - 1s - loss: 0.7835 - val_loss: 1.2264
Epoch 1973/5000
26/26 - 1s - loss: 0.7813 - val_loss: 1.2263
Epoch 1974/5000
26/26 - 1s - loss: 0.7815 - val_loss: 1.2279
Epoch 1975/5000
26/26 - 1s - loss: 0.7812 - val_loss: 1.2273
Epoch 1976/5000
26/26 - 1s - loss: 0.7797 - val_loss: 1.2273
Epoch 1977/5000
26/26 - 1s - loss: 0.7788 - val_loss: 1.2262
Epoch 1978/5000
26/26 - 1s - loss: 0.7803 - val_loss: 1.2268
Epoch 1979/5000
26/26 - 1s - loss: 0.7779 - val_loss: 1.2252
Epoch 1980/5000
26/26 - 1s - loss: 0.7786 - val_loss: 1.2268
Epoch 01980: val_loss did not improve from 1.22631
Epoch 1981/5000
26/26 - 1s - loss: 0.7808 - val_loss: 1.2237
Epoch 1982/5000
26/26 - 1s - loss: 0.7775 - val_loss: 1.2241
Epoch 1983/5000
26/26 - 1s - loss: 0.7792 - val_loss: 1.2238
Epoch 1984/5000
26/26 - 1s - loss: 0.7771 - val_loss: 1.2239
Epoch 1985/5000
26/26 - 1s - loss: 0.7778 - val_loss: 1.2236
Epoch 1986/5000
26/26 - 1s - loss: 0.7785 - val_loss: 1.2229
Epoch 1987/5000
26/26 - 1s - loss: 0.7770 - val_loss: 1.2235
Epoch 1988/5000
26/26 - 1s - loss: 0.7767 - val_loss: 1.2230
Epoch 1989/5000
26/26 - 1s - loss: 0.7756 - val_loss: 1.2229
Epoch 1990/5000
26/26 - 1s - loss: 0.7756 - val_loss: 1.2214
Epoch 01990: val_loss improved from 1.22631 to 1.22144, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 1991/5000
26/26 - 1s - loss: 0.7746 - val_loss: 1.2219
Epoch 1992/5000
26/26 - 1s - loss: 0.7726 - val_loss: 1.2202
Epoch 1993/5000
26/26 - 1s - loss: 0.7745 - val_loss: 1.2196
Epoch 1994/5000
26/26 - 1s - loss: 0.7736 - val_loss: 1.2196
Epoch 1995/5000
26/26 - 1s - loss: 0.7722 - val_loss: 1.2191
Epoch 1996/5000
26/26 - 1s - loss: 0.7723 - val_loss: 1.2187
Epoch 1997/5000
26/26 - 1s - loss: 0.7733 - val_loss: 1.2179
Epoch 1998/5000
26/26 - 1s - loss: 0.7725 - val_loss: 1.2184
Epoch 1999/5000
26/26 - 1s - loss: 0.7737 - val_loss: 1.2184
Epoch 2000/5000
26/26 - 1s - loss: 0.7724 - val_loss: 1.2188
Epoch 02000: val_loss improved from 1.22144 to 1.21876, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 2001/5000
26/26 - 1s - loss: 0.7704 - val_loss: 1.2166
Epoch 2002/5000
26/26 - 1s - loss: 0.7712 - val_loss: 1.2154
Epoch 2003/5000
26/26 - 1s - loss: 0.7699 - val_loss: 1.2163
Epoch 2004/5000
26/26 - 1s - loss: 0.7706 - val_loss: 1.2163
Epoch 2005/5000
26/26 - 1s - loss: 0.7696 - val_loss: 1.2165
Epoch 2006/5000
26/26 - 1s - loss: 0.7690 - val_loss: 1.2138
Epoch 2007/5000
26/26 - 1s - loss: 0.7700 - val_loss: 1.2155
Epoch 2008/5000
26/26 - 1s - loss: 0.7692 - val_loss: 1.2138
Epoch 2009/5000
26/26 - 1s - loss: 0.7714 - val_loss: 1.2142
Epoch 2010/5000
26/26 - 1s - loss: 0.7677 - val_loss: 1.2156
Epoch 02010: val_loss improved from 1.21876 to 1.21559, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 2011/5000
26/26 - 1s - loss: 0.7690 - val_loss: 1.2142
Epoch 2012/5000
26/26 - 1s - loss: 0.7677 - val_loss: 1.2127
Epoch 2013/5000
26/26 - 1s - loss: 0.7651 - val_loss: 1.2133
Epoch 2014/5000
26/26 - 1s - loss: 0.7673 - val_loss: 1.2128
Epoch 2015/5000
26/26 - 1s - loss: 0.7660 - val_loss: 1.2128
Epoch 2016/5000
26/26 - 1s - loss: 0.7676 - val_loss: 1.2139
Epoch 2017/5000
26/26 - 1s - loss: 0.7660 - val_loss: 1.2140
Epoch 2018/5000
26/26 - 1s - loss: 0.7654 - val_loss: 1.2121
Epoch 2019/5000
26/26 - 1s - loss: 0.7640 - val_loss: 1.2105
Epoch 2020/5000
26/26 - 1s - loss: 0.7646 - val_loss: 1.2110
Epoch 02020: val_loss improved from 1.21559 to 1.21097, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 2021/5000
26/26 - 1s - loss: 0.7645 - val_loss: 1.2123
Epoch 2022/5000
26/26 - 1s - loss: 0.7652 - val_loss: 1.2103
Epoch 2023/5000
26/26 - 1s - loss: 0.7630 - val_loss: 1.2092
Epoch 2024/5000
26/26 - 2s - loss: 0.7645 - val_loss: 1.2099
Epoch 2025/5000
26/26 - 1s - loss: 0.7635 - val_loss: 1.2094
Epoch 2026/5000
26/26 - 1s - loss: 0.7629 - val_loss: 1.2096
Epoch 2027/5000
26/26 - 1s - loss: 0.7619 - val_loss: 1.2089
Epoch 2028/5000
26/26 - 1s - loss: 0.7638 - val_loss: 1.2097
Epoch 2029/5000
26/26 - 1s - loss: 0.7607 - val_loss: 1.2082
Epoch 2030/5000
26/26 - 1s - loss: 0.7613 - val_loss: 1.2087
Epoch 02030: val_loss improved from 1.21097 to 1.20873, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 2031/5000
26/26 - 1s - loss: 0.7616 - val_loss: 1.2082
Epoch 2032/5000
26/26 - 2s - loss: 0.7606 - val_loss: 1.2074
Epoch 2033/5000
26/26 - 1s - loss: 0.7614 - val_loss: 1.2058
Epoch 2034/5000
26/26 - 1s - loss: 0.7594 - val_loss: 1.2072
Epoch 2035/5000
26/26 - 1s - loss: 0.7598 - val_loss: 1.2057
Epoch 2036/5000
26/26 - 1s - loss: 0.7598 - val_loss: 1.2061
Epoch 2037/5000
26/26 - 1s - loss: 0.7583 - val_loss: 1.2052
Epoch 2038/5000
26/26 - 1s - loss: 0.7593 - val_loss: 1.2051
Epoch 2039/5000
26/26 - 1s - loss: 0.7589 - val_loss: 1.2057
Epoch 2040/5000
26/26 - 1s - loss: 0.7570 - val_loss: 1.2057
Epoch 02040: val_loss improved from 1.20873 to 1.20568, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 2041/5000
26/26 - 1s - loss: 0.7584 - val_loss: 1.2050
Epoch 2042/5000
26/26 - 1s - loss: 0.7581 - val_loss: 1.2031
Epoch 2043/5000
26/26 - 1s - loss: 0.7578 - val_loss: 1.2030
Epoch 2044/5000
26/26 - 1s - loss: 0.7549 - val_loss: 1.2034
Epoch 2045/5000
26/26 - 1s - loss: 0.7583 - val_loss: 1.2016
Epoch 2046/5000
26/26 - 1s - loss: 0.7547 - val_loss: 1.2012
Epoch 2047/5000
26/26 - 1s - loss: 0.7562 - val_loss: 1.2025
Epoch 2048/5000
26/26 - 1s - loss: 0.7570 - val_loss: 1.2025
Epoch 2049/5000
26/26 - 1s - loss: 0.7557 - val_loss: 1.2015
Epoch 2050/5000
26/26 - 1s - loss: 0.7546 - val_loss: 1.2021
Epoch 02050: val_loss improved from 1.20568 to 1.20212, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 2051/5000
26/26 - 1s - loss: 0.7546 - val_loss: 1.2021
Epoch 2052/5000
26/26 - 1s - loss: 0.7533 - val_loss: 1.2005
Epoch 2053/5000
26/26 - 1s - loss: 0.7546 - val_loss: 1.2005
Epoch 2054/5000
26/26 - 1s - loss: 0.7530 - val_loss: 1.1976
Epoch 2055/5000
26/26 - 1s - loss: 0.7535 - val_loss: 1.1994
Epoch 2056/5000
26/26 - 2s - loss: 0.7518 - val_loss: 1.2005
Epoch 2057/5000
26/26 - 1s - loss: 0.7531 - val_loss: 1.2000
Epoch 2058/5000
26/26 - 1s - loss: 0.7515 - val_loss: 1.1982
Epoch 2059/5000
26/26 - 1s - loss: 0.7511 - val_loss: 1.1979
Epoch 2060/5000
26/26 - 1s - loss: 0.7509 - val_loss: 1.1976
Epoch 02060: val_loss improved from 1.20212 to 1.19757, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 2061/5000
26/26 - 1s - loss: 0.7504 - val_loss: 1.1975
Epoch 2062/5000
26/26 - 1s - loss: 0.7511 - val_loss: 1.1983
Epoch 2063/5000
26/26 - 1s - loss: 0.7496 - val_loss: 1.1988
Epoch 2064/5000
26/26 - 1s - loss: 0.7507 - val_loss: 1.1986
Epoch 2065/5000
26/26 - 2s - loss: 0.7500 - val_loss: 1.1975
Epoch 2066/5000
26/26 - 1s - loss: 0.7498 - val_loss: 1.1965
Epoch 2067/5000
26/26 - 1s - loss: 0.7474 - val_loss: 1.1959
Epoch 2068/5000
26/26 - 1s - loss: 0.7491 - val_loss: 1.1968
Epoch 2069/5000
26/26 - 1s - loss: 0.7489 - val_loss: 1.1973
Epoch 2070/5000
26/26 - 1s - loss: 0.7474 - val_loss: 1.1954
Epoch 02070: val_loss improved from 1.19757 to 1.19541, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 2071/5000
26/26 - 1s - loss: 0.7487 - val_loss: 1.1952
Epoch 2072/5000
26/26 - 1s - loss: 0.7480 - val_loss: 1.1931
Epoch 2073/5000
26/26 - 1s - loss: 0.7483 - val_loss: 1.1940
Epoch 2074/5000
26/26 - 1s - loss: 0.7475 - val_loss: 1.1939
Epoch 2075/5000
26/26 - 1s - loss: 0.7481 - val_loss: 1.1929
Epoch 2076/5000
26/26 - 1s - loss: 0.7456 - val_loss: 1.1934
Epoch 2077/5000
26/26 - 1s - loss: 0.7449 - val_loss: 1.1934
Epoch 2078/5000
26/26 - 1s - loss: 0.7450 - val_loss: 1.1918
Epoch 2079/5000
26/26 - 1s - loss: 0.7449 - val_loss: 1.1928
Epoch 2080/5000
26/26 - 1s - loss: 0.7444 - val_loss: 1.1924
Epoch 02080: val_loss improved from 1.19541 to 1.19241, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 2081/5000
26/26 - 1s - loss: 0.7445 - val_loss: 1.1913
Epoch 2082/5000
26/26 - 1s - loss: 0.7443 - val_loss: 1.1908
Epoch 2083/5000
26/26 - 1s - loss: 0.7430 - val_loss: 1.1907
Epoch 2084/5000
26/26 - 1s - loss: 0.7434 - val_loss: 1.1910
Epoch 2085/5000
26/26 - 1s - loss: 0.7434 - val_loss: 1.1909
Epoch 2086/5000
26/26 - 1s - loss: 0.7419 - val_loss: 1.1905
Epoch 2087/5000
26/26 - 1s - loss: 0.7418 - val_loss: 1.1907
Epoch 2088/5000
26/26 - 1s - loss: 0.7429 - val_loss: 1.1894
Epoch 2089/5000
26/26 - 1s - loss: 0.7419 - val_loss: 1.1909
Epoch 2090/5000
26/26 - 1s - loss: 0.7414 - val_loss: 1.1875
Epoch 02090: val_loss improved from 1.19241 to 1.18752, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 2091/5000
26/26 - 1s - loss: 0.7414 - val_loss: 1.1893
Epoch 2092/5000
26/26 - 1s - loss: 0.7413 - val_loss: 1.1876
Epoch 2093/5000
26/26 - 1s - loss: 0.7396 - val_loss: 1.1884
Epoch 2094/5000
26/26 - 1s - loss: 0.7400 - val_loss: 1.1863
Epoch 2095/5000
26/26 - 1s - loss: 0.7399 - val_loss: 1.1885
Epoch 2096/5000
26/26 - 1s - loss: 0.7394 - val_loss: 1.1884
Epoch 2097/5000
26/26 - 1s - loss: 0.7383 - val_loss: 1.1886
Epoch 2098/5000
26/26 - 1s - loss: 0.7373 - val_loss: 1.1876
Epoch 2099/5000
26/26 - 1s - loss: 0.7388 - val_loss: 1.1868
Epoch 2100/5000
26/26 - 1s - loss: 0.7384 - val_loss: 1.1855
Epoch 02100: val_loss improved from 1.18752 to 1.18547, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 2101/5000
26/26 - 1s - loss: 0.7374 - val_loss: 1.1840
Epoch 2102/5000
26/26 - 1s - loss: 0.7374 - val_loss: 1.1842
Epoch 2103/5000
26/26 - 1s - loss: 0.7398 - val_loss: 1.1855
Epoch 2104/5000
26/26 - 1s - loss: 0.7365 - val_loss: 1.1841
Epoch 2105/5000
26/26 - 1s - loss: 0.7375 - val_loss: 1.1824
Epoch 2106/5000
26/26 - 2s - loss: 0.7371 - val_loss: 1.1825
Epoch 2107/5000
26/26 - 2s - loss: 0.7356 - val_loss: 1.1832
Epoch 2108/5000
26/26 - 1s - loss: 0.7360 - val_loss: 1.1827
Epoch 2109/5000
26/26 - 1s - loss: 0.7353 - val_loss: 1.1810
Epoch 2110/5000
26/26 - 1s - loss: 0.7347 - val_loss: 1.1831
Epoch 02110: val_loss improved from 1.18547 to 1.18312, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 2111/5000
26/26 - 1s - loss: 0.7360 - val_loss: 1.1822
Epoch 2112/5000
26/26 - 1s - loss: 0.7340 - val_loss: 1.1813
Epoch 2113/5000
26/26 - 2s - loss: 0.7337 - val_loss: 1.1807
Epoch 2114/5000
26/26 - 1s - loss: 0.7342 - val_loss: 1.1784
Epoch 2115/5000
26/26 - 1s - loss: 0.7340 - val_loss: 1.1794
Epoch 2116/5000
26/26 - 1s - loss: 0.7337 - val_loss: 1.1804
Epoch 2117/5000
26/26 - 1s - loss: 0.7331 - val_loss: 1.1790
Epoch 2118/5000
26/26 - 1s - loss: 0.7338 - val_loss: 1.1805
Epoch 2119/5000
26/26 - 1s - loss: 0.7314 - val_loss: 1.1784
Epoch 2120/5000
26/26 - 1s - loss: 0.7332 - val_loss: 1.1774
Epoch 02120: val_loss improved from 1.18312 to 1.17739, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 2121/5000
26/26 - 1s - loss: 0.7316 - val_loss: 1.1767
Epoch 2122/5000
26/26 - 1s - loss: 0.7326 - val_loss: 1.1777
Epoch 2123/5000
26/26 - 1s - loss: 0.7305 - val_loss: 1.1773
Epoch 2124/5000
26/26 - 1s - loss: 0.7309 - val_loss: 1.1786
Epoch 2125/5000
26/26 - 1s - loss: 0.7315 - val_loss: 1.1774
Epoch 2126/5000
26/26 - 1s - loss: 0.7303 - val_loss: 1.1771
Epoch 2127/5000
26/26 - 1s - loss: 0.7292 - val_loss: 1.1761
Epoch 2128/5000
26/26 - 1s - loss: 0.7292 - val_loss: 1.1757
Epoch 2129/5000
26/26 - 1s - loss: 0.7299 - val_loss: 1.1762
Epoch 2130/5000
26/26 - 1s - loss: 0.7304 - val_loss: 1.1780
Epoch 02130: val_loss did not improve from 1.17739
Epoch 2131/5000
26/26 - 1s - loss: 0.7286 - val_loss: 1.1761
Epoch 2132/5000
26/26 - 1s - loss: 0.7275 - val_loss: 1.1740
Epoch 2133/5000
26/26 - 1s - loss: 0.7275 - val_loss: 1.1759
Epoch 2134/5000
26/26 - 1s - loss: 0.7280 - val_loss: 1.1762
Epoch 2135/5000
26/26 - 1s - loss: 0.7275 - val_loss: 1.1744
Epoch 2136/5000
26/26 - 1s - loss: 0.7273 - val_loss: 1.1758
Epoch 2137/5000
26/26 - 1s - loss: 0.7269 - val_loss: 1.1746
Epoch 2138/5000
26/26 - 1s - loss: 0.7282 - val_loss: 1.1737
Epoch 2139/5000
26/26 - 1s - loss: 0.7261 - val_loss: 1.1761
Epoch 2140/5000
26/26 - 1s - loss: 0.7274 - val_loss: 1.1734
Epoch 02140: val_loss improved from 1.17739 to 1.17340, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 2141/5000
26/26 - 1s - loss: 0.7256 - val_loss: 1.1732
Epoch 2142/5000
26/26 - 1s - loss: 0.7256 - val_loss: 1.1731
Epoch 2143/5000
26/26 - 1s - loss: 0.7243 - val_loss: 1.1718
Epoch 2144/5000
26/26 - 1s - loss: 0.7247 - val_loss: 1.1713
Epoch 2145/5000
26/26 - 1s - loss: 0.7242 - val_loss: 1.1706
Epoch 2146/5000
26/26 - 1s - loss: 0.7230 - val_loss: 1.1719
Epoch 2147/5000
26/26 - 1s - loss: 0.7238 - val_loss: 1.1713
Epoch 2148/5000
26/26 - 2s - loss: 0.7224 - val_loss: 1.1689
Epoch 2149/5000
26/26 - 1s - loss: 0.7227 - val_loss: 1.1696
Epoch 2150/5000
26/26 - 1s - loss: 0.7217 - val_loss: 1.1684
Epoch 02150: val_loss improved from 1.17340 to 1.16842, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 2151/5000
26/26 - 1s - loss: 0.7221 - val_loss: 1.1696
Epoch 2152/5000
26/26 - 1s - loss: 0.7218 - val_loss: 1.1677
Epoch 2153/5000
26/26 - 1s - loss: 0.7224 - val_loss: 1.1689
Epoch 2154/5000
26/26 - 1s - loss: 0.7201 - val_loss: 1.1675
Epoch 2155/5000
26/26 - 1s - loss: 0.7229 - val_loss: 1.1696
Epoch 2156/5000
26/26 - 1s - loss: 0.7212 - val_loss: 1.1675
Epoch 2157/5000
26/26 - 2s - loss: 0.7207 - val_loss: 1.1674
Epoch 2158/5000
26/26 - 1s - loss: 0.7208 - val_loss: 1.1682
Epoch 2159/5000
26/26 - 1s - loss: 0.7192 - val_loss: 1.1672
Epoch 2160/5000
26/26 - 1s - loss: 0.7203 - val_loss: 1.1678
Epoch 02160: val_loss improved from 1.16842 to 1.16775, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 2161/5000
26/26 - 1s - loss: 0.7186 - val_loss: 1.1670
Epoch 2162/5000
26/26 - 1s - loss: 0.7178 - val_loss: 1.1658
Epoch 2163/5000
26/26 - 1s - loss: 0.7196 - val_loss: 1.1669
Epoch 2164/5000
26/26 - 1s - loss: 0.7172 - val_loss: 1.1657
Epoch 2165/5000
26/26 - 1s - loss: 0.7182 - val_loss: 1.1659
Epoch 2166/5000
26/26 - 1s - loss: 0.7186 - val_loss: 1.1664
Epoch 2167/5000
26/26 - 1s - loss: 0.7173 - val_loss: 1.1646
Epoch 2168/5000
26/26 - 1s - loss: 0.7178 - val_loss: 1.1642
Epoch 2169/5000
26/26 - 1s - loss: 0.7193 - val_loss: 1.1640
Epoch 2170/5000
26/26 - 1s - loss: 0.7178 - val_loss: 1.1641
Epoch 02170: val_loss improved from 1.16775 to 1.16414, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 2171/5000
26/26 - 1s - loss: 0.7175 - val_loss: 1.1632
Epoch 2172/5000
26/26 - 1s - loss: 0.7160 - val_loss: 1.1634
Epoch 2173/5000
26/26 - 1s - loss: 0.7149 - val_loss: 1.1639
Epoch 2174/5000
26/26 - 1s - loss: 0.7164 - val_loss: 1.1633
Epoch 2175/5000
26/26 - 1s - loss: 0.7156 - val_loss: 1.1626
Epoch 2176/5000
26/26 - 1s - loss: 0.7154 - val_loss: 1.1620
Epoch 2177/5000
26/26 - 1s - loss: 0.7146 - val_loss: 1.1627
Epoch 2178/5000
26/26 - 1s - loss: 0.7134 - val_loss: 1.1633
Epoch 2179/5000
26/26 - 2s - loss: 0.7139 - val_loss: 1.1611
Epoch 2180/5000
26/26 - 1s - loss: 0.7137 - val_loss: 1.1633
Epoch 02180: val_loss improved from 1.16414 to 1.16327, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 2181/5000
26/26 - 1s - loss: 0.7143 - val_loss: 1.1616
Epoch 2182/5000
26/26 - 1s - loss: 0.7131 - val_loss: 1.1624
Epoch 2183/5000
26/26 - 1s - loss: 0.7126 - val_loss: 1.1616
Epoch 2184/5000
26/26 - 1s - loss: 0.7132 - val_loss: 1.1617
Epoch 2185/5000
26/26 - 1s - loss: 0.7125 - val_loss: 1.1599
Epoch 2186/5000
26/26 - 1s - loss: 0.7104 - val_loss: 1.1609
Epoch 2187/5000
26/26 - 1s - loss: 0.7106 - val_loss: 1.1589
Epoch 2188/5000
26/26 - 1s - loss: 0.7124 - val_loss: 1.1590
Epoch 2189/5000
26/26 - 1s - loss: 0.7124 - val_loss: 1.1586
Epoch 2190/5000
26/26 - 1s - loss: 0.7106 - val_loss: 1.1592
Epoch 02190: val_loss improved from 1.16327 to 1.15924, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 2191/5000
26/26 - 1s - loss: 0.7095 - val_loss: 1.1583
Epoch 2192/5000
26/26 - 1s - loss: 0.7111 - val_loss: 1.1569
Epoch 2193/5000
26/26 - 1s - loss: 0.7103 - val_loss: 1.1579
Epoch 2194/5000
26/26 - 1s - loss: 0.7099 - val_loss: 1.1562
Epoch 2195/5000
26/26 - 1s - loss: 0.7098 - val_loss: 1.1576
Epoch 2196/5000
26/26 - 1s - loss: 0.7093 - val_loss: 1.1559
Epoch 2197/5000
26/26 - 1s - loss: 0.7087 - val_loss: 1.1556
Epoch 2198/5000
26/26 - 1s - loss: 0.7084 - val_loss: 1.1561
Epoch 2199/5000
26/26 - 1s - loss: 0.7090 - val_loss: 1.1558
Epoch 2200/5000
26/26 - 1s - loss: 0.7080 - val_loss: 1.1555
Epoch 02200: val_loss improved from 1.15924 to 1.15552, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 2201/5000
26/26 - 1s - loss: 0.7063 - val_loss: 1.1541
Epoch 2202/5000
26/26 - 1s - loss: 0.7084 - val_loss: 1.1559
Epoch 2203/5000
26/26 - 1s - loss: 0.7065 - val_loss: 1.1561
Epoch 2204/5000
26/26 - 1s - loss: 0.7055 - val_loss: 1.1544
Epoch 2205/5000
26/26 - 1s - loss: 0.7067 - val_loss: 1.1533
Epoch 2206/5000
26/26 - 1s - loss: 0.7075 - val_loss: 1.1545
Epoch 2207/5000
26/26 - 1s - loss: 0.7051 - val_loss: 1.1544
Epoch 2208/5000
26/26 - 1s - loss: 0.7073 - val_loss: 1.1518
Epoch 2209/5000
26/26 - 1s - loss: 0.7053 - val_loss: 1.1528
Epoch 2210/5000
26/26 - 1s - loss: 0.7047 - val_loss: 1.1514
Epoch 02210: val_loss improved from 1.15552 to 1.15140, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 2211/5000
26/26 - 1s - loss: 0.7050 - val_loss: 1.1525
Epoch 2212/5000
26/26 - 1s - loss: 0.7036 - val_loss: 1.1520
Epoch 2213/5000
26/26 - 1s - loss: 0.7050 - val_loss: 1.1530
Epoch 2214/5000
26/26 - 1s - loss: 0.7045 - val_loss: 1.1536
Epoch 2215/5000
26/26 - 1s - loss: 0.7041 - val_loss: 1.1508
Epoch 2216/5000
26/26 - 1s - loss: 0.7050 - val_loss: 1.1521
Epoch 2217/5000
26/26 - 1s - loss: 0.7019 - val_loss: 1.1510
Epoch 2218/5000
26/26 - 1s - loss: 0.7027 - val_loss: 1.1518
Epoch 2219/5000
26/26 - 1s - loss: 0.7022 - val_loss: 1.1513
Epoch 2220/5000
26/26 - 2s - loss: 0.7024 - val_loss: 1.1513
Epoch 02220: val_loss improved from 1.15140 to 1.15126, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 2221/5000
26/26 - 1s - loss: 0.7030 - val_loss: 1.1521
Epoch 2222/5000
26/26 - 1s - loss: 0.7014 - val_loss: 1.1505
Epoch 2223/5000
26/26 - 1s - loss: 0.7018 - val_loss: 1.1505
Epoch 2224/5000
26/26 - 1s - loss: 0.7008 - val_loss: 1.1500
Epoch 2225/5000
26/26 - 1s - loss: 0.7021 - val_loss: 1.1494
Epoch 2226/5000
26/26 - 1s - loss: 0.7012 - val_loss: 1.1500
Epoch 2227/5000
26/26 - 1s - loss: 0.7016 - val_loss: 1.1482
Epoch 2228/5000
26/26 - 1s - loss: 0.7003 - val_loss: 1.1479
Epoch 2229/5000
26/26 - 1s - loss: 0.6997 - val_loss: 1.1468
Epoch 2230/5000
26/26 - 1s - loss: 0.6998 - val_loss: 1.1470
Epoch 02230: val_loss improved from 1.15126 to 1.14696, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 2231/5000
26/26 - 1s - loss: 0.6986 - val_loss: 1.1484
Epoch 2232/5000
26/26 - 1s - loss: 0.6976 - val_loss: 1.1469
Epoch 2233/5000
26/26 - 1s - loss: 0.6978 - val_loss: 1.1467
Epoch 2234/5000
26/26 - 2s - loss: 0.7000 - val_loss: 1.1474
Epoch 2235/5000
26/26 - 1s - loss: 0.6977 - val_loss: 1.1469
Epoch 2236/5000
26/26 - 1s - loss: 0.6971 - val_loss: 1.1456
Epoch 2237/5000
26/26 - 1s - loss: 0.6969 - val_loss: 1.1459
Epoch 2238/5000
26/26 - 1s - loss: 0.6988 - val_loss: 1.1448
Epoch 2239/5000
26/26 - 1s - loss: 0.6971 - val_loss: 1.1442
Epoch 2240/5000
26/26 - 1s - loss: 0.6960 - val_loss: 1.1470
Epoch 02240: val_loss did not improve from 1.14696
Epoch 2241/5000
26/26 - 1s - loss: 0.6962 - val_loss: 1.1448
Epoch 2242/5000
26/26 - 1s - loss: 0.6957 - val_loss: 1.1424
Epoch 2243/5000
26/26 - 1s - loss: 0.6959 - val_loss: 1.1441
Epoch 2244/5000
26/26 - 1s - loss: 0.6955 - val_loss: 1.1444
Epoch 2245/5000
26/26 - 1s - loss: 0.6955 - val_loss: 1.1431
Epoch 2246/5000
26/26 - 1s - loss: 0.6965 - val_loss: 1.1443
Epoch 2247/5000
26/26 - 1s - loss: 0.6961 - val_loss: 1.1431
Epoch 2248/5000
26/26 - 1s - loss: 0.6942 - val_loss: 1.1428
Epoch 2249/5000
26/26 - 1s - loss: 0.6942 - val_loss: 1.1437
Epoch 2250/5000
26/26 - 1s - loss: 0.6933 - val_loss: 1.1419
Epoch 02250: val_loss improved from 1.14696 to 1.14192, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 2251/5000
26/26 - 1s - loss: 0.6944 - val_loss: 1.1436
Epoch 2252/5000
26/26 - 1s - loss: 0.6918 - val_loss: 1.1425
Epoch 2253/5000
26/26 - 1s - loss: 0.6925 - val_loss: 1.1423
Epoch 2254/5000
26/26 - 1s - loss: 0.6926 - val_loss: 1.1403
Epoch 2255/5000
26/26 - 1s - loss: 0.6932 - val_loss: 1.1421
Epoch 2256/5000
26/26 - 1s - loss: 0.6937 - val_loss: 1.1414
Epoch 2257/5000
26/26 - 1s - loss: 0.6919 - val_loss: 1.1404
Epoch 2258/5000
26/26 - 1s - loss: 0.6923 - val_loss: 1.1410
Epoch 2259/5000
26/26 - 1s - loss: 0.6916 - val_loss: 1.1395
Epoch 2260/5000
26/26 - 1s - loss: 0.6913 - val_loss: 1.1398
Epoch 02260: val_loss improved from 1.14192 to 1.13977, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 2261/5000
26/26 - 1s - loss: 0.6916 - val_loss: 1.1371
Epoch 2262/5000
26/26 - 1s - loss: 0.6902 - val_loss: 1.1392
Epoch 2263/5000
26/26 - 1s - loss: 0.6902 - val_loss: 1.1372
Epoch 2264/5000
26/26 - 1s - loss: 0.6894 - val_loss: 1.1373
Epoch 2265/5000
26/26 - 1s - loss: 0.6894 - val_loss: 1.1395
Epoch 2266/5000
26/26 - 1s - loss: 0.6894 - val_loss: 1.1373
Epoch 2267/5000
26/26 - 1s - loss: 0.6905 - val_loss: 1.1363
Epoch 2268/5000
26/26 - 1s - loss: 0.6874 - val_loss: 1.1374
Epoch 2269/5000
26/26 - 1s - loss: 0.6890 - val_loss: 1.1366
Epoch 2270/5000
26/26 - 1s - loss: 0.6886 - val_loss: 1.1355
Epoch 02270: val_loss improved from 1.13977 to 1.13553, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 2271/5000
26/26 - 1s - loss: 0.6886 - val_loss: 1.1348
Epoch 2272/5000
26/26 - 2s - loss: 0.6885 - val_loss: 1.1346
Epoch 2273/5000
26/26 - 1s - loss: 0.6872 - val_loss: 1.1361
Epoch 2274/5000
26/26 - 1s - loss: 0.6872 - val_loss: 1.1373
Epoch 2275/5000
26/26 - 1s - loss: 0.6865 - val_loss: 1.1354
Epoch 2276/5000
26/26 - 1s - loss: 0.6869 - val_loss: 1.1356
Epoch 2277/5000
26/26 - 1s - loss: 0.6864 - val_loss: 1.1362
Epoch 2278/5000
26/26 - 1s - loss: 0.6869 - val_loss: 1.1351
Epoch 2279/5000
26/26 - 1s - loss: 0.6873 - val_loss: 1.1340
Epoch 2280/5000
26/26 - 1s - loss: 0.6862 - val_loss: 1.1340
Epoch 02280: val_loss improved from 1.13553 to 1.13396, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 2281/5000
26/26 - 1s - loss: 0.6847 - val_loss: 1.1320
Epoch 2282/5000
26/26 - 1s - loss: 0.6851 - val_loss: 1.1321
Epoch 2283/5000
26/26 - 1s - loss: 0.6843 - val_loss: 1.1347
Epoch 2284/5000
26/26 - 1s - loss: 0.6833 - val_loss: 1.1336
Epoch 2285/5000
26/26 - 1s - loss: 0.6852 - val_loss: 1.1316
Epoch 2286/5000
26/26 - 1s - loss: 0.6849 - val_loss: 1.1345
Epoch 2287/5000
26/26 - 1s - loss: 0.6833 - val_loss: 1.1318
Epoch 2288/5000
26/26 - 1s - loss: 0.6844 - val_loss: 1.1317
Epoch 2289/5000
26/26 - 1s - loss: 0.6832 - val_loss: 1.1321
Epoch 2290/5000
26/26 - 1s - loss: 0.6831 - val_loss: 1.1313
Epoch 02290: val_loss improved from 1.13396 to 1.13127, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 2291/5000
26/26 - 1s - loss: 0.6832 - val_loss: 1.1307
Epoch 2292/5000
26/26 - 1s - loss: 0.6829 - val_loss: 1.1318
Epoch 2293/5000
26/26 - 1s - loss: 0.6823 - val_loss: 1.1326
Epoch 2294/5000
26/26 - 1s - loss: 0.6811 - val_loss: 1.1316
Epoch 2295/5000
26/26 - 1s - loss: 0.6818 - val_loss: 1.1302
Epoch 2296/5000
26/26 - 1s - loss: 0.6820 - val_loss: 1.1299
Epoch 2297/5000
26/26 - 1s - loss: 0.6808 - val_loss: 1.1285
Epoch 2298/5000
26/26 - 1s - loss: 0.6819 - val_loss: 1.1291
Epoch 2299/5000
26/26 - 1s - loss: 0.6798 - val_loss: 1.1270
Epoch 2300/5000
26/26 - 1s - loss: 0.6807 - val_loss: 1.1293
Epoch 02300: val_loss improved from 1.13127 to 1.12935, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 2301/5000
26/26 - 1s - loss: 0.6804 - val_loss: 1.1280
Epoch 2302/5000
26/26 - 1s - loss: 0.6810 - val_loss: 1.1280
Epoch 2303/5000
26/26 - 1s - loss: 0.6794 - val_loss: 1.1274
Epoch 2304/5000
26/26 - 1s - loss: 0.6778 - val_loss: 1.1289
Epoch 2305/5000
26/26 - 1s - loss: 0.6797 - val_loss: 1.1276
Epoch 2306/5000
26/26 - 1s - loss: 0.6801 - val_loss: 1.1262
Epoch 2307/5000
26/26 - 1s - loss: 0.6784 - val_loss: 1.1273
Epoch 2308/5000
26/26 - 1s - loss: 0.6774 - val_loss: 1.1276
Epoch 2309/5000
26/26 - 1s - loss: 0.6780 - val_loss: 1.1252
Epoch 2310/5000
26/26 - 1s - loss: 0.6768 - val_loss: 1.1256
Epoch 02310: val_loss improved from 1.12935 to 1.12564, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 2311/5000
26/26 - 1s - loss: 0.6769 - val_loss: 1.1271
Epoch 2312/5000
26/26 - 1s - loss: 0.6781 - val_loss: 1.1249
Epoch 2313/5000
26/26 - 1s - loss: 0.6768 - val_loss: 1.1246
Epoch 2314/5000
26/26 - 1s - loss: 0.6774 - val_loss: 1.1241
Epoch 2315/5000
26/26 - 1s - loss: 0.6766 - val_loss: 1.1269
Epoch 2316/5000
26/26 - 1s - loss: 0.6764 - val_loss: 1.1261
Epoch 2317/5000
26/26 - 1s - loss: 0.6764 - val_loss: 1.1240
Epoch 2318/5000
26/26 - 1s - loss: 0.6759 - val_loss: 1.1236
Epoch 2319/5000
26/26 - 1s - loss: 0.6748 - val_loss: 1.1250
Epoch 2320/5000
26/26 - 1s - loss: 0.6740 - val_loss: 1.1236
Epoch 02320: val_loss improved from 1.12564 to 1.12355, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 2321/5000
26/26 - 1s - loss: 0.6761 - val_loss: 1.1238
Epoch 2322/5000
26/26 - 1s - loss: 0.6741 - val_loss: 1.1233
Epoch 2323/5000
26/26 - 1s - loss: 0.6747 - val_loss: 1.1228
Epoch 2324/5000
26/26 - 1s - loss: 0.6744 - val_loss: 1.1224
Epoch 2325/5000
26/26 - 1s - loss: 0.6750 - val_loss: 1.1220
Epoch 2326/5000
26/26 - 1s - loss: 0.6718 - val_loss: 1.1209
Epoch 2327/5000
26/26 - 1s - loss: 0.6730 - val_loss: 1.1214
Epoch 2328/5000
26/26 - 1s - loss: 0.6728 - val_loss: 1.1229
Epoch 2329/5000
26/26 - 1s - loss: 0.6737 - val_loss: 1.1204
Epoch 2330/5000
26/26 - 1s - loss: 0.6738 - val_loss: 1.1214
Epoch 02330: val_loss improved from 1.12355 to 1.12143, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 2331/5000
26/26 - 1s - loss: 0.6728 - val_loss: 1.1215
Epoch 2332/5000
26/26 - 1s - loss: 0.6734 - val_loss: 1.1206
Epoch 2333/5000
26/26 - 1s - loss: 0.6710 - val_loss: 1.1208
Epoch 2334/5000
26/26 - 1s - loss: 0.6715 - val_loss: 1.1204
Epoch 2335/5000
26/26 - 1s - loss: 0.6710 - val_loss: 1.1209
Epoch 2336/5000
26/26 - 1s - loss: 0.6699 - val_loss: 1.1210
Epoch 2337/5000
26/26 - 1s - loss: 0.6701 - val_loss: 1.1199
Epoch 2338/5000
26/26 - 1s - loss: 0.6704 - val_loss: 1.1212
Epoch 2339/5000
26/26 - 1s - loss: 0.6702 - val_loss: 1.1196
Epoch 2340/5000
26/26 - 1s - loss: 0.6707 - val_loss: 1.1193
Epoch 02340: val_loss improved from 1.12143 to 1.11925, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 2341/5000
26/26 - 1s - loss: 0.6678 - val_loss: 1.1176
Epoch 2342/5000
26/26 - 1s - loss: 0.6694 - val_loss: 1.1178
Epoch 2343/5000
26/26 - 1s - loss: 0.6681 - val_loss: 1.1199
Epoch 2344/5000
26/26 - 1s - loss: 0.6688 - val_loss: 1.1173
Epoch 2345/5000
26/26 - 1s - loss: 0.6677 - val_loss: 1.1176
Epoch 2346/5000
26/26 - 1s - loss: 0.6674 - val_loss: 1.1186
Epoch 2347/5000
26/26 - 1s - loss: 0.6685 - val_loss: 1.1164
Epoch 2348/5000
26/26 - 1s - loss: 0.6689 - val_loss: 1.1174
Epoch 2349/5000
26/26 - 1s - loss: 0.6669 - val_loss: 1.1170
Epoch 2350/5000
26/26 - 1s - loss: 0.6684 - val_loss: 1.1158
Epoch 02350: val_loss improved from 1.11925 to 1.11584, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 2351/5000
26/26 - 1s - loss: 0.6672 - val_loss: 1.1163
Epoch 2352/5000
26/26 - 1s - loss: 0.6666 - val_loss: 1.1166
Epoch 2353/5000
26/26 - 1s - loss: 0.6669 - val_loss: 1.1181
Epoch 2354/5000
26/26 - 1s - loss: 0.6677 - val_loss: 1.1158
Epoch 2355/5000
26/26 - 2s - loss: 0.6668 - val_loss: 1.1146
Epoch 2356/5000
26/26 - 1s - loss: 0.6676 - val_loss: 1.1138
Epoch 2357/5000
26/26 - 1s - loss: 0.6655 - val_loss: 1.1134
Epoch 2358/5000
26/26 - 1s - loss: 0.6657 - val_loss: 1.1135
Epoch 2359/5000
26/26 - 1s - loss: 0.6654 - val_loss: 1.1140
Epoch 2360/5000
26/26 - 1s - loss: 0.6653 - val_loss: 1.1141
Epoch 02360: val_loss improved from 1.11584 to 1.11406, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 2361/5000
26/26 - 1s - loss: 0.6646 - val_loss: 1.1143
Epoch 2362/5000
26/26 - 1s - loss: 0.6636 - val_loss: 1.1131
Epoch 2363/5000
26/26 - 1s - loss: 0.6645 - val_loss: 1.1120
Epoch 2364/5000
26/26 - 1s - loss: 0.6643 - val_loss: 1.1135
Epoch 2365/5000
26/26 - 1s - loss: 0.6647 - val_loss: 1.1127
Epoch 2366/5000
26/26 - 1s - loss: 0.6636 - val_loss: 1.1130
Epoch 2367/5000
26/26 - 1s - loss: 0.6637 - val_loss: 1.1130
Epoch 2368/5000
26/26 - 1s - loss: 0.6630 - val_loss: 1.1124
Epoch 2369/5000
26/26 - 1s - loss: 0.6617 - val_loss: 1.1132
Epoch 2370/5000
26/26 - 1s - loss: 0.6638 - val_loss: 1.1126
Epoch 02370: val_loss improved from 1.11406 to 1.11257, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 2371/5000
26/26 - 2s - loss: 0.6604 - val_loss: 1.1112
Epoch 2372/5000
26/26 - 1s - loss: 0.6618 - val_loss: 1.1109
Epoch 2373/5000
26/26 - 1s - loss: 0.6624 - val_loss: 1.1099
Epoch 2374/5000
26/26 - 1s - loss: 0.6616 - val_loss: 1.1105
Epoch 2375/5000
26/26 - 1s - loss: 0.6607 - val_loss: 1.1095
Epoch 2376/5000
26/26 - 1s - loss: 0.6618 - val_loss: 1.1114
Epoch 2377/5000
26/26 - 1s - loss: 0.6618 - val_loss: 1.1095
Epoch 2378/5000
26/26 - 1s - loss: 0.6609 - val_loss: 1.1104
Epoch 2379/5000
26/26 - 1s - loss: 0.6600 - val_loss: 1.1087
Epoch 2380/5000
26/26 - 1s - loss: 0.6613 - val_loss: 1.1094
Epoch 02380: val_loss improved from 1.11257 to 1.10942, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 2381/5000
26/26 - 1s - loss: 0.6599 - val_loss: 1.1081
Epoch 2382/5000
26/26 - 1s - loss: 0.6586 - val_loss: 1.1085
Epoch 2383/5000
26/26 - 1s - loss: 0.6577 - val_loss: 1.1076
Epoch 2384/5000
26/26 - 1s - loss: 0.6591 - val_loss: 1.1083
Epoch 2385/5000
26/26 - 1s - loss: 0.6580 - val_loss: 1.1076
Epoch 2386/5000
26/26 - 1s - loss: 0.6581 - val_loss: 1.1077
Epoch 2387/5000
26/26 - 1s - loss: 0.6586 - val_loss: 1.1075
Epoch 2388/5000
26/26 - 1s - loss: 0.6598 - val_loss: 1.1061
Epoch 2389/5000
26/26 - 1s - loss: 0.6569 - val_loss: 1.1075
Epoch 2390/5000
26/26 - 1s - loss: 0.6587 - val_loss: 1.1066
Epoch 02390: val_loss improved from 1.10942 to 1.10663, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 2391/5000
26/26 - 1s - loss: 0.6566 - val_loss: 1.1051
Epoch 2392/5000
26/26 - 1s - loss: 0.6575 - val_loss: 1.1056
Epoch 2393/5000
26/26 - 1s - loss: 0.6565 - val_loss: 1.1070
Epoch 2394/5000
26/26 - 1s - loss: 0.6567 - val_loss: 1.1057
Epoch 2395/5000
26/26 - 1s - loss: 0.6569 - val_loss: 1.1047
Epoch 2396/5000
26/26 - 2s - loss: 0.6560 - val_loss: 1.1047
Epoch 2397/5000
26/26 - 1s - loss: 0.6560 - val_loss: 1.1051
Epoch 2398/5000
26/26 - 1s - loss: 0.6560 - val_loss: 1.1043
Epoch 2399/5000
26/26 - 1s - loss: 0.6554 - val_loss: 1.1045
Epoch 2400/5000
26/26 - 1s - loss: 0.6554 - val_loss: 1.1043
Epoch 02400: val_loss improved from 1.10663 to 1.10429, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 2401/5000
26/26 - 1s - loss: 0.6549 - val_loss: 1.1026
Epoch 2402/5000
26/26 - 1s - loss: 0.6542 - val_loss: 1.1040
Epoch 2403/5000
26/26 - 1s - loss: 0.6559 - val_loss: 1.1032
Epoch 2404/5000
26/26 - 1s - loss: 0.6539 - val_loss: 1.1024
Epoch 2405/5000
26/26 - 1s - loss: 0.6530 - val_loss: 1.1032
Epoch 2406/5000
26/26 - 1s - loss: 0.6534 - val_loss: 1.1024
Epoch 2407/5000
26/26 - 1s - loss: 0.6531 - val_loss: 1.1029
Epoch 2408/5000
26/26 - 1s - loss: 0.6532 - val_loss: 1.1029
Epoch 2409/5000
26/26 - 1s - loss: 0.6521 - val_loss: 1.1008
Epoch 2410/5000
26/26 - 1s - loss: 0.6545 - val_loss: 1.0995
Epoch 02410: val_loss improved from 1.10429 to 1.09948, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 2411/5000
26/26 - 1s - loss: 0.6529 - val_loss: 1.1012
Epoch 2412/5000
26/26 - 1s - loss: 0.6518 - val_loss: 1.1010
Epoch 2413/5000
26/26 - 1s - loss: 0.6510 - val_loss: 1.1007
Epoch 2414/5000
26/26 - 1s - loss: 0.6508 - val_loss: 1.1027
Epoch 2415/5000
26/26 - 1s - loss: 0.6523 - val_loss: 1.1014
Epoch 2416/5000
26/26 - 1s - loss: 0.6523 - val_loss: 1.1018
Epoch 2417/5000
26/26 - 1s - loss: 0.6500 - val_loss: 1.1010
Epoch 2418/5000
26/26 - 1s - loss: 0.6506 - val_loss: 1.0995
Epoch 2419/5000
26/26 - 1s - loss: 0.6507 - val_loss: 1.0998
Epoch 2420/5000
26/26 - 1s - loss: 0.6509 - val_loss: 1.1001
Epoch 02420: val_loss did not improve from 1.09948
Epoch 2421/5000
26/26 - 1s - loss: 0.6502 - val_loss: 1.0996
Epoch 2422/5000
26/26 - 1s - loss: 0.6507 - val_loss: 1.0988
Epoch 2423/5000
26/26 - 1s - loss: 0.6493 - val_loss: 1.0993
Epoch 2424/5000
26/26 - 1s - loss: 0.6489 - val_loss: 1.0985
Epoch 2425/5000
26/26 - 1s - loss: 0.6489 - val_loss: 1.0986
Epoch 2426/5000
26/26 - 1s - loss: 0.6492 - val_loss: 1.0983
Epoch 2427/5000
26/26 - 1s - loss: 0.6486 - val_loss: 1.0975
Epoch 2428/5000
26/26 - 1s - loss: 0.6496 - val_loss: 1.0983
Epoch 2429/5000
26/26 - 1s - loss: 0.6479 - val_loss: 1.0968
Epoch 2430/5000
26/26 - 1s - loss: 0.6478 - val_loss: 1.0967
Epoch 02430: val_loss improved from 1.09948 to 1.09672, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 2431/5000
26/26 - 1s - loss: 0.6482 - val_loss: 1.0975
Epoch 2432/5000
26/26 - 1s - loss: 0.6477 - val_loss: 1.0972
Epoch 2433/5000
26/26 - 1s - loss: 0.6482 - val_loss: 1.0973
Epoch 2434/5000
26/26 - 1s - loss: 0.6469 - val_loss: 1.0969
Epoch 2435/5000
26/26 - 1s - loss: 0.6468 - val_loss: 1.0965
Epoch 2436/5000
26/26 - 1s - loss: 0.6458 - val_loss: 1.0953
Epoch 2437/5000
26/26 - 1s - loss: 0.6461 - val_loss: 1.0949
Epoch 2438/5000
26/26 - 1s - loss: 0.6467 - val_loss: 1.0954
Epoch 2439/5000
26/26 - 2s - loss: 0.6455 - val_loss: 1.0951
Epoch 2440/5000
26/26 - 1s - loss: 0.6451 - val_loss: 1.0954
Epoch 02440: val_loss improved from 1.09672 to 1.09536, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 2441/5000
26/26 - 1s - loss: 0.6447 - val_loss: 1.0955
Epoch 2442/5000
26/26 - 1s - loss: 0.6450 - val_loss: 1.0957
Epoch 2443/5000
26/26 - 1s - loss: 0.6435 - val_loss: 1.0944
Epoch 2444/5000
26/26 - 1s - loss: 0.6464 - val_loss: 1.0936
Epoch 2445/5000
26/26 - 1s - loss: 0.6442 - val_loss: 1.0945
Epoch 2446/5000
26/26 - 1s - loss: 0.6444 - val_loss: 1.0938
Epoch 2447/5000
26/26 - 1s - loss: 0.6436 - val_loss: 1.0930
Epoch 2448/5000
26/26 - 1s - loss: 0.6448 - val_loss: 1.0934
Epoch 2449/5000
26/26 - 1s - loss: 0.6433 - val_loss: 1.0930
Epoch 2450/5000
26/26 - 1s - loss: 0.6431 - val_loss: 1.0942
Epoch 02450: val_loss improved from 1.09536 to 1.09416, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 2451/5000
26/26 - 1s - loss: 0.6435 - val_loss: 1.0945
Epoch 2452/5000
26/26 - 1s - loss: 0.6436 - val_loss: 1.0931
Epoch 2453/5000
26/26 - 1s - loss: 0.6426 - val_loss: 1.0910
Epoch 2454/5000
26/26 - 1s - loss: 0.6439 - val_loss: 1.0945
Epoch 2455/5000
26/26 - 1s - loss: 0.6434 - val_loss: 1.0922
Epoch 2456/5000
26/26 - 1s - loss: 0.6425 - val_loss: 1.0922
Epoch 2457/5000
26/26 - 1s - loss: 0.6412 - val_loss: 1.0912
Epoch 2458/5000
26/26 - 2s - loss: 0.6421 - val_loss: 1.0935
Epoch 2459/5000
26/26 - 1s - loss: 0.6418 - val_loss: 1.0913
Epoch 2460/5000
26/26 - 1s - loss: 0.6416 - val_loss: 1.0911
Epoch 02460: val_loss improved from 1.09416 to 1.09107, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 2461/5000
26/26 - 1s - loss: 0.6410 - val_loss: 1.0909
Epoch 2462/5000
26/26 - 1s - loss: 0.6399 - val_loss: 1.0900
Epoch 2463/5000
26/26 - 1s - loss: 0.6401 - val_loss: 1.0911
Epoch 2464/5000
26/26 - 1s - loss: 0.6400 - val_loss: 1.0890
Epoch 2465/5000
26/26 - 1s - loss: 0.6409 - val_loss: 1.0902
Epoch 2466/5000
26/26 - 1s - loss: 0.6392 - val_loss: 1.0896
Epoch 2467/5000
26/26 - 1s - loss: 0.6399 - val_loss: 1.0894
Epoch 2468/5000
26/26 - 1s - loss: 0.6390 - val_loss: 1.0876
Epoch 2469/5000
26/26 - 1s - loss: 0.6386 - val_loss: 1.0892
Epoch 2470/5000
26/26 - 1s - loss: 0.6386 - val_loss: 1.0876
Epoch 02470: val_loss improved from 1.09107 to 1.08757, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 2471/5000
26/26 - 1s - loss: 0.6387 - val_loss: 1.0899
Epoch 2472/5000
26/26 - 1s - loss: 0.6394 - val_loss: 1.0890
Epoch 2473/5000
26/26 - 1s - loss: 0.6384 - val_loss: 1.0877
Epoch 2474/5000
26/26 - 1s - loss: 0.6392 - val_loss: 1.0861
Epoch 2475/5000
26/26 - 1s - loss: 0.6397 - val_loss: 1.0859
Epoch 2476/5000
26/26 - 1s - loss: 0.6378 - val_loss: 1.0868
Epoch 2477/5000
26/26 - 1s - loss: 0.6370 - val_loss: 1.0861
Epoch 2478/5000
26/26 - 1s - loss: 0.6386 - val_loss: 1.0848
Epoch 2479/5000
26/26 - 1s - loss: 0.6361 - val_loss: 1.0856
Epoch 2480/5000
26/26 - 1s - loss: 0.6367 - val_loss: 1.0860
Epoch 02480: val_loss improved from 1.08757 to 1.08604, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 2481/5000
26/26 - 1s - loss: 0.6370 - val_loss: 1.0853
Epoch 2482/5000
26/26 - 1s - loss: 0.6373 - val_loss: 1.0850
Epoch 2483/5000
26/26 - 1s - loss: 0.6362 - val_loss: 1.0847
Epoch 2484/5000
26/26 - 1s - loss: 0.6351 - val_loss: 1.0868
Epoch 2485/5000
26/26 - 1s - loss: 0.6352 - val_loss: 1.0858
Epoch 2486/5000
26/26 - 1s - loss: 0.6343 - val_loss: 1.0854
Epoch 2487/5000
26/26 - 1s - loss: 0.6351 - val_loss: 1.0849
Epoch 2488/5000
26/26 - 1s - loss: 0.6350 - val_loss: 1.0846
Epoch 2489/5000
26/26 - 1s - loss: 0.6341 - val_loss: 1.0836
Epoch 2490/5000
26/26 - 1s - loss: 0.6349 - val_loss: 1.0834
Epoch 02490: val_loss improved from 1.08604 to 1.08339, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 2491/5000
26/26 - 1s - loss: 0.6348 - val_loss: 1.0828
Epoch 2492/5000
26/26 - 1s - loss: 0.6351 - val_loss: 1.0830
Epoch 2493/5000
26/26 - 1s - loss: 0.6326 - val_loss: 1.0835
Epoch 2494/5000
26/26 - 2s - loss: 0.6329 - val_loss: 1.0829
Epoch 2495/5000
26/26 - 1s - loss: 0.6320 - val_loss: 1.0826
Epoch 2496/5000
26/26 - 1s - loss: 0.6323 - val_loss: 1.0830
Epoch 2497/5000
26/26 - 1s - loss: 0.6324 - val_loss: 1.0825
Epoch 2498/5000
26/26 - 1s - loss: 0.6329 - val_loss: 1.0821
Epoch 2499/5000
26/26 - 1s - loss: 0.6329 - val_loss: 1.0820
Epoch 2500/5000
26/26 - 1s - loss: 0.6323 - val_loss: 1.0833
Epoch 02500: val_loss improved from 1.08339 to 1.08328, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 2501/5000
26/26 - 1s - loss: 0.6312 - val_loss: 1.0825
Epoch 2502/5000
26/26 - 1s - loss: 0.6306 - val_loss: 1.0812
Epoch 2503/5000
26/26 - 1s - loss: 0.6305 - val_loss: 1.0809
Epoch 2504/5000
26/26 - 1s - loss: 0.6299 - val_loss: 1.0808
Epoch 2505/5000
26/26 - 1s - loss: 0.6319 - val_loss: 1.0819
Epoch 2506/5000
26/26 - 1s - loss: 0.6313 - val_loss: 1.0790
Epoch 2507/5000
26/26 - 1s - loss: 0.6311 - val_loss: 1.0789
Epoch 2508/5000
26/26 - 1s - loss: 0.6306 - val_loss: 1.0789
Epoch 2509/5000
26/26 - 1s - loss: 0.6303 - val_loss: 1.0778
Epoch 2510/5000
26/26 - 1s - loss: 0.6290 - val_loss: 1.0786
Epoch 02510: val_loss improved from 1.08328 to 1.07857, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 2511/5000
26/26 - 1s - loss: 0.6296 - val_loss: 1.0787
Epoch 2512/5000
26/26 - 1s - loss: 0.6304 - val_loss: 1.0795
Epoch 2513/5000
26/26 - 1s - loss: 0.6289 - val_loss: 1.0781
Epoch 2514/5000
26/26 - 1s - loss: 0.6288 - val_loss: 1.0780
Epoch 2515/5000
26/26 - 1s - loss: 0.6281 - val_loss: 1.0781
Epoch 2516/5000
26/26 - 1s - loss: 0.6288 - val_loss: 1.0788
Epoch 2517/5000
26/26 - 1s - loss: 0.6280 - val_loss: 1.0783
Epoch 2518/5000
26/26 - 1s - loss: 0.6286 - val_loss: 1.0770
Epoch 2519/5000
26/26 - 1s - loss: 0.6274 - val_loss: 1.0779
Epoch 2520/5000
26/26 - 2s - loss: 0.6280 - val_loss: 1.0787
Epoch 02520: val_loss did not improve from 1.07857
Epoch 2521/5000
26/26 - 1s - loss: 0.6298 - val_loss: 1.0771
Epoch 2522/5000
26/26 - 1s - loss: 0.6278 - val_loss: 1.0758
Epoch 2523/5000
26/26 - 1s - loss: 0.6273 - val_loss: 1.0771
Epoch 2524/5000
26/26 - 1s - loss: 0.6278 - val_loss: 1.0773
Epoch 2525/5000
26/26 - 1s - loss: 0.6270 - val_loss: 1.0752
Epoch 2526/5000
26/26 - 1s - loss: 0.6277 - val_loss: 1.0771
Epoch 2527/5000
26/26 - 1s - loss: 0.6265 - val_loss: 1.0768
Epoch 2528/5000
26/26 - 1s - loss: 0.6248 - val_loss: 1.0771
Epoch 2529/5000
26/26 - 2s - loss: 0.6263 - val_loss: 1.0746
Epoch 2530/5000
26/26 - 1s - loss: 0.6262 - val_loss: 1.0754
Epoch 02530: val_loss improved from 1.07857 to 1.07536, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 2531/5000
26/26 - 1s - loss: 0.6254 - val_loss: 1.0749
Epoch 2532/5000
26/26 - 1s - loss: 0.6262 - val_loss: 1.0743
Epoch 2533/5000
26/26 - 1s - loss: 0.6244 - val_loss: 1.0755
Epoch 2534/5000
26/26 - 1s - loss: 0.6238 - val_loss: 1.0742
Epoch 2535/5000
26/26 - 1s - loss: 0.6231 - val_loss: 1.0750
Epoch 2536/5000
26/26 - 1s - loss: 0.6246 - val_loss: 1.0735
Epoch 2537/5000
26/26 - 1s - loss: 0.6248 - val_loss: 1.0731
Epoch 2538/5000
26/26 - 1s - loss: 0.6242 - val_loss: 1.0729
Epoch 2539/5000
26/26 - 1s - loss: 0.6226 - val_loss: 1.0739
Epoch 2540/5000
26/26 - 2s - loss: 0.6224 - val_loss: 1.0722
Epoch 02540: val_loss improved from 1.07536 to 1.07217, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 2541/5000
26/26 - 1s - loss: 0.6233 - val_loss: 1.0711
Epoch 2542/5000
26/26 - 1s - loss: 0.6218 - val_loss: 1.0733
Epoch 2543/5000
26/26 - 1s - loss: 0.6227 - val_loss: 1.0723
Epoch 2544/5000
26/26 - 1s - loss: 0.6238 - val_loss: 1.0724
Epoch 2545/5000
26/26 - 1s - loss: 0.6219 - val_loss: 1.0724
Epoch 2546/5000
26/26 - 1s - loss: 0.6231 - val_loss: 1.0724
Epoch 2547/5000
26/26 - 1s - loss: 0.6212 - val_loss: 1.0717
Epoch 2548/5000
26/26 - 1s - loss: 0.6213 - val_loss: 1.0705
Epoch 2549/5000
26/26 - 1s - loss: 0.6210 - val_loss: 1.0723
Epoch 2550/5000
26/26 - 1s - loss: 0.6211 - val_loss: 1.0719
Epoch 02550: val_loss improved from 1.07217 to 1.07192, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 2551/5000
26/26 - 1s - loss: 0.6208 - val_loss: 1.0711
Epoch 2552/5000
26/26 - 1s - loss: 0.6209 - val_loss: 1.0720
Epoch 2553/5000
26/26 - 1s - loss: 0.6209 - val_loss: 1.0710
Epoch 2554/5000
26/26 - 1s - loss: 0.6204 - val_loss: 1.0700
Epoch 2555/5000
26/26 - 1s - loss: 0.6207 - val_loss: 1.0712
Epoch 2556/5000
26/26 - 1s - loss: 0.6205 - val_loss: 1.0701
Epoch 2557/5000
26/26 - 1s - loss: 0.6201 - val_loss: 1.0710
Epoch 2558/5000
26/26 - 1s - loss: 0.6205 - val_loss: 1.0689
Epoch 2559/5000
26/26 - 1s - loss: 0.6190 - val_loss: 1.0684
Epoch 2560/5000
26/26 - 1s - loss: 0.6191 - val_loss: 1.0684
Epoch 02560: val_loss improved from 1.07192 to 1.06842, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 2561/5000
26/26 - 1s - loss: 0.6190 - val_loss: 1.0683
Epoch 2562/5000
26/26 - 2s - loss: 0.6196 - val_loss: 1.0664
Epoch 2563/5000
26/26 - 1s - loss: 0.6195 - val_loss: 1.0680
Epoch 2564/5000
26/26 - 1s - loss: 0.6201 - val_loss: 1.0689
Epoch 2565/5000
26/26 - 1s - loss: 0.6181 - val_loss: 1.0683
Epoch 2566/5000
26/26 - 1s - loss: 0.6187 - val_loss: 1.0688
Epoch 2567/5000
26/26 - 1s - loss: 0.6186 - val_loss: 1.0673
Epoch 2568/5000
26/26 - 1s - loss: 0.6185 - val_loss: 1.0661
Epoch 2569/5000
26/26 - 1s - loss: 0.6187 - val_loss: 1.0653
Epoch 2570/5000
26/26 - 1s - loss: 0.6185 - val_loss: 1.0659
Epoch 02570: val_loss improved from 1.06842 to 1.06588, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 2571/5000
26/26 - 2s - loss: 0.6164 - val_loss: 1.0659
Epoch 2572/5000
26/26 - 1s - loss: 0.6167 - val_loss: 1.0662
Epoch 2573/5000
26/26 - 1s - loss: 0.6164 - val_loss: 1.0666
Epoch 2574/5000
26/26 - 1s - loss: 0.6171 - val_loss: 1.0655
Epoch 2575/5000
26/26 - 1s - loss: 0.6166 - val_loss: 1.0661
Epoch 2576/5000
26/26 - 1s - loss: 0.6166 - val_loss: 1.0670
Epoch 2577/5000
26/26 - 1s - loss: 0.6163 - val_loss: 1.0655
Epoch 2578/5000
26/26 - 1s - loss: 0.6154 - val_loss: 1.0645
Epoch 2579/5000
26/26 - 1s - loss: 0.6157 - val_loss: 1.0652
Epoch 2580/5000
26/26 - 1s - loss: 0.6150 - val_loss: 1.0649
Epoch 02580: val_loss improved from 1.06588 to 1.06486, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 2581/5000
26/26 - 1s - loss: 0.6145 - val_loss: 1.0645
Epoch 2582/5000
26/26 - 1s - loss: 0.6133 - val_loss: 1.0646
Epoch 2583/5000
26/26 - 1s - loss: 0.6141 - val_loss: 1.0640
Epoch 2584/5000
26/26 - 1s - loss: 0.6147 - val_loss: 1.0637
Epoch 2585/5000
26/26 - 1s - loss: 0.6128 - val_loss: 1.0626
Epoch 2586/5000
26/26 - 1s - loss: 0.6156 - val_loss: 1.0641
Epoch 2587/5000
26/26 - 1s - loss: 0.6145 - val_loss: 1.0650
Epoch 2588/5000
26/26 - 1s - loss: 0.6149 - val_loss: 1.0635
Epoch 2589/5000
26/26 - 1s - loss: 0.6139 - val_loss: 1.0643
Epoch 2590/5000
26/26 - 1s - loss: 0.6136 - val_loss: 1.0631
Epoch 02590: val_loss improved from 1.06486 to 1.06306, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 2591/5000
26/26 - 1s - loss: 0.6124 - val_loss: 1.0628
Epoch 2592/5000
26/26 - 1s - loss: 0.6138 - val_loss: 1.0621
Epoch 2593/5000
26/26 - 1s - loss: 0.6116 - val_loss: 1.0621
Epoch 2594/5000
26/26 - 1s - loss: 0.6125 - val_loss: 1.0640
Epoch 2595/5000
26/26 - 1s - loss: 0.6118 - val_loss: 1.0621
Epoch 2596/5000
26/26 - 1s - loss: 0.6129 - val_loss: 1.0605
Epoch 2597/5000
26/26 - 1s - loss: 0.6108 - val_loss: 1.0609
Epoch 2598/5000
26/26 - 1s - loss: 0.6137 - val_loss: 1.0603
Epoch 2599/5000
26/26 - 1s - loss: 0.6112 - val_loss: 1.0610
Epoch 2600/5000
26/26 - 1s - loss: 0.6114 - val_loss: 1.0612
Epoch 02600: val_loss improved from 1.06306 to 1.06120, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 2601/5000
26/26 - 1s - loss: 0.6127 - val_loss: 1.0621
Epoch 2602/5000
26/26 - 1s - loss: 0.6116 - val_loss: 1.0610
Epoch 2603/5000
26/26 - 2s - loss: 0.6117 - val_loss: 1.0609
Epoch 2604/5000
26/26 - 1s - loss: 0.6093 - val_loss: 1.0597
Epoch 2605/5000
26/26 - 1s - loss: 0.6109 - val_loss: 1.0615
Epoch 2606/5000
26/26 - 1s - loss: 0.6107 - val_loss: 1.0604
Epoch 2607/5000
26/26 - 1s - loss: 0.6108 - val_loss: 1.0587
Epoch 2608/5000
26/26 - 1s - loss: 0.6095 - val_loss: 1.0608
Epoch 2609/5000
26/26 - 2s - loss: 0.6084 - val_loss: 1.0592
Epoch 2610/5000
26/26 - 1s - loss: 0.6091 - val_loss: 1.0581
Epoch 02610: val_loss improved from 1.06120 to 1.05807, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 2611/5000
26/26 - 1s - loss: 0.6095 - val_loss: 1.0587
Epoch 2612/5000
26/26 - 1s - loss: 0.6093 - val_loss: 1.0582
Epoch 2613/5000
26/26 - 1s - loss: 0.6094 - val_loss: 1.0584
Epoch 2614/5000
26/26 - 1s - loss: 0.6090 - val_loss: 1.0586
Epoch 2615/5000
26/26 - 1s - loss: 0.6082 - val_loss: 1.0585
Epoch 2616/5000
26/26 - 2s - loss: 0.6080 - val_loss: 1.0578
Epoch 2617/5000
26/26 - 1s - loss: 0.6076 - val_loss: 1.0582
Epoch 2618/5000
26/26 - 1s - loss: 0.6082 - val_loss: 1.0589
Epoch 2619/5000
26/26 - 1s - loss: 0.6072 - val_loss: 1.0563
Epoch 2620/5000
26/26 - 1s - loss: 0.6071 - val_loss: 1.0586
Epoch 02620: val_loss did not improve from 1.05807
Epoch 2621/5000
26/26 - 1s - loss: 0.6082 - val_loss: 1.0557
Epoch 2622/5000
26/26 - 1s - loss: 0.6062 - val_loss: 1.0572
Epoch 2623/5000
26/26 - 1s - loss: 0.6071 - val_loss: 1.0577
Epoch 2624/5000
26/26 - 1s - loss: 0.6055 - val_loss: 1.0554
Epoch 2625/5000
26/26 - 1s - loss: 0.6070 - val_loss: 1.0552
Epoch 2626/5000
26/26 - 1s - loss: 0.6068 - val_loss: 1.0561
Epoch 2627/5000
26/26 - 1s - loss: 0.6062 - val_loss: 1.0556
Epoch 2628/5000
26/26 - 1s - loss: 0.6052 - val_loss: 1.0549
Epoch 2629/5000
26/26 - 1s - loss: 0.6058 - val_loss: 1.0560
Epoch 2630/5000
26/26 - 1s - loss: 0.6045 - val_loss: 1.0539
Epoch 02630: val_loss improved from 1.05807 to 1.05395, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 2631/5000
26/26 - 1s - loss: 0.6051 - val_loss: 1.0538
Epoch 2632/5000
26/26 - 1s - loss: 0.6064 - val_loss: 1.0560
Epoch 2633/5000
26/26 - 1s - loss: 0.6055 - val_loss: 1.0561
Epoch 2634/5000
26/26 - 1s - loss: 0.6046 - val_loss: 1.0554
Epoch 2635/5000
26/26 - 1s - loss: 0.6029 - val_loss: 1.0536
Epoch 2636/5000
26/26 - 1s - loss: 0.6040 - val_loss: 1.0530
Epoch 2637/5000
26/26 - 1s - loss: 0.6038 - val_loss: 1.0534
Epoch 2638/5000
26/26 - 1s - loss: 0.6037 - val_loss: 1.0527
Epoch 2639/5000
26/26 - 1s - loss: 0.6046 - val_loss: 1.0539
Epoch 2640/5000
26/26 - 1s - loss: 0.6033 - val_loss: 1.0532
Epoch 02640: val_loss improved from 1.05395 to 1.05322, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 2641/5000
26/26 - 1s - loss: 0.6034 - val_loss: 1.0528
Epoch 2642/5000
26/26 - 1s - loss: 0.6027 - val_loss: 1.0520
Epoch 2643/5000
26/26 - 1s - loss: 0.6018 - val_loss: 1.0532
Epoch 2644/5000
26/26 - 2s - loss: 0.6035 - val_loss: 1.0523
Epoch 2645/5000
26/26 - 2s - loss: 0.6026 - val_loss: 1.0512
Epoch 2646/5000
26/26 - 1s - loss: 0.6023 - val_loss: 1.0538
Epoch 2647/5000
26/26 - 1s - loss: 0.6020 - val_loss: 1.0514
Epoch 2648/5000
26/26 - 1s - loss: 0.6018 - val_loss: 1.0510
Epoch 2649/5000
26/26 - 1s - loss: 0.6011 - val_loss: 1.0524
Epoch 2650/5000
26/26 - 1s - loss: 0.6016 - val_loss: 1.0513
Epoch 02650: val_loss improved from 1.05322 to 1.05126, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 2651/5000
26/26 - 1s - loss: 0.6008 - val_loss: 1.0515
Epoch 2652/5000
26/26 - 1s - loss: 0.6005 - val_loss: 1.0521
Epoch 2653/5000
26/26 - 1s - loss: 0.6015 - val_loss: 1.0500
Epoch 2654/5000
26/26 - 1s - loss: 0.6014 - val_loss: 1.0514
Epoch 2655/5000
26/26 - 1s - loss: 0.6000 - val_loss: 1.0506
Epoch 2656/5000
26/26 - 1s - loss: 0.6003 - val_loss: 1.0491
Epoch 2657/5000
26/26 - 1s - loss: 0.6016 - val_loss: 1.0483
Epoch 2658/5000
26/26 - 1s - loss: 0.5999 - val_loss: 1.0500
Epoch 2659/5000
26/26 - 1s - loss: 0.6007 - val_loss: 1.0495
Epoch 2660/5000
26/26 - 1s - loss: 0.5986 - val_loss: 1.0497
Epoch 02660: val_loss improved from 1.05126 to 1.04969, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 2661/5000
26/26 - 1s - loss: 0.6008 - val_loss: 1.0477
Epoch 2662/5000
26/26 - 1s - loss: 0.5981 - val_loss: 1.0492
Epoch 2663/5000
26/26 - 1s - loss: 0.5987 - val_loss: 1.0488
Epoch 2664/5000
26/26 - 1s - loss: 0.5994 - val_loss: 1.0492
Epoch 2665/5000
26/26 - 1s - loss: 0.5991 - val_loss: 1.0473
Epoch 2666/5000
26/26 - 1s - loss: 0.5992 - val_loss: 1.0480
Epoch 2667/5000
26/26 - 2s - loss: 0.5971 - val_loss: 1.0493
Epoch 2668/5000
26/26 - 1s - loss: 0.5976 - val_loss: 1.0472
Epoch 2669/5000
26/26 - 1s - loss: 0.5965 - val_loss: 1.0475
Epoch 2670/5000
26/26 - 1s - loss: 0.5974 - val_loss: 1.0485
Epoch 02670: val_loss improved from 1.04969 to 1.04853, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 2671/5000
26/26 - 2s - loss: 0.5982 - val_loss: 1.0492
Epoch 2672/5000
26/26 - 1s - loss: 0.5974 - val_loss: 1.0474
Epoch 2673/5000
26/26 - 1s - loss: 0.5977 - val_loss: 1.0483
Epoch 2674/5000
26/26 - 1s - loss: 0.5972 - val_loss: 1.0478
Epoch 2675/5000
26/26 - 1s - loss: 0.5976 - val_loss: 1.0465
Epoch 2676/5000
26/26 - 1s - loss: 0.5973 - val_loss: 1.0473
Epoch 2677/5000
26/26 - 1s - loss: 0.5968 - val_loss: 1.0468
Epoch 2678/5000
26/26 - 1s - loss: 0.5967 - val_loss: 1.0466
Epoch 2679/5000
26/26 - 1s - loss: 0.5964 - val_loss: 1.0455
Epoch 2680/5000
26/26 - 1s - loss: 0.5958 - val_loss: 1.0461
Epoch 02680: val_loss improved from 1.04853 to 1.04607, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 2681/5000
26/26 - 1s - loss: 0.5964 - val_loss: 1.0462
Epoch 2682/5000
26/26 - 1s - loss: 0.5950 - val_loss: 1.0443
Epoch 2683/5000
26/26 - 1s - loss: 0.5943 - val_loss: 1.0457
Epoch 2684/5000
26/26 - 1s - loss: 0.5952 - val_loss: 1.0455
Epoch 2685/5000
26/26 - 1s - loss: 0.5937 - val_loss: 1.0446
Epoch 2686/5000
26/26 - 1s - loss: 0.5945 - val_loss: 1.0438
Epoch 2687/5000
26/26 - 1s - loss: 0.5947 - val_loss: 1.0431
Epoch 2688/5000
26/26 - 1s - loss: 0.5942 - val_loss: 1.0442
Epoch 2689/5000
26/26 - 1s - loss: 0.5935 - val_loss: 1.0448
Epoch 2690/5000
26/26 - 1s - loss: 0.5946 - val_loss: 1.0454
Epoch 02690: val_loss improved from 1.04607 to 1.04535, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 2691/5000
26/26 - 1s - loss: 0.5944 - val_loss: 1.0454
Epoch 2692/5000
26/26 - 1s - loss: 0.5938 - val_loss: 1.0442
Epoch 2693/5000
26/26 - 1s - loss: 0.5937 - val_loss: 1.0448
Epoch 2694/5000
26/26 - 1s - loss: 0.5943 - val_loss: 1.0440
Epoch 2695/5000
26/26 - 1s - loss: 0.5940 - val_loss: 1.0416
Epoch 2696/5000
26/26 - 1s - loss: 0.5936 - val_loss: 1.0429
Epoch 2697/5000
26/26 - 1s - loss: 0.5926 - val_loss: 1.0433
Epoch 2698/5000
26/26 - 1s - loss: 0.5921 - val_loss: 1.0413
Epoch 2699/5000
26/26 - 1s - loss: 0.5920 - val_loss: 1.0407
Epoch 2700/5000
26/26 - 1s - loss: 0.5923 - val_loss: 1.0418
Epoch 02700: val_loss improved from 1.04535 to 1.04176, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 2701/5000
26/26 - 1s - loss: 0.5914 - val_loss: 1.0406
Epoch 2702/5000
26/26 - 2s - loss: 0.5917 - val_loss: 1.0422
Epoch 2703/5000
26/26 - 1s - loss: 0.5916 - val_loss: 1.0434
Epoch 2704/5000
26/26 - 1s - loss: 0.5916 - val_loss: 1.0419
Epoch 2705/5000
26/26 - 1s - loss: 0.5915 - val_loss: 1.0430
Epoch 2706/5000
26/26 - 1s - loss: 0.5911 - val_loss: 1.0405
Epoch 2707/5000
26/26 - 1s - loss: 0.5906 - val_loss: 1.0425
Epoch 2708/5000
26/26 - 1s - loss: 0.5903 - val_loss: 1.0406
Epoch 2709/5000
26/26 - 1s - loss: 0.5899 - val_loss: 1.0423
Epoch 2710/5000
26/26 - 1s - loss: 0.5904 - val_loss: 1.0413
Epoch 02710: val_loss improved from 1.04176 to 1.04128, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 2711/5000
26/26 - 1s - loss: 0.5906 - val_loss: 1.0395
Epoch 2712/5000
26/26 - 1s - loss: 0.5898 - val_loss: 1.0395
Epoch 2713/5000
26/26 - 1s - loss: 0.5896 - val_loss: 1.0399
Epoch 2714/5000
26/26 - 1s - loss: 0.5902 - val_loss: 1.0390
Epoch 2715/5000
26/26 - 1s - loss: 0.5891 - val_loss: 1.0387
Epoch 2716/5000
26/26 - 1s - loss: 0.5885 - val_loss: 1.0373
Epoch 2717/5000
26/26 - 2s - loss: 0.5889 - val_loss: 1.0376
Epoch 2718/5000
26/26 - 1s - loss: 0.5893 - val_loss: 1.0378
Epoch 2719/5000
26/26 - 2s - loss: 0.5876 - val_loss: 1.0368
Epoch 2720/5000
26/26 - 1s - loss: 0.5877 - val_loss: 1.0375
Epoch 02720: val_loss improved from 1.04128 to 1.03747, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 2721/5000
26/26 - 1s - loss: 0.5877 - val_loss: 1.0367
Epoch 2722/5000
26/26 - 1s - loss: 0.5875 - val_loss: 1.0368
Epoch 2723/5000
26/26 - 1s - loss: 0.5878 - val_loss: 1.0388
Epoch 2724/5000
26/26 - 1s - loss: 0.5873 - val_loss: 1.0382
Epoch 2725/5000
26/26 - 2s - loss: 0.5887 - val_loss: 1.0376
Epoch 2726/5000
26/26 - 2s - loss: 0.5866 - val_loss: 1.0358
Epoch 2727/5000
26/26 - 1s - loss: 0.5872 - val_loss: 1.0363
Epoch 2728/5000
26/26 - 1s - loss: 0.5868 - val_loss: 1.0375
Epoch 2729/5000
26/26 - 1s - loss: 0.5868 - val_loss: 1.0362
Epoch 2730/5000
26/26 - 2s - loss: 0.5873 - val_loss: 1.0379
Epoch 02730: val_loss did not improve from 1.03747
Epoch 2731/5000
26/26 - 1s - loss: 0.5873 - val_loss: 1.0379
Epoch 2732/5000
26/26 - 2s - loss: 0.5858 - val_loss: 1.0371
Epoch 2733/5000
26/26 - 1s - loss: 0.5860 - val_loss: 1.0367
Epoch 2734/5000
26/26 - 1s - loss: 0.5870 - val_loss: 1.0360
Epoch 2735/5000
26/26 - 1s - loss: 0.5859 - val_loss: 1.0356
Epoch 2736/5000
26/26 - 1s - loss: 0.5847 - val_loss: 1.0357
Epoch 2737/5000
26/26 - 1s - loss: 0.5849 - val_loss: 1.0351
Epoch 2738/5000
26/26 - 1s - loss: 0.5861 - val_loss: 1.0345
Epoch 2739/5000
26/26 - 1s - loss: 0.5854 - val_loss: 1.0346
Epoch 2740/5000
26/26 - 1s - loss: 0.5840 - val_loss: 1.0341
Epoch 02740: val_loss improved from 1.03747 to 1.03413, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 2741/5000
26/26 - 1s - loss: 0.5841 - val_loss: 1.0344
Epoch 2742/5000
26/26 - 1s - loss: 0.5855 - val_loss: 1.0346
Epoch 2743/5000
26/26 - 1s - loss: 0.5841 - val_loss: 1.0346
Epoch 2744/5000
26/26 - 1s - loss: 0.5845 - val_loss: 1.0341
Epoch 2745/5000
26/26 - 1s - loss: 0.5846 - val_loss: 1.0350
Epoch 2746/5000
26/26 - 1s - loss: 0.5829 - val_loss: 1.0351
Epoch 2747/5000
26/26 - 1s - loss: 0.5840 - val_loss: 1.0334
Epoch 2748/5000
26/26 - 1s - loss: 0.5836 - val_loss: 1.0348
Epoch 2749/5000
26/26 - 1s - loss: 0.5835 - val_loss: 1.0326
Epoch 2750/5000
26/26 - 1s - loss: 0.5836 - val_loss: 1.0339
Epoch 02750: val_loss improved from 1.03413 to 1.03392, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 2751/5000
26/26 - 1s - loss: 0.5842 - val_loss: 1.0315
Epoch 2752/5000
26/26 - 1s - loss: 0.5824 - val_loss: 1.0342
Epoch 2753/5000
26/26 - 1s - loss: 0.5828 - val_loss: 1.0329
Epoch 2754/5000
26/26 - 1s - loss: 0.5841 - val_loss: 1.0331
Epoch 2755/5000
26/26 - 1s - loss: 0.5837 - val_loss: 1.0333
Epoch 2756/5000
26/26 - 1s - loss: 0.5817 - val_loss: 1.0324
Epoch 2757/5000
26/26 - 1s - loss: 0.5823 - val_loss: 1.0318
Epoch 2758/5000
26/26 - 1s - loss: 0.5820 - val_loss: 1.0320
Epoch 2759/5000
26/26 - 1s - loss: 0.5823 - val_loss: 1.0333
Epoch 2760/5000
26/26 - 1s - loss: 0.5822 - val_loss: 1.0331
Epoch 02760: val_loss improved from 1.03392 to 1.03307, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 2761/5000
26/26 - 1s - loss: 0.5808 - val_loss: 1.0332
Epoch 2762/5000
26/26 - 1s - loss: 0.5821 - val_loss: 1.0319
Epoch 2763/5000
26/26 - 1s - loss: 0.5802 - val_loss: 1.0333
Epoch 2764/5000
26/26 - 2s - loss: 0.5810 - val_loss: 1.0306
Epoch 2765/5000
26/26 - 1s - loss: 0.5801 - val_loss: 1.0302
Epoch 2766/5000
26/26 - 1s - loss: 0.5796 - val_loss: 1.0300
Epoch 2767/5000
26/26 - 1s - loss: 0.5806 - val_loss: 1.0299
Epoch 2768/5000
26/26 - 1s - loss: 0.5809 - val_loss: 1.0319
Epoch 2769/5000
26/26 - 1s - loss: 0.5803 - val_loss: 1.0293
Epoch 2770/5000
26/26 - 1s - loss: 0.5783 - val_loss: 1.0296
Epoch 02770: val_loss improved from 1.03307 to 1.02956, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 2771/5000
26/26 - 1s - loss: 0.5799 - val_loss: 1.0303
Epoch 2772/5000
26/26 - 1s - loss: 0.5793 - val_loss: 1.0293
Epoch 2773/5000
26/26 - 1s - loss: 0.5799 - val_loss: 1.0285
Epoch 2774/5000
26/26 - 1s - loss: 0.5791 - val_loss: 1.0273
Epoch 2775/5000
26/26 - 1s - loss: 0.5784 - val_loss: 1.0288
Epoch 2776/5000
26/26 - 1s - loss: 0.5791 - val_loss: 1.0277
Epoch 2777/5000
26/26 - 1s - loss: 0.5795 - val_loss: 1.0290
Epoch 2778/5000
26/26 - 1s - loss: 0.5771 - val_loss: 1.0279
Epoch 2779/5000
26/26 - 1s - loss: 0.5783 - val_loss: 1.0280
Epoch 2780/5000
26/26 - 1s - loss: 0.5778 - val_loss: 1.0274
Epoch 02780: val_loss improved from 1.02956 to 1.02738, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 2781/5000
26/26 - 1s - loss: 0.5783 - val_loss: 1.0271
Epoch 2782/5000
26/26 - 1s - loss: 0.5785 - val_loss: 1.0274
Epoch 2783/5000
26/26 - 1s - loss: 0.5776 - val_loss: 1.0277
Epoch 2784/5000
26/26 - 1s - loss: 0.5786 - val_loss: 1.0278
Epoch 2785/5000
26/26 - 1s - loss: 0.5782 - val_loss: 1.0276
Epoch 2786/5000
26/26 - 1s - loss: 0.5767 - val_loss: 1.0262
Epoch 2787/5000
26/26 - 1s - loss: 0.5761 - val_loss: 1.0284
Epoch 2788/5000
26/26 - 1s - loss: 0.5761 - val_loss: 1.0262
Epoch 2789/5000
26/26 - 1s - loss: 0.5773 - val_loss: 1.0264
Epoch 2790/5000
26/26 - 1s - loss: 0.5763 - val_loss: 1.0271
Epoch 02790: val_loss improved from 1.02738 to 1.02712, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 2791/5000
26/26 - 1s - loss: 0.5768 - val_loss: 1.0228
Epoch 2792/5000
26/26 - 1s - loss: 0.5758 - val_loss: 1.0256
Epoch 2793/5000
26/26 - 1s - loss: 0.5758 - val_loss: 1.0251
Epoch 2794/5000
26/26 - 1s - loss: 0.5763 - val_loss: 1.0251
Epoch 2795/5000
26/26 - 2s - loss: 0.5751 - val_loss: 1.0243
Epoch 2796/5000
26/26 - 1s - loss: 0.5750 - val_loss: 1.0249
Epoch 2797/5000
26/26 - 1s - loss: 0.5754 - val_loss: 1.0252
Epoch 2798/5000
26/26 - 1s - loss: 0.5748 - val_loss: 1.0236
Epoch 2799/5000
26/26 - 1s - loss: 0.5741 - val_loss: 1.0239
Epoch 2800/5000
26/26 - 2s - loss: 0.5754 - val_loss: 1.0239
Epoch 02800: val_loss improved from 1.02712 to 1.02386, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 2801/5000
26/26 - 1s - loss: 0.5739 - val_loss: 1.0228
Epoch 2802/5000
26/26 - 1s - loss: 0.5738 - val_loss: 1.0226
Epoch 2803/5000
26/26 - 1s - loss: 0.5745 - val_loss: 1.0254
Epoch 2804/5000
26/26 - 1s - loss: 0.5735 - val_loss: 1.0252
Epoch 2805/5000
26/26 - 1s - loss: 0.5748 - val_loss: 1.0235
Epoch 2806/5000
26/26 - 1s - loss: 0.5742 - val_loss: 1.0258
Epoch 2807/5000
26/26 - 1s - loss: 0.5741 - val_loss: 1.0249
Epoch 2808/5000
26/26 - 1s - loss: 0.5738 - val_loss: 1.0241
Epoch 2809/5000
26/26 - 2s - loss: 0.5727 - val_loss: 1.0236
Epoch 2810/5000
26/26 - 1s - loss: 0.5721 - val_loss: 1.0239
Epoch 02810: val_loss did not improve from 1.02386
Epoch 2811/5000
26/26 - 1s - loss: 0.5734 - val_loss: 1.0225
Epoch 2812/5000
26/26 - 1s - loss: 0.5735 - val_loss: 1.0228
Epoch 2813/5000
26/26 - 1s - loss: 0.5721 - val_loss: 1.0230
Epoch 2814/5000
26/26 - 1s - loss: 0.5720 - val_loss: 1.0235
Epoch 2815/5000
26/26 - 1s - loss: 0.5723 - val_loss: 1.0211
Epoch 2816/5000
26/26 - 1s - loss: 0.5721 - val_loss: 1.0222
Epoch 2817/5000
26/26 - 1s - loss: 0.5709 - val_loss: 1.0229
Epoch 2818/5000
26/26 - 1s - loss: 0.5712 - val_loss: 1.0210
Epoch 2819/5000
26/26 - 1s - loss: 0.5716 - val_loss: 1.0221
Epoch 2820/5000
26/26 - 2s - loss: 0.5720 - val_loss: 1.0209
Epoch 02820: val_loss improved from 1.02386 to 1.02095, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 2821/5000
26/26 - 1s - loss: 0.5714 - val_loss: 1.0215
Epoch 2822/5000
26/26 - 1s - loss: 0.5715 - val_loss: 1.0223
Epoch 2823/5000
26/26 - 1s - loss: 0.5719 - val_loss: 1.0214
Epoch 2824/5000
26/26 - 1s - loss: 0.5715 - val_loss: 1.0205
Epoch 2825/5000
26/26 - 1s - loss: 0.5709 - val_loss: 1.0195
Epoch 2826/5000
26/26 - 1s - loss: 0.5699 - val_loss: 1.0206
Epoch 2827/5000
26/26 - 1s - loss: 0.5711 - val_loss: 1.0211
Epoch 2828/5000
26/26 - 1s - loss: 0.5702 - val_loss: 1.0215
Epoch 2829/5000
26/26 - 1s - loss: 0.5709 - val_loss: 1.0211
Epoch 2830/5000
26/26 - 1s - loss: 0.5697 - val_loss: 1.0206
Epoch 02830: val_loss improved from 1.02095 to 1.02060, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 2831/5000
26/26 - 1s - loss: 0.5709 - val_loss: 1.0193
Epoch 2832/5000
26/26 - 1s - loss: 0.5686 - val_loss: 1.0181
Epoch 2833/5000
26/26 - 1s - loss: 0.5694 - val_loss: 1.0197
Epoch 2834/5000
26/26 - 1s - loss: 0.5699 - val_loss: 1.0197
Epoch 2835/5000
26/26 - 1s - loss: 0.5694 - val_loss: 1.0206
Epoch 2836/5000
26/26 - 1s - loss: 0.5680 - val_loss: 1.0204
Epoch 2837/5000
26/26 - 1s - loss: 0.5692 - val_loss: 1.0188
Epoch 2838/5000
26/26 - 1s - loss: 0.5679 - val_loss: 1.0179
Epoch 2839/5000
26/26 - 1s - loss: 0.5679 - val_loss: 1.0196
Epoch 2840/5000
26/26 - 1s - loss: 0.5679 - val_loss: 1.0180
Epoch 02840: val_loss improved from 1.02060 to 1.01803, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 2841/5000
26/26 - 1s - loss: 0.5674 - val_loss: 1.0175
Epoch 2842/5000
26/26 - 1s - loss: 0.5676 - val_loss: 1.0186
Epoch 2843/5000
26/26 - 1s - loss: 0.5677 - val_loss: 1.0174
Epoch 2844/5000
26/26 - 1s - loss: 0.5668 - val_loss: 1.0184
Epoch 2845/5000
26/26 - 1s - loss: 0.5674 - val_loss: 1.0164
Epoch 2846/5000
26/26 - 1s - loss: 0.5680 - val_loss: 1.0165
Epoch 2847/5000
26/26 - 1s - loss: 0.5662 - val_loss: 1.0153
Epoch 2848/5000
26/26 - 1s - loss: 0.5667 - val_loss: 1.0184
Epoch 2849/5000
26/26 - 1s - loss: 0.5675 - val_loss: 1.0168
Epoch 2850/5000
26/26 - 1s - loss: 0.5676 - val_loss: 1.0166
Epoch 02850: val_loss improved from 1.01803 to 1.01662, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 2851/5000
26/26 - 1s - loss: 0.5655 - val_loss: 1.0154
Epoch 2852/5000
26/26 - 1s - loss: 0.5663 - val_loss: 1.0167
Epoch 2853/5000
26/26 - 2s - loss: 0.5670 - val_loss: 1.0145
Epoch 2854/5000
26/26 - 1s - loss: 0.5661 - val_loss: 1.0165
Epoch 2855/5000
26/26 - 1s - loss: 0.5658 - val_loss: 1.0149
Epoch 2856/5000
26/26 - 1s - loss: 0.5657 - val_loss: 1.0147
Epoch 2857/5000
26/26 - 2s - loss: 0.5640 - val_loss: 1.0144
Epoch 2858/5000
26/26 - 1s - loss: 0.5647 - val_loss: 1.0148
Epoch 2859/5000
26/26 - 1s - loss: 0.5650 - val_loss: 1.0143
Epoch 2860/5000
26/26 - 1s - loss: 0.5643 - val_loss: 1.0142
Epoch 02860: val_loss improved from 1.01662 to 1.01417, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 2861/5000
26/26 - 1s - loss: 0.5653 - val_loss: 1.0153
Epoch 2862/5000
26/26 - 1s - loss: 0.5635 - val_loss: 1.0145
Epoch 2863/5000
26/26 - 1s - loss: 0.5645 - val_loss: 1.0149
Epoch 2864/5000
26/26 - 1s - loss: 0.5643 - val_loss: 1.0146
Epoch 2865/5000
26/26 - 1s - loss: 0.5634 - val_loss: 1.0149
Epoch 2866/5000
26/26 - 1s - loss: 0.5631 - val_loss: 1.0140
Epoch 2867/5000
26/26 - 2s - loss: 0.5638 - val_loss: 1.0142
Epoch 2868/5000
26/26 - 2s - loss: 0.5639 - val_loss: 1.0137
Epoch 2869/5000
26/26 - 1s - loss: 0.5638 - val_loss: 1.0130
Epoch 2870/5000
26/26 - 1s - loss: 0.5629 - val_loss: 1.0124
Epoch 02870: val_loss improved from 1.01417 to 1.01242, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 2871/5000
26/26 - 1s - loss: 0.5638 - val_loss: 1.0133
Epoch 2872/5000
26/26 - 1s - loss: 0.5624 - val_loss: 1.0120
Epoch 2873/5000
26/26 - 1s - loss: 0.5638 - val_loss: 1.0114
Epoch 2874/5000
26/26 - 1s - loss: 0.5619 - val_loss: 1.0133
Epoch 2875/5000
26/26 - 1s - loss: 0.5629 - val_loss: 1.0116
Epoch 2876/5000
26/26 - 1s - loss: 0.5623 - val_loss: 1.0108
Epoch 2877/5000
26/26 - 1s - loss: 0.5623 - val_loss: 1.0139
Epoch 2878/5000
26/26 - 1s - loss: 0.5635 - val_loss: 1.0135
Epoch 2879/5000
26/26 - 1s - loss: 0.5622 - val_loss: 1.0116
Epoch 2880/5000
26/26 - 1s - loss: 0.5616 - val_loss: 1.0128
Epoch 02880: val_loss did not improve from 1.01242
Epoch 2881/5000
26/26 - 1s - loss: 0.5604 - val_loss: 1.0134
Epoch 2882/5000
26/26 - 1s - loss: 0.5613 - val_loss: 1.0108
Epoch 2883/5000
26/26 - 1s - loss: 0.5607 - val_loss: 1.0116
Epoch 2884/5000
26/26 - 1s - loss: 0.5618 - val_loss: 1.0109
Epoch 2885/5000
26/26 - 1s - loss: 0.5621 - val_loss: 1.0124
Epoch 2886/5000
26/26 - 1s - loss: 0.5609 - val_loss: 1.0114
Epoch 2887/5000
26/26 - 1s - loss: 0.5612 - val_loss: 1.0106
Epoch 2888/5000
26/26 - 1s - loss: 0.5604 - val_loss: 1.0094
Epoch 2889/5000
26/26 - 1s - loss: 0.5602 - val_loss: 1.0112
Epoch 2890/5000
26/26 - 1s - loss: 0.5592 - val_loss: 1.0109
Epoch 02890: val_loss improved from 1.01242 to 1.01086, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 2891/5000
26/26 - 1s - loss: 0.5608 - val_loss: 1.0112
Epoch 2892/5000
26/26 - 1s - loss: 0.5591 - val_loss: 1.0092
Epoch 2893/5000
26/26 - 1s - loss: 0.5584 - val_loss: 1.0091
Epoch 2894/5000
26/26 - 1s - loss: 0.5591 - val_loss: 1.0084
Epoch 2895/5000
26/26 - 1s - loss: 0.5598 - val_loss: 1.0092
Epoch 2896/5000
26/26 - 1s - loss: 0.5588 - val_loss: 1.0087
Epoch 2897/5000
26/26 - 1s - loss: 0.5590 - val_loss: 1.0090
Epoch 2898/5000
26/26 - 2s - loss: 0.5588 - val_loss: 1.0080
Epoch 2899/5000
26/26 - 1s - loss: 0.5577 - val_loss: 1.0101
Epoch 2900/5000
26/26 - 1s - loss: 0.5581 - val_loss: 1.0073
Epoch 02900: val_loss improved from 1.01086 to 1.00727, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 2901/5000
26/26 - 1s - loss: 0.5578 - val_loss: 1.0086
Epoch 2902/5000
26/26 - 1s - loss: 0.5579 - val_loss: 1.0102
Epoch 2903/5000
26/26 - 1s - loss: 0.5583 - val_loss: 1.0087
Epoch 2904/5000
26/26 - 1s - loss: 0.5577 - val_loss: 1.0084
Epoch 2905/5000
26/26 - 1s - loss: 0.5588 - val_loss: 1.0093
Epoch 2906/5000
26/26 - 1s - loss: 0.5574 - val_loss: 1.0069
Epoch 2907/5000
26/26 - 1s - loss: 0.5578 - val_loss: 1.0089
Epoch 2908/5000
26/26 - 2s - loss: 0.5568 - val_loss: 1.0091
Epoch 2909/5000
26/26 - 1s - loss: 0.5576 - val_loss: 1.0082
Epoch 2910/5000
26/26 - 1s - loss: 0.5568 - val_loss: 1.0069
Epoch 02910: val_loss improved from 1.00727 to 1.00687, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 2911/5000
26/26 - 1s - loss: 0.5585 - val_loss: 1.0062
Epoch 2912/5000
26/26 - 1s - loss: 0.5558 - val_loss: 1.0068
Epoch 2913/5000
26/26 - 1s - loss: 0.5573 - val_loss: 1.0066
Epoch 2914/5000
26/26 - 1s - loss: 0.5558 - val_loss: 1.0064
Epoch 2915/5000
26/26 - 1s - loss: 0.5576 - val_loss: 1.0068
Epoch 2916/5000
26/26 - 1s - loss: 0.5567 - val_loss: 1.0059
Epoch 2917/5000
26/26 - 1s - loss: 0.5560 - val_loss: 1.0046
Epoch 2918/5000
26/26 - 1s - loss: 0.5556 - val_loss: 1.0058
Epoch 2919/5000
26/26 - 1s - loss: 0.5553 - val_loss: 1.0051
Epoch 2920/5000
26/26 - 1s - loss: 0.5549 - val_loss: 1.0062
Epoch 02920: val_loss improved from 1.00687 to 1.00624, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 2921/5000
26/26 - 1s - loss: 0.5556 - val_loss: 1.0061
Epoch 2922/5000
26/26 - 1s - loss: 0.5560 - val_loss: 1.0055
Epoch 2923/5000
26/26 - 1s - loss: 0.5568 - val_loss: 1.0042
Epoch 2924/5000
26/26 - 1s - loss: 0.5560 - val_loss: 1.0053
Epoch 2925/5000
26/26 - 1s - loss: 0.5540 - val_loss: 1.0038
Epoch 2926/5000
26/26 - 1s - loss: 0.5539 - val_loss: 1.0069
Epoch 2927/5000
26/26 - 1s - loss: 0.5548 - val_loss: 1.0054
Epoch 2928/5000
26/26 - 1s - loss: 0.5541 - val_loss: 1.0056
Epoch 2929/5000
26/26 - 1s - loss: 0.5548 - val_loss: 1.0056
Epoch 2930/5000
26/26 - 1s - loss: 0.5554 - val_loss: 1.0039
Epoch 02930: val_loss improved from 1.00624 to 1.00385, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 2931/5000
26/26 - 1s - loss: 0.5535 - val_loss: 1.0047
Epoch 2932/5000
26/26 - 1s - loss: 0.5542 - val_loss: 1.0054
Epoch 2933/5000
26/26 - 1s - loss: 0.5542 - val_loss: 1.0031
Epoch 2934/5000
26/26 - 1s - loss: 0.5546 - val_loss: 1.0031
Epoch 2935/5000
26/26 - 1s - loss: 0.5521 - val_loss: 1.0029
Epoch 2936/5000
26/26 - 1s - loss: 0.5530 - val_loss: 1.0040
Epoch 2937/5000
26/26 - 1s - loss: 0.5530 - val_loss: 1.0035
Epoch 2938/5000
26/26 - 1s - loss: 0.5531 - val_loss: 1.0029
Epoch 2939/5000
26/26 - 1s - loss: 0.5528 - val_loss: 1.0020
Epoch 2940/5000
26/26 - 1s - loss: 0.5532 - val_loss: 1.0030
Epoch 02940: val_loss improved from 1.00385 to 1.00296, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 2941/5000
26/26 - 1s - loss: 0.5524 - val_loss: 1.0037
Epoch 2942/5000
26/26 - 1s - loss: 0.5521 - val_loss: 1.0038
Epoch 2943/5000
26/26 - 1s - loss: 0.5512 - val_loss: 1.0025
Epoch 2944/5000
26/26 - 1s - loss: 0.5526 - val_loss: 1.0025
Epoch 2945/5000
26/26 - 1s - loss: 0.5516 - val_loss: 1.0027
Epoch 2946/5000
26/26 - 1s - loss: 0.5509 - val_loss: 1.0006
Epoch 2947/5000
26/26 - 1s - loss: 0.5514 - val_loss: 1.0010
Epoch 2948/5000
26/26 - 1s - loss: 0.5520 - val_loss: 1.0009
Epoch 2949/5000
26/26 - 1s - loss: 0.5515 - val_loss: 1.0013
Epoch 2950/5000
26/26 - 1s - loss: 0.5521 - val_loss: 1.0006
Epoch 02950: val_loss improved from 1.00296 to 1.00059, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 2951/5000
26/26 - 1s - loss: 0.5513 - val_loss: 1.0009
Epoch 2952/5000
26/26 - 1s - loss: 0.5504 - val_loss: 1.0006
Epoch 2953/5000
26/26 - 1s - loss: 0.5503 - val_loss: 1.0007
Epoch 2954/5000
26/26 - 1s - loss: 0.5519 - val_loss: 1.0008
Epoch 2955/5000
26/26 - 1s - loss: 0.5499 - val_loss: 1.0003
Epoch 2956/5000
26/26 - 2s - loss: 0.5495 - val_loss: 1.0012
Epoch 2957/5000
26/26 - 1s - loss: 0.5501 - val_loss: 0.9987
Epoch 2958/5000
26/26 - 1s - loss: 0.5501 - val_loss: 0.9991
Epoch 2959/5000
26/26 - 1s - loss: 0.5492 - val_loss: 0.9993
Epoch 2960/5000
26/26 - 1s - loss: 0.5496 - val_loss: 0.9992
Epoch 02960: val_loss improved from 1.00059 to 0.99921, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 2961/5000
26/26 - 1s - loss: 0.5500 - val_loss: 0.9993
Epoch 2962/5000
26/26 - 1s - loss: 0.5489 - val_loss: 0.9972
Epoch 2963/5000
26/26 - 1s - loss: 0.5490 - val_loss: 0.9995
Epoch 2964/5000
26/26 - 1s - loss: 0.5494 - val_loss: 0.9993
Epoch 2965/5000
26/26 - 1s - loss: 0.5481 - val_loss: 0.9989
Epoch 2966/5000
26/26 - 1s - loss: 0.5480 - val_loss: 0.9990
Epoch 2967/5000
26/26 - 1s - loss: 0.5495 - val_loss: 0.9980
Epoch 2968/5000
26/26 - 1s - loss: 0.5483 - val_loss: 0.9998
Epoch 2969/5000
26/26 - 1s - loss: 0.5480 - val_loss: 0.9985
Epoch 2970/5000
26/26 - 1s - loss: 0.5477 - val_loss: 0.9991
Epoch 02970: val_loss improved from 0.99921 to 0.99909, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 2971/5000
26/26 - 1s - loss: 0.5491 - val_loss: 0.9992
Epoch 2972/5000
26/26 - 1s - loss: 0.5477 - val_loss: 0.9967
Epoch 2973/5000
26/26 - 1s - loss: 0.5479 - val_loss: 0.9979
Epoch 2974/5000
26/26 - 1s - loss: 0.5465 - val_loss: 0.9991
Epoch 2975/5000
26/26 - 1s - loss: 0.5463 - val_loss: 0.9980
Epoch 2976/5000
26/26 - 1s - loss: 0.5473 - val_loss: 0.9986
Epoch 2977/5000
26/26 - 1s - loss: 0.5467 - val_loss: 0.9979
Epoch 2978/5000
26/26 - 1s - loss: 0.5470 - val_loss: 0.9950
Epoch 2979/5000
26/26 - 1s - loss: 0.5467 - val_loss: 0.9964
Epoch 2980/5000
26/26 - 1s - loss: 0.5466 - val_loss: 0.9972
Epoch 02980: val_loss improved from 0.99909 to 0.99720, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 2981/5000
26/26 - 1s - loss: 0.5460 - val_loss: 0.9989
Epoch 2982/5000
26/26 - 1s - loss: 0.5456 - val_loss: 0.9976
Epoch 2983/5000
26/26 - 1s - loss: 0.5470 - val_loss: 0.9972
Epoch 2984/5000
26/26 - 1s - loss: 0.5461 - val_loss: 0.9967
Epoch 2985/5000
26/26 - 1s - loss: 0.5450 - val_loss: 0.9975
Epoch 2986/5000
26/26 - 1s - loss: 0.5465 - val_loss: 0.9970
Epoch 2987/5000
26/26 - 1s - loss: 0.5453 - val_loss: 0.9971
Epoch 2988/5000
26/26 - 1s - loss: 0.5450 - val_loss: 0.9981
Epoch 2989/5000
26/26 - 1s - loss: 0.5445 - val_loss: 0.9974
Epoch 2990/5000
26/26 - 1s - loss: 0.5452 - val_loss: 0.9960
Epoch 02990: val_loss improved from 0.99720 to 0.99601, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 2991/5000
26/26 - 1s - loss: 0.5455 - val_loss: 0.9952
Epoch 2992/5000
26/26 - 1s - loss: 0.5462 - val_loss: 0.9965
Epoch 2993/5000
26/26 - 1s - loss: 0.5456 - val_loss: 0.9964
Epoch 2994/5000
26/26 - 2s - loss: 0.5455 - val_loss: 0.9969
Epoch 2995/5000
26/26 - 1s - loss: 0.5446 - val_loss: 0.9952
Epoch 2996/5000
26/26 - 1s - loss: 0.5447 - val_loss: 0.9943
Epoch 2997/5000
26/26 - 1s - loss: 0.5450 - val_loss: 0.9956
Epoch 2998/5000
26/26 - 1s - loss: 0.5446 - val_loss: 0.9951
Epoch 2999/5000
26/26 - 1s - loss: 0.5447 - val_loss: 0.9951
Epoch 3000/5000
26/26 - 1s - loss: 0.5443 - val_loss: 0.9928
Epoch 03000: val_loss improved from 0.99601 to 0.99281, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 3001/5000
26/26 - 1s - loss: 0.5438 - val_loss: 0.9935
Epoch 3002/5000
26/26 - 1s - loss: 0.5428 - val_loss: 0.9931
Epoch 3003/5000
26/26 - 1s - loss: 0.5447 - val_loss: 0.9941
Epoch 3004/5000
26/26 - 1s - loss: 0.5431 - val_loss: 0.9924
Epoch 3005/5000
26/26 - 1s - loss: 0.5441 - val_loss: 0.9929
Epoch 3006/5000
26/26 - 1s - loss: 0.5434 - val_loss: 0.9930
Epoch 3007/5000
26/26 - 1s - loss: 0.5425 - val_loss: 0.9927
Epoch 3008/5000
26/26 - 1s - loss: 0.5431 - val_loss: 0.9936
Epoch 3009/5000
26/26 - 1s - loss: 0.5428 - val_loss: 0.9922
Epoch 3010/5000
26/26 - 1s - loss: 0.5422 - val_loss: 0.9922
Epoch 03010: val_loss improved from 0.99281 to 0.99216, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 3011/5000
26/26 - 1s - loss: 0.5433 - val_loss: 0.9922
Epoch 3012/5000
26/26 - 1s - loss: 0.5417 - val_loss: 0.9918
Epoch 3013/5000
26/26 - 2s - loss: 0.5416 - val_loss: 0.9921
Epoch 3014/5000
26/26 - 1s - loss: 0.5429 - val_loss: 0.9937
Epoch 3015/5000
26/26 - 1s - loss: 0.5428 - val_loss: 0.9923
Epoch 3016/5000
26/26 - 1s - loss: 0.5421 - val_loss: 0.9932
Epoch 3017/5000
26/26 - 1s - loss: 0.5416 - val_loss: 0.9907
Epoch 3018/5000
26/26 - 1s - loss: 0.5404 - val_loss: 0.9907
Epoch 3019/5000
26/26 - 1s - loss: 0.5416 - val_loss: 0.9909
Epoch 3020/5000
26/26 - 1s - loss: 0.5415 - val_loss: 0.9893
Epoch 03020: val_loss improved from 0.99216 to 0.98930, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 3021/5000
26/26 - 1s - loss: 0.5424 - val_loss: 0.9902
Epoch 3022/5000
26/26 - 1s - loss: 0.5410 - val_loss: 0.9908
Epoch 3023/5000
26/26 - 1s - loss: 0.5405 - val_loss: 0.9913
Epoch 3024/5000
26/26 - 1s - loss: 0.5407 - val_loss: 0.9897
Epoch 3025/5000
26/26 - 1s - loss: 0.5408 - val_loss: 0.9915
Epoch 3026/5000
26/26 - 1s - loss: 0.5397 - val_loss: 0.9919
Epoch 3027/5000
26/26 - 1s - loss: 0.5399 - val_loss: 0.9900
Epoch 3028/5000
26/26 - 1s - loss: 0.5408 - val_loss: 0.9904
Epoch 3029/5000
26/26 - 1s - loss: 0.5395 - val_loss: 0.9902
Epoch 3030/5000
26/26 - 1s - loss: 0.5396 - val_loss: 0.9897
Epoch 03030: val_loss did not improve from 0.98930
Epoch 3031/5000
26/26 - 1s - loss: 0.5395 - val_loss: 0.9898
Epoch 3032/5000
26/26 - 2s - loss: 0.5392 - val_loss: 0.9895
Epoch 3033/5000
26/26 - 1s - loss: 0.5396 - val_loss: 0.9902
Epoch 3034/5000
26/26 - 1s - loss: 0.5399 - val_loss: 0.9909
Epoch 3035/5000
26/26 - 1s - loss: 0.5385 - val_loss: 0.9908
Epoch 3036/5000
26/26 - 1s - loss: 0.5391 - val_loss: 0.9882
Epoch 3037/5000
26/26 - 1s - loss: 0.5385 - val_loss: 0.9915
Epoch 3038/5000
26/26 - 1s - loss: 0.5381 - val_loss: 0.9892
Epoch 3039/5000
26/26 - 1s - loss: 0.5384 - val_loss: 0.9894
Epoch 3040/5000
26/26 - 1s - loss: 0.5382 - val_loss: 0.9885
Epoch 03040: val_loss improved from 0.98930 to 0.98850, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 3041/5000
26/26 - 1s - loss: 0.5389 - val_loss: 0.9892
Epoch 3042/5000
26/26 - 1s - loss: 0.5380 - val_loss: 0.9876
Epoch 3043/5000
26/26 - 1s - loss: 0.5380 - val_loss: 0.9880
Epoch 3044/5000
26/26 - 1s - loss: 0.5381 - val_loss: 0.9883
Epoch 3045/5000
26/26 - 1s - loss: 0.5380 - val_loss: 0.9889
Epoch 3046/5000
26/26 - 1s - loss: 0.5369 - val_loss: 0.9879
Epoch 3047/5000
26/26 - 1s - loss: 0.5372 - val_loss: 0.9864
Epoch 3048/5000
26/26 - 1s - loss: 0.5374 - val_loss: 0.9875
Epoch 3049/5000
26/26 - 1s - loss: 0.5377 - val_loss: 0.9880
Epoch 3050/5000
26/26 - 1s - loss: 0.5366 - val_loss: 0.9872
Epoch 03050: val_loss improved from 0.98850 to 0.98716, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 3051/5000
26/26 - 1s - loss: 0.5360 - val_loss: 0.9899
Epoch 3052/5000
26/26 - 1s - loss: 0.5371 - val_loss: 0.9886
Epoch 3053/5000
26/26 - 1s - loss: 0.5367 - val_loss: 0.9858
Epoch 3054/5000
26/26 - 2s - loss: 0.5367 - val_loss: 0.9873
Epoch 3055/5000
26/26 - 1s - loss: 0.5364 - val_loss: 0.9868
Epoch 3056/5000
26/26 - 1s - loss: 0.5363 - val_loss: 0.9896
Epoch 3057/5000
26/26 - 1s - loss: 0.5369 - val_loss: 0.9858
Epoch 3058/5000
26/26 - 1s - loss: 0.5372 - val_loss: 0.9852
Epoch 3059/5000
26/26 - 1s - loss: 0.5358 - val_loss: 0.9873
Epoch 3060/5000
26/26 - 1s - loss: 0.5359 - val_loss: 0.9866
Epoch 03060: val_loss improved from 0.98716 to 0.98659, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 3061/5000
26/26 - 1s - loss: 0.5351 - val_loss: 0.9856
Epoch 3062/5000
26/26 - 1s - loss: 0.5349 - val_loss: 0.9850
Epoch 3063/5000
26/26 - 1s - loss: 0.5360 - val_loss: 0.9849
Epoch 3064/5000
26/26 - 1s - loss: 0.5361 - val_loss: 0.9863
Epoch 3065/5000
26/26 - 1s - loss: 0.5360 - val_loss: 0.9856
Epoch 3066/5000
26/26 - 1s - loss: 0.5353 - val_loss: 0.9840
Epoch 3067/5000
26/26 - 1s - loss: 0.5344 - val_loss: 0.9843
Epoch 3068/5000
26/26 - 1s - loss: 0.5353 - val_loss: 0.9837
Epoch 3069/5000
26/26 - 1s - loss: 0.5338 - val_loss: 0.9836
Epoch 3070/5000
26/26 - 1s - loss: 0.5333 - val_loss: 0.9848
Epoch 03070: val_loss improved from 0.98659 to 0.98477, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 3071/5000
26/26 - 1s - loss: 0.5339 - val_loss: 0.9837
Epoch 3072/5000
26/26 - 1s - loss: 0.5351 - val_loss: 0.9835
Epoch 3073/5000
26/26 - 1s - loss: 0.5328 - val_loss: 0.9827
Epoch 3074/5000
26/26 - 1s - loss: 0.5346 - val_loss: 0.9828
Epoch 3075/5000
26/26 - 2s - loss: 0.5328 - val_loss: 0.9857
Epoch 3076/5000
26/26 - 1s - loss: 0.5336 - val_loss: 0.9832
Epoch 3077/5000
26/26 - 1s - loss: 0.5336 - val_loss: 0.9846
Epoch 3078/5000
26/26 - 1s - loss: 0.5336 - val_loss: 0.9833
Epoch 3079/5000
26/26 - 1s - loss: 0.5333 - val_loss: 0.9826
Epoch 3080/5000
26/26 - 1s - loss: 0.5326 - val_loss: 0.9829
Epoch 03080: val_loss improved from 0.98477 to 0.98286, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 3081/5000
26/26 - 1s - loss: 0.5332 - val_loss: 0.9824
Epoch 3082/5000
26/26 - 1s - loss: 0.5330 - val_loss: 0.9822
Epoch 3083/5000
26/26 - 1s - loss: 0.5321 - val_loss: 0.9824
Epoch 3084/5000
26/26 - 1s - loss: 0.5320 - val_loss: 0.9828
Epoch 3085/5000
26/26 - 2s - loss: 0.5329 - val_loss: 0.9824
Epoch 3086/5000
26/26 - 2s - loss: 0.5322 - val_loss: 0.9829
Epoch 3087/5000
26/26 - 1s - loss: 0.5322 - val_loss: 0.9818
Epoch 3088/5000
26/26 - 1s - loss: 0.5316 - val_loss: 0.9836
Epoch 3089/5000
26/26 - 1s - loss: 0.5328 - val_loss: 0.9823
Epoch 3090/5000
26/26 - 1s - loss: 0.5324 - val_loss: 0.9817
Epoch 03090: val_loss improved from 0.98286 to 0.98174, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 3091/5000
26/26 - 1s - loss: 0.5327 - val_loss: 0.9823
Epoch 3092/5000
26/26 - 1s - loss: 0.5322 - val_loss: 0.9830
Epoch 3093/5000
26/26 - 1s - loss: 0.5317 - val_loss: 0.9796
Epoch 3094/5000
26/26 - 1s - loss: 0.5305 - val_loss: 0.9821
Epoch 3095/5000
26/26 - 2s - loss: 0.5309 - val_loss: 0.9815
Epoch 3096/5000
26/26 - 1s - loss: 0.5321 - val_loss: 0.9822
Epoch 3097/5000
26/26 - 1s - loss: 0.5310 - val_loss: 0.9806
Epoch 3098/5000
26/26 - 1s - loss: 0.5300 - val_loss: 0.9803
Epoch 3099/5000
26/26 - 1s - loss: 0.5304 - val_loss: 0.9816
Epoch 3100/5000
26/26 - 1s - loss: 0.5309 - val_loss: 0.9803
Epoch 03100: val_loss improved from 0.98174 to 0.98029, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 3101/5000
26/26 - 1s - loss: 0.5302 - val_loss: 0.9818
Epoch 3102/5000
26/26 - 1s - loss: 0.5302 - val_loss: 0.9793
Epoch 3103/5000
26/26 - 1s - loss: 0.5285 - val_loss: 0.9803
Epoch 3104/5000
26/26 - 1s - loss: 0.5296 - val_loss: 0.9812
Epoch 3105/5000
26/26 - 1s - loss: 0.5301 - val_loss: 0.9794
Epoch 3106/5000
26/26 - 1s - loss: 0.5292 - val_loss: 0.9796
Epoch 3107/5000
26/26 - 1s - loss: 0.5290 - val_loss: 0.9813
Epoch 3108/5000
26/26 - 1s - loss: 0.5305 - val_loss: 0.9800
Epoch 3109/5000
26/26 - 1s - loss: 0.5293 - val_loss: 0.9807
Epoch 3110/5000
26/26 - 1s - loss: 0.5293 - val_loss: 0.9790
Epoch 03110: val_loss improved from 0.98029 to 0.97902, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 3111/5000
26/26 - 1s - loss: 0.5288 - val_loss: 0.9788
Epoch 3112/5000
26/26 - 1s - loss: 0.5289 - val_loss: 0.9807
Epoch 3113/5000
26/26 - 1s - loss: 0.5290 - val_loss: 0.9795
Epoch 3114/5000
26/26 - 1s - loss: 0.5281 - val_loss: 0.9786
Epoch 3115/5000
26/26 - 1s - loss: 0.5291 - val_loss: 0.9782
Epoch 3116/5000
26/26 - 1s - loss: 0.5293 - val_loss: 0.9792
Epoch 3117/5000
26/26 - 1s - loss: 0.5275 - val_loss: 0.9797
Epoch 3118/5000
26/26 - 1s - loss: 0.5278 - val_loss: 0.9780
Epoch 3119/5000
26/26 - 1s - loss: 0.5277 - val_loss: 0.9785
Epoch 3120/5000
26/26 - 1s - loss: 0.5274 - val_loss: 0.9781
Epoch 03120: val_loss improved from 0.97902 to 0.97810, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 3121/5000
26/26 - 1s - loss: 0.5282 - val_loss: 0.9777
Epoch 3122/5000
26/26 - 1s - loss: 0.5271 - val_loss: 0.9767
Epoch 3123/5000
26/26 - 1s - loss: 0.5279 - val_loss: 0.9771
Epoch 3124/5000
26/26 - 1s - loss: 0.5274 - val_loss: 0.9767
Epoch 3125/5000
26/26 - 2s - loss: 0.5272 - val_loss: 0.9773
Epoch 3126/5000
26/26 - 1s - loss: 0.5273 - val_loss: 0.9775
Epoch 3127/5000
26/26 - 1s - loss: 0.5272 - val_loss: 0.9782
Epoch 3128/5000
26/26 - 1s - loss: 0.5267 - val_loss: 0.9774
Epoch 3129/5000
26/26 - 1s - loss: 0.5271 - val_loss: 0.9778
Epoch 3130/5000
26/26 - 1s - loss: 0.5263 - val_loss: 0.9781
Epoch 03130: val_loss did not improve from 0.97810
Epoch 3131/5000
26/26 - 1s - loss: 0.5263 - val_loss: 0.9772
Epoch 3132/5000
26/26 - 1s - loss: 0.5274 - val_loss: 0.9782
Epoch 3133/5000
26/26 - 1s - loss: 0.5248 - val_loss: 0.9775
Epoch 3134/5000
26/26 - 1s - loss: 0.5261 - val_loss: 0.9763
Epoch 3135/5000
26/26 - 1s - loss: 0.5260 - val_loss: 0.9767
Epoch 3136/5000
26/26 - 1s - loss: 0.5245 - val_loss: 0.9769
Epoch 3137/5000
26/26 - 1s - loss: 0.5262 - val_loss: 0.9769
Epoch 3138/5000
26/26 - 1s - loss: 0.5259 - val_loss: 0.9751
Epoch 3139/5000
26/26 - 1s - loss: 0.5275 - val_loss: 0.9761
Epoch 3140/5000
26/26 - 1s - loss: 0.5262 - val_loss: 0.9771
Epoch 03140: val_loss improved from 0.97810 to 0.97713, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 3141/5000
26/26 - 1s - loss: 0.5240 - val_loss: 0.9764
Epoch 3142/5000
26/26 - 1s - loss: 0.5251 - val_loss: 0.9749
Epoch 3143/5000
26/26 - 1s - loss: 0.5255 - val_loss: 0.9764
Epoch 3144/5000
26/26 - 1s - loss: 0.5251 - val_loss: 0.9764
Epoch 3145/5000
26/26 - 1s - loss: 0.5257 - val_loss: 0.9763
Epoch 3146/5000
26/26 - 1s - loss: 0.5250 - val_loss: 0.9764
Epoch 3147/5000
26/26 - 1s - loss: 0.5255 - val_loss: 0.9748
Epoch 3148/5000
26/26 - 1s - loss: 0.5246 - val_loss: 0.9751
Epoch 3149/5000
26/26 - 1s - loss: 0.5250 - val_loss: 0.9754
Epoch 3150/5000
26/26 - 1s - loss: 0.5240 - val_loss: 0.9740
Epoch 03150: val_loss improved from 0.97713 to 0.97403, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 3151/5000
26/26 - 2s - loss: 0.5234 - val_loss: 0.9740
Epoch 3152/5000
26/26 - 1s - loss: 0.5248 - val_loss: 0.9746
Epoch 3153/5000
26/26 - 1s - loss: 0.5241 - val_loss: 0.9739
Epoch 3154/5000
26/26 - 1s - loss: 0.5239 - val_loss: 0.9738
Epoch 3155/5000
26/26 - 2s - loss: 0.5223 - val_loss: 0.9746
Epoch 3156/5000
26/26 - 1s - loss: 0.5228 - val_loss: 0.9756
Epoch 3157/5000
26/26 - 1s - loss: 0.5228 - val_loss: 0.9744
Epoch 3158/5000
26/26 - 1s - loss: 0.5230 - val_loss: 0.9742
Epoch 3159/5000
26/26 - 1s - loss: 0.5232 - val_loss: 0.9736
Epoch 3160/5000
26/26 - 1s - loss: 0.5222 - val_loss: 0.9748
Epoch 03160: val_loss did not improve from 0.97403
Epoch 3161/5000
26/26 - 1s - loss: 0.5224 - val_loss: 0.9737
Epoch 3162/5000
26/26 - 1s - loss: 0.5221 - val_loss: 0.9739
Epoch 3163/5000
26/26 - 1s - loss: 0.5245 - val_loss: 0.9739
Epoch 3164/5000
26/26 - 2s - loss: 0.5222 - val_loss: 0.9738
Epoch 3165/5000
26/26 - 1s - loss: 0.5230 - val_loss: 0.9730
Epoch 3166/5000
26/26 - 1s - loss: 0.5217 - val_loss: 0.9732
Epoch 3167/5000
26/26 - 1s - loss: 0.5221 - val_loss: 0.9734
Epoch 3168/5000
26/26 - 1s - loss: 0.5206 - val_loss: 0.9736
Epoch 3169/5000
26/26 - 1s - loss: 0.5214 - val_loss: 0.9739
Epoch 3170/5000
26/26 - 1s - loss: 0.5214 - val_loss: 0.9715
Epoch 03170: val_loss improved from 0.97403 to 0.97148, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 3171/5000
26/26 - 1s - loss: 0.5213 - val_loss: 0.9744
Epoch 3172/5000
26/26 - 1s - loss: 0.5222 - val_loss: 0.9728
Epoch 3173/5000
26/26 - 1s - loss: 0.5216 - val_loss: 0.9720
Epoch 3174/5000
26/26 - 1s - loss: 0.5210 - val_loss: 0.9731
Epoch 3175/5000
26/26 - 1s - loss: 0.5205 - val_loss: 0.9719
Epoch 3176/5000
26/26 - 2s - loss: 0.5205 - val_loss: 0.9720
Epoch 3177/5000
26/26 - 2s - loss: 0.5216 - val_loss: 0.9713
Epoch 3178/5000
26/26 - 1s - loss: 0.5195 - val_loss: 0.9736
Epoch 3179/5000
26/26 - 1s - loss: 0.5208 - val_loss: 0.9715
Epoch 3180/5000
26/26 - 1s - loss: 0.5212 - val_loss: 0.9708
Epoch 03180: val_loss improved from 0.97148 to 0.97076, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 3181/5000
26/26 - 1s - loss: 0.5201 - val_loss: 0.9715
Epoch 3182/5000
26/26 - 1s - loss: 0.5214 - val_loss: 0.9722
Epoch 3183/5000
26/26 - 1s - loss: 0.5196 - val_loss: 0.9714
Epoch 3184/5000
26/26 - 1s - loss: 0.5203 - val_loss: 0.9693
Epoch 3185/5000
26/26 - 1s - loss: 0.5183 - val_loss: 0.9713
Epoch 3186/5000
26/26 - 1s - loss: 0.5198 - val_loss: 0.9709
Epoch 3187/5000
26/26 - 2s - loss: 0.5196 - val_loss: 0.9705
Epoch 3188/5000
26/26 - 1s - loss: 0.5184 - val_loss: 0.9695
Epoch 3189/5000
26/26 - 1s - loss: 0.5194 - val_loss: 0.9709
Epoch 3190/5000
26/26 - 1s - loss: 0.5199 - val_loss: 0.9712
Epoch 03190: val_loss did not improve from 0.97076
Epoch 3191/5000
26/26 - 1s - loss: 0.5198 - val_loss: 0.9711
Epoch 3192/5000
26/26 - 1s - loss: 0.5187 - val_loss: 0.9708
Epoch 3193/5000
26/26 - 1s - loss: 0.5197 - val_loss: 0.9721
Epoch 3194/5000
26/26 - 1s - loss: 0.5193 - val_loss: 0.9695
Epoch 3195/5000
26/26 - 1s - loss: 0.5194 - val_loss: 0.9699
Epoch 3196/5000
26/26 - 1s - loss: 0.5177 - val_loss: 0.9691
Epoch 3197/5000
26/26 - 1s - loss: 0.5194 - val_loss: 0.9692
Epoch 3198/5000
26/26 - 1s - loss: 0.5188 - val_loss: 0.9698
Epoch 3199/5000
26/26 - 1s - loss: 0.5181 - val_loss: 0.9694
Epoch 3200/5000
26/26 - 1s - loss: 0.5183 - val_loss: 0.9689
Epoch 03200: val_loss improved from 0.97076 to 0.96893, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 3201/5000
26/26 - 1s - loss: 0.5168 - val_loss: 0.9689
Epoch 3202/5000
26/26 - 1s - loss: 0.5172 - val_loss: 0.9693
Epoch 3203/5000
26/26 - 1s - loss: 0.5173 - val_loss: 0.9676
Epoch 3204/5000
26/26 - 1s - loss: 0.5174 - val_loss: 0.9680
Epoch 3205/5000
26/26 - 1s - loss: 0.5172 - val_loss: 0.9687
Epoch 3206/5000
26/26 - 1s - loss: 0.5190 - val_loss: 0.9693
Epoch 3207/5000
26/26 - 1s - loss: 0.5173 - val_loss: 0.9681
Epoch 3208/5000
26/26 - 1s - loss: 0.5159 - val_loss: 0.9689
Epoch 3209/5000
26/26 - 1s - loss: 0.5173 - val_loss: 0.9677
Epoch 3210/5000
26/26 - 2s - loss: 0.5169 - val_loss: 0.9685
Epoch 03210: val_loss improved from 0.96893 to 0.96851, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 3211/5000
26/26 - 1s - loss: 0.5164 - val_loss: 0.9673
Epoch 3212/5000
26/26 - 1s - loss: 0.5152 - val_loss: 0.9679
Epoch 3213/5000
26/26 - 1s - loss: 0.5170 - val_loss: 0.9691
Epoch 3214/5000
26/26 - 1s - loss: 0.5168 - val_loss: 0.9688
Epoch 3215/5000
26/26 - 1s - loss: 0.5165 - val_loss: 0.9677
Epoch 3216/5000
26/26 - 1s - loss: 0.5162 - val_loss: 0.9666
Epoch 3217/5000
26/26 - 2s - loss: 0.5145 - val_loss: 0.9678
Epoch 3218/5000
26/26 - 2s - loss: 0.5154 - val_loss: 0.9681
Epoch 3219/5000
26/26 - 1s - loss: 0.5154 - val_loss: 0.9653
Epoch 3220/5000
26/26 - 1s - loss: 0.5152 - val_loss: 0.9668
Epoch 03220: val_loss improved from 0.96851 to 0.96676, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 3221/5000
26/26 - 1s - loss: 0.5155 - val_loss: 0.9676
Epoch 3222/5000
26/26 - 1s - loss: 0.5161 - val_loss: 0.9687
Epoch 3223/5000
26/26 - 1s - loss: 0.5151 - val_loss: 0.9658
Epoch 3224/5000
26/26 - 1s - loss: 0.5146 - val_loss: 0.9666
Epoch 3225/5000
26/26 - 2s - loss: 0.5151 - val_loss: 0.9660
Epoch 3226/5000
26/26 - 2s - loss: 0.5152 - val_loss: 0.9660
Epoch 3227/5000
26/26 - 2s - loss: 0.5157 - val_loss: 0.9668
Epoch 3228/5000
26/26 - 2s - loss: 0.5157 - val_loss: 0.9668
Epoch 3229/5000
26/26 - 1s - loss: 0.5154 - val_loss: 0.9681
Epoch 3230/5000
26/26 - 1s - loss: 0.5143 - val_loss: 0.9659
Epoch 03230: val_loss improved from 0.96676 to 0.96591, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 3231/5000
26/26 - 1s - loss: 0.5151 - val_loss: 0.9654
Epoch 3232/5000
26/26 - 2s - loss: 0.5152 - val_loss: 0.9662
Epoch 3233/5000
26/26 - 1s - loss: 0.5138 - val_loss: 0.9645
Epoch 3234/5000
26/26 - 1s - loss: 0.5156 - val_loss: 0.9648
Epoch 3235/5000
26/26 - 1s - loss: 0.5137 - val_loss: 0.9658
Epoch 3236/5000
26/26 - 2s - loss: 0.5138 - val_loss: 0.9651
Epoch 3237/5000
26/26 - 1s - loss: 0.5142 - val_loss: 0.9639
Epoch 3238/5000
26/26 - 1s - loss: 0.5144 - val_loss: 0.9632
Epoch 3239/5000
26/26 - 1s - loss: 0.5141 - val_loss: 0.9631
Epoch 3240/5000
26/26 - 1s - loss: 0.5128 - val_loss: 0.9642
Epoch 03240: val_loss improved from 0.96591 to 0.96419, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 3241/5000
26/26 - 1s - loss: 0.5132 - val_loss: 0.9631
Epoch 3242/5000
26/26 - 1s - loss: 0.5143 - val_loss: 0.9635
Epoch 3243/5000
26/26 - 1s - loss: 0.5130 - val_loss: 0.9641
Epoch 3244/5000
26/26 - 1s - loss: 0.5120 - val_loss: 0.9627
Epoch 3245/5000
26/26 - 1s - loss: 0.5132 - val_loss: 0.9647
Epoch 3246/5000
26/26 - 1s - loss: 0.5140 - val_loss: 0.9656
Epoch 3247/5000
26/26 - 1s - loss: 0.5123 - val_loss: 0.9638
Epoch 3248/5000
26/26 - 1s - loss: 0.5122 - val_loss: 0.9637
Epoch 3249/5000
26/26 - 1s - loss: 0.5121 - val_loss: 0.9634
Epoch 3250/5000
26/26 - 1s - loss: 0.5118 - val_loss: 0.9643
Epoch 03250: val_loss did not improve from 0.96419
Epoch 3251/5000
26/26 - 1s - loss: 0.5117 - val_loss: 0.9647
Epoch 3252/5000
26/26 - 1s - loss: 0.5136 - val_loss: 0.9636
Epoch 3253/5000
26/26 - 1s - loss: 0.5118 - val_loss: 0.9649
Epoch 3254/5000
26/26 - 1s - loss: 0.5124 - val_loss: 0.9645
Epoch 3255/5000
26/26 - 1s - loss: 0.5100 - val_loss: 0.9638
Epoch 3256/5000
26/26 - 1s - loss: 0.5118 - val_loss: 0.9634
Epoch 3257/5000
26/26 - 1s - loss: 0.5114 - val_loss: 0.9629
Epoch 3258/5000
26/26 - 2s - loss: 0.5104 - val_loss: 0.9629
Epoch 3259/5000
26/26 - 2s - loss: 0.5119 - val_loss: 0.9636
Epoch 3260/5000
26/26 - 1s - loss: 0.5107 - val_loss: 0.9628
Epoch 03260: val_loss improved from 0.96419 to 0.96281, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 3261/5000
26/26 - 1s - loss: 0.5111 - val_loss: 0.9625
Epoch 3262/5000
26/26 - 1s - loss: 0.5110 - val_loss: 0.9618
Epoch 3263/5000
26/26 - 1s - loss: 0.5101 - val_loss: 0.9618
Epoch 3264/5000
26/26 - 1s - loss: 0.5096 - val_loss: 0.9622
Epoch 3265/5000
26/26 - 1s - loss: 0.5103 - val_loss: 0.9628
Epoch 3266/5000
26/26 - 1s - loss: 0.5099 - val_loss: 0.9624
Epoch 3267/5000
26/26 - 1s - loss: 0.5112 - val_loss: 0.9623
Epoch 3268/5000
26/26 - 1s - loss: 0.5105 - val_loss: 0.9616
Epoch 3269/5000
26/26 - 1s - loss: 0.5100 - val_loss: 0.9604
Epoch 3270/5000
26/26 - 1s - loss: 0.5104 - val_loss: 0.9600
Epoch 03270: val_loss improved from 0.96281 to 0.95998, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 3271/5000
26/26 - 1s - loss: 0.5089 - val_loss: 0.9615
Epoch 3272/5000
26/26 - 1s - loss: 0.5098 - val_loss: 0.9621
Epoch 3273/5000
26/26 - 1s - loss: 0.5095 - val_loss: 0.9608
Epoch 3274/5000
26/26 - 1s - loss: 0.5094 - val_loss: 0.9612
Epoch 3275/5000
26/26 - 1s - loss: 0.5087 - val_loss: 0.9614
Epoch 3276/5000
26/26 - 1s - loss: 0.5098 - val_loss: 0.9614
Epoch 3277/5000
26/26 - 2s - loss: 0.5088 - val_loss: 0.9611
Epoch 3278/5000
26/26 - 1s - loss: 0.5096 - val_loss: 0.9593
Epoch 3279/5000
26/26 - 1s - loss: 0.5096 - val_loss: 0.9603
Epoch 3280/5000
26/26 - 2s - loss: 0.5081 - val_loss: 0.9595
Epoch 03280: val_loss improved from 0.95998 to 0.95951, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 3281/5000
26/26 - 1s - loss: 0.5089 - val_loss: 0.9589
Epoch 3282/5000
26/26 - 1s - loss: 0.5087 - val_loss: 0.9601
Epoch 3283/5000
26/26 - 1s - loss: 0.5078 - val_loss: 0.9591
Epoch 3284/5000
26/26 - 1s - loss: 0.5088 - val_loss: 0.9594
Epoch 3285/5000
26/26 - 1s - loss: 0.5079 - val_loss: 0.9593
Epoch 3286/5000
26/26 - 1s - loss: 0.5086 - val_loss: 0.9604
Epoch 3287/5000
26/26 - 1s - loss: 0.5075 - val_loss: 0.9601
Epoch 3288/5000
26/26 - 1s - loss: 0.5077 - val_loss: 0.9590
Epoch 3289/5000
26/26 - 2s - loss: 0.5073 - val_loss: 0.9581
Epoch 3290/5000
26/26 - 1s - loss: 0.5073 - val_loss: 0.9591
Epoch 03290: val_loss improved from 0.95951 to 0.95913, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 3291/5000
26/26 - 1s - loss: 0.5084 - val_loss: 0.9586
Epoch 3292/5000
26/26 - 1s - loss: 0.5073 - val_loss: 0.9577
Epoch 3293/5000
26/26 - 1s - loss: 0.5065 - val_loss: 0.9591
Epoch 3294/5000
26/26 - 1s - loss: 0.5065 - val_loss: 0.9568
Epoch 3295/5000
26/26 - 1s - loss: 0.5073 - val_loss: 0.9601
Epoch 3296/5000
26/26 - 1s - loss: 0.5072 - val_loss: 0.9587
Epoch 3297/5000
26/26 - 1s - loss: 0.5066 - val_loss: 0.9590
Epoch 3298/5000
26/26 - 1s - loss: 0.5072 - val_loss: 0.9578
Epoch 3299/5000
26/26 - 2s - loss: 0.5069 - val_loss: 0.9579
Epoch 3300/5000
26/26 - 1s - loss: 0.5069 - val_loss: 0.9572
Epoch 03300: val_loss improved from 0.95913 to 0.95722, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 3301/5000
26/26 - 1s - loss: 0.5063 - val_loss: 0.9575
Epoch 3302/5000
26/26 - 1s - loss: 0.5050 - val_loss: 0.9574
Epoch 3303/5000
26/26 - 1s - loss: 0.5057 - val_loss: 0.9570
Epoch 3304/5000
26/26 - 1s - loss: 0.5056 - val_loss: 0.9567
Epoch 3305/5000
26/26 - 1s - loss: 0.5052 - val_loss: 0.9582
Epoch 3306/5000
26/26 - 1s - loss: 0.5068 - val_loss: 0.9569
Epoch 3307/5000
26/26 - 1s - loss: 0.5062 - val_loss: 0.9589
Epoch 3308/5000
26/26 - 1s - loss: 0.5066 - val_loss: 0.9583
Epoch 3309/5000
26/26 - 1s - loss: 0.5057 - val_loss: 0.9578
Epoch 3310/5000
26/26 - 1s - loss: 0.5050 - val_loss: 0.9565
Epoch 03310: val_loss improved from 0.95722 to 0.95648, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 3311/5000
26/26 - 1s - loss: 0.5070 - val_loss: 0.9562
Epoch 3312/5000
26/26 - 2s - loss: 0.5059 - val_loss: 0.9556
Epoch 3313/5000
26/26 - 1s - loss: 0.5052 - val_loss: 0.9570
Epoch 3314/5000
26/26 - 2s - loss: 0.5056 - val_loss: 0.9565
Epoch 3315/5000
26/26 - 1s - loss: 0.5040 - val_loss: 0.9582
Epoch 3316/5000
26/26 - 2s - loss: 0.5058 - val_loss: 0.9571
Epoch 3317/5000
26/26 - 1s - loss: 0.5054 - val_loss: 0.9567
Epoch 3318/5000
26/26 - 1s - loss: 0.5051 - val_loss: 0.9573
Epoch 3319/5000
26/26 - 1s - loss: 0.5042 - val_loss: 0.9562
Epoch 3320/5000
26/26 - 1s - loss: 0.5032 - val_loss: 0.9558
Epoch 03320: val_loss improved from 0.95648 to 0.95580, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 3321/5000
26/26 - 1s - loss: 0.5039 - val_loss: 0.9558
Epoch 3322/5000
26/26 - 1s - loss: 0.5038 - val_loss: 0.9556
Epoch 3323/5000
26/26 - 1s - loss: 0.5047 - val_loss: 0.9549
Epoch 3324/5000
26/26 - 1s - loss: 0.5053 - val_loss: 0.9565
Epoch 3325/5000
26/26 - 2s - loss: 0.5036 - val_loss: 0.9565
Epoch 3326/5000
26/26 - 1s - loss: 0.5045 - val_loss: 0.9563
Epoch 3327/5000
26/26 - 1s - loss: 0.5042 - val_loss: 0.9564
Epoch 3328/5000
26/26 - 1s - loss: 0.5021 - val_loss: 0.9557
Epoch 3329/5000
26/26 - 1s - loss: 0.5039 - val_loss: 0.9546
Epoch 3330/5000
26/26 - 1s - loss: 0.5038 - val_loss: 0.9550
Epoch 03330: val_loss improved from 0.95580 to 0.95503, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 3331/5000
26/26 - 1s - loss: 0.5031 - val_loss: 0.9553
Epoch 3332/5000
26/26 - 1s - loss: 0.5042 - val_loss: 0.9530
Epoch 3333/5000
26/26 - 1s - loss: 0.5026 - val_loss: 0.9559
Epoch 3334/5000
26/26 - 1s - loss: 0.5040 - val_loss: 0.9528
Epoch 3335/5000
26/26 - 1s - loss: 0.5024 - val_loss: 0.9539
Epoch 3336/5000
26/26 - 1s - loss: 0.5020 - val_loss: 0.9554
Epoch 3337/5000
26/26 - 1s - loss: 0.5027 - val_loss: 0.9546
Epoch 3338/5000
26/26 - 2s - loss: 0.5027 - val_loss: 0.9542
Epoch 3339/5000
26/26 - 1s - loss: 0.5021 - val_loss: 0.9553
Epoch 3340/5000
26/26 - 2s - loss: 0.5034 - val_loss: 0.9530
Epoch 03340: val_loss improved from 0.95503 to 0.95302, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 3341/5000
26/26 - 1s - loss: 0.5022 - val_loss: 0.9541
Epoch 3342/5000
26/26 - 1s - loss: 0.5021 - val_loss: 0.9530
Epoch 3343/5000
26/26 - 1s - loss: 0.5017 - val_loss: 0.9537
Epoch 3344/5000
26/26 - 1s - loss: 0.5021 - val_loss: 0.9529
Epoch 3345/5000
26/26 - 1s - loss: 0.5020 - val_loss: 0.9542
Epoch 3346/5000
26/26 - 1s - loss: 0.5007 - val_loss: 0.9531
Epoch 3347/5000
26/26 - 1s - loss: 0.5016 - val_loss: 0.9533
Epoch 3348/5000
26/26 - 1s - loss: 0.5009 - val_loss: 0.9535
Epoch 3349/5000
26/26 - 1s - loss: 0.5022 - val_loss: 0.9525
Epoch 3350/5000
26/26 - 1s - loss: 0.5015 - val_loss: 0.9528
Epoch 03350: val_loss improved from 0.95302 to 0.95282, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 3351/5000
26/26 - 1s - loss: 0.5022 - val_loss: 0.9500
Epoch 3352/5000
26/26 - 2s - loss: 0.5012 - val_loss: 0.9508
Epoch 3353/5000
26/26 - 1s - loss: 0.5010 - val_loss: 0.9530
Epoch 3354/5000
26/26 - 2s - loss: 0.4996 - val_loss: 0.9517
Epoch 3355/5000
26/26 - 1s - loss: 0.5006 - val_loss: 0.9518
Epoch 3356/5000
26/26 - 1s - loss: 0.5010 - val_loss: 0.9537
Epoch 3357/5000
26/26 - 1s - loss: 0.5006 - val_loss: 0.9515
Epoch 3358/5000
26/26 - 1s - loss: 0.5002 - val_loss: 0.9530
Epoch 3359/5000
26/26 - 1s - loss: 0.5005 - val_loss: 0.9515
Epoch 3360/5000
26/26 - 1s - loss: 0.5014 - val_loss: 0.9520
Epoch 03360: val_loss improved from 0.95282 to 0.95199, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 3361/5000
26/26 - 1s - loss: 0.4995 - val_loss: 0.9530
Epoch 3362/5000
26/26 - 1s - loss: 0.5001 - val_loss: 0.9539
Epoch 3363/5000
26/26 - 1s - loss: 0.5011 - val_loss: 0.9524
Epoch 3364/5000
26/26 - 1s - loss: 0.5002 - val_loss: 0.9507
Epoch 3365/5000
26/26 - 1s - loss: 0.4994 - val_loss: 0.9529
Epoch 3366/5000
26/26 - 1s - loss: 0.5002 - val_loss: 0.9509
Epoch 3367/5000
26/26 - 1s - loss: 0.4998 - val_loss: 0.9517
Epoch 3368/5000
26/26 - 1s - loss: 0.4987 - val_loss: 0.9523
Epoch 3369/5000
26/26 - 1s - loss: 0.4989 - val_loss: 0.9490
Epoch 3370/5000
26/26 - 1s - loss: 0.4994 - val_loss: 0.9504
Epoch 03370: val_loss improved from 0.95199 to 0.95042, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 3371/5000
26/26 - 1s - loss: 0.5010 - val_loss: 0.9484
Epoch 3372/5000
26/26 - 1s - loss: 0.4979 - val_loss: 0.9507
Epoch 3373/5000
26/26 - 1s - loss: 0.4996 - val_loss: 0.9486
Epoch 3374/5000
26/26 - 1s - loss: 0.4986 - val_loss: 0.9498
Epoch 3375/5000
26/26 - 1s - loss: 0.4984 - val_loss: 0.9507
Epoch 3376/5000
26/26 - 1s - loss: 0.4982 - val_loss: 0.9500
Epoch 3377/5000
26/26 - 1s - loss: 0.4977 - val_loss: 0.9494
Epoch 3378/5000
26/26 - 1s - loss: 0.4987 - val_loss: 0.9500
Epoch 3379/5000
26/26 - 1s - loss: 0.4986 - val_loss: 0.9491
Epoch 3380/5000
26/26 - 1s - loss: 0.4984 - val_loss: 0.9484
Epoch 03380: val_loss improved from 0.95042 to 0.94837, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 3381/5000
26/26 - 2s - loss: 0.4978 - val_loss: 0.9506
Epoch 3382/5000
26/26 - 1s - loss: 0.4988 - val_loss: 0.9486
Epoch 3383/5000
26/26 - 1s - loss: 0.4979 - val_loss: 0.9494
Epoch 3384/5000
26/26 - 1s - loss: 0.4984 - val_loss: 0.9488
Epoch 3385/5000
26/26 - 1s - loss: 0.4969 - val_loss: 0.9484
Epoch 3386/5000
26/26 - 1s - loss: 0.4966 - val_loss: 0.9495
Epoch 3387/5000
26/26 - 1s - loss: 0.4984 - val_loss: 0.9498
Epoch 3388/5000
26/26 - 2s - loss: 0.4978 - val_loss: 0.9495
Epoch 3389/5000
26/26 - 1s - loss: 0.4978 - val_loss: 0.9472
Epoch 3390/5000
26/26 - 1s - loss: 0.4979 - val_loss: 0.9487
Epoch 03390: val_loss did not improve from 0.94837
Epoch 3391/5000
26/26 - 1s - loss: 0.4963 - val_loss: 0.9485
Epoch 3392/5000
26/26 - 2s - loss: 0.4968 - val_loss: 0.9491
Epoch 3393/5000
26/26 - 1s - loss: 0.4971 - val_loss: 0.9483
Epoch 3394/5000
26/26 - 1s - loss: 0.4970 - val_loss: 0.9470
Epoch 3395/5000
26/26 - 1s - loss: 0.4974 - val_loss: 0.9492
Epoch 3396/5000
26/26 - 1s - loss: 0.4967 - val_loss: 0.9483
Epoch 3397/5000
26/26 - 1s - loss: 0.4964 - val_loss: 0.9480
Epoch 3398/5000
26/26 - 1s - loss: 0.4971 - val_loss: 0.9484
Epoch 3399/5000
26/26 - 1s - loss: 0.4966 - val_loss: 0.9490
Epoch 3400/5000
26/26 - 2s - loss: 0.4964 - val_loss: 0.9479
Epoch 03400: val_loss improved from 0.94837 to 0.94793, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 3401/5000
26/26 - 1s - loss: 0.4962 - val_loss: 0.9480
Epoch 3402/5000
26/26 - 1s - loss: 0.4967 - val_loss: 0.9463
Epoch 3403/5000
26/26 - 1s - loss: 0.4954 - val_loss: 0.9468
Epoch 3404/5000
26/26 - 1s - loss: 0.4949 - val_loss: 0.9474
Epoch 3405/5000
26/26 - 1s - loss: 0.4957 - val_loss: 0.9474
Epoch 3406/5000
26/26 - 1s - loss: 0.4948 - val_loss: 0.9463
Epoch 3407/5000
26/26 - 1s - loss: 0.4968 - val_loss: 0.9494
Epoch 3408/5000
26/26 - 1s - loss: 0.4951 - val_loss: 0.9465
Epoch 3409/5000
26/26 - 1s - loss: 0.4942 - val_loss: 0.9468
Epoch 3410/5000
26/26 - 2s - loss: 0.4967 - val_loss: 0.9475
Epoch 03410: val_loss improved from 0.94793 to 0.94748, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 3411/5000
26/26 - 1s - loss: 0.4947 - val_loss: 0.9501
Epoch 3412/5000
26/26 - 1s - loss: 0.4957 - val_loss: 0.9470
Epoch 3413/5000
26/26 - 1s - loss: 0.4933 - val_loss: 0.9474
Epoch 3414/5000
26/26 - 1s - loss: 0.4952 - val_loss: 0.9472
Epoch 3415/5000
26/26 - 1s - loss: 0.4937 - val_loss: 0.9482
Epoch 3416/5000
26/26 - 1s - loss: 0.4957 - val_loss: 0.9468
Epoch 3417/5000
26/26 - 1s - loss: 0.4951 - val_loss: 0.9469
Epoch 3418/5000
26/26 - 1s - loss: 0.4940 - val_loss: 0.9458
Epoch 3419/5000
26/26 - 1s - loss: 0.4943 - val_loss: 0.9472
Epoch 3420/5000
26/26 - 1s - loss: 0.4933 - val_loss: 0.9464
Epoch 03420: val_loss improved from 0.94748 to 0.94636, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 3421/5000
26/26 - 1s - loss: 0.4937 - val_loss: 0.9473
Epoch 3422/5000
26/26 - 1s - loss: 0.4937 - val_loss: 0.9468
Epoch 3423/5000
26/26 - 1s - loss: 0.4935 - val_loss: 0.9460
Epoch 3424/5000
26/26 - 1s - loss: 0.4934 - val_loss: 0.9466
Epoch 3425/5000
26/26 - 1s - loss: 0.4939 - val_loss: 0.9470
Epoch 3426/5000
26/26 - 1s - loss: 0.4942 - val_loss: 0.9473
Epoch 3427/5000
26/26 - 2s - loss: 0.4925 - val_loss: 0.9449
Epoch 3428/5000
26/26 - 1s - loss: 0.4942 - val_loss: 0.9454
Epoch 3429/5000
26/26 - 1s - loss: 0.4937 - val_loss: 0.9459
Epoch 3430/5000
26/26 - 1s - loss: 0.4921 - val_loss: 0.9454
Epoch 03430: val_loss improved from 0.94636 to 0.94535, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 3431/5000
26/26 - 1s - loss: 0.4934 - val_loss: 0.9469
Epoch 3432/5000
26/26 - 2s - loss: 0.4931 - val_loss: 0.9449
Epoch 3433/5000
26/26 - 1s - loss: 0.4926 - val_loss: 0.9450
Epoch 3434/5000
26/26 - 1s - loss: 0.4932 - val_loss: 0.9442
Epoch 3435/5000
26/26 - 1s - loss: 0.4928 - val_loss: 0.9424
Epoch 3436/5000
26/26 - 1s - loss: 0.4925 - val_loss: 0.9454
Epoch 3437/5000
26/26 - 1s - loss: 0.4922 - val_loss: 0.9440
Epoch 3438/5000
26/26 - 1s - loss: 0.4934 - val_loss: 0.9425
Epoch 3439/5000
26/26 - 1s - loss: 0.4912 - val_loss: 0.9455
Epoch 3440/5000
26/26 - 1s - loss: 0.4932 - val_loss: 0.9437
Epoch 03440: val_loss improved from 0.94535 to 0.94368, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 3441/5000
26/26 - 1s - loss: 0.4921 - val_loss: 0.9443
Epoch 3442/5000
26/26 - 1s - loss: 0.4929 - val_loss: 0.9435
Epoch 3443/5000
26/26 - 1s - loss: 0.4923 - val_loss: 0.9443
Epoch 3444/5000
26/26 - 1s - loss: 0.4916 - val_loss: 0.9435
Epoch 3445/5000
26/26 - 1s - loss: 0.4915 - val_loss: 0.9422
Epoch 3446/5000
26/26 - 1s - loss: 0.4911 - val_loss: 0.9440
Epoch 3447/5000
26/26 - 2s - loss: 0.4919 - val_loss: 0.9430
Epoch 3448/5000
26/26 - 1s - loss: 0.4926 - val_loss: 0.9439
Epoch 3449/5000
26/26 - 1s - loss: 0.4913 - val_loss: 0.9451
Epoch 3450/5000
26/26 - 1s - loss: 0.4922 - val_loss: 0.9440
Epoch 03450: val_loss did not improve from 0.94368
Epoch 3451/5000
26/26 - 1s - loss: 0.4910 - val_loss: 0.9441
Epoch 3452/5000
26/26 - 1s - loss: 0.4903 - val_loss: 0.9438
Epoch 3453/5000
26/26 - 1s - loss: 0.4907 - val_loss: 0.9421
Epoch 3454/5000
26/26 - 1s - loss: 0.4917 - val_loss: 0.9433
Epoch 3455/5000
26/26 - 1s - loss: 0.4896 - val_loss: 0.9438
Epoch 3456/5000
26/26 - 1s - loss: 0.4908 - val_loss: 0.9431
Epoch 3457/5000
26/26 - 2s - loss: 0.4908 - val_loss: 0.9445
Epoch 3458/5000
26/26 - 1s - loss: 0.4905 - val_loss: 0.9421
Epoch 3459/5000
26/26 - 1s - loss: 0.4902 - val_loss: 0.9427
Epoch 3460/5000
26/26 - 1s - loss: 0.4901 - val_loss: 0.9419
Epoch 03460: val_loss improved from 0.94368 to 0.94186, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-3.model.weights.hdf5
Epoch 3461/5000
26/26 - 1s - loss: 0.4900 - val_loss: 0.9425
Epoch 3462/5000
26/26 - 2s - loss: 0.4906 - val_loss: 0.9423
Epoch 3463/5000
26/26 - 1s - loss: 0.4896 - val_loss: 0.9431
Epoch 3464/5000
26/26 - 2s - loss: 0.4902 - val_loss: 0.9418
Epoch 3465/5000
26/26 - 1s - loss: 0.4908 - val_loss: 0.9402
Epoch 3466/5000
26/26 - 1s - loss: 0.4893 - val_loss: 0.9407
Epoch 3467/5000
26/26 - 1s - loss: 0.4898 - val_loss: 0.9441
Epoch 3468/5000
26/26 - 1s - loss: 0.4911 - val_loss: 0.9415
Epoch 3469/5000
26/26 - 1s - loss: 0.4898 - val_loss: 0.9429
Epoch 3470/5000
26/26 - 1s - loss: 0.4891 - val_loss: 0.9428
Epoch 03470: val_loss did not improve from 0.94186
Epoch 3471/5000
26/26 - 1s - loss: 0.4887 - val_loss: 0.9437
Epoch 3472/5000
26/26 - 2s - loss: 0.4898 - val_loss: 0.9409
Epoch 3473/5000
26/26 - 1s - loss: 0.4890 - val_loss: 0.9432
Epoch 3474/5000
26/26 - 1s - loss: 0.4869 - val_loss: 0.9402
Epoch 3475/5000
26/26 - 1s - loss: 0.4888 - val_loss: 0.9408
Epoch 3476/5000
26/26 - 1s - loss: 0.4890 - val_loss: 0.9422
Epoch 3477/5000
26/26 - 1s - loss: 0.4886 - val_loss: 0.9401
Epoch 3478/5000
26/26 - 1s - loss: 0.4896 - val_loss: 0.9425
Epoch 3479/5000
26/26 - 2s - loss: 0.4886 - val_loss: 0.9416
Epoch 3480/5000
26/26 - 1s - loss: 0.4883 - val_loss: 0.9420
Epoch 03480: val_loss did not improve from 0.94186
Epoch 3481/5000
26/26 - 1s - loss: 0.4889 - val_loss: 0.9419
Epoch 3482/5000
26/26 - 1s - loss: 0.4873 - val_loss: 0.9406
Epoch 3483/5000
26/26 - 2s - loss: 0.4875 - val_loss: 0.9414
Epoch 3484/5000
26/26 - 1s - loss: 0.4875 - val_loss: 0.9416
Epoch 3485/5000
INFO     Computation time for training the single-label model for AR: 84.41 min
INFO     Network type: REG
INFO     Model: "sequential"
INFO     _________________________________________________________________
INFO     Layer (type)                 Output Shape              Param #
INFO     =================================================================
INFO     dense (Dense)                (None, 1024)              2098176
INFO     _________________________________________________________________
INFO     dropout (Dropout)            (None, 1024)              0
INFO     _________________________________________________________________
INFO     dense_1 (Dense)              (None, 512)               524800
INFO     _________________________________________________________________
INFO     dropout_1 (Dropout)          (None, 512)               0
INFO     _________________________________________________________________
INFO     dense_2 (Dense)              (None, 256)               131328
INFO     _________________________________________________________________
INFO     dropout_2 (Dropout)          (None, 256)               0
INFO     _________________________________________________________________
INFO     dense_3 (Dense)              (None, 128)               32896
INFO     _________________________________________________________________
INFO     dropout_3 (Dropout)          (None, 128)               0
INFO     _________________________________________________________________
INFO     dense_4 (Dense)              (None, 1)                 129
INFO     =================================================================
INFO     Total params: 2,787,329
INFO     Trainable params: 2,787,329
INFO     Non-trainable params: 0
INFO     _________________________________________________________________
26/26 - 1s - loss: 0.4881 - val_loss: 0.9405
Restoring model weights from the end of the best epoch.
Epoch 03485: early stopping
INFO     Evaluating trained model 'AR single-labeled Fold-3' on test data
INFO     Training of fold number: 4
INFO     Training sample distribution: train data: {-1.2016366720199585: 9, -1.2016363143920898: 4, -1.2016353607177734: 4, -1.201636552810669: 4, -1.2016369104385376: 3, -1.201636791229248: 3, -1.2016383409500122: 3, -1.2016342878341675: 3, -1.201637625694275: 3, -1.20163094997406: 3, -1.2016347646713257: 2, -1.2016384601593018: 2, -1.201633095741272: 2, -1.2016377449035645: 2, -1.2015819549560547: 2, -1.2016355991363525: 2, -1.2016191482543945: 2, -1.2016315460205078: 2, -1.201629877090454: 2, -1.2009780406951904: 2, -1.2016041278839111: 2, -1.2016310691833496: 2, -1.201596975326538: 2, -1.2016339302062988: 2, -1.2016327381134033: 2, -1.2016288042068481: 2, -1.2016290426254272: 2, -1.2016326189041138: 2, -1.2016351222991943: 2, -1.2016254663467407: 2, -1.2016304731369019: 2, -1.201635479927063: 2, -1.2016324996948242: 2, -1.2016303539276123: 2, -1.201627254486084: 2, 0.25592291355133057: 1, 0.2900018095970154: 1, 0.5601941347122192: 1, 1.6106586456298828: 1, 1.168498158454895: 1, -1.0873279571533203: 1, -0.2516374886035919: 1, 0.10525540262460709: 1, -0.4459679424762726: 1, 0.3664189279079437: 1, -0.3040960133075714: 1, 1.4836735725402832: 1, 0.6158161163330078: 1, 0.831580638885498: 1, 1.4237861633300781: 1, -0.6966411471366882: 1, 0.6442342400550842: 1, 0.7778733968734741: 1, -0.5348793864250183: 1, -0.37317758798599243: 1, 1.5720155239105225: 1, 0.8825770616531372: 1, -0.5706648826599121: 1, -0.4307803809642792: 1, 1.0551327466964722: 1, 1.6063342094421387: 1, 1.605971336364746: 1, 0.2661615014076233: 1, 1.3996703624725342: 1, 0.22961212694644928: 1, 0.8664619326591492: 1, 0.32629087567329407: 1, 1.415509581565857: 1, 1.6089627742767334: 1, 1.5149681568145752: 1, 0.19487899541854858: 1, -0.06838630884885788: 1, 0.31109386682510376: 1, 0.5247108340263367: 1, 0.8396581411361694: 1, 1.8254425525665283: 1, 0.6829988956451416: 1, 0.31096193194389343: 1, 0.5732383131980896: 1, 0.2610865831375122: 1, 0.0860099047422409: 1, -1.201627492904663: 1, 0.28807583451271057: 1, 0.4933412969112396: 1, 0.3488617241382599: 1, -0.5330178141593933: 1, -0.02133699133992195: 1, 0.25783771276474: 1, -0.4253336787223816: 1, -0.008235386572778225: 1, 0.2470504194498062: 1, -0.9223102331161499: 1, -0.297276109457016: 1, 1.5978947877883911: 1, -0.3203604817390442: 1, -1.201623558998108: 1, 1.1970174312591553: 1, 1.5667779445648193: 1, -0.09881063550710678: 1, -0.41311657428741455: 1, 0.7218993902206421: 1, -0.7750424146652222: 1, 0.2713952362537384: 1, -1.1963087320327759: 1, -0.2558962106704712: 1, 0.002237366745248437: 1, -1.2016195058822632: 1, -0.16027171909809113: 1, 0.02149348333477974: 1, -0.425843745470047: 1, 1.4308977127075195: 1, 1.4458893537521362: 1, 0.22109845280647278: 1, 1.0667099952697754: 1, 0.06667295098304749: 1, 1.5455868244171143: 1, 0.3489625155925751: 1, -0.3774075210094452: 1, -0.16630633175373077: 1, 1.5805106163024902: 1, -0.31709083914756775: 1, 0.1385408341884613: 1, 0.7171676754951477: 1, 1.5370275974273682: 1, 0.31334978342056274: 1, -0.040320102125406265: 1, 0.17100460827350616: 1, -0.993471086025238: 1, 0.8569538593292236: 1, -0.2393776774406433: 1, 0.6820655465126038: 1, 0.8028292059898376: 1, 0.6465133428573608: 1, 0.7402693033218384: 1, 0.540539026260376: 1, 0.10738043487071991: 1, 0.9149655103683472: 1, 1.4755654335021973: 1, -1.160044550895691: 1, 1.166581153869629: 1, 1.2997812032699585: 1, 1.2994223833084106: 1, 1.4923255443572998: 1, 0.5403873920440674: 1, -0.20045150816440582: 1, 1.4424492120742798: 1, 0.36003923416137695: 1, -1.1746125221252441: 1, 0.7614589929580688: 1, 0.3443826735019684: 1, 0.6386443972587585: 1, -0.24653010070323944: 1, 1.5124294757843018: 1, -0.22351212799549103: 1, 0.9285130500793457: 1, 0.5170465111732483: 1, -1.1701372861862183: 1, 0.8274267315864563: 1, -0.2711203992366791: 1, 1.457643747329712: 1, -0.48075738549232483: 1, -0.13643299043178558: 1, 0.16810350120067596: 1, 0.49536043405532837: 1, 0.23716896772384644: 1, -0.10035426914691925: 1, -1.189130187034607: 1, 0.10213274508714676: 1, 0.34950539469718933: 1, -1.1950759887695312: 1, -0.17338967323303223: 1, 0.34191834926605225: 1, -0.10739652067422867: 1, 1.4653488397598267: 1, 0.912930965423584: 1, 0.20390741527080536: 1, 1.4158756732940674: 1, 1.5277636051177979: 1, 1.3042255640029907: 1, 1.571059226989746: 1, 1.2753889560699463: 1, 1.4251669645309448: 1, 0.663663923740387: 1, -1.0201867818832397: 1, 0.15129715204238892: 1, -0.21738800406455994: 1, 0.2609155476093292: 1, -1.194837212562561: 1, -0.30124133825302124: 1, 0.3422311544418335: 1, 1.5035943984985352: 1, -0.3344474732875824: 1, 0.7412875294685364: 1, 0.2500914931297302: 1, -0.7282882928848267: 1, 0.9650787115097046: 1, 1.0995265245437622: 1, -0.9669303297996521: 1, 1.1683435440063477: 1, 0.695344090461731: 1, 0.21625953912734985: 1, -0.19042591750621796: 1, 1.9054079055786133: 1, 0.7279888391494751: 1, -0.31671836972236633: 1, 0.29451489448547363: 1, 1.1508636474609375: 1, 0.15692748129367828: 1, 1.090896725654602: 1, 0.7471779584884644: 1, 0.36155056953430176: 1, -0.4993174076080322: 1, -0.29657527804374695: 1, -0.6546655297279358: 1, 0.21768306195735931: 1, -0.07565759867429733: 1, -1.198028802871704: 1, -1.1962871551513672: 1, -1.201625108718872: 1, -0.7543148398399353: 1, -1.1961579322814941: 1, -0.7196366786956787: 1, -0.5718616843223572: 1, 1.7539664506912231: 1, -0.005905755329877138: 1, -1.1402051448822021: 1, 0.8374537825584412: 1, -0.5042855143547058: 1, -0.26343750953674316: 1, 1.4295574426651: 1, -0.00951747503131628: 1, -0.09802207350730896: 1, 0.9686956405639648: 1, -0.5639210939407349: 1, -0.39423879981040955: 1, -0.4268537759780884: 1, 0.16395387053489685: 1, -0.29486238956451416: 1, -1.1980621814727783: 1, -1.2009947299957275: 1, -1.1927686929702759: 1, -1.1962217092514038: 1, 0.5436715483665466: 1, -1.2007057666778564: 1, -1.1639471054077148: 1, -1.2007761001586914: 1, -1.2015410661697388: 1, -1.045175313949585: 1, -1.1976145505905151: 1, -1.197718858718872: 1, -0.7745821475982666: 1, -1.1987890005111694: 1, -1.1964974403381348: 1, -1.1852269172668457: 1, -1.1929398775100708: 1, -1.1945018768310547: 1, -0.34237241744995117: 1, -1.197901725769043: 1, -1.1866750717163086: 1, -1.1237342357635498: 1, -1.1664562225341797: 1, -1.1972938776016235: 1, 0.005114047322422266: 1, 0.5479292273521423: 1, -0.32787415385246277: 1, -1.1323087215423584: 1, -0.8897131681442261: 1, 1.8768579959869385: 1, 0.6321052312850952: 1, 1.7107861042022705: 1, 1.8398889303207397: 1, 1.8953936100006104: 1, -1.009895920753479: 1, -0.30134135484695435: 1, 1.3204209804534912: 1, -1.2016119956970215: 1, -0.4007585644721985: 1, -0.116549052298069: 1, 0.2115481197834015: 1, -0.47023993730545044: 1, 1.0165932178497314: 1, 1.477781891822815: 1, -0.03796708956360817: 1, -0.6538054943084717: 1, -0.408608615398407: 1, 1.5324621200561523: 1, -1.1897622346878052: 1, -0.46823152899742126: 1, -0.5717604756355286: 1, 0.6560998558998108: 1, 0.21325090527534485: 1, 0.2407364845275879: 1, 1.0926192998886108: 1, 1.4029184579849243: 1, -0.3579169511795044: 1, -1.177890419960022: 1, 1.603068470954895: 1, -0.8235211968421936: 1, 0.2787120044231415: 1, -0.2865978181362152: 1, -0.683555543422699: 1, 1.4387927055358887: 1, 0.18236172199249268: 1, 1.2996296882629395: 1, -0.30772721767425537: 1, 0.5139665007591248: 1, 1.3062021732330322: 1, -0.37773171067237854: 1, -0.3334684669971466: 1, 0.7675377726554871: 1, 1.6281145811080933: 1, -0.883184015750885: 1, -0.19289743900299072: 1, 0.0290671493858099: 1, -1.1476235389709473: 1, 1.520749807357788: 1, 0.21878471970558167: 1, 0.9611315131187439: 1, -0.916256844997406: 1, 0.339458167552948: 1, -1.137963891029358: 1, -0.9888867139816284: 1, -0.9931707978248596: 1, -1.2013576030731201: 1, -1.2016162872314453: 1, -0.3419588804244995: 1, -0.8519163131713867: 1, 0.6741006970405579: 1, -0.506174623966217: 1, -1.055851936340332: 1, -1.2015936374664307: 1, -0.6465518474578857: 1, 0.8243502378463745: 1, -1.2014310359954834: 1, 0.603252649307251: 1, -0.4781683385372162: 1, -0.5828713178634644: 1, -1.201597809791565: 1, 0.1308048814535141: 1, -0.1883729249238968: 1, -0.46576231718063354: 1, -1.2016302347183228: 1, -0.8657602667808533: 1, 0.16982176899909973: 1, 1.1950836181640625: 1, 0.34516385197639465: 1, -0.5620038509368896: 1, -0.059458885341882706: 1, 0.007953275926411152: 1, -0.5027830004692078: 1, -0.399366557598114: 1, 1.3625078201293945: 1, 0.12603989243507385: 1, 1.621954083442688: 1, 1.5633379220962524: 1, 1.593016266822815: 1, 0.32263097167015076: 1, 1.1153340339660645: 1, 0.44327667355537415: 1, 1.4907145500183105: 1, 1.5627902746200562: 1, 0.670230507850647: 1, 0.007752139586955309: 1, -1.2012096643447876: 1, 0.37413451075553894: 1, -1.2015149593353271: 1, -1.2004907131195068: 1, -1.1482487916946411: 1, -1.1996524333953857: 1, -0.4154701232910156: 1, -1.1205617189407349: 1, -0.09781666100025177: 1, -0.06465810537338257: 1, -1.2015609741210938: 1, -0.2831217646598816: 1, -0.8503693342208862: 1, -1.2014657258987427: 1, -1.186226725578308: 1, -1.1972260475158691: 1, 0.39954620599746704: 1, -0.6686071157455444: 1, -1.1674489974975586: 1, 1.14208984375: 1, -1.1830931901931763: 1, -0.6864959597587585: 1, -0.33821895718574524: 1, -1.1987498998641968: 1, 0.4001854956150055: 1, -0.512914776802063: 1, 0.034827083349227905: 1, -0.37911587953567505: 1, -1.0070439577102661: 1, -0.7784246206283569: 1, -1.2014739513397217: 1, -1.201564908027649: 1, -1.201509714126587: 1, -1.1959139108657837: 1, -1.2007629871368408: 1, -1.2015974521636963: 1, -1.2015589475631714: 1, -0.7713357210159302: 1, -1.2016221284866333: 1, 0.812082827091217: 1, -1.201629400253296: 1, -0.21383443474769592: 1, 0.21984633803367615: 1, -1.2006030082702637: 1, -1.201606273651123: 1, -1.2015992403030396: 1, 0.6647039651870728: 1, 0.46835482120513916: 1, -1.201348900794983: 1, -1.1382324695587158: 1, -0.8027163147926331: 1, -1.2002341747283936: 1, -1.2014697790145874: 1, 0.13578376173973083: 1, 1.6003168821334839: 1, 0.7950616478919983: 1, -1.1138004064559937: 1, -1.1573199033737183: 1, 0.7257186770439148: 1, -1.098894715309143: 1, -0.24214474856853485: 1, -0.3594827950000763: 1, -1.1312958002090454: 1, -1.2013144493103027: 1, -1.111975073814392: 1, -1.0634658336639404: 1, -0.8993450403213501: 1, -1.1072322130203247: 1, 1.3899286985397339: 1, -1.1329883337020874: 1, 1.8358889818191528: 1, -0.8241826295852661: 1, -1.0799649953842163: 1, -1.1919200420379639: 1, -0.07864277809858322: 1, -0.8634552955627441: 1, 0.4990178942680359: 1, -0.4610443115234375: 1, -0.9751180410385132: 1, 1.5516252517700195: 1, 1.577986240386963: 1, -1.1560314893722534: 1, -0.8920286297798157: 1, -0.5754680633544922: 1, -0.18136551976203918: 1, -0.5884833335876465: 1, 0.7908697128295898: 1, 0.10187211632728577: 1, -0.9047161340713501: 1, -1.0422825813293457: 1, -0.6316778063774109: 1, 1.4887964725494385: 1, 0.7494425773620605: 1, -0.556195080280304: 1, 1.3791615962982178: 1, 1.788353681564331: 1, -0.896551251411438: 1, 0.43905025720596313: 1, 1.8447388410568237: 1, -1.1907743215560913: 1, -1.1941906213760376: 1, -0.26542410254478455: 1, -1.0395952463150024: 1, -0.3497014045715332: 1, -1.0090559720993042: 1, -1.1442792415618896: 1, 0.2268732339143753: 1, -0.5990430116653442: 1, -1.1570571660995483: 1, -0.26597675681114197: 1, 1.3982725143432617: 1, -1.0213873386383057: 1, 1.281778335571289: 1, -1.1798655986785889: 1, -0.4367877244949341: 1, -1.1983946561813354: 1, -0.7376867532730103: 1, 1.2280430793762207: 1, 0.16367900371551514: 1, -0.700495719909668: 1, 0.15106460452079773: 1, -0.9755853414535522: 1, 0.5357815027236938: 1, -1.1196062564849854: 1, -1.17500901222229: 1, -1.1970343589782715: 1, -0.03840658441185951: 1, -0.9793020486831665: 1, 1.324471354484558: 1, -1.2006902694702148: 1, -1.0523524284362793: 1, -1.1004241704940796: 1, 0.8691079020500183: 1, -1.0893759727478027: 1, -1.1569617986679077: 1, 1.7476170063018799: 1, -0.12463472783565521: 1, -1.0407896041870117: 1, -1.1999518871307373: 1, 0.06941241025924683: 1, 1.5657020807266235: 1, -1.1800106763839722: 1, 0.006390336435288191: 1, -1.1358855962753296: 1, -1.18631112575531: 1, -0.3605387806892395: 1, 0.292474627494812: 1, -0.9981153011322021: 1, -0.9736310243606567: 1, 0.9790754318237305: 1, 0.20924636721611023: 1, -0.5120261311531067: 1, 0.05307941138744354: 1, 0.6492028832435608: 1, 0.46832725405693054: 1, 0.20812031626701355: 1, -0.9288858771324158: 1, 1.192413568496704: 1, -0.20539666712284088: 1, -0.8917420506477356: 1, -1.0858170986175537: 1, -1.024053692817688: 1, -0.7701697945594788: 1, -0.5982488989830017: 1, -0.3573164939880371: 1, 1.3890480995178223: 1, -1.1605626344680786: 1, -0.8580590486526489: 1, -0.7244925498962402: 1, -1.1898142099380493: 1, -1.1212905645370483: 1, 0.5211433172225952: 1, 0.0663938894867897: 1, 1.4690353870391846: 1, 0.17207567393779755: 1, -0.981871485710144: 1, -1.1791499853134155: 1, -1.078048825263977: 1, 1.536348581314087: 1, 1.8480533361434937: 1, 1.0025136470794678: 1, -1.094030499458313: 1, -0.9951556921005249: 1, -0.8108161091804504: 1, -1.188598394393921: 1, 0.638069212436676: 1, 1.0112850666046143: 1, -0.49170398712158203: 1, -0.8270519375801086: 1, -0.04211708903312683: 1, -0.974824070930481: 1, -0.7166287899017334: 1, -1.19014310836792: 1, -0.67027348279953: 1, 1.8432141542434692: 1, -0.8671839237213135: 1, -0.8936908841133118: 1, -1.1343045234680176: 1, 1.4807751178741455: 1, -1.0776159763336182: 1, 0.6570013761520386: 1, -0.9803085327148438: 1, 0.5575955510139465: 1, 0.39953649044036865: 1, 0.4876076281070709: 1, 1.0163378715515137: 1, -0.9919153451919556: 1, -0.233070969581604: 1, 0.5474697947502136: 1, -0.7628646492958069: 1, 1.114488959312439: 1, -0.35407769680023193: 1, -1.0393953323364258: 1, 1.3862724304199219: 1, 1.6304298639297485: 1, 1.6431117057800293: 1, -0.1989370882511139: 1, -1.0411947965621948: 1, -0.923246443271637: 1, -1.0519613027572632: 1, -1.0751264095306396: 1, -0.7096079587936401: 1, -1.197619080543518: 1, -0.8923998475074768: 1, -1.2003082036972046: 1, -0.4400210678577423: 1, -1.1370965242385864: 1, -0.8680421113967896: 1, 0.2903974652290344: 1, -1.193503737449646: 1, -0.45158419013023376: 1, -0.930094301700592: 1, -1.1848548650741577: 1, -0.6311256289482117: 1, -0.19042661786079407: 1, -1.1041064262390137: 1, -1.1646332740783691: 1, -1.1712636947631836: 1, 0.3200169503688812: 1, 3.259727716445923: 1, 0.025435535237193108: 1, -0.09378552436828613: 1, -0.8962797522544861: 1, 0.30057549476623535: 1, 0.9532740116119385: 1, 0.8790757060050964: 1, -0.9627416729927063: 1, -0.608140766620636: 1, -1.0076677799224854: 1, 1.0116826295852661: 1, -1.2001420259475708: 1, -1.2009141445159912: 1, -1.0507465600967407: 1, -1.1325860023498535: 1, 1.8212419748306274: 1, -0.9299888610839844: 1, -1.1659823656082153: 1, -1.0714704990386963: 1, -0.4354245066642761: 1, -0.5724524259567261: 1, -0.7811231017112732: 1, 0.4252239763736725: 1, -0.16512484848499298: 1, 1.5870535373687744: 1, 1.0355088710784912: 1, -1.107023000717163: 1, -0.16252407431602478: 1, 0.9657272696495056: 1, 0.27488669753074646: 1, 1.2950940132141113: 1, 0.7110782265663147: 1, 1.210314393043518: 1, 0.5456136465072632: 1, 0.6780659556388855: 1, -0.01062939316034317: 1, 1.2984728813171387: 1, 1.3494455814361572: 1, 0.6696186065673828: 1, -0.6162902116775513: 1, 1.4811328649520874: 1, -0.8986467123031616: 1, -0.15746326744556427: 1, -1.1914112567901611: 1, 1.5811641216278076: 1, -0.25767362117767334: 1, 1.0303751230239868: 1, 1.2882025241851807: 1, 0.784543514251709: 1, 0.8268551230430603: 1, 0.2808535397052765: 1, 0.2640135586261749: 1, -0.26631277799606323: 1, 1.4656307697296143: 1, 0.16298139095306396: 1, 0.02830575592815876: 1, 0.6718612909317017: 1, 1.3615977764129639: 1, -0.9881011247634888: 1, -1.2016340494155884: 1, 1.7417840957641602: 1, 0.9129876494407654: 1, -0.7180438041687012: 1, 1.0402084589004517: 1, -0.7170498371124268: 1, 0.6606081128120422: 1, 1.5755736827850342: 1, 0.8417104482650757: 1, 0.8741052150726318: 1, 0.8892757296562195: 1, 0.4746771454811096: 1, 0.5222756862640381: 1, -0.027011625468730927: 1, 1.3250715732574463: 1, 0.12876805663108826: 1, 0.7825468182563782: 1, 1.6178103685379028: 1, -1.2016338109970093: 1, 0.15925046801567078: 1, -0.37734130024909973: 1, 1.1998642683029175: 1, 1.3784377574920654: 1, 1.2730098962783813: 1, 0.7611046433448792: 1, 1.2712597846984863: 1, 1.573736548423767: 1, 1.4694101810455322: 1, 1.9331234693527222: 1, 0.3604428768157959: 1, 1.427628755569458: 1, 0.35480692982673645: 1, 0.699573814868927: 1, 0.3411409258842468: 1, 0.394859254360199: 1, 0.23328566551208496: 1, -0.030014334246516228: 1, -0.4297850430011749: 1, 1.5701237916946411: 1, -0.930620014667511: 1, -0.8510425090789795: 1, 0.71575528383255: 1, -0.5881722569465637: 1, 0.465168297290802: 1, 0.2403424084186554: 1, -0.5288078188896179: 1, 1.224327802658081: 1, -0.9218448400497437: 1, 0.2308363914489746: 1, 1.1008855104446411: 1, 1.4938230514526367: 1, 1.1383615732192993: 1, 0.8504006266593933: 1, 1.2588475942611694: 1, -0.24819114804267883: 1, -0.08030800521373749: 1, 0.36215391755104065: 1, -0.5825971961021423: 1, 1.427193522453308: 1, -0.16857680678367615: 1, -0.39828142523765564: 1, 0.7533062696456909: 1, 1.5823100805282593: 1, 0.9470359086990356: 1, 0.003300704760476947: 1, 0.16384904086589813: 1, 0.1881372481584549: 1, -1.1895103454589844: 1, 1.1112544536590576: 1, -0.48030614852905273: 1, 1.9067574739456177: 1, 0.7951827049255371: 1, 0.07571198046207428: 1, -0.8877851366996765: 1, 1.6006011962890625: 1, -0.9158876538276672: 1, 0.342952162027359: 1, -0.9859256744384766: 1, 1.425097107887268: 1, -0.13197508454322815: 1, -1.1913617849349976: 1, -1.0719594955444336: 1, 1.287703037261963: 1, 0.35307395458221436: 1, 1.4413039684295654: 1, -1.1940946578979492: 1, -1.1507015228271484: 1, -1.1607003211975098: 1, 1.9238320589065552: 1, 1.1217374801635742: 1, -0.09434226900339127: 1, 1.6284458637237549: 1, -0.6876721978187561: 1, -0.2101057469844818: 1, -0.748826265335083: 1, 0.22341269254684448: 1, -1.199570655822754: 1, -0.5242934823036194: 1, -1.1873440742492676: 1, -0.8747767210006714: 1, 1.4774726629257202: 1, -0.731924295425415: 1, -0.05734042823314667: 1, -0.45086827874183655: 1, -1.0084080696105957: 1, -0.9050359129905701: 1, -0.5784834027290344: 1, -0.9131625890731812: 1, -1.1544640064239502: 1, -0.9035985469818115: 1, 1.7851945161819458: 1, 1.2659807205200195: 1, -0.8956496715545654: 1, -1.0024443864822388: 1, -0.8754668831825256: 1, -1.1969212293624878: 1, -1.07969331741333: 1, -0.5022063255310059: 1, -0.021469445899128914: 1, 0.36046868562698364: 1, 0.12815426290035248: 1, 1.7614071369171143: 1, 0.3255096673965454: 1, 0.9005318284034729: 1, -0.18478137254714966: 1, 0.3698660731315613: 1, 0.2775695323944092: 1, 0.28860634565353394: 1, 0.2634084224700928: 1, -0.32526895403862: 1, 0.1708119511604309: 1, 1.4644434452056885: 1, 1.053705096244812: 1, 1.001185417175293: 1, 0.6039040088653564: 1, -1.031872034072876: 1, 0.1467430293560028: 1, 0.3286954462528229: 1, 0.46373531222343445: 1, -0.016411839053034782: 1, 0.10751932114362717: 1, 0.9138351678848267: 1, -0.514695405960083: 1, 1.0683897733688354: 1, -0.8486149311065674: 1, -0.23679472506046295: 1, -0.14438967406749725: 1, -0.4185396134853363: 1, 0.2964378595352173: 1, 0.7527873516082764: 1, 0.03332117572426796: 1, 0.002877143444493413: 1, 1.4062000513076782: 1, -0.2782626748085022: 1, -0.047685928642749786: 1, -0.19720852375030518: 1, 0.3313300311565399: 1, -0.11414719372987747: 1, -0.18094922602176666: 1, -0.10225572437047958: 1, 0.9568199515342712: 1, -0.16261765360832214: 1, -0.1799832135438919: 1, 0.1897212415933609: 1, 0.900846004486084: 1, 0.32160520553588867: 1, -0.07945768535137177: 1, -0.3424704372882843: 1, -1.1767117977142334: 1, -1.102870225906372: 1, -0.4105451703071594: 1, 1.280470848083496: 1, 0.28526046872138977: 1, -1.2013123035430908: 1, 0.4860861599445343: 1, 1.6493014097213745: 1, -1.1416743993759155: 1, -0.9635469913482666: 1, -0.7447215914726257: 1, -0.35984915494918823: 1, -0.9806917309761047: 1, 1.3466922044754028: 1, -1.2016212940216064: 1, 0.454349547624588: 1, -0.5300003886222839: 1, 1.2604477405548096: 1, 1.2889325618743896: 1, 0.35712626576423645: 1, 1.9024626016616821: 1, 1.6134487390518188: 1, 1.5396013259887695: 1, -1.0555497407913208: 1, 1.2202951908111572: 1, -1.2014974355697632: 1, 0.11243647336959839: 1, -1.201619267463684: 1, -0.2495342493057251: 1, -0.26848453283309937: 1, 0.3117649555206299: 1, 0.5131713151931763: 1, 1.76934015750885: 1, -1.2016271352767944: 1, 0.9834553003311157: 1, -1.2006548643112183: 1, -1.1663979291915894: 1, 1.8360271453857422: 1, 1.5100682973861694: 1, -1.2016189098358154: 1, -1.1264077425003052: 1, 1.0540943145751953: 1, 1.5793328285217285: 1, 0.42848649621009827: 1, -0.41689032316207886: 1, -0.2686515748500824: 1, 1.8881990909576416: 1, -1.061113953590393: 1, 1.7208062410354614: 1, 1.633592128753662: 1, -1.2002416849136353: 1, 1.92928946018219: 1, -0.4422439932823181: 1, -0.6532332301139832: 1, 0.9807740449905396: 1, 1.1523829698562622: 1, 1.796111822128296: 1, 1.6013163328170776: 1, 0.3571586012840271: 1, 1.8800233602523804: 1, -0.7931265234947205: 1, -0.5870760679244995: 1, 0.5326334834098816: 1, -0.9877029061317444: 1, -0.8662946820259094: 1, 0.5422061085700989: 1, 1.5719680786132812: 1, -1.2016080617904663: 1, 0.23050570487976074: 1, 0.8521577715873718: 1, -0.009660118259489536: 1, 1.114802360534668: 1, -0.7990305423736572: 1, -1.174880027770996: 1, -1.201361894607544: 1, -1.0791629552841187: 1, -1.2016111612319946: 1, 1.9020731449127197: 1, 1.3496158123016357: 1, -1.088794231414795: 1, -0.7490406036376953: 1, -1.042400598526001: 1, 0.10260368138551712: 1, 1.4280599355697632: 1, -0.5811882019042969: 1, 1.2709132432937622: 1, 0.804813027381897: 1, 0.9758270978927612: 1, -0.253825306892395: 1, 0.14208731055259705: 1, -0.10222127288579941: 1, 1.7350994348526: 1, -0.44384631514549255: 1, 1.7166484594345093: 1, 1.5665374994277954: 1, 0.5419895052909851: 1, -1.0994056463241577: 1, 1.901644229888916: 1, 0.5713223814964294: 1, 1.5698747634887695: 1, -1.2016352415084839: 1, -0.4567924439907074: 1, 1.7558945417404175: 1, -0.22097758948802948: 1, -0.2709258496761322: 1, -0.180640310049057: 1, -0.6814844608306885: 1, 0.4076603651046753: 1, 1.6906383037567139: 1, 1.8846700191497803: 1, 0.6905592679977417: 1, 1.5815212726593018: 1, 1.1628241539001465: 1, 1.7223035097122192: 1, 0.34220972657203674: 1, -0.7435175180435181: 1, 0.6320856809616089: 1, 1.9327584505081177: 1, 0.6905925273895264: 1, 1.4221618175506592: 1, 1.4504951238632202: 1, -1.054260492324829: 1, -0.10395042598247528: 1, -0.0515512116253376: 1, 1.3716888427734375: 1, -0.5665901303291321: 1, 0.48093563318252563: 1, 0.9761800765991211: 1, -0.4791189730167389: 1, -0.4220041036605835: 1, -0.2772659659385681: 1, 0.7481110692024231: 1, -0.08275699615478516: 1, 1.4337563514709473: 1, 0.2889866232872009: 1, -0.22210338711738586: 1, 0.9173278212547302: 1, 0.14515815675258636: 1, 0.7045637369155884: 1, -0.03429622948169708: 1, 0.03225273638963699: 1, 1.590468406677246: 1, 1.3033519983291626: 1, 0.5755087733268738: 1, 0.19334031641483307: 1, -1.0097246170043945: 1, 0.39559683203697205: 1, -0.1754150390625: 1, -0.03317539766430855: 1, 1.7539278268814087: 1, 1.0263571739196777: 1, 0.949316143989563: 1, 1.6153100728988647: 1, 1.865704894065857: 1, 0.591416597366333: 1, 0.24442099034786224: 1, 0.7021965980529785: 1, 1.1086541414260864: 1, -0.025178229436278343: 1, 1.4182209968566895: 1, -0.2232077419757843: 1, 1.0101338624954224: 1, 0.9832457304000854: 1, 0.014584558084607124: 1, 1.891126036643982: 1, 0.8425688147544861: 1, 0.5497986674308777: 1, 0.08808748424053192: 1, 1.6464146375656128: 1, -1.0590986013412476: 1, 1.9098440408706665: 1, 1.6939563751220703: 1, 0.523788332939148: 1, 0.2675780653953552: 1, 1.6441566944122314: 1, 1.4320223331451416: 1, 0.27008119225502014: 1, 0.5438616275787354: 1, 1.7069745063781738: 1, 0.9952530264854431: 1, 1.7395148277282715: 1, 1.0135881900787354: 1, 0.9889004230499268: 1, 0.5330069065093994: 1, 1.0885628461837769: 1, -0.2079317569732666: 1, 1.612230896949768: 1, 0.1083393469452858: 1, -0.3399538993835449: 1, 0.723430871963501: 1, 0.9554869532585144: 1, 1.6500415802001953: 1, 0.6886505484580994: 1, 1.8263726234436035: 1, 1.7850080728530884: 1, -0.2531147599220276: 1, 0.9454357028007507: 1, 0.9327585697174072: 1, 0.8505056500434875: 1, 0.521833598613739: 1, 0.18436919152736664: 1, 1.7429335117340088: 1, 1.7397531270980835: 1, 0.6789939403533936: 1, -0.9450681209564209: 1, -1.2015763521194458: 1, 0.6071187853813171: 1, 1.3225599527359009: 1, 0.38025134801864624: 1, 0.9784976243972778: 1, 1.7376213073730469: 1, 1.9389169216156006: 1, 1.2539737224578857: 1, 1.610649824142456: 1, -1.0571757555007935: 1, 0.8005363941192627: 1, -0.2255825698375702: 1, 1.2595564126968384: 1, -1.012891411781311: 1, -0.13542917370796204: 1, -1.2016215324401855: 1, 0.963599681854248: 1, 0.6376197934150696: 1, 0.7080846428871155: 1, -1.2015223503112793: 1, 0.9156394600868225: 1, -1.2016057968139648: 1, 0.8071705102920532: 1, 1.6581584215164185: 1, 0.562964141368866: 1, -0.0861414223909378: 1, 0.20394694805145264: 1, 0.780617356300354: 1, 1.8291547298431396: 1, 1.6759966611862183: 1, 1.0316096544265747: 1, -1.2015867233276367: 1, -0.538144052028656: 1, 1.7389863729476929: 1, 1.896134853363037: 1, -1.040601134300232: 1, -0.8136187195777893: 1, 1.519827961921692: 1, 1.3174397945404053: 1, 1.16111421585083: 1, -0.7696746587753296: 1, 1.0987391471862793: 1, 1.2727433443069458: 1, 0.261216938495636: 1, -0.43114355206489563: 1, -0.7838892936706543: 1, 1.271135687828064: 1, 0.4253986179828644: 1, 1.3490053415298462: 1, -0.08804576843976974: 1, 1.3129916191101074: 1, -0.06900987029075623: 1, 1.8261455297470093: 1, -1.2016295194625854: 1, -0.3587249517440796: 1, -0.40272125601768494: 1, -1.130736231803894: 1, -0.24564200639724731: 1, -1.2007230520248413: 1, -0.20700129866600037: 1, -0.6896651983261108: 1, 0.8895251154899597: 1, 1.5920872688293457: 1, 1.5496872663497925: 1, -1.2016278505325317: 1, -1.1936933994293213: 1, 0.4616207182407379: 1, -1.2016236782073975: 1, 0.8969712257385254: 1, -0.8653848767280579: 1, 1.8480027914047241: 1, -0.7542659640312195: 1, -1.1320221424102783: 1, -0.34224218130111694: 1, -1.0611162185668945: 1, 0.4620521366596222: 1, 0.5805153250694275: 1, 1.7425659894943237: 1, 1.0934252738952637: 1, -0.9789384603500366: 1, -1.14544677734375: 1, -0.6306192278862: 1, 0.9750880599021912: 1, 1.3510494232177734: 1, 1.6034055948257446: 1, -0.6586742401123047: 1, 0.1718989461660385: 1, -1.200726866722107: 1, -0.13886263966560364: 1, -0.24342182278633118: 1, -0.7379482984542847: 1, -0.5476839542388916: 1, -1.1302224397659302: 1, 1.3847272396087646: 1, 1.110692024230957: 1, -0.8793452382087708: 1, 1.7776927947998047: 1, 1.2369016408920288: 1, -0.20698606967926025: 1, -1.002498984336853: 1, -1.201613187789917: 1, 0.08071509003639221: 1, 1.446770191192627: 1, 1.7338974475860596: 1, -1.1562846899032593: 1, 1.760259985923767: 1, -0.11584310233592987: 1, -0.9069592952728271: 1, 1.8416829109191895: 1, 1.7511584758758545: 1, 1.788615107536316: 1, 0.8217967748641968: 1, 0.8788592219352722: 1, 1.8749902248382568: 1, 1.5984208583831787: 1, 0.63859623670578: 1, 1.7614209651947021: 1, 0.8861773610115051: 1, -0.010684509761631489: 1, 1.887890100479126: 1, 0.09368380159139633: 1, 1.4813575744628906: 1, -0.907404899597168: 1, 1.2843722105026245: 1, -0.22918304800987244: 1, 1.8706549406051636: 1, 0.43482455611228943: 1, 1.8285820484161377: 1, 1.2302463054656982: 1, 0.8335886001586914: 1, -0.8916471600532532: 1, 0.04224063828587532: 1, 1.8896540403366089: 1, 0.6508381366729736: 1, 0.9216548800468445: 1, 0.8007993698120117: 1, -0.4143541157245636: 1, 0.25442907214164734: 1, 1.6676998138427734: 1, -1.2014168500900269: 1, 0.9160022139549255: 1, 0.10331395268440247: 1, 0.9006003737449646: 1, 1.0329307317733765: 1, 1.0343732833862305: 1, 0.05316608399152756: 1, 1.4475048780441284: 1, 1.6221997737884521: 1, 1.9172451496124268: 1, -0.796614408493042: 1, 1.7114263772964478: 1, -0.8602240085601807: 1, -1.156775951385498: 1, 1.5742621421813965: 1, 0.1478143036365509: 1, 0.35647323727607727: 1, 1.5142070055007935: 1, -0.420527845621109: 1, -0.5466570258140564: 1, 1.6787821054458618: 1, 1.8354456424713135: 1, 1.8180814981460571: 1, 1.4667671918869019: 1, 1.455495834350586: 1, -1.1579643487930298: 1, 0.8198267817497253: 1, 1.8067305088043213: 1, -0.24759358167648315: 1, -0.655949056148529: 1, 1.5291143655776978: 1, 1.7461694478988647: 1, 1.8303353786468506: 1, 0.46223318576812744: 1, 1.127722144126892: 1, -0.4194781482219696: 1, 1.7499927282333374: 1, 1.473099708557129: 1, 1.7661612033843994: 1, 0.9352988600730896: 1, 1.597861409187317: 1, 0.9059203267097473: 1, 1.0176738500595093: 1, 0.9797016382217407: 1, 1.746630072593689: 1, 0.9968640804290771: 1, 1.5016316175460815: 1, 1.6772441864013672: 1, 5.270293235778809: 1, 1.7267848253250122: 1, -0.5458275079727173: 1, -0.7692103385925293: 1, 1.0276689529418945: 1, 0.019096076488494873: 1, 1.8225407600402832: 1, -0.1335328370332718: 1, 1.7602899074554443: 1, 1.8266348838806152: 1, 1.058468222618103: 1, 0.32922476530075073: 1, 0.43645578622817993: 1, 0.26557838916778564: 1, 1.1236602067947388: 1, -0.4369107186794281: 1, -0.03591597080230713: 1, -0.14395083487033844: 1, 0.2143784463405609: 1, 0.171223446726799: 1, 1.4956955909729004: 1, 1.9232004880905151: 1, -0.33291712403297424: 1, -0.3439752459526062: 1, -0.6786049008369446: 1, -0.07709828019142151: 1, -0.48933231830596924: 1, 1.3399251699447632: 1, -0.7822737693786621: 1, -0.28641819953918457: 1, -1.1991149187088013: 1, 0.6785101294517517: 1, 0.29917436838150024: 1, 0.6026607155799866: 1, -1.0817426443099976: 1, -1.2014148235321045: 1, -0.31910526752471924: 1, -1.193153738975525: 1, -0.37577909231185913: 1, -0.7031784653663635: 1, 0.3244344890117645: 1, 1.3759360313415527: 1, -0.8751227855682373: 1, 1.456301212310791: 1, 1.542358636856079: 1, -0.46590861678123474: 1, -0.6440175771713257: 1, -0.5345551371574402: 1, 1.2142442464828491: 1, -0.9877877235412598: 1, -0.3422335684299469: 1, -0.9143604040145874: 1, -0.29665860533714294: 1, -0.4729682505130768: 1, -0.032225385308265686: 1, 1.2115764617919922: 1, -0.558110237121582: 1, 0.9276106357574463: 1, -0.47733497619628906: 1, -0.014680324122309685: 1, -0.24040797352790833: 1, -0.6007823944091797: 1, -0.7505127787590027: 1, -1.1891201734542847: 1, -0.44544142484664917: 1, -0.7293344736099243: 1, -0.8671026229858398: 1, 1.4193427562713623: 1, 1.8792171478271484: 1, -0.42767956852912903: 1, -0.0695866122841835: 1, 1.93668532371521: 1, 0.16830426454544067: 1, 0.3824501037597656: 1, 0.1589841991662979: 1, 1.8601784706115723: 1, -0.3237372636795044: 1, 1.8596769571304321: 1, 0.15484686195850372: 1, 1.2151012420654297: 1, 1.7544053792953491: 1, -0.5209143161773682: 1, -1.1991721391677856: 1, -0.2183121144771576: 1, -0.906280517578125: 1, -0.7995052933692932: 1, -0.22242674231529236: 1, 1.7963730096817017: 1, -0.9981749057769775: 1, 1.6570682525634766: 1, 1.1691573858261108: 1, 1.058951735496521: 1, -0.06596078723669052: 1, -0.5857658982276917: 1, -1.1479105949401855: 1, -0.5629591941833496: 1, -0.24207068979740143: 1, -0.3858093321323395: 1, 1.3953920602798462: 1, 1.878305196762085: 1, 1.5919677019119263: 1, 1.7606215476989746: 1, 0.6444322466850281: 1, -0.3169466555118561: 1, 1.3363116979599: 1, -0.9762966632843018: 1, -0.8443267941474915: 1, -1.1333893537521362: 1, 0.22667939960956573: 1, 1.6555308103561401: 1, 0.6444940567016602: 1, -0.5378462672233582: 1, -0.21501778066158295: 1, -0.0690179392695427: 1, 0.006228437647223473: 1, -0.3081098794937134: 1, 0.7785534858703613: 1, 0.6150393486022949: 1, -0.29989245533943176: 1, 1.7616217136383057: 1, -0.4605187475681305: 1, -0.2670007050037384: 1, 0.3248298168182373: 1, 1.4998488426208496: 1, -0.879592776298523: 1, -0.288830429315567: 1, -0.8833669424057007: 1, -0.007546336855739355: 1, -0.766767144203186: 1, -0.16766004264354706: 1, -0.5713714957237244: 1, -1.0160380601882935: 1, -0.5338919758796692: 1, -0.4835919141769409: 1, -0.13618627190589905: 1, 0.9851498603820801: 1, -0.5030357241630554: 1, 0.672914445400238: 1, -1.1766016483306885: 1, 1.376474142074585: 1, -0.200442373752594: 1, -0.5582360029220581: 1, -0.35491982102394104: 1, 0.16428887844085693: 1, 1.3883335590362549: 1, -0.47762712836265564: 1, -0.9396678805351257: 1, 1.0024393796920776: 1, -0.1308683305978775: 1, -0.5413272380828857: 1, -0.7269378900527954: 1, 0.46070119738578796: 1, 0.15043634176254272: 1, -0.7417653203010559: 1, 0.4303920269012451: 1, -0.13842415809631348: 1, 1.491416096687317: 1, -0.17455770075321198: 1, 1.4534815549850464: 1, -0.30685290694236755: 1, -0.46731990575790405: 1, -0.5843047499656677: 1, -0.1967451572418213: 1, 0.8612715601921082: 1, 0.47734835743904114: 1, 1.5933094024658203: 1, -0.4548023045063019: 1, 1.6648616790771484: 1, -1.0031712055206299: 1, -0.3702247440814972: 1, 0.7159720659255981: 1, -0.0832226425409317: 1, 1.5722923278808594: 1, -0.0211151335388422: 1, 0.19413244724273682: 1, -0.06991042196750641: 1, 0.2551988363265991: 1, 1.483720302581787: 1, 1.4788439273834229: 1, 1.2964091300964355: 1, 1.5233625173568726: 1, 1.3913298845291138: 1, -0.9003047943115234: 1, 1.569981575012207: 1, -0.92970210313797: 1, 0.9121710062026978: 1, -0.34284013509750366: 1, -0.47152507305145264: 1, -0.23732632398605347: 1, -1.093284010887146: 1, -0.6234576106071472: 1, -0.023513898253440857: 1, 0.9130324721336365: 1, 0.025362450629472733: 1, 0.7212937474250793: 1, 0.48826223611831665: 1, -0.5381679534912109: 1, 0.5300026535987854: 1, 0.3571058511734009: 1, 0.04851381108164787: 1, -0.5174428224563599: 1, -0.3349834084510803: 1, -0.21868036687374115: 1, 0.6790488958358765: 1, 1.2261123657226562: 1, 0.37810415029525757: 1, -0.5210259556770325: 1, -0.49563685059547424: 1, -0.8521113991737366: 1, -1.179208755493164: 1, 1.3037792444229126: 1, 0.17652635276317596: 1, -0.36514076590538025: 1, 0.31867286562919617: 1, 1.7650138139724731: 1, 1.5116616487503052: 1, -0.24450848996639252: 1, -0.3431845009326935: 1, 1.4984081983566284: 1, 1.0491001605987549: 1, 0.2092854529619217: 1, 0.15644948184490204: 1, 0.3744315505027771: 1, -0.24634599685668945: 1, 1.852098822593689: 1, 0.3915187418460846: 1, 0.49887171387672424: 1, 0.7721536755561829: 1, 0.10666077584028244: 1, -1.2015986442565918: 1, -0.46826034784317017: 1, 0.956155002117157: 1, -0.9726563692092896: 1, -1.0813374519348145: 1, 1.525660514831543: 1, 0.7304840087890625: 1, -0.6562255024909973: 1, -0.3255411982536316: 1, -0.061833277344703674: 1, -0.26246213912963867: 1, 0.3141234815120697: 1, 1.0318180322647095: 1, 1.6552691459655762: 1, 0.5530001521110535: 1, -0.4845651388168335: 1, 0.8109627962112427: 1, 2.0270323753356934: 1, 0.04068145155906677: 1, -0.39972129464149475: 1, -0.5899453163146973: 1, -1.1006834506988525: 1, 1.6034691333770752: 1, 0.3640252351760864: 1, -0.9720814824104309: 1, -0.18857857584953308: 1, 1.5428107976913452: 1, 0.5874748826026917: 1, -0.5802597403526306: 1, -1.0406304597854614: 1, -0.3135724663734436: 1, -1.1350090503692627: 1, 1.7555813789367676: 1, -1.1569743156433105: 1, -0.6055639982223511: 1, -0.28438881039619446: 1, 0.12007596343755722: 1, -0.22874142229557037: 1, -1.2016154527664185: 1, -1.2016232013702393: 1, -0.6276612877845764: 1, -0.3100615441799164: 1, -0.22727444767951965: 1, 1.692647099494934: 1, 1.8799982070922852: 1, 1.2042120695114136: 1, -0.15924076735973358: 1, -0.8965012431144714: 1, 1.7392624616622925: 1, 0.4917255938053131: 1, 1.6278821229934692: 1, 0.11675674468278885: 1, 0.16001862287521362: 1, 0.3740043342113495: 1, 0.056893277913331985: 1, 0.0023630079813301563: 1, 0.24924464523792267: 1, -0.7589280009269714: 1, -0.03557446971535683: 1, 0.7833142876625061: 1, -0.7948459982872009: 1, 0.81379234790802: 1, 0.7566526532173157: 1, -0.6431723237037659: 1, 0.04079057276248932: 1, -0.024080123752355576: 1, -0.3426608741283417: 1, 0.6405824422836304: 1, 1.6683679819107056: 1, -0.8678878545761108: 1, 1.905008316040039: 1, -0.14373785257339478: 1, 0.611980676651001: 1, 0.5500550270080566: 1, -0.5307965278625488: 1, -0.27935805916786194: 1, -0.9310538172721863: 1, 1.4624748229980469: 1, 0.8171444535255432: 1, 0.46222802996635437: 1, 0.1769435554742813: 1, 1.2401331663131714: 1, -1.2016234397888184: 1, 0.7334970831871033: 1, -0.2089931219816208: 1, -0.2558027505874634: 1, -0.4171464443206787: 1, -0.9147067666053772: 1, -0.04817575961351395: 1, -0.563433825969696: 1, 0.5143935680389404: 1, 0.4647309482097626: 1, -0.29477736353874207: 1, -0.6015652418136597: 1, 0.012895430438220501: 1, -0.36108481884002686: 1, -0.8523722290992737: 1, -0.7296833395957947: 1, 0.7230984568595886: 1, -1.2016171216964722: 1, -1.0483030080795288: 1, -1.201569676399231: 1, -0.9971945881843567: 1, -1.1498054265975952: 1, -1.110012173652649: 1, -0.6403390765190125: 1, -1.1406184434890747: 1, -0.9465845823287964: 1, 4.282702445983887: 1, -1.1494790315628052: 1, -1.201560139656067: 1, -1.1635701656341553: 1, -1.2015472650527954: 1, -0.3383774757385254: 1, -1.1422733068466187: 1, -0.5884281992912292: 1, -0.7939702272415161: 1, 1.896323323249817: 1, -0.3123464286327362: 1, -1.201271653175354: 1, -0.4653182625770569: 1, -1.1008306741714478: 1, -0.37186357378959656: 1, -0.27475300431251526: 1, 0.4937743842601776: 1, -0.539626955986023: 1, -0.7983001470565796: 1, -0.7234905958175659: 1, 1.5011951923370361: 1, 1.571593999862671: 1, -1.1689732074737549: 1, -0.44949105381965637: 1, -0.26545509696006775: 1, -0.9465891718864441: 1, -1.1831696033477783: 1, -0.6922621726989746: 1, -0.26783594489097595: 1, -0.8816695213317871: 1, -1.1108695268630981: 1, -1.0460647344589233: 1, -1.1475187540054321: 1, -1.201583981513977: 1, -1.2016146183013916: 1, -1.1971925497055054: 1, 0.5123543739318848: 1, -1.1336802244186401: 1, -1.1232612133026123: 1, -0.392406165599823: 1, -0.8350648880004883: 1, -1.103760838508606: 1, -1.188301682472229: 1, -0.8279891610145569: 1, -1.1760764122009277: 1, -0.2970311641693115: 1, -0.7790790796279907: 1, -1.0625981092453003: 1, -0.6904935240745544: 1, -1.1205850839614868: 1, 1.9273109436035156: 1, 0.3162490129470825: 1, 1.552185297012329: 1, -0.13741447031497955: 1, -1.1448076963424683: 1, -0.5809049010276794: 1, -0.8602992296218872: 1, 0.9462845921516418: 1, -1.2013746500015259: 1, -1.1829675436019897: 1, 4.112330913543701: 1, -1.1969398260116577: 1, -1.201536774635315: 1, -1.1448140144348145: 1, 4.5118513107299805: 1, 0.8056516647338867: 1, -1.201555848121643: 1, -0.9751223921775818: 1, -1.199577808380127: 1, -0.7289202809333801: 1, -1.1515345573425293: 1, -1.1694631576538086: 1, -0.13113076984882355: 1, 3.341240167617798: 1, -1.1624175310134888: 1, -1.1994960308074951: 1, -0.2223142683506012: 1, 0.7078472375869751: 1, -0.5361114740371704: 1, -1.1236215829849243: 1, 1.4680920839309692: 1, -1.1851680278778076: 1, -1.1999285221099854: 1, -0.4707425832748413: 1, -0.6006320714950562: 1, -1.051085352897644: 1, -0.5369265675544739: 1, -0.03033183142542839: 1, -0.10903797298669815: 1, -0.7482910752296448: 1, -1.15325927734375: 1, -1.0079307556152344: 1, -1.1992580890655518: 1, 0.9220188856124878: 1, -1.0164505243301392: 1, -0.7683218121528625: 1, -1.150406837463379: 1, -0.1598413586616516: 1, 0.032617583870887756: 1, -1.1747305393218994: 1, 0.4001394212245941: 1, -0.7597726583480835: 1, -1.0657732486724854: 1, -0.3311309218406677: 1, -0.4976504147052765: 1, -1.1774768829345703: 1, -0.6715388298034668: 1, -1.2001534700393677: 1, -0.35893186926841736: 1, -0.8982016444206238: 1, 1.2778337001800537: 1, -1.193078875541687: 1, -1.186597228050232: 1, -1.1420694589614868: 1, 0.02189079485833645: 1, -1.169080138206482: 1, -0.3552163541316986: 1, -0.9202075004577637: 1, -1.2008600234985352: 1, -0.11124473065137863: 1, 0.7242860198020935: 1, 1.2057219743728638: 1, -0.7557051777839661: 1, 0.7875488996505737: 1, 0.8666924238204956: 1, -1.1981743574142456: 1, -0.16389766335487366: 1, 1.488932728767395: 1, 0.8550933599472046: 1, -1.2011059522628784: 1, -0.05180063098669052: 1, -1.2016228437423706: 1, 0.3949566185474396: 1, -1.1074978113174438: 1, -0.14771254360675812: 1, 1.1114397048950195: 1, 0.68455970287323: 1, -0.05808640271425247: 1, 0.03431294485926628: 1, -0.881252646446228: 1, -1.1873303651809692: 1, 0.17733901739120483: 1, -0.04230939969420433: 1, -0.35624369978904724: 1, -0.3301823139190674: 1, -1.1322532892227173: 1, 0.49005118012428284: 1, -0.7893494963645935: 1, -1.192280888557434: 1, -1.2016328573226929: 1, 0.5382747054100037: 1, -0.25587567687034607: 1, 1.0084635019302368: 1, 0.810942530632019: 1, -0.0674244612455368: 1, 0.6161129474639893: 1, -0.9743238091468811: 1, 0.9156388640403748: 1, 0.6273993253707886: 1, 1.257509708404541: 1, -0.40984249114990234: 1, -0.04764068126678467: 1, -0.6981508731842041: 1, -0.01552529539912939: 1, -0.9249427914619446: 1, -0.17132940888404846: 1, -1.20045006275177: 1, -0.14265145361423492: 1, 0.5761882066726685: 1, -0.03975825384259224: 1, -1.2010724544525146: 1, -1.1809542179107666: 1, 0.013616573065519333: 1, -1.1908975839614868: 1, -0.8764730095863342: 1, -0.24605818092823029: 1, -0.5374748706817627: 1, -0.171332448720932: 1, -0.8429122567176819: 1, 0.040903303772211075: 1, 1.0700626373291016: 1, 0.1461368203163147: 1, -0.17077305912971497: 1, 0.029728559777140617: 1, -1.1866058111190796: 1, -0.41031748056411743: 1, -0.3642917275428772: 1, -0.5142630338668823: 1, 0.7907249331474304: 1, -0.13443662226200104: 1, 0.03998654708266258: 1, -0.002722974168136716: 1, -1.1028809547424316: 1, -1.1580485105514526: 1, 0.8628989458084106: 1, -0.24963954091072083: 1, -1.1379677057266235: 1, -0.5910298824310303: 1, 1.0215860605239868: 1, 0.9452794194221497: 1, 1.1708778142929077: 1, 0.3778545558452606: 1, 1.1708914041519165: 1, 0.4334481656551361: 1, -0.13237591087818146: 1, 0.6366953253746033: 1, 1.177069067955017: 1, 0.8209275603294373: 1, -0.8366453051567078: 1, -0.3122004270553589: 1, -1.1760457754135132: 1, -1.0666824579238892: 1, -0.3503449857234955: 1, -1.2002339363098145: 1, -0.2299073189496994: 1, 0.9924389719963074: 1, -1.0224525928497314: 1, -0.21156159043312073: 1, -0.39197561144828796: 1, -1.1276121139526367: 1, 0.004596139770001173: 1, -0.042717207223176956: 1, -0.6867349743843079: 1, -0.5342384576797485: 1, -1.2015101909637451: 1, -0.39157962799072266: 1, -0.09844925999641418: 1, -0.5935716032981873: 1, 1.0410280227661133: 1, -1.1992988586425781: 1, -1.1941806077957153: 1, -0.3055903911590576: 1, 1.1094199419021606: 1, 1.0952398777008057: 1, 1.258391261100769: 1, -0.3075839579105377: 1, -0.29541367292404175: 1, 0.3790889084339142: 1, 0.7587617635726929: 1, 0.6585915088653564: 1, -1.2009185552597046: 1, 0.043348729610443115: 1, -1.192414402961731: 1, -0.4258005619049072: 1, 0.19746625423431396: 1, 0.7787987589836121: 1, 1.1413462162017822: 1, 0.7848809361457825: 1, -0.6031686067581177: 1, 0.3299405574798584: 1, 0.9907589554786682: 1, 1.143233299255371: 1, -0.861803412437439: 1, -0.671938955783844: 1, 0.8696494698524475: 1, -0.7864221334457397: 1, -0.136034294962883: 1, -0.32589226961135864: 1, -1.201613426208496: 1, -0.07435453683137894: 1, -0.19534148275852203: 1, -0.15606260299682617: 1, -0.7601802349090576: 1, -1.1549781560897827: 1, -0.5920382142066956: 1, 0.8677871227264404: 1, 0.669349730014801: 1, 0.5067287683486938: 1, 1.0160198211669922: 1, 0.5890273451805115: 1, 0.8621184229850769: 1, -0.5897277593612671: 1, 0.050834186375141144: 1, -1.192929744720459: 1, -0.08644621819257736: 1, -0.7811260223388672: 1, -0.7514510154724121: 1, -1.09720778465271: 1, -0.06760650873184204: 1, -1.201612949371338: 1, -0.5272284746170044: 1, -1.1375747919082642: 1, -1.006115436553955: 1, 1.0866621732711792: 1, -0.3076569437980652: 1, 1.0350605249404907: 1, -0.1227283924818039: 1, -0.7380070686340332: 1, 0.8076177835464478: 1, -0.0404001921415329: 1, 0.6649335622787476: 1, -0.11562295258045197: 1, 1.1690500974655151: 1, -0.7266088128089905: 1, 0.022439440712332726: 1, -0.618209183216095: 1, -0.3780987560749054: 1, 0.0014178809942677617: 1, 1.1997345685958862: 1, -0.36745402216911316: 1, -0.26567548513412476: 1, 0.5923774242401123: 1, -0.19212104380130768: 1, -1.2016348838806152: 1, 0.9105262160301208: 1, 0.9495259523391724: 1, 0.15262554585933685: 1, 0.6296795010566711: 1, 1.0150052309036255: 1, -0.21699510514736176: 1, -0.01294313371181488: 1, -0.1045512706041336: 1, 0.9348665475845337: 1, 0.6152291297912598: 1, -1.0131398439407349: 1, 0.01674770377576351: 1, -0.003650385420769453: 1, -0.3348834216594696: 1, 1.182131290435791: 1, -0.12709279358386993: 1, -0.7992537021636963: 1, -1.1966403722763062: 1, -0.13060888648033142: 1, -0.8245752453804016: 1, -1.198868989944458: 1, 1.1278204917907715: 1, 1.1562855243682861: 1, -0.21517714858055115: 1, 0.9726781249046326: 1, 0.008013189770281315: 1, -0.2588912844657898: 1, 1.3108209371566772: 1, -0.5649014711380005: 1, -0.6869497895240784: 1, -0.09314227104187012: 1, 1.274375557899475: 1, 0.8297379016876221: 1, 0.05186900869011879: 1, -0.5160067081451416: 1, -0.4923703670501709: 1, -1.0983095169067383: 1, 0.21761490404605865: 1, 0.43160757422447205: 1, -0.809281051158905: 1, 1.1265980005264282: 1, 0.02654222585260868: 1, 0.017293494194746017: 1, -0.253052294254303: 1, -0.8852211833000183: 1, -0.12913857400417328: 1, 1.2238743305206299: 1, -1.194933295249939: 1, 0.167218878865242: 1, 1.1819933652877808: 1, -0.5622422695159912: 1, -1.194725751876831: 1, 0.12959904968738556: 1, -1.1653438806533813: 1, -0.27802959084510803: 1, 0.09289468824863434: 1, 1.0399528741836548: 1, 0.5615567564964294: 1, 1.196900725364685: 1, -0.09158548712730408: 1, -0.4809620976448059: 1, 0.2788977324962616: 1, -0.1383906453847885: 1, -0.17342792451381683: 1, 1.2882169485092163: 1, -1.1826090812683105: 1, 0.7786849141120911: 1, -0.19042982161045074: 1, -1.200330376625061: 1, 1.0692790746688843: 1, -0.257107138633728: 1, 0.606712281703949: 1, -0.5744653940200806: 1, -1.182140827178955: 1, 0.028185075148940086: 1, -0.27063482999801636: 1, 0.666642963886261: 1, 0.2699461281299591: 1, -0.024909501895308495: 1, 0.6744810342788696: 1, 1.30207359790802: 1, -0.32544630765914917: 1, 1.2993144989013672: 1, -1.1338447332382202: 1, -0.5088394284248352: 1, -0.3548189401626587: 1, 1.2035483121871948: 1, 1.0808086395263672: 1, -1.2008949518203735: 1, -0.455705851316452: 1, -0.9566242694854736: 1, 0.033837273716926575: 1, -0.999508798122406: 1, -0.05626079440116882: 1, 1.0766181945800781: 1, 0.762050449848175: 1, -0.22990889847278595: 1, 1.2829649448394775: 1, 0.03461417928338051: 1, 0.7100340127944946: 1, 1.030333161354065: 1, 0.5583640336990356: 1, 0.9881972074508667: 1, -1.109034538269043: 1, 0.05442709103226662: 1, -0.26975223422050476: 1, -0.5908447504043579: 1, -1.1654343605041504: 1, -0.7650251388549805: 1, -1.1914066076278687: 1, -0.16474246978759766: 1, 1.1363762617111206: 1, -1.2016372680664062: 1, -1.1513574123382568: 1, 0.8519235253334045: 1, 1.3037681579589844: 1, -0.03992554917931557: 1, 1.0595874786376953: 1, -0.07969757169485092: 1, -1.0076838731765747: 1, -0.2553582787513733: 1, -0.2878003716468811: 1, -0.16514527797698975: 1, 1.023429274559021: 1, -0.7420557737350464: 1, -0.3634156584739685: 1, 1.1214849948883057: 1, -1.1934514045715332: 1, -0.11618001013994217: 1, 0.5070648193359375: 1, -0.019765598699450493: 1, -0.42906203866004944: 1, 1.1092147827148438: 1, -0.7010860443115234: 1, -0.01721060648560524: 1, -0.2265346348285675: 1, -0.9494366645812988: 1, -0.18796367943286896: 1, 1.043671727180481: 1, -0.15634003281593323: 1, -0.34825557470321655: 1, -0.21185417473316193: 1, 0.7746310830116272: 1, -1.1586220264434814: 1, -0.3917173445224762: 1, -0.02432066947221756: 1, 0.2549906373023987: 1, 0.7206960916519165: 1, 0.6609118580818176: 1, -0.6903147101402283: 1, -0.6155209541320801: 1, -1.193916916847229: 1, -0.15946216881275177: 1, -0.06410756707191467: 1, 0.6174435615539551: 1, 0.7346874475479126: 1, -0.29025569558143616: 1, -0.016240552067756653: 1, -0.15591250360012054: 1, -0.07907160371541977: 1, -0.0026253368705511093: 1, -1.0697368383407593: 1, -0.08915898948907852: 1, 0.021945519372820854: 1, 0.004652172327041626: 1, -0.32367783784866333: 1, -0.5897085666656494: 1, -0.23161835968494415: 1, -0.19999556243419647: 1, 0.26051849126815796: 1, 0.053155358880758286: 1, 1.0157313346862793: 1, -0.056251220405101776: 1, 1.1558300256729126: 1, 0.048983871936798096: 1, -0.9567206501960754: 1, -1.0963338613510132: 1, -0.07029284536838531: 1, -0.6019781231880188: 1, 0.6389100551605225: 1, -0.4774172008037567: 1, 0.6907184720039368: 1, -1.162119746208191: 1, -0.63347989320755: 1, -0.5631559491157532: 1, -1.1520825624465942: 1, -1.167817234992981: 1, -0.9714952111244202: 1, -1.0810678005218506: 1, -0.6395750641822815: 1, -1.1800310611724854: 1, -1.194927453994751: 1, -1.1923733949661255: 1, -1.2006478309631348: 1, -1.1784441471099854: 1, -1.1339654922485352: 1, -1.1858601570129395: 1, -0.8829452991485596: 1, -1.1954984664916992: 1, -0.6137503981590271: 1, -0.788809597492218: 1, -1.1685314178466797: 1, -1.0831377506256104: 1, -1.161582589149475: 1, -1.0068711042404175: 1, -1.0668615102767944: 1, -1.191677212715149: 1, -1.0135008096694946: 1, -1.1744723320007324: 1, -0.9300684928894043: 1, -1.193282127380371: 1, -1.157931923866272: 1, -0.7443411946296692: 1, -0.6325321793556213: 1, -0.8867827653884888: 1, -1.194840431213379: 1, -1.0467379093170166: 1, -1.0309724807739258: 1, -1.1989647150039673: 1, -0.8834558129310608: 1, -0.6588079929351807: 1, -1.1501390933990479: 1, -1.201562523841858: 1, -0.6635865569114685: 1, -0.5772318840026855: 1, -1.194190263748169: 1, -0.19914157688617706: 1, -0.2236318439245224: 1, -1.200749397277832: 1, -1.17979097366333: 1, -0.7897263169288635: 1, -1.1350377798080444: 1, -1.126016616821289: 1, -1.1855055093765259: 1, -0.6185922622680664: 1, -0.8899372816085815: 1, -0.7500604391098022: 1, -1.2013036012649536: 1, -1.1936330795288086: 1, -1.170555830001831: 1, -0.957834780216217: 1, -1.0740134716033936: 1, -1.1981604099273682: 1, -1.2015275955200195: 1, -1.2016135454177856: 1, -1.1405699253082275: 1, -1.0521938800811768: 1, -1.0014375448226929: 1, -1.2015451192855835: 1, -1.0986745357513428: 1, -1.1066040992736816: 1, -1.1595861911773682: 1, -1.1821529865264893: 1, -1.076475739479065: 1, -1.2009553909301758: 1, -1.2012261152267456: 1, -1.2014809846878052: 1, -1.1558103561401367: 1, -1.0323843955993652: 1, -0.862193763256073: 1, -1.180148720741272: 1, -1.201621413230896: 1, -1.2008585929870605: 1, -0.9933805465698242: 1, -1.2016000747680664: 1, -0.832706868648529: 1, -1.1563661098480225: 1, -0.9732658267021179: 1, -1.1349726915359497: 1, -1.1859160661697388: 1, -1.1969208717346191: 1, -1.1333073377609253: 1, -1.1816785335540771: 1, -0.17838822305202484: 1, -0.5632508397102356: 1, -1.2016375064849854: 1, -1.1857632398605347: 1, -1.198327660560608: 1, -0.40758535265922546: 1, -1.091170310974121: 1, -0.5115338563919067: 1, -0.2650972902774811: 1, -1.1344302892684937: 1, -0.4444347023963928: 1, -0.21827441453933716: 1, -0.7001152634620667: 1, -1.0461647510528564: 1, -0.9021695256233215: 1, -0.8267379403114319: 1, -1.1486388444900513: 1, -0.25539615750312805: 1, -1.1933567523956299: 1, -0.86516273021698: 1, -1.0501618385314941: 1, -0.9225814342498779: 1, -1.200010895729065: 1, -0.9485399723052979: 1, -1.087907314300537: 1, -0.3156169652938843: 1, -1.2006280422210693: 1, -1.1703253984451294: 1, -1.144382119178772: 1, -0.9726890325546265: 1, -0.9449849724769592: 1, -1.0933367013931274: 1, -0.7420345544815063: 1, 1.7253838777542114: 1, -0.4794164001941681: 1, 5.037554740905762: 1, -1.193596363067627: 1, 4.863577365875244: 1, -0.622269868850708: 1, 5.001932144165039: 1, 4.900933742523193: 1, -1.1445417404174805: 1, -0.7939543128013611: 1, 4.302996635437012: 1, 4.58308219909668: 1, -0.07361169159412384: 1, -1.1480247974395752: 1, 4.875144004821777: 1, 5.066896438598633: 1, 5.072229385375977: 1, 0.4937030076980591: 1, 5.05051851272583: 1, 3.830434560775757: 1, -1.1136521100997925: 1, 5.006033897399902: 1, 5.012721538543701: 1, -1.1754673719406128: 1, -1.1181161403656006: 1, -1.1434556245803833: 1, -1.2015254497528076: 1, -1.1150031089782715: 1, 0.29684698581695557: 1, -0.7968862652778625: 1, -1.145255446434021: 1, -0.9272821545600891: 1, -1.02364981174469: 1, -0.5409148931503296: 1, -0.9141108393669128: 1, -0.712040364742279: 1, -0.8772833347320557: 1, -0.5385211110115051: 1, -0.9129625558853149: 1, -1.0223268270492554: 1, -0.7736660242080688: 1, -0.41631850600242615: 1, -1.0399388074874878: 1, -1.1062737703323364: 1, -0.8366922736167908: 1, -1.0281414985656738: 1, -1.1463063955307007: 1, -1.05448579788208: 1, -0.4774998426437378: 1, -0.75212162733078: 1, -0.8758900761604309: 1, -0.4437340795993805: 1, -1.1940333843231201: 1, -1.0246316194534302: 1, -0.1397843062877655: 1, -0.47614291310310364: 1, -0.8666650652885437: 1, -0.8599510192871094: 1, -0.8791208267211914: 1, -1.0074002742767334: 1, -0.9824236631393433: 1, -0.5638068318367004: 1, -1.00275719165802: 1, -0.6867276430130005: 1, -0.669135332107544: 1, -1.2014120817184448: 1, -0.6967374682426453: 1, -1.0416630506515503: 1, 0.216363787651062: 1, -0.38882899284362793: 1, -1.1946135759353638: 1, -0.5497206449508667: 1, -1.167614459991455: 1, -0.6818874478340149: 1, -1.1019858121871948: 1, -1.1624016761779785: 1, -0.7073397040367126: 1, -0.8054187893867493: 1, 1.7609484195709229: 1, -1.201631784439087: 1, -0.7198786735534668: 1, -0.1808653175830841: 1, -0.17489729821681976: 1, 1.0823159217834473: 1, 0.5601547360420227: 1, 0.6412140727043152: 1, 1.3093386888504028: 1, 1.5706232786178589: 1, 0.5863446593284607: 1, -0.7622874975204468: 1, -0.34684932231903076: 1, 1.585684895515442: 1, -1.2015637159347534: 1, -1.201583743095398: 1, -1.199397087097168: 1, 0.009732205420732498: 1, -1.1621276140213013: 1, -0.45371508598327637: 1, -0.411021888256073: 1, 0.6509128212928772: 1, 1.2748090028762817: 1, 0.2817704975605011: 1, 1.036679744720459: 1, 0.2436400204896927: 1, -0.23251420259475708: 1, -0.15451399981975555: 1, 1.094826102256775: 1, -0.1682627946138382: 1, 1.593440055847168: 1, 1.0043728351593018: 1, 0.6101611256599426: 1, 1.6096405982971191: 1, 0.9798484444618225: 1, -0.6111128330230713: 1, -0.433001846075058: 1, -1.0877141952514648: 1, 0.4647965729236603: 1, 0.9053057432174683: 1, 1.6532078981399536: 1, 1.5002496242523193: 1, -1.1626108884811401: 1, 0.5944019556045532: 1, 1.8255465030670166: 1, -0.44575440883636475: 1, 1.2443398237228394: 1, 0.8030492067337036: 1, -1.2016305923461914: 1, -1.2016229629516602: 1, -0.3316475749015808: 1, -1.084214448928833: 1, 0.21256738901138306: 1, -0.8594576716423035: 1, -0.35862404108047485: 1, 0.605282723903656: 1, -1.1689379215240479: 1, -0.8211961388587952: 1, 0.369517058134079: 1, 1.2405850887298584: 1, 0.842113196849823: 1, -0.9499993324279785: 1, 0.5858985185623169: 1, 0.48456287384033203: 1, -0.30494359135627747: 1, -0.5739816427230835: 1, 0.7670885920524597: 1, -1.2016373872756958: 1, -0.576421856880188: 1, 0.821479320526123: 1, -1.1690752506256104: 1, 1.2671806812286377: 1, 0.880368709564209: 1, 0.5329916477203369: 1, 0.5780434608459473: 1, 1.229008674621582: 1, -0.7776852250099182: 1, -0.32108673453330994: 1, -1.1028145551681519: 1, -0.4003660976886749: 1, -0.1892606019973755: 1, -0.8111313581466675: 1, -0.4005826711654663: 1, -1.1959316730499268: 1, -0.9352316856384277: 1, -0.5334532856941223: 1, -0.5885310769081116: 1, 1.2156920433044434: 1, 0.6975425481796265: 1, 0.9127581715583801: 1, 0.4960012137889862: 1, 1.0147548913955688: 1, -0.35478705167770386: 1, -0.44190311431884766: 1, -1.1707801818847656: 1, 1.2256038188934326: 1, 0.9517895579338074: 1, 0.8852934837341309: 1, -1.2010202407836914: 1, 1.5164122581481934: 1, -0.7359880805015564: 1, -0.459964781999588: 1, -1.1054575443267822: 1, 0.741217315196991: 1, 1.8886953592300415: 1, 0.19575154781341553: 1, -1.1993547677993774: 1, -1.2002415657043457: 1, -1.1929975748062134: 1, 0.3444092869758606: 1, -1.2014538049697876: 1, 0.7184975147247314: 1, -1.157366394996643: 1, -1.194710373878479: 1, 0.6086026430130005: 1, 0.275499552488327: 1, -1.171040415763855: 1, -1.1539435386657715: 1, -1.1912822723388672: 1, -1.1865227222442627: 1, -1.1973918676376343: 1, -0.4641222357749939: 1, -1.195172905921936: 1, -1.1910632848739624: 1, -0.6273359060287476: 1, 5.645717620849609: 1, -1.18364417552948: 1, -0.5079992413520813: 1, -1.1857080459594727: 1, -1.0951330661773682: 1, -1.0865159034729004: 1, -1.1643073558807373: 1, -1.1970044374465942: 1, -0.40432149171829224: 1, -1.1610828638076782: 1, -1.188031554222107: 1, -0.6870049834251404: 1, -1.1676386594772339: 1, -1.186979055404663: 1, -1.1963162422180176: 1, -1.201351284980774: 1, -1.0963668823242188: 1, -0.9010990262031555: 1, -0.5026354193687439: 1, -1.1365838050842285: 1, -1.196357011795044: 1, -1.1999870538711548: 1, -0.7029581665992737: 1, -1.0972120761871338: 1, -1.1882411241531372: 1, -1.1971783638000488: 1, -0.4477367699146271: 1, -1.016434669494629: 1, -0.31994664669036865: 1, -0.7167530059814453: 1, 0.18603742122650146: 1, -0.9458178281784058: 1, -0.39169713854789734: 1, 0.7324214577674866: 1, -1.201418161392212: 1, 1.6113003492355347: 1, 0.29118791222572327: 1, -0.2950673997402191: 1, -0.15851348638534546: 1, -1.1721446514129639: 1, -0.23552395403385162: 1, 1.6066374778747559: 1, 0.8136993050575256: 1, 0.5599294900894165: 1, -0.6900844573974609: 1, -0.726171612739563: 1, 0.7067909836769104: 1, -1.2015913724899292: 1, -1.188680648803711: 1, -0.40274444222450256: 1, -1.201563835144043: 1, -1.1986582279205322: 1, -1.198758602142334: 1, -1.1819733381271362: 1, -1.1612766981124878: 1, -1.1862393617630005: 1, -1.1970906257629395: 1, -1.1995155811309814: 1, -0.9958106875419617: 1, 0.4647902548313141: 1, -1.1924408674240112: 1, -1.1728081703186035: 1, -1.1948115825653076: 1, -1.1685289144515991: 1, -1.1994209289550781: 1, -1.173497200012207: 1, -0.638097882270813: 1, -0.6645460724830627: 1, -1.2015430927276611: 1, 0.23683039844036102: 1, 0.05473056063055992: 1, -0.04312499612569809: 1, 1.0467203855514526: 1, -0.35746997594833374: 1, -1.1579349040985107: 1, 1.6200270652770996: 1, 0.2561040222644806: 1, 0.6983891129493713: 1, 1.431997537612915: 1, 0.06706182658672333: 1, 0.18826737999916077: 1, 0.30889958143234253: 1, -0.632728099822998: 1, 0.3140702247619629: 1, -0.10326965153217316: 1, -0.11514480412006378: 1, 0.12041562795639038: 1, -0.6454114317893982: 1, 0.7718502283096313: 1, 0.05425111949443817: 1, 0.3626178801059723: 1, 0.4515918791294098: 1, 0.20913533866405487: 1, 1.622856616973877: 1, -0.40100592374801636: 1, -0.20442236959934235: 1, 1.4727312326431274: 1, 0.7984791398048401: 1, 1.914624810218811: 1, 0.3338538706302643: 1, 0.06557659059762955: 1, 0.11681299656629562: 1, 0.2539007067680359: 1, 1.3110779523849487: 1, 1.55558443069458: 1, -0.16230207681655884: 1, 1.4751214981079102: 1, 0.3319496512413025: 1, 0.858607292175293: 1, 1.282376766204834: 1, 0.7782679200172424: 1, 1.0659617185592651: 1, 0.987504780292511: 1, 0.22794750332832336: 1, 1.1887938976287842: 1, 0.9434877038002014: 1, -0.1031346470117569: 1, -0.22873257100582123: 1, 0.22853031754493713: 1, -0.6209532618522644: 1, -0.0526600144803524: 1, -0.21601253747940063: 1, 0.33931559324264526: 1, 1.3110328912734985: 1, 1.5503476858139038: 1, 0.24058645963668823: 1, 1.556430697441101: 1, 0.6647680401802063: 1, -0.10372047126293182: 1, 1.1246896982192993: 1, 0.7825908064842224: 1, 1.260263442993164: 1, 1.375723958015442: 1, 1.4145740270614624: 1, 0.7438257932662964: 1, -0.2681446373462677: 1, 0.18295887112617493: 1, 0.2222539335489273: 1, 0.3024345636367798: 1, 0.276736855506897: 1, 1.6643083095550537: 1, 1.4752575159072876: 1, -0.10448039323091507: 1, -0.19414900243282318: 1, -1.1556631326675415: 1, -0.2946179509162903: 1, 0.07517638802528381: 1, 0.2801852524280548: 1, -0.6031805276870728: 1, 0.592692494392395: 1, 0.3425867557525635: 1, 1.3231751918792725: 1, 0.35902467370033264: 1, 1.561331033706665: 1, 0.23767612874507904: 1, -0.13019442558288574: 1, 0.14948004484176636: 1, -0.1196289211511612: 1, 0.11391282081604004: 1, 1.2999117374420166: 1, -0.44012218713760376: 1, 1.4899855852127075: 1, 0.3157300651073456: 1, 1.5044559240341187: 1, -0.6113037467002869: 1, 1.1449819803237915: 1, -0.9236016869544983: 1, 0.7352478504180908: 1, 1.0590577125549316: 1, 1.4972656965255737: 1, 1.0554699897766113: 1, -0.31073465943336487: 1, 0.21012525260448456: 1, 0.753506600856781: 1, 0.8502970337867737: 1, 1.0611008405685425: 1, -0.20083148777484894: 1, -1.0867854356765747: 1, 0.9222752451896667: 1, 1.0225938558578491: 1, -0.16844011843204498: 1, -0.22581757605075836: 1, -0.4890022277832031: 1, 0.8458276391029358: 1, 0.29691436886787415: 1, 1.0407052040100098: 1, 0.8794386982917786: 1, 1.244690179824829: 1, 0.35881760716438293: 1, 1.5059622526168823: 1, 1.059990644454956: 1, -0.4757806956768036: 1, 0.021963730454444885: 1, 0.3591007590293884: 1, -0.0920228511095047: 1, 1.5724915266036987: 1, 0.7346283793449402: 1, 1.466652274131775: 1, 1.111115574836731: 1, 1.189407229423523: 1, 0.05546194687485695: 1, -1.140577793121338: 1, 0.31476086378097534: 1, -1.201634168624878: 1, -1.1903178691864014: 1, -1.171350359916687: 1, -1.150158166885376: 1, -1.201610803604126: 1, -1.1100589036941528: 1, 0.3900337219238281: 1, -0.6426911950111389: 1, -0.9999420642852783: 1, -1.201622486114502: 1, -0.8209242224693298: 1, -1.2014681100845337: 1, -0.06736226379871368: 1, 0.34270647168159485: 1, -0.5888482928276062: 1, -0.5093275308609009: 1, -1.2015849351882935: 1, -0.0922759622335434: 1, 0.3153885304927826: 1, 0.9252344369888306: 1, 0.10832516103982925: 1, 1.5909359455108643: 1, 0.017385760322213173: 1, -0.014552570879459381: 1, -0.5199218392372131: 1, 0.5634517669677734: 1, 0.9224774241447449: 1, 1.533963680267334: 1, 1.3359390497207642: 1, 1.4527193307876587: 1, -0.03743843734264374: 1, 1.5988372564315796: 1, 1.3192821741104126: 1, 1.533202886581421: 1, 0.31844547390937805: 1, 0.19674475491046906: 1, -0.3147757053375244: 1, -0.1758507341146469: 1, -0.11516252905130386: 1, 0.7623692750930786: 1, -1.1871042251586914: 1, 0.7778939008712769: 1, 1.5112932920455933: 1, -0.1960129290819168: 1, -1.1865193843841553: 1, -1.0057076215744019: 1, 1.466456651687622: 1, -0.5517370104789734: 1, 1.3268651962280273: 1, 0.01178812701255083: 1, -0.3219013214111328: 1, 0.8655160665512085: 1, 0.8642333745956421: 1, 0.3231067657470703: 1, -0.5888111591339111: 1, 0.5367603302001953: 1, -0.06798223406076431: 1, 1.3918044567108154: 1, 1.3780083656311035: 1, 1.1870133876800537: 1, 1.126368761062622: 1, 1.2197668552398682: 1, 0.004675476811826229: 1, 0.7520656585693359: 1, 1.0209075212478638: 1, 1.5651975870132446: 1, 0.7781831622123718: 1, 1.4282907247543335: 1, -0.11230547726154327: 1, 0.19260334968566895: 1, 0.36868804693222046: 1, -0.48821160197257996: 1, 1.4599251747131348: 1, 1.5357881784439087: 1, 1.6108869314193726: 1, 1.0432312488555908: 1, -1.1632437705993652: 1, -1.1837621927261353: 1, 1.5597952604293823: 1, 1.5033998489379883: 1, 0.22126778960227966: 1, 0.09744428098201752: 1, -0.8828660249710083: 1, -0.19494549930095673: 1, -0.16245944797992706: 1, 0.05755604803562164: 1, 0.31122344732284546: 1, 1.5557059049606323: 1, -0.7541334629058838: 1, 0.2642950117588043: 1, 0.8805737495422363: 1, 0.9053120613098145: 1, -0.17545948922634125: 1, 0.263934850692749: 1, 0.6183062195777893: 1, 1.3463716506958008: 1, 1.3844947814941406: 1, 0.24872112274169922: 1, -0.6971253156661987: 1, 1.3494865894317627: 1, 1.2778698205947876: 1, 1.3597513437271118: 1, 1.368375539779663: 1, 0.8933335542678833: 1, 0.0863155722618103: 1, -0.11868320405483246: 1, 0.3126457929611206: 1, 1.5239653587341309: 1, 0.5069756507873535: 1, 1.5916389226913452: 1, 1.3772739171981812: 1, 0.19180983304977417: 1, -0.43688738346099854: 1, 0.8997297883033752: 1, 0.9983642101287842: 1, 1.5009726285934448: 1, 1.133412480354309: 1, 0.8847638368606567: 1, 1.3578754663467407: 1, 1.547389030456543: 1, -0.07051629573106766: 1, 0.11520501971244812: 1, 0.7684015035629272: 1, 0.22540217638015747: 1, -0.9639919996261597: 1, 0.7280606031417847: 1, 1.5217971801757812: 1, 0.008224710822105408: 1, -0.30212122201919556: 1, -0.091583751142025: 1, 0.2824403941631317: 1, -0.009973025880753994: 1, 0.3276282548904419: 1, 0.3697844445705414: 1, -1.2015975713729858: 1, 1.2598285675048828: 1, -1.086830973625183: 1, -0.6757361888885498: 1, 0.6463801264762878: 1, 1.4938876628875732: 1, 0.7275790572166443: 1, 1.556864857673645: 1, -0.20395949482917786: 1, -0.04237562045454979: 1, -0.13872799277305603: 1, 0.8999701142311096: 1, -1.1716605424880981: 1, 0.2692132890224457: 1, 1.3759030103683472: 1, -0.025239035487174988: 1, 1.4103199243545532: 1, 1.4127790927886963: 1, 0.8287729024887085: 1, -0.11932859569787979: 1, 1.5179330110549927: 1, -0.20943275094032288: 1, 0.3431006968021393: 1, 0.35639217495918274: 1, 0.7253832817077637: 1, 1.5550216436386108: 1, 0.80833899974823: 1, 2.0702569484710693: 1, 0.9344565272331238: 1, -1.1040070056915283: 1, -0.004799619782716036: 1, 1.1921144723892212: 1, 0.7161386013031006: 1, 0.27865079045295715: 1, 0.7332795858383179: 1, -0.5194405913352966: 1, 0.009009350091218948: 1, 1.4580349922180176: 1, 0.9064841866493225: 1, 1.0842758417129517: 1, 0.7159395813941956: 1, -0.20360040664672852: 1, -0.10053509473800659: 1, 0.5472216606140137: 1, 0.8098611235618591: 1, 0.6104775071144104: 1, 0.39806419610977173: 1, 1.473071813583374: 1, 1.069445013999939: 1, 1.5127235651016235: 1, -0.30856987833976746: 1, 0.1911333203315735: 1, 0.39437854290008545: 1, 1.4644227027893066: 1, 0.2846507132053375: 1, 1.5425565242767334: 1, -0.41700541973114014: 1, 1.5978120565414429: 1, 0.881543755531311: 1, -1.1900941133499146: 1, 0.3661552369594574: 1, 1.5401815176010132: 1, 1.4411144256591797: 1, -0.15476621687412262: 1, 1.2591054439544678: 1, 1.622040867805481: 1, 1.0418422222137451: 1, 1.3920340538024902: 1, 1.0482161045074463: 1, 0.44933900237083435: 1, 0.5573470592498779: 1, -0.5775644183158875: 1, 0.81052166223526: 1, 1.215183138847351: 1, 1.3900312185287476: 1, 0.3554361164569855: 1, 0.9935461282730103: 1, 0.2826254069805145: 1, -0.3440307080745697: 1, 0.3602719008922577: 1, 0.04298178479075432: 1, 0.30982303619384766: 1, 0.2596873939037323: 1, 1.4639532566070557: 1, 1.1571227312088013: 1, -0.30866673588752747: 1, 1.540098786354065: 1, 1.4051384925842285: 1, 0.02118554152548313: 1, 1.1854524612426758: 1, 1.5040947198867798: 1, 0.8433452844619751: 1, 0.8512904644012451: 1, 0.5769325494766235: 1, 1.5063830614089966: 1, -0.4644731879234314: 1, 1.4151798486709595: 1, -0.11862857639789581: 1, -0.11317183822393417: 1, 0.3683989942073822: 1, 0.1563034951686859: 1, 0.35628634691238403: 1, -1.0716415643692017: 1, 0.30258285999298096: 1, -0.4737872779369354: 1, -0.2417261004447937: 1, 1.1731380224227905: 1, 0.6668532490730286: 1, 1.0762102603912354: 1, 1.4793431758880615: 1, 0.5000193119049072: 1, 0.9953116178512573: 1, 1.302850365638733: 1, 0.7008569240570068: 1, 0.9266675710678101: 1, 0.8342987895011902: 1, 0.31707215309143066: 1, 0.968934953212738: 1, 0.98076331615448: 1, 1.0041277408599854: 1, 0.23464715480804443: 1, -1.1742432117462158: 1, 0.9422100782394409: 1, 1.4056357145309448: 1, 2.4050354957580566: 1, -0.09898030012845993: 1, 0.29443448781967163: 1, 0.06307864934206009: 1, -0.20047886669635773: 1, 0.9952307939529419: 1, 0.8222253322601318: 1, -0.28925973176956177: 1, 1.5599995851516724: 1, -1.0314160585403442: 1, 1.5686708688735962: 1, -0.08143828064203262: 1, 0.2925977110862732: 1, -0.2210041582584381: 1, 0.20136801898479462: 1, -1.180970549583435: 1, -1.2015372514724731: 1, 0.8834699392318726: 1, -0.24570585787296295: 1, -0.28629931807518005: 1, -0.2671576142311096: 1, 0.009196557104587555: 1, 1.256977915763855: 1, 0.23997803032398224: 1, 0.24701066315174103: 1, 1.1922990083694458: 1, -0.8705235719680786: 1, -1.1247143745422363: 1, -0.007039392367005348: 1, -0.6103007793426514: 1, -0.621107280254364: 1, 0.23272329568862915: 1, 1.0945255756378174: 1, -0.9168434739112854: 1, -0.8882886171340942: 1, -0.09275590628385544: 1, -0.06864805519580841: 1, -0.20541705191135406: 1, 0.8532567620277405: 1, 0.23268936574459076: 1, 0.889022946357727: 1, 0.9730016589164734: 1, 1.009254813194275: 1, -0.8759819269180298: 1, 0.7264453172683716: 1, 1.2736730575561523: 1, -0.16393840312957764: 1, 0.840314507484436: 1, -0.24846623837947845: 1, -1.2016277313232422: 1, -1.2004797458648682: 1, -0.0411478653550148: 1, -1.1211071014404297: 1, -0.42487016320228577: 1, 0.5979693531990051: 1, 0.8318881988525391: 1, 1.0863690376281738: 1, 0.5795131325721741: 1, 0.7288377285003662: 1, 1.3004077672958374: 1, -1.0489486455917358: 1, -0.012521528638899326: 1, -0.4453357756137848: 1, -0.12074612081050873: 1, 1.2192449569702148: 1, 0.6013088822364807: 1, -0.35842111706733704: 1, -1.126828908920288: 1, -0.2664187550544739: 1, 0.729964017868042: 1, -0.8322789072990417: 1, -0.33581042289733887: 1, 0.4259852468967438: 1, -0.2597183287143707: 1, 0.6872115731239319: 1, 1.035197138786316: 1, -0.1517976075410843: 1, 1.196738839149475: 1, -0.16826894879341125: 1, -0.09905305504798889: 1, 1.2749443054199219: 1, 1.113309621810913: 1, 1.2229245901107788: 1, -1.2015588283538818: 1, -0.7430113554000854: 1, 1.3061707019805908: 1, -1.1755220890045166: 1, -1.0965174436569214: 1, -0.08886344730854034: 1, 0.015011060051620007: 1, 1.2978951930999756: 1, 0.8119110465049744: 1, 1.185150146484375: 1, -0.25068432092666626: 1, 0.9848727583885193: 1, -0.1916339248418808: 1, 0.938378632068634: 1, 0.9840258359909058: 1, 0.008470889180898666: 1, -0.2164342701435089: 1, -0.39580973982810974: 1, -1.201277732849121: 1, -0.27776581048965454: 1, -1.1879688501358032: 1, 1.2413678169250488: 1, 1.2350752353668213: 1, -0.2299586683511734: 1, 0.9413108229637146: 1, 0.8562594652175903: 1, 0.8952587246894836: 1, -0.4684484004974365: 1, 1.150696039199829: 1, 0.6777730584144592: 1, 1.0972230434417725: 1, 0.7325264811515808: 1, 1.1694233417510986: 1, 0.838370680809021: 1, -0.18254651129245758: 1, 0.004906347021460533: 1, -0.15267345309257507: 1, -0.19495585560798645: 1, -0.3641962707042694: 1, -1.2016282081604004: 1, 1.0062413215637207: 1, -0.02250954695045948: 1, -0.23942992091178894: 1, -0.11078709363937378: 1, -1.095828652381897: 1, 1.2373515367507935: 1, -0.586733877658844: 1, -0.3953478932380676: 1, 0.7884150743484497: 1, -0.4121406674385071: 1, -1.0349785089492798: 1, 1.0134633779525757: 1, 0.44458866119384766: 1, 1.0013794898986816: 1, -0.9308528304100037: 1, -0.9307324290275574: 1, -0.35524412989616394: 1, -0.23531334102153778: 1, 0.0796559751033783: 1, -0.1827705055475235: 1, -0.11709770560264587: 1, -0.025104349479079247: 1, -0.17582924664020538: 1, 0.32448264956474304: 1, 1.2197721004486084: 1, -0.7624772787094116: 1, 0.6605957746505737: 1, -0.6921688914299011: 1, -0.019019143655896187: 1, -0.16533638536930084: 1, 1.104429006576538: 1, 0.525627076625824: 1, 0.31131237745285034: 1, 1.2513697147369385: 1, -0.6482374668121338: 1, -1.1878859996795654: 1, -0.8494775891304016: 1, -1.1774789094924927: 1, -0.12005668878555298: 1, 1.1650327444076538: 1, 1.1985303163528442: 1, -0.13838951289653778: 1, -0.39703771471977234: 1, -1.0583271980285645: 1, -0.24156887829303741: 1, -0.816495954990387: 1, 0.033690646290779114: 1, 0.8838387131690979: 1, -0.9900954961776733: 1, -0.1855221837759018: 1, 1.0187321901321411: 1, 1.1683013439178467: 1, -1.1928194761276245: 1, 1.1942393779754639: 1, 1.179612159729004: 1, 1.2479115724563599: 1, -0.22866979241371155: 1, -1.1798354387283325: 1, 0.6535213589668274: 1, 1.2529374361038208: 1, -0.37873539328575134: 1, 1.2188843488693237: 1, 0.9235488772392273: 1, 0.4542813301086426: 1, 0.8476697206497192: 1, -0.22304323315620422: 1, -0.12660875916481018: 1, 0.4205377995967865: 1, -0.6826592683792114: 1, 0.41254743933677673: 1, -0.42109400033950806: 1, 0.042822714895009995: 1, 1.1279875040054321: 1, 0.7561059594154358: 1, -0.11954380571842194: 1, -1.1981785297393799: 1, 0.1316474825143814: 1, 1.039072036743164: 1, -0.0449078269302845: 1, -0.24153171479701996: 1, -0.8713244199752808: 1, -0.14272816479206085: 1, -1.2016297578811646: 1, 1.1865397691726685: 1, 0.02958042360842228: 1, -0.04175446555018425: 1, -0.2098180055618286: 1, -0.1759326308965683: 1, -0.19223442673683167: 1, 0.7244752645492554: 1, -0.29299479722976685: 1, 0.4987215995788574: 1, -1.1905653476715088: 1, 1.310013771057129: 1, -0.41403406858444214: 1, -1.201407790184021: 1, -1.2015341520309448: 1, -1.1987295150756836: 1, -0.6771114468574524: 1, -1.1967262029647827: 1, -0.07099064439535141: 1, 0.1497194468975067: 1, -1.2014774084091187: 1, -1.2015197277069092: 1, -1.201634407043457: 1, -1.201493740081787: 1, -1.2016335725784302: 1, 0.029840881004929543: 1, -0.7989376187324524: 1, -1.193373441696167: 1, 0.3015405535697937: 1, -1.1923046112060547: 1, -0.053435858339071274: 1, -1.1363192796707153: 1, -1.0203382968902588: 1, 0.2464127391576767: 1, -1.2016382217407227: 1, -1.201630711555481: 1, -1.2016255855560303: 1, -1.2016292810440063: 1, -1.2016286849975586: 1, 0.36895880103111267: 1, -1.2016350030899048: 1, 0.0891273021697998: 1, -1.201523780822754: 1, -1.2015316486358643: 1, -1.1484540700912476: 1, -1.201595425605774: 1, -1.201473593711853: 1, 0.19668835401535034: 1, 2.26233172416687: 1, -1.2015864849090576: 1, -0.848010778427124: 1, -0.3404213488101959: 1, -1.1948002576828003: 1, -0.5937166213989258: 1, 4.363720893859863: 1, -1.1943039894104004: 1, -1.2015916109085083: 1, -1.179785132408142: 1, -0.11716806888580322: 1, -0.14289136230945587: 1, 0.9705496430397034: 1, -0.4132004976272583: 1, -1.2015154361724854: 1, -1.2011888027191162: 1, -0.006573406048119068: 1, 0.07849415391683578: 1, 0.86496901512146: 1, -0.21048426628112793: 1, 0.8498935103416443: 1, -1.201079249382019: 1, -0.9951181411743164: 1, -1.2016206979751587: 1, -1.2003253698349: 1, 0.5647678971290588: 1, -1.1779894828796387: 1, -0.9857455492019653: 1, 0.15111742913722992: 1, -0.009843221865594387: 1, -1.2016316652297974: 1, -0.5475073456764221: 1, -1.2014092206954956: 1, -1.2012838125228882: 1, -1.2016196250915527: 1, -1.2016048431396484: 1, -1.201635718345642: 1, -1.2015061378479004: 1, -0.5612114071846008: 1, -1.2014601230621338: 1, -0.4687884449958801: 1, -1.2016345262527466: 1, -1.1849600076675415: 1, -1.2015478610992432: 1, -0.08719541132450104: 1, 0.1322988122701645: 1, -1.2016332149505615: 1, -1.0628424882888794: 1, -0.6676098704338074: 1, -0.6758065819740295: 1, -1.1954847574234009: 1, -1.2016379833221436: 1, -1.194821834564209: 1, -1.2013384103775024: 1, -1.1490250825881958: 1, 0.1581522524356842: 1, -0.28742867708206177: 1, -0.518195390701294: 1, -0.5834662318229675: 1, -0.9676279425621033: 1, -1.2015115022659302: 1, -1.1257261037826538: 1, -1.2016159296035767: 1, -1.201620101928711: 1, -1.201588749885559: 1, -1.2015702724456787: 1, -1.1634924411773682: 1, -0.4360010027885437: 1, -0.0580214224755764: 1, 0.8548043966293335: 1, 1.2064505815505981: 1, 0.18457916378974915: 1, 0.9529565572738647: 1, 0.01943967677652836: 1, 0.04932570457458496: 1, -0.9972583055496216: 1, 1.2879470586776733: 1, -0.9601404666900635: 1, -0.8751402497291565: 1, -1.1107620000839233: 1, 0.9265954494476318: 1, 0.9066123366355896: 1, -0.9734629392623901: 1, -1.193730115890503: 1, -0.19123347103595734: 1, 0.8673886060714722: 1, -0.011501064524054527: 1, -0.3390061855316162: 1, -0.3879204988479614: 1, 0.04192548990249634: 1, -1.1947468519210815: 1, -0.14633353054523468: 1, 0.09383262693881989: 1, 0.36468705534935: 1, 0.5362502932548523: 1, 0.5997416377067566: 1, 0.6232529878616333: 1, -0.14243346452713013: 1, -1.1812138557434082: 1, -0.05695538595318794: 1, 0.8218473196029663: 1, 0.2588636577129364: 1, 0.9394524693489075: 1, -0.1427413374185562: 1, -0.0332360677421093: 1, -0.34523066878318787: 1, 0.01664043217897415: 1, -0.6421114802360535: 1, -0.3469093143939972: 1, -0.029267514124512672: 1, -1.1963841915130615: 1, -0.21881107985973358: 1, 0.33690160512924194: 1, 0.3886134922504425: 1, -1.175829291343689: 1, -0.16572129726409912: 1, 0.8222996592521667: 1, -0.47594985365867615: 1, 0.5694330334663391: 1, 1.0520529747009277: 1, 0.9786769151687622: 1, -0.10290281474590302: 1, -0.040736664086580276: 1, -0.2904500663280487: 1, -0.16829612851142883: 1, -0.1688508540391922: 1, 0.6481048464775085: 1, 1.4819786548614502: 1, -0.2126571536064148: 1, 1.018097996711731: 1, -0.3213060796260834: 1, -1.1992485523223877: 1, 0.2079305797815323: 1, -0.10643515735864639: 1, -1.1586899757385254: 1, -1.0591267347335815: 1, -1.2015695571899414: 1, -1.2016273736953735: 1, 1.0808240175247192: 1, 0.09893729537725449: 1, 3.9672465324401855: 1, 1.1995047330856323: 1, -0.463926762342453: 1, -0.37206733226776123: 1, 0.7298784255981445: 1, 0.8610304594039917: 1, 1.0769490003585815: 1, -0.11945565789937973: 1, 1.2526917457580566: 1, 0.3951069414615631: 1, 1.0069524049758911: 1, 0.8965467214584351: 1, -1.0558182001113892: 1, 1.1649004220962524: 1, -0.34107181429862976: 1, -0.33075013756752014: 1, 1.1717408895492554: 1, -0.24580752849578857: 1, -0.10037212073802948: 1, -1.0237786769866943: 1, 0.9712859988212585: 1, 1.176633596420288: 1, -0.04390183836221695: 1, -1.2003904581069946: 1} test data: {-1.2016253471374512: 2, -1.201635479927063: 2, -1.2016377449035645: 2, -1.2016324996948242: 2, -1.201621651649475: 2, -1.2016383409500122: 2, 0.11128426343202591: 1, -0.16336557269096375: 1, 1.1870161294937134: 1, 0.5256680846214294: 1, 1.3986883163452148: 1, 1.4535586833953857: 1, 1.2952117919921875: 1, 0.5299685597419739: 1, 1.478103518486023: 1, -0.4970245659351349: 1, 0.08119866997003555: 1, 1.3501269817352295: 1, -0.35442060232162476: 1, 0.20332399010658264: 1, 0.8313724994659424: 1, 0.6573249697685242: 1, -0.6442912220954895: 1, 0.2669691741466522: 1, 1.5410983562469482: 1, 1.3818247318267822: 1, -0.24468590319156647: 1, -0.9010018706321716: 1, 0.057195477187633514: 1, 1.3083291053771973: 1, 1.6950387954711914: 1, 0.3245508372783661: 1, 1.2379846572875977: 1, 0.0010552277090027928: 1, -1.0873202085494995: 1, 0.31522658467292786: 1, 0.2743425965309143: 1, -0.026365874335169792: 1, 0.015656888484954834: 1, 0.643775999546051: 1, 1.5088152885437012: 1, 0.17204155027866364: 1, 0.04832748696208: 1, -0.17784874141216278: 1, 1.595760464668274: 1, 0.8440163731575012: 1, 0.26908448338508606: 1, 0.06883639097213745: 1, 0.2954026460647583: 1, -0.18418414890766144: 1, 0.9998847246170044: 1, -1.1840753555297852: 1, 0.21748410165309906: 1, -0.1421377956867218: 1, 0.9703378081321716: 1, -1.1506352424621582: 1, -1.201622486114502: 1, 0.1561044156551361: 1, 0.7523799538612366: 1, -1.201615810394287: 1, -1.2014594078063965: 1, -0.27123570442199707: 1, 0.5888639092445374: 1, -1.201310396194458: 1, -0.9657180905342102: 1, -1.2000701427459717: 1, -0.9927355051040649: 1, -1.196489930152893: 1, -1.1871466636657715: 1, -1.1939657926559448: 1, -1.2016159296035767: 1, -1.2008297443389893: 1, -1.1962995529174805: 1, -1.1956098079681396: 1, -1.1269394159317017: 1, -0.15980762243270874: 1, -0.8842195272445679: 1, -1.201614260673523: 1, -1.2016280889511108: 1, 1.4500402212142944: 1, -1.1999688148498535: 1, 1.6047093868255615: 1, -1.1937233209609985: 1, -1.1647279262542725: 1, -1.2015366554260254: 1, -0.6204319000244141: 1, -1.2013626098632812: 1, -1.201349139213562: 1, -1.092740535736084: 1, -1.1730265617370605: 1, 0.031881630420684814: 1, -1.2016046047210693: 1, -0.4260459542274475: 1, -0.3525408208370209: 1, -1.180529236793518: 1, -1.1526696681976318: 1, -1.0348321199417114: 1, -1.2015955448150635: 1, -1.2016127109527588: 1, 0.10008653253316879: 1, -0.9153497219085693: 1, -0.5025617480278015: 1, 1.088638186454773: 1, 1.286658525466919: 1, 1.1079081296920776: 1, -0.2589719295501709: 1, -1.1845873594284058: 1, -0.36594024300575256: 1, -1.194373369216919: 1, -0.9642779231071472: 1, -1.2012841701507568: 1, -0.2147151529788971: 1, -0.9940837025642395: 1, -0.8935588002204895: 1, -1.18907630443573: 1, -1.0768063068389893: 1, -0.6980454921722412: 1, -1.199107050895691: 1, -1.1412583589553833: 1, 0.6965684294700623: 1, -0.9432769417762756: 1, -0.41735291481018066: 1, 1.5167564153671265: 1, -0.2154475301504135: 1, -0.04164140671491623: 1, -1.0622771978378296: 1, 0.19889964163303375: 1, 1.9090871810913086: 1, -0.9982122778892517: 1, -1.1946264505386353: 1, -1.115770697593689: 1, 0.6749281883239746: 1, 0.31239357590675354: 1, 0.3124358654022217: 1, 0.9760450720787048: 1, 0.3074534237384796: 1, -0.9134752750396729: 1, -0.9460977911949158: 1, 0.49092090129852295: 1, -0.08619289845228195: 1, 1.4876383543014526: 1, -0.90742427110672: 1, -0.3465023636817932: 1, 1.0554637908935547: 1, -1.0539400577545166: 1, -0.8890491724014282: 1, 0.6293842196464539: 1, 1.1045739650726318: 1, 0.04404761642217636: 1, 1.4842547178268433: 1, 0.2662027180194855: 1, 1.6904795169830322: 1, -1.0221163034439087: 1, -0.1391475349664688: 1, -0.5620161294937134: 1, -0.43377333879470825: 1, 0.5685755014419556: 1, 1.0202414989471436: 1, 1.0541952848434448: 1, 0.11351441591978073: 1, 0.1011820137500763: 1, 1.495969295501709: 1, 1.0170906782150269: 1, 1.3002121448516846: 1, 0.4723914861679077: 1, -0.06996402144432068: 1, 1.3024239540100098: 1, 1.2427196502685547: 1, 0.22981515526771545: 1, 0.5786248445510864: 1, -1.113705039024353: 1, 0.4633817672729492: 1, 1.4536134004592896: 1, -0.004194003064185381: 1, -0.22176611423492432: 1, 0.2726168930530548: 1, 0.03207547590136528: 1, 0.4048600494861603: 1, 0.637361466884613: 1, 1.7721431255340576: 1, -0.6611031889915466: 1, 0.9499619007110596: 1, 0.9666041731834412: 1, 1.3437533378601074: 1, -0.8407037854194641: 1, -0.5144169330596924: 1, -0.5850008726119995: 1, 0.9346560835838318: 1, 1.1575602293014526: 1, 0.40835040807724: 1, -0.8622487187385559: 1, -0.38419657945632935: 1, -0.27164560556411743: 1, -0.0960833728313446: 1, -0.8234555125236511: 1, -0.9339156150817871: 1, 0.739153265953064: 1, 0.7724436521530151: 1, 1.3262649774551392: 1, 0.3607413172721863: 1, 1.366266131401062: 1, -0.16177639365196228: 1, -0.539240837097168: 1, -0.589444637298584: 1, 0.31799715757369995: 1, -0.2302703857421875: 1, 1.4342314004898071: 1, 0.48444247245788574: 1, -0.04758370667695999: 1, 1.8893579244613647: 1, 0.29674577713012695: 1, 0.7860816717147827: 1, 1.0254307985305786: 1, -0.12864308059215546: 1, -0.31705647706985474: 1, 0.8423707485198975: 1, -0.19816404581069946: 1, 0.7741647958755493: 1, 0.5059344172477722: 1, 0.26564425230026245: 1, 0.766596257686615: 1, 1.5315228700637817: 1, 0.27857378125190735: 1, 1.7298698425292969: 1, 0.19680047035217285: 1, -0.962668240070343: 1, -1.0099024772644043: 1, -0.807515025138855: 1, -1.2016263008117676: 1, 1.3301595449447632: 1, 1.1998889446258545: 1, 1.545008897781372: 1, 0.04922454059123993: 1, -0.1501469612121582: 1, -0.2874172031879425: 1, 0.6704513430595398: 1, 1.8505216836929321: 1, 1.5747997760772705: 1, -0.01187801081687212: 1, 1.6697852611541748: 1, 0.21643884479999542: 1, 0.21555371582508087: 1, 1.7097703218460083: 1, -0.09393322467803955: 1, 0.11937177926301956: 1, 1.8530430793762207: 1, 1.6143009662628174: 1, -0.4433523118495941: 1, -0.23398016393184662: 1, -0.4275071322917938: 1, 1.231010913848877: 1, 1.5299123525619507: 1, 1.9158005714416504: 1, 1.6179100275039673: 1, -0.22299611568450928: 1, 1.6735926866531372: 1, 0.8739101886749268: 1, -0.9704957008361816: 1, 1.5992790460586548: 1, -0.864342451095581: 1, -1.2015928030014038: 1, 1.2849032878875732: 1, 1.0356202125549316: 1, -0.21755054593086243: 1, 0.12914451956748962: 1, 0.3816104531288147: 1, -1.085170865058899: 1, 0.8646785616874695: 1, 0.2591017186641693: 1, -1.201594352722168: 1, 0.5681759119033813: 1, 1.4854387044906616: 1, -1.1658029556274414: 1, 1.0590068101882935: 1, -1.0813920497894287: 1, 0.10180643945932388: 1, 0.6801310777664185: 1, -1.2011404037475586: 1, 1.339892864227295: 1, 1.1129239797592163: 1, 1.3335411548614502: 1, -0.9954119920730591: 1, 0.9158507585525513: 1, 1.0331618785858154: 1, -0.11489463597536087: 1, 1.9223994016647339: 1, 0.13984030485153198: 1, -1.2016136646270752: 1, 1.5722275972366333: 1, 0.5880253314971924: 1, -0.2810556888580322: 1, 0.6003371477127075: 1, 1.8529928922653198: 1, 1.0374422073364258: 1, 1.9196945428848267: 1, -0.5369133949279785: 1, -0.94952791929245: 1, -0.7923315763473511: 1, -0.8557604551315308: 1, 1.4848576784133911: 1, -1.2014458179473877: 1, 1.636014699935913: 1, -0.9175146222114563: 1, -0.2059621661901474: 1, 1.703546404838562: 1, 0.7037346363067627: 1, 1.0119178295135498: 1, -0.7876200675964355: 1, -0.25207433104515076: 1, -0.9834045767784119: 1, 0.28549933433532715: 1, -0.8708242177963257: 1, 1.8017815351486206: 1, -0.6655375957489014: 1, 0.3796524703502655: 1, 0.5128249526023865: 1, -0.8232764601707458: 1, 1.6888245344161987: 1, -1.0713788270950317: 1, -0.2992294430732727: 1, 1.6146701574325562: 1, -0.29448574781417847: 1, -0.014453819021582603: 1, -0.238590806722641: 1, -0.8769359588623047: 1, 0.012689988128840923: 1, -0.3753896951675415: 1, 1.052090048789978: 1, -0.466978520154953: 1, 1.8227369785308838: 1, -0.7915438413619995: 1, -0.4592617452144623: 1, -1.0387167930603027: 1, -0.8232591152191162: 1, 0.273947536945343: 1, 1.187366247177124: 1, 0.3213244378566742: 1, 1.6493046283721924: 1, -0.5222632884979248: 1, -0.27864202857017517: 1, -0.9769929647445679: 1, -0.4147615134716034: 1, -0.6085047125816345: 1, 0.7517246007919312: 1, 1.5550175905227661: 1, 0.7365891337394714: 1, 1.0860453844070435: 1, -0.8472578525543213: 1, -0.2937408983707428: 1, 1.9010276794433594: 1, -0.5880576372146606: 1, -0.3907565474510193: 1, -1.1868388652801514: 1, -0.9987406134605408: 1, 0.9746946096420288: 1, -0.5378177762031555: 1, -0.960394024848938: 1, -1.1109116077423096: 1, 0.06790906190872192: 1, 3.323413610458374: 1, 0.017721591517329216: 1, -1.1957098245620728: 1, 1.1735796928405762: 1, -0.3992172181606293: 1, -0.25122812390327454: 1, -0.9730278849601746: 1, -1.1925454139709473: 1, -1.2002716064453125: 1, -1.1945738792419434: 1, -1.2015953063964844: 1, -1.1953339576721191: 1, -1.2015608549118042: 1, -1.1207038164138794: 1, -1.2015869617462158: 1, -1.1680830717086792: 1, -0.4622204601764679: 1, -1.155177116394043: 1, 0.5790113210678101: 1, 0.2097160518169403: 1, 1.2531977891921997: 1, 1.7796648740768433: 1, 0.2126206010580063: 1, -1.0319366455078125: 1, 0.2422785758972168: 1, -0.21730461716651917: 1, -0.03495830297470093: 1, -0.3189155161380768: 1, -0.364163339138031: 1, -0.30786851048469543: 1, -0.9930511116981506: 1, 0.5354000329971313: 1, -0.34326422214508057: 1, -0.9299617409706116: 1, 1.8707987070083618: 1, -1.185569167137146: 1, 1.6845048666000366: 1, -0.46113380789756775: 1, -0.8456059098243713: 1, -1.0356733798980713: 1, 0.43118155002593994: 1, -0.2761753499507904: 1, 0.7185215353965759: 1, -0.608808159828186: 1, 0.13262629508972168: 1, 1.629822015762329: 1, -0.43086937069892883: 1, -0.4168057143688202: 1, -1.1617506742477417: 1, -1.061142921447754: 1, 0.18035785853862762: 1, -0.8952922821044922: 1, -0.19083698093891144: 1, 0.6448225975036621: 1, -0.870884895324707: 1, 1.25115966796875: 1, -0.5807573795318604: 1, -1.20154869556427: 1, 1.1807068586349487: 1, -0.8395327925682068: 1, -0.7184330821037292: 1, -1.0178923606872559: 1, -1.083876609802246: 1, -1.0825824737548828: 1, -1.1735186576843262: 1, -0.16215069591999054: 1, -1.187011957168579: 1, -1.1756606101989746: 1, 0.8018452525138855: 1, -1.1992239952087402: 1, 0.264765202999115: 1, -0.8984062075614929: 1, -1.2016023397445679: 1, 0.5111210942268372: 1, -0.12521179020404816: 1, -0.5657321810722351: 1, -0.39080610871315: 1, -0.07614605128765106: 1, -0.005674127489328384: 1, 1.2313082218170166: 1, -1.0416687726974487: 1, -1.144519329071045: 1, 1.1463862657546997: 1, -1.154268741607666: 1, -1.171905517578125: 1, 1.123262643814087: 1, 0.6591589450836182: 1, -0.8244988322257996: 1, -0.5816406607627869: 1, -0.6496835947036743: 1, -0.7252834439277649: 1, -0.7019102573394775: 1, 1.0577486753463745: 1, -0.8736492395401001: 1, -1.2016358375549316: 1, -0.4656817615032196: 1, -1.0180860757827759: 1, -1.195853590965271: 1, -0.5976389050483704: 1, 1.0670119524002075: 1, 1.282943606376648: 1, 0.9692907333374023: 1, -1.2016355991363525: 1, 1.3031054735183716: 1, -1.1632294654846191: 1, 1.0962333679199219: 1, -1.1260875463485718: 1, -1.1991511583328247: 1, -1.0054869651794434: 1, -0.002401667181402445: 1, -1.2016304731369019: 1, -0.6266202926635742: 1, -0.5931430459022522: 1, 0.9011586904525757: 1, -0.23146884143352509: 1, 0.12906195223331451: 1, 0.9278587102890015: 1, -0.3034926950931549: 1, -1.1993433237075806: 1, -0.06601618975400925: 1, -1.2016302347183228: 1, 0.2524677515029907: 1, 1.00320303440094: 1, -0.24131079018115997: 1, -0.048064157366752625: 1, -0.024461327120661736: 1, -0.2519146203994751: 1, -0.06836508214473724: 1, -0.15388239920139313: 1, -1.155191421508789: 1, -1.0579359531402588: 1, -0.9430715441703796: 1, -0.27813780307769775: 1, -0.20172715187072754: 1, -0.4824763238430023: 1, 1.2888545989990234: 1, -1.1091188192367554: 1, 0.8944936394691467: 1, -1.1906999349594116: 1, -0.012192374095320702: 1, -1.1882494688034058: 1, -0.0996609777212143: 1, -1.1139642000198364: 1, 0.944926917552948: 1, 0.00948107335716486: 1, 0.4138732850551605: 1, -0.7673166990280151: 1, -0.15097910165786743: 1, -0.7642948627471924: 1, -0.10418325662612915: 1, -0.5050346255302429: 1, -1.200080394744873: 1, -0.03235474228858948: 1, -0.8901136517524719: 1, -0.3561877906322479: 1, 0.05797763168811798: 1, 1.4735376834869385: 1, -1.1509727239608765: 1, -1.2005667686462402: 1, -1.0699774026870728: 1, -1.0538722276687622: 1, -0.5520062446594238: 1, -0.7228063344955444: 1, -0.9656466245651245: 1, -0.694696843624115: 1, -0.6411266922950745: 1, -1.026742935180664: 1, -0.4338090121746063: 1, -1.1729844808578491: 1, -1.1691778898239136: 1, -1.010536789894104: 1, -1.1722731590270996: 1, -0.4475323557853699: 1, -1.1582452058792114: 1, -1.0160771608352661: 1, -1.1721937656402588: 1, -1.0433918237686157: 1, -1.1470420360565186: 1, -0.5804309248924255: 1, -1.1568188667297363: 1, -1.1212965250015259: 1, 0.18425099551677704: 1, -1.0620733499526978: 1, -1.1790684461593628: 1, 4.722275257110596: 1, 0.939198911190033: 1, -1.1364892721176147: 1, -1.1785725355148315: 1, -0.4824954569339752: 1, -0.6635236740112305: 1, -1.0589720010757446: 1, -1.1036909818649292: 1, -1.1981457471847534: 1, -1.1891672611236572: 1, -1.1977088451385498: 1, -1.1986464262008667: 1, -1.0186684131622314: 1, -1.1468044519424438: 1, -0.967963457107544: 1, -0.9453350901603699: 1, -1.1503796577453613: 1, -1.189503788948059: 1, -0.4648326635360718: 1, -1.0434380769729614: 1, -0.9539603590965271: 1, -1.175873041152954: 1, -1.1832531690597534: 1, -0.750208854675293: 1, 0.6880602836608887: 1, -1.2008846998214722: 1, -1.0349972248077393: 1, -1.0542218685150146: 1, -0.6460915207862854: 1, -1.0081939697265625: 1, -1.1948130130767822: 1, -1.1990872621536255: 1, -0.17054051160812378: 1, -1.1964715719223022: 1, 0.3594037592411041: 1, 0.6052579283714294: 1, -0.9966712594032288: 1, 0.8645955324172974: 1, -0.7970835566520691: 1, 0.5490202307701111: 1, -0.16204535961151123: 1, 0.8697875142097473: 1, 0.7108749151229858: 1, 1.1564749479293823: 1, 0.7936035394668579: 1, -0.9092623591423035: 1, -0.8635501265525818: 1, 1.7719837427139282: 1, -0.9986592531204224: 1, -1.1950762271881104: 1, -1.196737289428711: 1, -0.6249680519104004: 1, -1.1552096605300903: 1, -1.1905823945999146: 1, -1.1300313472747803: 1, -1.1961623430252075: 1, -1.1871360540390015: 1, -0.6205415725708008: 1, -0.7456731200218201: 1, -1.0944702625274658: 1, -1.1929867267608643: 1, -1.1482324600219727: 1, -1.1485586166381836: 1, -0.6259949207305908: 1, -1.1594713926315308: 1, -1.164048194885254: 1, -1.175083041191101: 1, -0.23024950921535492: 1, -1.1811505556106567: 1, 1.6846264600753784: 1, -1.2010711431503296: 1, -1.1966667175292969: 1, 0.05971863865852356: 1, 1.1462621688842773: 1, -0.5774872899055481: 1, -0.9130086302757263: 1, 1.379611611366272: 1, 1.5148382186889648: 1, -0.10369612276554108: 1, 1.481839656829834: 1, 0.5713070034980774: 1, 0.1928955614566803: 1, 0.23876602947711945: 1, -0.1709771454334259: 1, -0.812441349029541: 1, 0.6854439377784729: 1, 1.1509000062942505: 1, 0.2568971812725067: 1, 0.35236239433288574: 1, -0.05793027952313423: 1, 1.233654499053955: 1, 1.3589857816696167: 1, -0.11455459892749786: 1, 0.340713769197464: 1, 0.822748601436615: 1, 0.1144905686378479: 1, 0.2816910445690155: 1, 0.339995801448822: 1, 0.18870316445827484: 1, 1.4786081314086914: 1, 1.4364150762557983: 1, 0.002965901279821992: 1, -1.1383882761001587: 1, -0.416104793548584: 1, -0.11705746501684189: 1, -1.1623584032058716: 1, -0.8935246467590332: 1, 0.4845307767391205: 1, 0.021630888804793358: 1, -1.1736985445022583: 1, 0.26258811354637146: 1, -0.22985504567623138: 1, -0.06609977036714554: 1, 0.11165464669466019: 1, 0.853833794593811: 1, 1.7802213430404663: 1, 0.2615222632884979: 1, 0.7675086855888367: 1, 0.11682117730379105: 1, 0.21266861259937286: 1, -0.03138621151447296: 1, 1.256608247756958: 1, 0.9693878293037415: 1, 0.11328306794166565: 1, 0.5611000061035156: 1, 0.2748369872570038: 1, 0.041503969579935074: 1, 1.471611738204956: 1, 1.4821670055389404: 1, 1.4069277048110962: 1, 0.2389289289712906: 1, 1.1072840690612793: 1, 1.5512200593948364: 1, 0.29415011405944824: 1, 0.8618729114532471: 1, 0.9195832014083862: 1, -0.16789886355400085: 1, -1.1504056453704834: 1, 0.4243623912334442: 1, 1.405086636543274: 1, -0.2038322389125824: 1, 1.1847658157348633: 1, -0.56696617603302: 1, 0.7092652916908264: 1, 0.0384686179459095: 1, 0.8865175843238831: 1, 0.7922017574310303: 1, 1.5027039051055908: 1, -0.9046985507011414: 1, 0.8533394932746887: 1, 0.6865427494049072: 1, 0.8744557499885559: 1, 1.4529069662094116: 1, 1.5731940269470215: 1, 1.2513844966888428: 1, 1.3862555027008057: 1, 1.2155990600585938: 1, 0.8500742316246033: 1, 1.007346510887146: 1, -0.6390724778175354: 1, -0.6223658919334412: 1, 0.3499395549297333: 1, -0.1565595418214798: 1, 1.7404301166534424: 1, -0.3739321231842041: 1, 0.5241292119026184: 1, 1.916335940361023: 1, -0.1884830892086029: 1, -0.2168547362089157: 1, 0.6513680219650269: 1, -0.040548212826251984: 1, -0.20986565947532654: 1, -1.024586796760559: 1, 0.8943637013435364: 1, -1.2016351222991943: 1, -1.2016295194625854: 1, 0.9015107750892639: 1, 1.150854468345642: 1, 1.0578463077545166: 1, -0.5372467637062073: 1, 1.2327803373336792: 1, -0.20062661170959473: 1, -1.1656997203826904: 1, -0.5479841232299805: 1, -0.41674312949180603: 1, 0.654328465461731: 1, 0.23339834809303284: 1, -0.002975589595735073: 1, -0.674019455909729: 1, -1.201635718345642: 1, -1.1972460746765137: 1, 0.055386364459991455: 1, 0.8631362915039062: 1, 0.008864369243383408: 1, 1.2822530269622803: 1, 0.008748067542910576: 1, 0.6657525897026062: 1, -0.9053927063941956: 1, -0.37779542803764343: 1, -0.5749701261520386: 1, 0.29174771904945374: 1, 0.15707539021968842: 1, -1.20163094997406: 1, -1.2004859447479248: 1, -0.12876513600349426: 1, -1.1697360277175903: 1, -0.028692159801721573: 1, -0.11206144839525223: 1, 0.41225412487983704: 1, -1.1068792343139648: 1, 0.07662157714366913: 1, 0.5745441317558289: 1, 1.2464758157730103: 1, 1.2668349742889404: 1, -1.1104997396469116: 1, 0.34432777762413025: 1, -0.32669803500175476: 1, 0.5908839702606201: 1, 1.0087279081344604: 1, 1.2926610708236694: 1, -0.34817975759506226: 1, -1.1852235794067383: 1, -0.20500345528125763: 1, -0.7914682626724243: 1, 0.02054119110107422: 1, 0.9305616617202759: 1, 0.4251300096511841: 1, -0.9032037854194641: 1, 0.047311048954725266: 1, 1.2726079225540161: 1, 0.30879271030426025: 1, 0.005373222753405571: 1, 1.0674824714660645: 1, -0.21433545649051666: 1, 0.1331777125597: 1, -1.2015215158462524: 1, -1.2016299962997437: 1, -1.2016327381134033: 1, 0.469992071390152: 1, -1.2016345262527466: 1, -1.2015831470489502: 1, -1.2016384601593018: 1, -1.1318936347961426: 1, -1.2013877630233765: 1, -1.2016361951828003: 1, -0.8910970091819763: 1, -1.1979888677597046: 1, -0.4057319462299347: 1, -0.045158740133047104: 1, -0.11703558266162872: 1, 1.295539140701294: 1, 0.29349300265312195: 1, 1.0347987413406372: 1, 1.28579580783844: 1, 0.6233600974082947: 1, -1.0537559986114502: 1, -0.5104274749755859: 1, 0.5468651056289673: 1, 0.637002170085907: 1, -1.2005233764648438: 1, -0.28555259108543396: 1, -0.5884501934051514: 1, -0.8740671277046204: 1, 1.1960792541503906: 1, 0.08744630217552185: 1, -1.2016375064849854: 1, 0.27843114733695984: 1, 0.6842910647392273: 1, 0.5252094268798828: 1, -0.14631414413452148: 1, -0.84548020362854: 1, 1.200035810470581: 1, -0.30726683139801025: 1, 1.2916063070297241: 1, -0.015705665573477745: 1, -0.7448302507400513: 1}
INFO     Network type: REG
INFO     Model: "sequential_1"
INFO     _________________________________________________________________
INFO     Layer (type)                 Output Shape              Param #
INFO     =================================================================
INFO     dense_5 (Dense)              (None, 1024)              2098176
INFO     _________________________________________________________________
INFO     dropout_4 (Dropout)          (None, 1024)              0
INFO     _________________________________________________________________
INFO     dense_6 (Dense)              (None, 512)               524800
INFO     _________________________________________________________________
INFO     dropout_5 (Dropout)          (None, 512)               0
INFO     _________________________________________________________________
INFO     dense_7 (Dense)              (None, 256)               131328
INFO     _________________________________________________________________
INFO     dropout_6 (Dropout)          (None, 256)               0
INFO     _________________________________________________________________
INFO     dense_8 (Dense)              (None, 128)               32896
INFO     _________________________________________________________________
INFO     dropout_7 (Dropout)          (None, 128)               0
INFO     _________________________________________________________________
INFO     dense_9 (Dense)              (None, 1)                 129
INFO     =================================================================
INFO     Total params: 2,787,329
INFO     Trainable params: 2,787,329
INFO     Non-trainable params: 0
INFO     _________________________________________________________________
WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.
WARNING  `period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.
Epoch 1/5000
26/26 - 2s - loss: 4.7148 - val_loss: 4.6761
Epoch 2/5000
26/26 - 1s - loss: 4.6605 - val_loss: 4.6270
Epoch 3/5000
26/26 - 1s - loss: 4.6236 - val_loss: 4.5968
Epoch 4/5000
26/26 - 1s - loss: 4.5978 - val_loss: 4.5708
Epoch 5/5000
26/26 - 1s - loss: 4.5715 - val_loss: 4.5461
Epoch 6/5000
26/26 - 1s - loss: 4.5482 - val_loss: 4.5243
Epoch 7/5000
26/26 - 1s - loss: 4.5237 - val_loss: 4.5008
Epoch 8/5000
26/26 - 1s - loss: 4.5003 - val_loss: 4.4785
Epoch 9/5000
26/26 - 1s - loss: 4.4784 - val_loss: 4.4597
Epoch 10/5000
26/26 - 1s - loss: 4.4571 - val_loss: 4.4440
Epoch 00010: val_loss improved from inf to 4.44400, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 11/5000
26/26 - 1s - loss: 4.4397 - val_loss: 4.4301
Epoch 12/5000
26/26 - 1s - loss: 4.4215 - val_loss: 4.4166
Epoch 13/5000
26/26 - 1s - loss: 4.4056 - val_loss: 4.4035
Epoch 14/5000
26/26 - 1s - loss: 4.3872 - val_loss: 4.3906
Epoch 15/5000
26/26 - 1s - loss: 4.3709 - val_loss: 4.3776
Epoch 16/5000
26/26 - 1s - loss: 4.3536 - val_loss: 4.3648
Epoch 17/5000
26/26 - 2s - loss: 4.3372 - val_loss: 4.3525
Epoch 18/5000
26/26 - 1s - loss: 4.3225 - val_loss: 4.3403
Epoch 19/5000
26/26 - 1s - loss: 4.3074 - val_loss: 4.3288
Epoch 20/5000
26/26 - 1s - loss: 4.2907 - val_loss: 4.3169
Epoch 00020: val_loss improved from 4.44400 to 4.31695, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 21/5000
26/26 - 1s - loss: 4.2753 - val_loss: 4.3050
Epoch 22/5000
26/26 - 1s - loss: 4.2602 - val_loss: 4.2940
Epoch 23/5000
26/26 - 1s - loss: 4.2448 - val_loss: 4.2828
Epoch 24/5000
26/26 - 1s - loss: 4.2302 - val_loss: 4.2719
Epoch 25/5000
26/26 - 1s - loss: 4.2146 - val_loss: 4.2611
Epoch 26/5000
26/26 - 1s - loss: 4.1996 - val_loss: 4.2503
Epoch 27/5000
26/26 - 1s - loss: 4.1853 - val_loss: 4.2390
Epoch 28/5000
26/26 - 1s - loss: 4.1727 - val_loss: 4.2285
Epoch 29/5000
26/26 - 1s - loss: 4.1545 - val_loss: 4.2186
Epoch 30/5000
26/26 - 1s - loss: 4.1436 - val_loss: 4.2090
Epoch 00030: val_loss improved from 4.31695 to 4.20899, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 31/5000
26/26 - 1s - loss: 4.1270 - val_loss: 4.1998
Epoch 32/5000
26/26 - 1s - loss: 4.1145 - val_loss: 4.1883
Epoch 33/5000
26/26 - 1s - loss: 4.1010 - val_loss: 4.1785
Epoch 34/5000
26/26 - 1s - loss: 4.0867 - val_loss: 4.1693
Epoch 35/5000
26/26 - 1s - loss: 4.0707 - val_loss: 4.1593
Epoch 36/5000
26/26 - 1s - loss: 4.0608 - val_loss: 4.1496
Epoch 37/5000
26/26 - 1s - loss: 4.0462 - val_loss: 4.1397
Epoch 38/5000
26/26 - 1s - loss: 4.0336 - val_loss: 4.1306
Epoch 39/5000
26/26 - 1s - loss: 4.0195 - val_loss: 4.1221
Epoch 40/5000
26/26 - 1s - loss: 4.0044 - val_loss: 4.1128
Epoch 00040: val_loss improved from 4.20899 to 4.11283, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 41/5000
26/26 - 1s - loss: 3.9945 - val_loss: 4.1032
Epoch 42/5000
26/26 - 1s - loss: 3.9803 - val_loss: 4.0940
Epoch 43/5000
26/26 - 1s - loss: 3.9694 - val_loss: 4.0854
Epoch 44/5000
26/26 - 1s - loss: 3.9556 - val_loss: 4.0762
Epoch 45/5000
26/26 - 1s - loss: 3.9435 - val_loss: 4.0690
Epoch 46/5000
26/26 - 1s - loss: 3.9309 - val_loss: 4.0603
Epoch 47/5000
26/26 - 1s - loss: 3.9195 - val_loss: 4.0537
Epoch 48/5000
26/26 - 1s - loss: 3.9086 - val_loss: 4.0443
Epoch 49/5000
26/26 - 1s - loss: 3.8964 - val_loss: 4.0365
Epoch 50/5000
26/26 - 1s - loss: 3.8859 - val_loss: 4.0296
Epoch 00050: val_loss improved from 4.11283 to 4.02963, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 51/5000
26/26 - 1s - loss: 3.8748 - val_loss: 4.0212
Epoch 52/5000
26/26 - 1s - loss: 3.8604 - val_loss: 4.0132
Epoch 53/5000
26/26 - 1s - loss: 3.8515 - val_loss: 4.0065
Epoch 54/5000
26/26 - 1s - loss: 3.8377 - val_loss: 3.9987
Epoch 55/5000
26/26 - 1s - loss: 3.8295 - val_loss: 3.9911
Epoch 56/5000
26/26 - 1s - loss: 3.8208 - val_loss: 3.9843
Epoch 57/5000
26/26 - 1s - loss: 3.8067 - val_loss: 3.9768
Epoch 58/5000
26/26 - 1s - loss: 3.7986 - val_loss: 3.9711
Epoch 59/5000
26/26 - 1s - loss: 3.7888 - val_loss: 3.9633
Epoch 60/5000
26/26 - 1s - loss: 3.7738 - val_loss: 3.9564
Epoch 00060: val_loss improved from 4.02963 to 3.95640, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 61/5000
26/26 - 1s - loss: 3.7664 - val_loss: 3.9488
Epoch 62/5000
26/26 - 1s - loss: 3.7555 - val_loss: 3.9423
Epoch 63/5000
26/26 - 1s - loss: 3.7474 - val_loss: 3.9352
Epoch 64/5000
26/26 - 1s - loss: 3.7356 - val_loss: 3.9289
Epoch 65/5000
26/26 - 1s - loss: 3.7271 - val_loss: 3.9231
Epoch 66/5000
26/26 - 1s - loss: 3.7177 - val_loss: 3.9165
Epoch 67/5000
26/26 - 1s - loss: 3.7067 - val_loss: 3.9099
Epoch 68/5000
26/26 - 1s - loss: 3.6979 - val_loss: 3.9034
Epoch 69/5000
26/26 - 1s - loss: 3.6883 - val_loss: 3.8971
Epoch 70/5000
26/26 - 1s - loss: 3.6805 - val_loss: 3.8918
Epoch 00070: val_loss improved from 3.95640 to 3.89183, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 71/5000
26/26 - 1s - loss: 3.6736 - val_loss: 3.8861
Epoch 72/5000
26/26 - 1s - loss: 3.6613 - val_loss: 3.8804
Epoch 73/5000
26/26 - 1s - loss: 3.6534 - val_loss: 3.8738
Epoch 74/5000
26/26 - 1s - loss: 3.6440 - val_loss: 3.8679
Epoch 75/5000
26/26 - 1s - loss: 3.6342 - val_loss: 3.8620
Epoch 76/5000
26/26 - 1s - loss: 3.6292 - val_loss: 3.8552
Epoch 77/5000
26/26 - 1s - loss: 3.6202 - val_loss: 3.8490
Epoch 78/5000
26/26 - 1s - loss: 3.6139 - val_loss: 3.8441
Epoch 79/5000
26/26 - 2s - loss: 3.6037 - val_loss: 3.8371
Epoch 80/5000
26/26 - 1s - loss: 3.5947 - val_loss: 3.8304
Epoch 00080: val_loss improved from 3.89183 to 3.83039, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 81/5000
26/26 - 1s - loss: 3.5858 - val_loss: 3.8251
Epoch 82/5000
26/26 - 1s - loss: 3.5790 - val_loss: 3.8199
Epoch 83/5000
26/26 - 1s - loss: 3.5723 - val_loss: 3.8134
Epoch 84/5000
26/26 - 1s - loss: 3.5644 - val_loss: 3.8078
Epoch 85/5000
26/26 - 1s - loss: 3.5563 - val_loss: 3.8022
Epoch 86/5000
26/26 - 1s - loss: 3.5501 - val_loss: 3.7978
Epoch 87/5000
26/26 - 1s - loss: 3.5389 - val_loss: 3.7914
Epoch 88/5000
26/26 - 1s - loss: 3.5343 - val_loss: 3.7859
Epoch 89/5000
26/26 - 1s - loss: 3.5278 - val_loss: 3.7807
Epoch 90/5000
26/26 - 1s - loss: 3.5190 - val_loss: 3.7753
Epoch 00090: val_loss improved from 3.83039 to 3.77531, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 91/5000
26/26 - 1s - loss: 3.5123 - val_loss: 3.7687
Epoch 92/5000
26/26 - 1s - loss: 3.5000 - val_loss: 3.7656
Epoch 93/5000
26/26 - 1s - loss: 3.4977 - val_loss: 3.7583
Epoch 94/5000
26/26 - 1s - loss: 3.4904 - val_loss: 3.7531
Epoch 95/5000
26/26 - 1s - loss: 3.4856 - val_loss: 3.7497
Epoch 96/5000
26/26 - 1s - loss: 3.4776 - val_loss: 3.7425
Epoch 97/5000
26/26 - 1s - loss: 3.4651 - val_loss: 3.7371
Epoch 98/5000
26/26 - 1s - loss: 3.4596 - val_loss: 3.7318
Epoch 99/5000
26/26 - 2s - loss: 3.4559 - val_loss: 3.7268
Epoch 100/5000
26/26 - 1s - loss: 3.4493 - val_loss: 3.7223
Epoch 00100: val_loss improved from 3.77531 to 3.72227, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 101/5000
26/26 - 1s - loss: 3.4432 - val_loss: 3.7166
Epoch 102/5000
26/26 - 1s - loss: 3.4346 - val_loss: 3.7114
Epoch 103/5000
26/26 - 1s - loss: 3.4293 - val_loss: 3.7061
Epoch 104/5000
26/26 - 1s - loss: 3.4228 - val_loss: 3.7015
Epoch 105/5000
26/26 - 1s - loss: 3.4181 - val_loss: 3.6973
Epoch 106/5000
26/26 - 1s - loss: 3.4104 - val_loss: 3.6922
Epoch 107/5000
26/26 - 1s - loss: 3.4051 - val_loss: 3.6867
Epoch 108/5000
26/26 - 1s - loss: 3.3979 - val_loss: 3.6820
Epoch 109/5000
26/26 - 1s - loss: 3.3913 - val_loss: 3.6774
Epoch 110/5000
26/26 - 1s - loss: 3.3857 - val_loss: 3.6721
Epoch 00110: val_loss improved from 3.72227 to 3.67206, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 111/5000
26/26 - 1s - loss: 3.3788 - val_loss: 3.6671
Epoch 112/5000
26/26 - 1s - loss: 3.3760 - val_loss: 3.6620
Epoch 113/5000
26/26 - 1s - loss: 3.3703 - val_loss: 3.6577
Epoch 114/5000
26/26 - 1s - loss: 3.3620 - val_loss: 3.6533
Epoch 115/5000
26/26 - 1s - loss: 3.3570 - val_loss: 3.6485
Epoch 116/5000
26/26 - 1s - loss: 3.3488 - val_loss: 3.6440
Epoch 117/5000
26/26 - 1s - loss: 3.3422 - val_loss: 3.6405
Epoch 118/5000
26/26 - 1s - loss: 3.3406 - val_loss: 3.6349
Epoch 119/5000
26/26 - 1s - loss: 3.3342 - val_loss: 3.6291
Epoch 120/5000
26/26 - 1s - loss: 3.3265 - val_loss: 3.6246
Epoch 00120: val_loss improved from 3.67206 to 3.62465, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 121/5000
26/26 - 1s - loss: 3.3228 - val_loss: 3.6203
Epoch 122/5000
26/26 - 1s - loss: 3.3153 - val_loss: 3.6151
Epoch 123/5000
26/26 - 1s - loss: 3.3109 - val_loss: 3.6106
Epoch 124/5000
26/26 - 1s - loss: 3.3050 - val_loss: 3.6058
Epoch 125/5000
26/26 - 1s - loss: 3.2989 - val_loss: 3.6012
Epoch 126/5000
26/26 - 1s - loss: 3.2973 - val_loss: 3.5959
Epoch 127/5000
26/26 - 1s - loss: 3.2888 - val_loss: 3.5913
Epoch 128/5000
26/26 - 1s - loss: 3.2830 - val_loss: 3.5869
Epoch 129/5000
26/26 - 1s - loss: 3.2777 - val_loss: 3.5835
Epoch 130/5000
26/26 - 1s - loss: 3.2727 - val_loss: 3.5783
Epoch 00130: val_loss improved from 3.62465 to 3.57827, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 131/5000
26/26 - 1s - loss: 3.2657 - val_loss: 3.5747
Epoch 132/5000
26/26 - 1s - loss: 3.2628 - val_loss: 3.5712
Epoch 133/5000
26/26 - 1s - loss: 3.2565 - val_loss: 3.5678
Epoch 134/5000
26/26 - 1s - loss: 3.2530 - val_loss: 3.5612
Epoch 135/5000
26/26 - 1s - loss: 3.2448 - val_loss: 3.5567
Epoch 136/5000
26/26 - 1s - loss: 3.2415 - val_loss: 3.5526
Epoch 137/5000
26/26 - 1s - loss: 3.2369 - val_loss: 3.5475
Epoch 138/5000
26/26 - 1s - loss: 3.2304 - val_loss: 3.5430
Epoch 139/5000
26/26 - 1s - loss: 3.2261 - val_loss: 3.5387
Epoch 140/5000
26/26 - 2s - loss: 3.2213 - val_loss: 3.5351
Epoch 00140: val_loss improved from 3.57827 to 3.53510, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 141/5000
26/26 - 2s - loss: 3.2176 - val_loss: 3.5310
Epoch 142/5000
26/26 - 1s - loss: 3.2100 - val_loss: 3.5272
Epoch 143/5000
26/26 - 1s - loss: 3.2061 - val_loss: 3.5238
Epoch 144/5000
26/26 - 1s - loss: 3.2020 - val_loss: 3.5187
Epoch 145/5000
26/26 - 1s - loss: 3.1973 - val_loss: 3.5143
Epoch 146/5000
26/26 - 1s - loss: 3.1933 - val_loss: 3.5086
Epoch 147/5000
26/26 - 1s - loss: 3.1815 - val_loss: 3.5043
Epoch 148/5000
26/26 - 1s - loss: 3.1822 - val_loss: 3.5002
Epoch 149/5000
26/26 - 1s - loss: 3.1791 - val_loss: 3.4952
Epoch 150/5000
26/26 - 1s - loss: 3.1716 - val_loss: 3.4911
Epoch 00150: val_loss improved from 3.53510 to 3.49113, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 151/5000
26/26 - 1s - loss: 3.1680 - val_loss: 3.4876
Epoch 152/5000
26/26 - 1s - loss: 3.1614 - val_loss: 3.4835
Epoch 153/5000
26/26 - 1s - loss: 3.1591 - val_loss: 3.4791
Epoch 154/5000
26/26 - 1s - loss: 3.1535 - val_loss: 3.4751
Epoch 155/5000
26/26 - 1s - loss: 3.1501 - val_loss: 3.4723
Epoch 156/5000
26/26 - 1s - loss: 3.1427 - val_loss: 3.4673
Epoch 157/5000
26/26 - 1s - loss: 3.1380 - val_loss: 3.4625
Epoch 158/5000
26/26 - 1s - loss: 3.1357 - val_loss: 3.4594
Epoch 159/5000
26/26 - 2s - loss: 3.1315 - val_loss: 3.4547
Epoch 160/5000
26/26 - 1s - loss: 3.1247 - val_loss: 3.4502
Epoch 00160: val_loss improved from 3.49113 to 3.45022, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 161/5000
26/26 - 1s - loss: 3.1196 - val_loss: 3.4473
Epoch 162/5000
26/26 - 1s - loss: 3.1178 - val_loss: 3.4444
Epoch 163/5000
26/26 - 1s - loss: 3.1128 - val_loss: 3.4399
Epoch 164/5000
26/26 - 1s - loss: 3.1072 - val_loss: 3.4354
Epoch 165/5000
26/26 - 1s - loss: 3.1017 - val_loss: 3.4316
Epoch 166/5000
26/26 - 1s - loss: 3.0993 - val_loss: 3.4276
Epoch 167/5000
26/26 - 1s - loss: 3.0949 - val_loss: 3.4256
Epoch 168/5000
26/26 - 1s - loss: 3.0893 - val_loss: 3.4225
Epoch 169/5000
26/26 - 1s - loss: 3.0878 - val_loss: 3.4176
Epoch 170/5000
26/26 - 1s - loss: 3.0797 - val_loss: 3.4148
Epoch 00170: val_loss improved from 3.45022 to 3.41484, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 171/5000
26/26 - 1s - loss: 3.0775 - val_loss: 3.4111
Epoch 172/5000
26/26 - 1s - loss: 3.0726 - val_loss: 3.4072
Epoch 173/5000
26/26 - 1s - loss: 3.0687 - val_loss: 3.4019
Epoch 174/5000
26/26 - 1s - loss: 3.0665 - val_loss: 3.3982
Epoch 175/5000
26/26 - 1s - loss: 3.0562 - val_loss: 3.3945
Epoch 176/5000
26/26 - 1s - loss: 3.0532 - val_loss: 3.3904
Epoch 177/5000
26/26 - 1s - loss: 3.0505 - val_loss: 3.3867
Epoch 178/5000
26/26 - 1s - loss: 3.0456 - val_loss: 3.3828
Epoch 179/5000
26/26 - 1s - loss: 3.0428 - val_loss: 3.3792
Epoch 180/5000
26/26 - 1s - loss: 3.0398 - val_loss: 3.3747
Epoch 00180: val_loss improved from 3.41484 to 3.37475, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 181/5000
26/26 - 2s - loss: 3.0338 - val_loss: 3.3710
Epoch 182/5000
26/26 - 1s - loss: 3.0266 - val_loss: 3.3664
Epoch 183/5000
26/26 - 1s - loss: 3.0261 - val_loss: 3.3640
Epoch 184/5000
26/26 - 1s - loss: 3.0230 - val_loss: 3.3606
Epoch 185/5000
26/26 - 1s - loss: 3.0163 - val_loss: 3.3558
Epoch 186/5000
26/26 - 1s - loss: 3.0137 - val_loss: 3.3518
Epoch 187/5000
26/26 - 1s - loss: 3.0103 - val_loss: 3.3471
Epoch 188/5000
26/26 - 1s - loss: 3.0067 - val_loss: 3.3451
Epoch 189/5000
26/26 - 1s - loss: 3.0038 - val_loss: 3.3399
Epoch 190/5000
26/26 - 1s - loss: 2.9961 - val_loss: 3.3366
Epoch 00190: val_loss improved from 3.37475 to 3.33665, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 191/5000
26/26 - 1s - loss: 2.9936 - val_loss: 3.3340
Epoch 192/5000
26/26 - 1s - loss: 2.9895 - val_loss: 3.3302
Epoch 193/5000
26/26 - 1s - loss: 2.9860 - val_loss: 3.3268
Epoch 194/5000
26/26 - 1s - loss: 2.9831 - val_loss: 3.3231
Epoch 195/5000
26/26 - 1s - loss: 2.9775 - val_loss: 3.3201
Epoch 196/5000
26/26 - 1s - loss: 2.9745 - val_loss: 3.3155
Epoch 197/5000
26/26 - 1s - loss: 2.9691 - val_loss: 3.3128
Epoch 198/5000
26/26 - 1s - loss: 2.9657 - val_loss: 3.3092
Epoch 199/5000
26/26 - 1s - loss: 2.9594 - val_loss: 3.3069
Epoch 200/5000
26/26 - 1s - loss: 2.9569 - val_loss: 3.3012
Epoch 00200: val_loss improved from 3.33665 to 3.30124, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 201/5000
26/26 - 1s - loss: 2.9545 - val_loss: 3.2981
Epoch 202/5000
26/26 - 1s - loss: 2.9482 - val_loss: 3.2943
Epoch 203/5000
26/26 - 1s - loss: 2.9468 - val_loss: 3.2903
Epoch 204/5000
26/26 - 1s - loss: 2.9399 - val_loss: 3.2873
Epoch 205/5000
26/26 - 1s - loss: 2.9385 - val_loss: 3.2844
Epoch 206/5000
26/26 - 1s - loss: 2.9323 - val_loss: 3.2795
Epoch 207/5000
26/26 - 1s - loss: 2.9330 - val_loss: 3.2763
Epoch 208/5000
26/26 - 1s - loss: 2.9249 - val_loss: 3.2719
Epoch 209/5000
26/26 - 1s - loss: 2.9218 - val_loss: 3.2705
Epoch 210/5000
26/26 - 1s - loss: 2.9184 - val_loss: 3.2676
Epoch 00210: val_loss improved from 3.30124 to 3.26761, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 211/5000
26/26 - 1s - loss: 2.9130 - val_loss: 3.2631
Epoch 212/5000
26/26 - 1s - loss: 2.9114 - val_loss: 3.2598
Epoch 213/5000
26/26 - 1s - loss: 2.9115 - val_loss: 3.2554
Epoch 214/5000
26/26 - 1s - loss: 2.9037 - val_loss: 3.2513
Epoch 215/5000
26/26 - 1s - loss: 2.8989 - val_loss: 3.2473
Epoch 216/5000
26/26 - 1s - loss: 2.8983 - val_loss: 3.2441
Epoch 217/5000
26/26 - 1s - loss: 2.8922 - val_loss: 3.2413
Epoch 218/5000
26/26 - 1s - loss: 2.8893 - val_loss: 3.2373
Epoch 219/5000
26/26 - 1s - loss: 2.8861 - val_loss: 3.2342
Epoch 220/5000
26/26 - 1s - loss: 2.8813 - val_loss: 3.2308
Epoch 00220: val_loss improved from 3.26761 to 3.23079, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 221/5000
26/26 - 1s - loss: 2.8781 - val_loss: 3.2274
Epoch 222/5000
26/26 - 2s - loss: 2.8740 - val_loss: 3.2257
Epoch 223/5000
26/26 - 1s - loss: 2.8717 - val_loss: 3.2209
Epoch 224/5000
26/26 - 1s - loss: 2.8669 - val_loss: 3.2185
Epoch 225/5000
26/26 - 1s - loss: 2.8658 - val_loss: 3.2137
Epoch 226/5000
26/26 - 1s - loss: 2.8599 - val_loss: 3.2099
Epoch 227/5000
26/26 - 1s - loss: 2.8587 - val_loss: 3.2082
Epoch 228/5000
26/26 - 1s - loss: 2.8541 - val_loss: 3.2047
Epoch 229/5000
26/26 - 1s - loss: 2.8501 - val_loss: 3.2008
Epoch 230/5000
26/26 - 1s - loss: 2.8454 - val_loss: 3.1974
Epoch 00230: val_loss improved from 3.23079 to 3.19744, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 231/5000
26/26 - 1s - loss: 2.8444 - val_loss: 3.1932
Epoch 232/5000
26/26 - 1s - loss: 2.8394 - val_loss: 3.1907
Epoch 233/5000
26/26 - 1s - loss: 2.8371 - val_loss: 3.1885
Epoch 234/5000
26/26 - 1s - loss: 2.8336 - val_loss: 3.1874
Epoch 235/5000
26/26 - 1s - loss: 2.8293 - val_loss: 3.1825
Epoch 236/5000
26/26 - 1s - loss: 2.8240 - val_loss: 3.1783
Epoch 237/5000
26/26 - 1s - loss: 2.8200 - val_loss: 3.1743
Epoch 238/5000
26/26 - 1s - loss: 2.8169 - val_loss: 3.1719
Epoch 239/5000
26/26 - 1s - loss: 2.8139 - val_loss: 3.1701
Epoch 240/5000
26/26 - 1s - loss: 2.8090 - val_loss: 3.1657
Epoch 00240: val_loss improved from 3.19744 to 3.16566, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 241/5000
26/26 - 1s - loss: 2.8069 - val_loss: 3.1619
Epoch 242/5000
26/26 - 1s - loss: 2.8055 - val_loss: 3.1579
Epoch 243/5000
26/26 - 1s - loss: 2.7993 - val_loss: 3.1542
Epoch 244/5000
26/26 - 1s - loss: 2.7989 - val_loss: 3.1526
Epoch 245/5000
26/26 - 1s - loss: 2.7939 - val_loss: 3.1522
Epoch 246/5000
26/26 - 1s - loss: 2.7908 - val_loss: 3.1469
Epoch 247/5000
26/26 - 1s - loss: 2.7871 - val_loss: 3.1430
Epoch 248/5000
26/26 - 1s - loss: 2.7858 - val_loss: 3.1407
Epoch 249/5000
26/26 - 1s - loss: 2.7807 - val_loss: 3.1369
Epoch 250/5000
26/26 - 1s - loss: 2.7785 - val_loss: 3.1342
Epoch 00250: val_loss improved from 3.16566 to 3.13425, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 251/5000
26/26 - 1s - loss: 2.7733 - val_loss: 3.1316
Epoch 252/5000
26/26 - 1s - loss: 2.7725 - val_loss: 3.1301
Epoch 253/5000
26/26 - 1s - loss: 2.7666 - val_loss: 3.1260
Epoch 254/5000
26/26 - 1s - loss: 2.7640 - val_loss: 3.1224
Epoch 255/5000
26/26 - 1s - loss: 2.7613 - val_loss: 3.1181
Epoch 256/5000
26/26 - 1s - loss: 2.7593 - val_loss: 3.1155
Epoch 257/5000
26/26 - 1s - loss: 2.7520 - val_loss: 3.1135
Epoch 258/5000
26/26 - 1s - loss: 2.7509 - val_loss: 3.1083
Epoch 259/5000
26/26 - 1s - loss: 2.7475 - val_loss: 3.1062
Epoch 260/5000
26/26 - 1s - loss: 2.7467 - val_loss: 3.1039
Epoch 00260: val_loss improved from 3.13425 to 3.10394, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 261/5000
26/26 - 1s - loss: 2.7409 - val_loss: 3.1008
Epoch 262/5000
26/26 - 1s - loss: 2.7382 - val_loss: 3.0975
Epoch 263/5000
26/26 - 2s - loss: 2.7351 - val_loss: 3.0963
Epoch 264/5000
26/26 - 1s - loss: 2.7335 - val_loss: 3.0917
Epoch 265/5000
26/26 - 1s - loss: 2.7292 - val_loss: 3.0889
Epoch 266/5000
26/26 - 1s - loss: 2.7265 - val_loss: 3.0866
Epoch 267/5000
26/26 - 1s - loss: 2.7241 - val_loss: 3.0839
Epoch 268/5000
26/26 - 1s - loss: 2.7171 - val_loss: 3.0817
Epoch 269/5000
26/26 - 1s - loss: 2.7158 - val_loss: 3.0780
Epoch 270/5000
26/26 - 1s - loss: 2.7126 - val_loss: 3.0738
Epoch 00270: val_loss improved from 3.10394 to 3.07384, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 271/5000
26/26 - 1s - loss: 2.7095 - val_loss: 3.0700
Epoch 272/5000
26/26 - 1s - loss: 2.7090 - val_loss: 3.0674
Epoch 273/5000
26/26 - 1s - loss: 2.7062 - val_loss: 3.0643
Epoch 274/5000
26/26 - 1s - loss: 2.7006 - val_loss: 3.0607
Epoch 275/5000
26/26 - 1s - loss: 2.6985 - val_loss: 3.0583
Epoch 276/5000
26/26 - 1s - loss: 2.6977 - val_loss: 3.0561
Epoch 277/5000
26/26 - 1s - loss: 2.6935 - val_loss: 3.0518
Epoch 278/5000
26/26 - 1s - loss: 2.6883 - val_loss: 3.0504
Epoch 279/5000
26/26 - 1s - loss: 2.6843 - val_loss: 3.0467
Epoch 280/5000
26/26 - 1s - loss: 2.6829 - val_loss: 3.0457
Epoch 00280: val_loss improved from 3.07384 to 3.04573, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 281/5000
26/26 - 1s - loss: 2.6773 - val_loss: 3.0413
Epoch 282/5000
26/26 - 1s - loss: 2.6772 - val_loss: 3.0372
Epoch 283/5000
26/26 - 1s - loss: 2.6715 - val_loss: 3.0356
Epoch 284/5000
26/26 - 1s - loss: 2.6705 - val_loss: 3.0329
Epoch 285/5000
26/26 - 1s - loss: 2.6679 - val_loss: 3.0312
Epoch 286/5000
26/26 - 1s - loss: 2.6644 - val_loss: 3.0281
Epoch 287/5000
26/26 - 1s - loss: 2.6593 - val_loss: 3.0242
Epoch 288/5000
26/26 - 1s - loss: 2.6536 - val_loss: 3.0214
Epoch 289/5000
26/26 - 1s - loss: 2.6544 - val_loss: 3.0188
Epoch 290/5000
26/26 - 1s - loss: 2.6511 - val_loss: 3.0169
Epoch 00290: val_loss improved from 3.04573 to 3.01690, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 291/5000
26/26 - 1s - loss: 2.6493 - val_loss: 3.0134
Epoch 292/5000
26/26 - 1s - loss: 2.6470 - val_loss: 3.0101
Epoch 293/5000
26/26 - 1s - loss: 2.6440 - val_loss: 3.0059
Epoch 294/5000
26/26 - 1s - loss: 2.6401 - val_loss: 3.0027
Epoch 295/5000
26/26 - 1s - loss: 2.6374 - val_loss: 2.9993
Epoch 296/5000
26/26 - 1s - loss: 2.6318 - val_loss: 2.9969
Epoch 297/5000
26/26 - 1s - loss: 2.6302 - val_loss: 2.9937
Epoch 298/5000
26/26 - 1s - loss: 2.6283 - val_loss: 2.9911
Epoch 299/5000
26/26 - 1s - loss: 2.6247 - val_loss: 2.9870
Epoch 300/5000
26/26 - 1s - loss: 2.6194 - val_loss: 2.9851
Epoch 00300: val_loss improved from 3.01690 to 2.98514, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 301/5000
26/26 - 1s - loss: 2.6174 - val_loss: 2.9826
Epoch 302/5000
26/26 - 1s - loss: 2.6154 - val_loss: 2.9788
Epoch 303/5000
26/26 - 1s - loss: 2.6101 - val_loss: 2.9760
Epoch 304/5000
26/26 - 1s - loss: 2.6094 - val_loss: 2.9732
Epoch 305/5000
26/26 - 1s - loss: 2.6079 - val_loss: 2.9710
Epoch 306/5000
26/26 - 1s - loss: 2.6039 - val_loss: 2.9676
Epoch 307/5000
26/26 - 1s - loss: 2.5987 - val_loss: 2.9661
Epoch 308/5000
26/26 - 1s - loss: 2.5998 - val_loss: 2.9626
Epoch 309/5000
26/26 - 1s - loss: 2.5956 - val_loss: 2.9604
Epoch 310/5000
26/26 - 1s - loss: 2.5920 - val_loss: 2.9561
Epoch 00310: val_loss improved from 2.98514 to 2.95605, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 311/5000
26/26 - 1s - loss: 2.5906 - val_loss: 2.9533
Epoch 312/5000
26/26 - 1s - loss: 2.5851 - val_loss: 2.9512
Epoch 313/5000
26/26 - 1s - loss: 2.5824 - val_loss: 2.9485
Epoch 314/5000
26/26 - 1s - loss: 2.5807 - val_loss: 2.9460
Epoch 315/5000
26/26 - 1s - loss: 2.5778 - val_loss: 2.9422
Epoch 316/5000
26/26 - 1s - loss: 2.5767 - val_loss: 2.9395
Epoch 317/5000
26/26 - 1s - loss: 2.5701 - val_loss: 2.9376
Epoch 318/5000
26/26 - 1s - loss: 2.5660 - val_loss: 2.9338
Epoch 319/5000
26/26 - 1s - loss: 2.5664 - val_loss: 2.9314
Epoch 320/5000
26/26 - 1s - loss: 2.5640 - val_loss: 2.9285
Epoch 00320: val_loss improved from 2.95605 to 2.92854, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 321/5000
26/26 - 1s - loss: 2.5607 - val_loss: 2.9256
Epoch 322/5000
26/26 - 1s - loss: 2.5571 - val_loss: 2.9239
Epoch 323/5000
26/26 - 1s - loss: 2.5562 - val_loss: 2.9228
Epoch 324/5000
26/26 - 1s - loss: 2.5514 - val_loss: 2.9187
Epoch 325/5000
26/26 - 1s - loss: 2.5495 - val_loss: 2.9160
Epoch 326/5000
26/26 - 2s - loss: 2.5469 - val_loss: 2.9141
Epoch 327/5000
26/26 - 1s - loss: 2.5447 - val_loss: 2.9114
Epoch 328/5000
26/26 - 1s - loss: 2.5401 - val_loss: 2.9074
Epoch 329/5000
26/26 - 1s - loss: 2.5399 - val_loss: 2.9053
Epoch 330/5000
26/26 - 1s - loss: 2.5367 - val_loss: 2.9039
Epoch 00330: val_loss improved from 2.92854 to 2.90389, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 331/5000
26/26 - 1s - loss: 2.5313 - val_loss: 2.8996
Epoch 332/5000
26/26 - 1s - loss: 2.5289 - val_loss: 2.8974
Epoch 333/5000
26/26 - 1s - loss: 2.5291 - val_loss: 2.8942
Epoch 334/5000
26/26 - 1s - loss: 2.5257 - val_loss: 2.8923
Epoch 335/5000
26/26 - 1s - loss: 2.5208 - val_loss: 2.8899
Epoch 336/5000
26/26 - 1s - loss: 2.5159 - val_loss: 2.8868
Epoch 337/5000
26/26 - 1s - loss: 2.5147 - val_loss: 2.8850
Epoch 338/5000
26/26 - 1s - loss: 2.5099 - val_loss: 2.8821
Epoch 339/5000
26/26 - 1s - loss: 2.5108 - val_loss: 2.8790
Epoch 340/5000
26/26 - 1s - loss: 2.5068 - val_loss: 2.8766
Epoch 00340: val_loss improved from 2.90389 to 2.87665, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 341/5000
26/26 - 1s - loss: 2.5060 - val_loss: 2.8737
Epoch 342/5000
26/26 - 1s - loss: 2.5018 - val_loss: 2.8717
Epoch 343/5000
26/26 - 1s - loss: 2.4982 - val_loss: 2.8697
Epoch 344/5000
26/26 - 1s - loss: 2.4997 - val_loss: 2.8652
Epoch 345/5000
26/26 - 2s - loss: 2.4927 - val_loss: 2.8639
Epoch 346/5000
26/26 - 1s - loss: 2.4938 - val_loss: 2.8622
Epoch 347/5000
26/26 - 1s - loss: 2.4887 - val_loss: 2.8585
Epoch 348/5000
26/26 - 1s - loss: 2.4846 - val_loss: 2.8562
Epoch 349/5000
26/26 - 1s - loss: 2.4844 - val_loss: 2.8531
Epoch 350/5000
26/26 - 1s - loss: 2.4793 - val_loss: 2.8499
Epoch 00350: val_loss improved from 2.87665 to 2.84995, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 351/5000
26/26 - 1s - loss: 2.4768 - val_loss: 2.8510
Epoch 352/5000
26/26 - 1s - loss: 2.4764 - val_loss: 2.8468
Epoch 353/5000
26/26 - 1s - loss: 2.4730 - val_loss: 2.8424
Epoch 354/5000
26/26 - 1s - loss: 2.4697 - val_loss: 2.8395
Epoch 355/5000
26/26 - 1s - loss: 2.4684 - val_loss: 2.8375
Epoch 356/5000
26/26 - 1s - loss: 2.4648 - val_loss: 2.8363
Epoch 357/5000
26/26 - 1s - loss: 2.4653 - val_loss: 2.8326
Epoch 358/5000
26/26 - 1s - loss: 2.4616 - val_loss: 2.8307
Epoch 359/5000
26/26 - 1s - loss: 2.4574 - val_loss: 2.8283
Epoch 360/5000
26/26 - 1s - loss: 2.4543 - val_loss: 2.8258
Epoch 00360: val_loss improved from 2.84995 to 2.82581, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 361/5000
26/26 - 1s - loss: 2.4507 - val_loss: 2.8244
Epoch 362/5000
26/26 - 1s - loss: 2.4512 - val_loss: 2.8202
Epoch 363/5000
26/26 - 1s - loss: 2.4467 - val_loss: 2.8180
Epoch 364/5000
26/26 - 1s - loss: 2.4433 - val_loss: 2.8155
Epoch 365/5000
26/26 - 1s - loss: 2.4411 - val_loss: 2.8125
Epoch 366/5000
26/26 - 1s - loss: 2.4366 - val_loss: 2.8102
Epoch 367/5000
26/26 - 1s - loss: 2.4358 - val_loss: 2.8079
Epoch 368/5000
26/26 - 1s - loss: 2.4332 - val_loss: 2.8052
Epoch 369/5000
26/26 - 1s - loss: 2.4309 - val_loss: 2.8026
Epoch 370/5000
26/26 - 1s - loss: 2.4296 - val_loss: 2.7990
Epoch 00370: val_loss improved from 2.82581 to 2.79898, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 371/5000
26/26 - 1s - loss: 2.4264 - val_loss: 2.7982
Epoch 372/5000
26/26 - 1s - loss: 2.4241 - val_loss: 2.7951
Epoch 373/5000
26/26 - 1s - loss: 2.4211 - val_loss: 2.7933
Epoch 374/5000
26/26 - 1s - loss: 2.4175 - val_loss: 2.7906
Epoch 375/5000
26/26 - 1s - loss: 2.4144 - val_loss: 2.7876
Epoch 376/5000
26/26 - 1s - loss: 2.4148 - val_loss: 2.7859
Epoch 377/5000
26/26 - 1s - loss: 2.4115 - val_loss: 2.7834
Epoch 378/5000
26/26 - 1s - loss: 2.4067 - val_loss: 2.7813
Epoch 379/5000
26/26 - 1s - loss: 2.4061 - val_loss: 2.7807
Epoch 380/5000
26/26 - 1s - loss: 2.4046 - val_loss: 2.7778
Epoch 00380: val_loss improved from 2.79898 to 2.77783, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 381/5000
26/26 - 1s - loss: 2.3985 - val_loss: 2.7737
Epoch 382/5000
26/26 - 1s - loss: 2.3968 - val_loss: 2.7716
Epoch 383/5000
26/26 - 1s - loss: 2.3944 - val_loss: 2.7699
Epoch 384/5000
26/26 - 1s - loss: 2.3934 - val_loss: 2.7673
Epoch 385/5000
26/26 - 1s - loss: 2.3904 - val_loss: 2.7660
Epoch 386/5000
26/26 - 1s - loss: 2.3885 - val_loss: 2.7613
Epoch 387/5000
26/26 - 2s - loss: 2.3833 - val_loss: 2.7602
Epoch 388/5000
26/26 - 1s - loss: 2.3806 - val_loss: 2.7589
Epoch 389/5000
26/26 - 1s - loss: 2.3804 - val_loss: 2.7546
Epoch 390/5000
26/26 - 1s - loss: 2.3786 - val_loss: 2.7524
Epoch 00390: val_loss improved from 2.77783 to 2.75238, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 391/5000
26/26 - 1s - loss: 2.3776 - val_loss: 2.7514
Epoch 392/5000
26/26 - 1s - loss: 2.3737 - val_loss: 2.7458
Epoch 393/5000
26/26 - 1s - loss: 2.3717 - val_loss: 2.7455
Epoch 394/5000
26/26 - 1s - loss: 2.3685 - val_loss: 2.7435
Epoch 395/5000
26/26 - 1s - loss: 2.3648 - val_loss: 2.7400
Epoch 396/5000
26/26 - 1s - loss: 2.3629 - val_loss: 2.7390
Epoch 397/5000
26/26 - 1s - loss: 2.3596 - val_loss: 2.7379
Epoch 398/5000
26/26 - 1s - loss: 2.3560 - val_loss: 2.7330
Epoch 399/5000
26/26 - 1s - loss: 2.3534 - val_loss: 2.7306
Epoch 400/5000
26/26 - 1s - loss: 2.3542 - val_loss: 2.7295
Epoch 00400: val_loss improved from 2.75238 to 2.72948, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 401/5000
26/26 - 1s - loss: 2.3497 - val_loss: 2.7278
Epoch 402/5000
26/26 - 1s - loss: 2.3473 - val_loss: 2.7235
Epoch 403/5000
26/26 - 1s - loss: 2.3462 - val_loss: 2.7220
Epoch 404/5000
26/26 - 1s - loss: 2.3441 - val_loss: 2.7182
Epoch 405/5000
26/26 - 1s - loss: 2.3412 - val_loss: 2.7158
Epoch 406/5000
26/26 - 1s - loss: 2.3381 - val_loss: 2.7137
Epoch 407/5000
26/26 - 1s - loss: 2.3336 - val_loss: 2.7113
Epoch 408/5000
26/26 - 1s - loss: 2.3326 - val_loss: 2.7090
Epoch 409/5000
26/26 - 1s - loss: 2.3297 - val_loss: 2.7052
Epoch 410/5000
26/26 - 1s - loss: 2.3306 - val_loss: 2.7038
Epoch 00410: val_loss improved from 2.72948 to 2.70379, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 411/5000
26/26 - 1s - loss: 2.3266 - val_loss: 2.7016
Epoch 412/5000
26/26 - 1s - loss: 2.3236 - val_loss: 2.7002
Epoch 413/5000
26/26 - 1s - loss: 2.3245 - val_loss: 2.6982
Epoch 414/5000
26/26 - 1s - loss: 2.3209 - val_loss: 2.6949
Epoch 415/5000
26/26 - 1s - loss: 2.3185 - val_loss: 2.6932
Epoch 416/5000
26/26 - 1s - loss: 2.3132 - val_loss: 2.6911
Epoch 417/5000
26/26 - 1s - loss: 2.3134 - val_loss: 2.6882
Epoch 418/5000
26/26 - 1s - loss: 2.3112 - val_loss: 2.6880
Epoch 419/5000
26/26 - 1s - loss: 2.3082 - val_loss: 2.6841
Epoch 420/5000
26/26 - 1s - loss: 2.3034 - val_loss: 2.6813
Epoch 00420: val_loss improved from 2.70379 to 2.68134, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 421/5000
26/26 - 1s - loss: 2.3043 - val_loss: 2.6794
Epoch 422/5000
26/26 - 1s - loss: 2.2999 - val_loss: 2.6777
Epoch 423/5000
26/26 - 1s - loss: 2.2978 - val_loss: 2.6734
Epoch 424/5000
26/26 - 1s - loss: 2.2957 - val_loss: 2.6727
Epoch 425/5000
26/26 - 1s - loss: 2.2916 - val_loss: 2.6708
Epoch 426/5000
26/26 - 1s - loss: 2.2906 - val_loss: 2.6688
Epoch 427/5000
26/26 - 1s - loss: 2.2884 - val_loss: 2.6662
Epoch 428/5000
26/26 - 2s - loss: 2.2852 - val_loss: 2.6631
Epoch 429/5000
26/26 - 1s - loss: 2.2847 - val_loss: 2.6627
Epoch 430/5000
26/26 - 1s - loss: 2.2813 - val_loss: 2.6591
Epoch 00430: val_loss improved from 2.68134 to 2.65915, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 431/5000
26/26 - 1s - loss: 2.2797 - val_loss: 2.6587
Epoch 432/5000
26/26 - 1s - loss: 2.2756 - val_loss: 2.6566
Epoch 433/5000
26/26 - 1s - loss: 2.2758 - val_loss: 2.6556
Epoch 434/5000
26/26 - 1s - loss: 2.2721 - val_loss: 2.6509
Epoch 435/5000
26/26 - 1s - loss: 2.2700 - val_loss: 2.6493
Epoch 436/5000
26/26 - 1s - loss: 2.2678 - val_loss: 2.6453
Epoch 437/5000
26/26 - 1s - loss: 2.2638 - val_loss: 2.6436
Epoch 438/5000
26/26 - 1s - loss: 2.2624 - val_loss: 2.6433
Epoch 439/5000
26/26 - 1s - loss: 2.2601 - val_loss: 2.6403
Epoch 440/5000
26/26 - 1s - loss: 2.2587 - val_loss: 2.6382
Epoch 00440: val_loss improved from 2.65915 to 2.63824, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 441/5000
26/26 - 1s - loss: 2.2552 - val_loss: 2.6353
Epoch 442/5000
26/26 - 1s - loss: 2.2530 - val_loss: 2.6322
Epoch 443/5000
26/26 - 1s - loss: 2.2534 - val_loss: 2.6306
Epoch 444/5000
26/26 - 1s - loss: 2.2490 - val_loss: 2.6296
Epoch 445/5000
26/26 - 1s - loss: 2.2463 - val_loss: 2.6266
Epoch 446/5000
26/26 - 1s - loss: 2.2463 - val_loss: 2.6219
Epoch 447/5000
26/26 - 1s - loss: 2.2429 - val_loss: 2.6210
Epoch 448/5000
26/26 - 1s - loss: 2.2385 - val_loss: 2.6183
Epoch 449/5000
26/26 - 1s - loss: 2.2395 - val_loss: 2.6161
Epoch 450/5000
26/26 - 1s - loss: 2.2354 - val_loss: 2.6144
Epoch 00450: val_loss improved from 2.63824 to 2.61440, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 451/5000
26/26 - 1s - loss: 2.2333 - val_loss: 2.6128
Epoch 452/5000
26/26 - 1s - loss: 2.2310 - val_loss: 2.6114
Epoch 453/5000
26/26 - 2s - loss: 2.2292 - val_loss: 2.6083
Epoch 454/5000
26/26 - 1s - loss: 2.2282 - val_loss: 2.6068
Epoch 455/5000
26/26 - 1s - loss: 2.2274 - val_loss: 2.6045
Epoch 456/5000
26/26 - 1s - loss: 2.2218 - val_loss: 2.6038
Epoch 457/5000
26/26 - 1s - loss: 2.2211 - val_loss: 2.6011
Epoch 458/5000
26/26 - 1s - loss: 2.2178 - val_loss: 2.5993
Epoch 459/5000
26/26 - 1s - loss: 2.2150 - val_loss: 2.5981
Epoch 460/5000
26/26 - 1s - loss: 2.2130 - val_loss: 2.5944
Epoch 00460: val_loss improved from 2.61440 to 2.59438, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 461/5000
26/26 - 1s - loss: 2.2118 - val_loss: 2.5908
Epoch 462/5000
26/26 - 1s - loss: 2.2097 - val_loss: 2.5888
Epoch 463/5000
26/26 - 1s - loss: 2.2072 - val_loss: 2.5862
Epoch 464/5000
26/26 - 1s - loss: 2.2047 - val_loss: 2.5837
Epoch 465/5000
26/26 - 1s - loss: 2.2040 - val_loss: 2.5808
Epoch 466/5000
26/26 - 1s - loss: 2.1983 - val_loss: 2.5790
Epoch 467/5000
26/26 - 1s - loss: 2.1982 - val_loss: 2.5783
Epoch 468/5000
26/26 - 1s - loss: 2.1959 - val_loss: 2.5749
Epoch 469/5000
26/26 - 2s - loss: 2.1958 - val_loss: 2.5735
Epoch 470/5000
26/26 - 1s - loss: 2.1914 - val_loss: 2.5703
Epoch 00470: val_loss improved from 2.59438 to 2.57034, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 471/5000
26/26 - 1s - loss: 2.1922 - val_loss: 2.5698
Epoch 472/5000
26/26 - 1s - loss: 2.1888 - val_loss: 2.5659
Epoch 473/5000
26/26 - 1s - loss: 2.1834 - val_loss: 2.5649
Epoch 474/5000
26/26 - 1s - loss: 2.1836 - val_loss: 2.5650
Epoch 475/5000
26/26 - 1s - loss: 2.1813 - val_loss: 2.5629
Epoch 476/5000
26/26 - 1s - loss: 2.1782 - val_loss: 2.5598
Epoch 477/5000
26/26 - 1s - loss: 2.1759 - val_loss: 2.5569
Epoch 478/5000
26/26 - 1s - loss: 2.1741 - val_loss: 2.5555
Epoch 479/5000
26/26 - 1s - loss: 2.1731 - val_loss: 2.5525
Epoch 480/5000
26/26 - 1s - loss: 2.1698 - val_loss: 2.5509
Epoch 00480: val_loss improved from 2.57034 to 2.55093, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 481/5000
26/26 - 1s - loss: 2.1698 - val_loss: 2.5514
Epoch 482/5000
26/26 - 1s - loss: 2.1660 - val_loss: 2.5471
Epoch 483/5000
26/26 - 1s - loss: 2.1640 - val_loss: 2.5458
Epoch 484/5000
26/26 - 1s - loss: 2.1611 - val_loss: 2.5441
Epoch 485/5000
26/26 - 1s - loss: 2.1591 - val_loss: 2.5418
Epoch 486/5000
26/26 - 1s - loss: 2.1559 - val_loss: 2.5402
Epoch 487/5000
26/26 - 1s - loss: 2.1575 - val_loss: 2.5381
Epoch 488/5000
26/26 - 1s - loss: 2.1532 - val_loss: 2.5354
Epoch 489/5000
26/26 - 1s - loss: 2.1513 - val_loss: 2.5329
Epoch 490/5000
26/26 - 1s - loss: 2.1500 - val_loss: 2.5312
Epoch 00490: val_loss improved from 2.55093 to 2.53120, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 491/5000
26/26 - 1s - loss: 2.1463 - val_loss: 2.5306
Epoch 492/5000
26/26 - 1s - loss: 2.1470 - val_loss: 2.5275
Epoch 493/5000
26/26 - 1s - loss: 2.1424 - val_loss: 2.5245
Epoch 494/5000
26/26 - 1s - loss: 2.1416 - val_loss: 2.5223
Epoch 495/5000
26/26 - 1s - loss: 2.1396 - val_loss: 2.5204
Epoch 496/5000
26/26 - 1s - loss: 2.1386 - val_loss: 2.5207
Epoch 497/5000
26/26 - 1s - loss: 2.1351 - val_loss: 2.5177
Epoch 498/5000
26/26 - 1s - loss: 2.1321 - val_loss: 2.5156
Epoch 499/5000
26/26 - 1s - loss: 2.1312 - val_loss: 2.5124
Epoch 500/5000
26/26 - 1s - loss: 2.1290 - val_loss: 2.5104
Epoch 00500: val_loss improved from 2.53120 to 2.51037, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 501/5000
26/26 - 1s - loss: 2.1263 - val_loss: 2.5086
Epoch 502/5000
26/26 - 1s - loss: 2.1237 - val_loss: 2.5052
Epoch 503/5000
26/26 - 1s - loss: 2.1217 - val_loss: 2.5045
Epoch 504/5000
26/26 - 1s - loss: 2.1204 - val_loss: 2.5023
Epoch 505/5000
26/26 - 1s - loss: 2.1183 - val_loss: 2.4995
Epoch 506/5000
26/26 - 1s - loss: 2.1171 - val_loss: 2.4978
Epoch 507/5000
26/26 - 1s - loss: 2.1128 - val_loss: 2.4970
Epoch 508/5000
26/26 - 1s - loss: 2.1120 - val_loss: 2.4952
Epoch 509/5000
26/26 - 1s - loss: 2.1100 - val_loss: 2.4956
Epoch 510/5000
26/26 - 1s - loss: 2.1072 - val_loss: 2.4897
Epoch 00510: val_loss improved from 2.51037 to 2.48968, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 511/5000
26/26 - 1s - loss: 2.1054 - val_loss: 2.4877
Epoch 512/5000
26/26 - 1s - loss: 2.1039 - val_loss: 2.4864
Epoch 513/5000
26/26 - 1s - loss: 2.1030 - val_loss: 2.4855
Epoch 514/5000
26/26 - 1s - loss: 2.1004 - val_loss: 2.4838
Epoch 515/5000
26/26 - 1s - loss: 2.0976 - val_loss: 2.4801
Epoch 516/5000
26/26 - 1s - loss: 2.0963 - val_loss: 2.4800
Epoch 517/5000
26/26 - 1s - loss: 2.0953 - val_loss: 2.4779
Epoch 518/5000
26/26 - 1s - loss: 2.0930 - val_loss: 2.4756
Epoch 519/5000
26/26 - 1s - loss: 2.0890 - val_loss: 2.4729
Epoch 520/5000
26/26 - 1s - loss: 2.0875 - val_loss: 2.4712
Epoch 00520: val_loss improved from 2.48968 to 2.47118, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 521/5000
26/26 - 1s - loss: 2.0847 - val_loss: 2.4697
Epoch 522/5000
26/26 - 2s - loss: 2.0859 - val_loss: 2.4690
Epoch 523/5000
26/26 - 1s - loss: 2.0819 - val_loss: 2.4661
Epoch 524/5000
26/26 - 1s - loss: 2.0802 - val_loss: 2.4641
Epoch 525/5000
26/26 - 1s - loss: 2.0777 - val_loss: 2.4643
Epoch 526/5000
26/26 - 1s - loss: 2.0753 - val_loss: 2.4608
Epoch 527/5000
26/26 - 1s - loss: 2.0761 - val_loss: 2.4614
Epoch 528/5000
26/26 - 1s - loss: 2.0727 - val_loss: 2.4585
Epoch 529/5000
26/26 - 1s - loss: 2.0707 - val_loss: 2.4543
Epoch 530/5000
26/26 - 1s - loss: 2.0675 - val_loss: 2.4517
Epoch 00530: val_loss improved from 2.47118 to 2.45173, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 531/5000
26/26 - 1s - loss: 2.0648 - val_loss: 2.4498
Epoch 532/5000
26/26 - 1s - loss: 2.0657 - val_loss: 2.4484
Epoch 533/5000
26/26 - 1s - loss: 2.0640 - val_loss: 2.4466
Epoch 534/5000
26/26 - 1s - loss: 2.0586 - val_loss: 2.4448
Epoch 535/5000
26/26 - 1s - loss: 2.0587 - val_loss: 2.4435
Epoch 536/5000
26/26 - 1s - loss: 2.0566 - val_loss: 2.4409
Epoch 537/5000
26/26 - 1s - loss: 2.0538 - val_loss: 2.4409
Epoch 538/5000
26/26 - 1s - loss: 2.0531 - val_loss: 2.4381
Epoch 539/5000
26/26 - 1s - loss: 2.0504 - val_loss: 2.4359
Epoch 540/5000
26/26 - 1s - loss: 2.0459 - val_loss: 2.4330
Epoch 00540: val_loss improved from 2.45173 to 2.43296, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 541/5000
26/26 - 1s - loss: 2.0470 - val_loss: 2.4308
Epoch 542/5000
26/26 - 1s - loss: 2.0444 - val_loss: 2.4286
Epoch 543/5000
26/26 - 1s - loss: 2.0408 - val_loss: 2.4271
Epoch 544/5000
26/26 - 1s - loss: 2.0413 - val_loss: 2.4264
Epoch 545/5000
26/26 - 1s - loss: 2.0382 - val_loss: 2.4230
Epoch 546/5000
26/26 - 1s - loss: 2.0363 - val_loss: 2.4224
Epoch 547/5000
26/26 - 1s - loss: 2.0345 - val_loss: 2.4202
Epoch 548/5000
26/26 - 1s - loss: 2.0335 - val_loss: 2.4176
Epoch 549/5000
26/26 - 1s - loss: 2.0322 - val_loss: 2.4171
Epoch 550/5000
26/26 - 1s - loss: 2.0309 - val_loss: 2.4148
Epoch 00550: val_loss improved from 2.43296 to 2.41484, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 551/5000
26/26 - 1s - loss: 2.0255 - val_loss: 2.4122
Epoch 552/5000
26/26 - 1s - loss: 2.0258 - val_loss: 2.4101
Epoch 553/5000
26/26 - 1s - loss: 2.0235 - val_loss: 2.4076
Epoch 554/5000
26/26 - 1s - loss: 2.0228 - val_loss: 2.4097
Epoch 555/5000
26/26 - 1s - loss: 2.0188 - val_loss: 2.4066
Epoch 556/5000
26/26 - 1s - loss: 2.0178 - val_loss: 2.4040
Epoch 557/5000
26/26 - 1s - loss: 2.0154 - val_loss: 2.4019
Epoch 558/5000
26/26 - 1s - loss: 2.0123 - val_loss: 2.3991
Epoch 559/5000
26/26 - 1s - loss: 2.0128 - val_loss: 2.3983
Epoch 560/5000
26/26 - 1s - loss: 2.0112 - val_loss: 2.3949
Epoch 00560: val_loss improved from 2.41484 to 2.39493, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 561/5000
26/26 - 1s - loss: 2.0093 - val_loss: 2.3943
Epoch 562/5000
26/26 - 1s - loss: 2.0061 - val_loss: 2.3919
Epoch 563/5000
26/26 - 1s - loss: 2.0030 - val_loss: 2.3904
Epoch 564/5000
26/26 - 1s - loss: 2.0029 - val_loss: 2.3897
Epoch 565/5000
26/26 - 1s - loss: 1.9985 - val_loss: 2.3876
Epoch 566/5000
26/26 - 1s - loss: 1.9985 - val_loss: 2.3852
Epoch 567/5000
26/26 - 2s - loss: 1.9961 - val_loss: 2.3849
Epoch 568/5000
26/26 - 2s - loss: 1.9960 - val_loss: 2.3799
Epoch 569/5000
26/26 - 1s - loss: 1.9931 - val_loss: 2.3795
Epoch 570/5000
26/26 - 1s - loss: 1.9920 - val_loss: 2.3795
Epoch 00570: val_loss improved from 2.39493 to 2.37953, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 571/5000
26/26 - 1s - loss: 1.9885 - val_loss: 2.3767
Epoch 572/5000
26/26 - 1s - loss: 1.9867 - val_loss: 2.3751
Epoch 573/5000
26/26 - 1s - loss: 1.9842 - val_loss: 2.3741
Epoch 574/5000
26/26 - 1s - loss: 1.9834 - val_loss: 2.3702
Epoch 575/5000
26/26 - 1s - loss: 1.9842 - val_loss: 2.3689
Epoch 576/5000
26/26 - 1s - loss: 1.9790 - val_loss: 2.3674
Epoch 577/5000
26/26 - 1s - loss: 1.9768 - val_loss: 2.3658
Epoch 578/5000
26/26 - 1s - loss: 1.9769 - val_loss: 2.3637
Epoch 579/5000
26/26 - 1s - loss: 1.9736 - val_loss: 2.3618
Epoch 580/5000
26/26 - 1s - loss: 1.9731 - val_loss: 2.3594
Epoch 00580: val_loss improved from 2.37953 to 2.35940, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 581/5000
26/26 - 1s - loss: 1.9707 - val_loss: 2.3571
Epoch 582/5000
26/26 - 1s - loss: 1.9693 - val_loss: 2.3559
Epoch 583/5000
26/26 - 1s - loss: 1.9683 - val_loss: 2.3527
Epoch 584/5000
26/26 - 1s - loss: 1.9651 - val_loss: 2.3516
Epoch 585/5000
26/26 - 1s - loss: 1.9644 - val_loss: 2.3507
Epoch 586/5000
26/26 - 1s - loss: 1.9603 - val_loss: 2.3483
Epoch 587/5000
26/26 - 1s - loss: 1.9615 - val_loss: 2.3475
Epoch 588/5000
26/26 - 1s - loss: 1.9589 - val_loss: 2.3458
Epoch 589/5000
26/26 - 1s - loss: 1.9559 - val_loss: 2.3438
Epoch 590/5000
26/26 - 1s - loss: 1.9538 - val_loss: 2.3428
Epoch 00590: val_loss improved from 2.35940 to 2.34278, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 591/5000
26/26 - 1s - loss: 1.9524 - val_loss: 2.3400
Epoch 592/5000
26/26 - 1s - loss: 1.9498 - val_loss: 2.3401
Epoch 593/5000
26/26 - 1s - loss: 1.9490 - val_loss: 2.3365
Epoch 594/5000
26/26 - 1s - loss: 1.9475 - val_loss: 2.3353
Epoch 595/5000
26/26 - 1s - loss: 1.9429 - val_loss: 2.3325
Epoch 596/5000
26/26 - 1s - loss: 1.9443 - val_loss: 2.3325
Epoch 597/5000
26/26 - 1s - loss: 1.9422 - val_loss: 2.3305
Epoch 598/5000
26/26 - 1s - loss: 1.9412 - val_loss: 2.3283
Epoch 599/5000
26/26 - 1s - loss: 1.9402 - val_loss: 2.3286
Epoch 600/5000
26/26 - 1s - loss: 1.9351 - val_loss: 2.3259
Epoch 00600: val_loss improved from 2.34278 to 2.32594, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 601/5000
26/26 - 1s - loss: 1.9349 - val_loss: 2.3235
Epoch 602/5000
26/26 - 1s - loss: 1.9323 - val_loss: 2.3216
Epoch 603/5000
26/26 - 1s - loss: 1.9290 - val_loss: 2.3204
Epoch 604/5000
26/26 - 1s - loss: 1.9310 - val_loss: 2.3179
Epoch 605/5000
26/26 - 1s - loss: 1.9272 - val_loss: 2.3175
Epoch 606/5000
26/26 - 1s - loss: 1.9235 - val_loss: 2.3153
Epoch 607/5000
26/26 - 1s - loss: 1.9251 - val_loss: 2.3129
Epoch 608/5000
26/26 - 1s - loss: 1.9216 - val_loss: 2.3108
Epoch 609/5000
26/26 - 1s - loss: 1.9192 - val_loss: 2.3072
Epoch 610/5000
26/26 - 1s - loss: 1.9177 - val_loss: 2.3065
Epoch 00610: val_loss improved from 2.32594 to 2.30654, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 611/5000
26/26 - 1s - loss: 1.9182 - val_loss: 2.3048
Epoch 612/5000
26/26 - 1s - loss: 1.9148 - val_loss: 2.3039
Epoch 613/5000
26/26 - 1s - loss: 1.9124 - val_loss: 2.3005
Epoch 614/5000
26/26 - 1s - loss: 1.9128 - val_loss: 2.2999
Epoch 615/5000
26/26 - 1s - loss: 1.9100 - val_loss: 2.2987
Epoch 616/5000
26/26 - 1s - loss: 1.9092 - val_loss: 2.2968
Epoch 617/5000
26/26 - 1s - loss: 1.9067 - val_loss: 2.2961
Epoch 618/5000
26/26 - 1s - loss: 1.9052 - val_loss: 2.2940
Epoch 619/5000
26/26 - 1s - loss: 1.9036 - val_loss: 2.2928
Epoch 620/5000
26/26 - 1s - loss: 1.9031 - val_loss: 2.2894
Epoch 00620: val_loss improved from 2.30654 to 2.28940, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 621/5000
26/26 - 1s - loss: 1.8978 - val_loss: 2.2876
Epoch 622/5000
26/26 - 1s - loss: 1.8979 - val_loss: 2.2861
Epoch 623/5000
26/26 - 1s - loss: 1.8971 - val_loss: 2.2853
Epoch 624/5000
26/26 - 1s - loss: 1.8958 - val_loss: 2.2842
Epoch 625/5000
26/26 - 1s - loss: 1.8928 - val_loss: 2.2818
Epoch 626/5000
26/26 - 1s - loss: 1.8913 - val_loss: 2.2806
Epoch 627/5000
26/26 - 1s - loss: 1.8884 - val_loss: 2.2782
Epoch 628/5000
26/26 - 1s - loss: 1.8883 - val_loss: 2.2758
Epoch 629/5000
26/26 - 1s - loss: 1.8878 - val_loss: 2.2749
Epoch 630/5000
26/26 - 1s - loss: 1.8850 - val_loss: 2.2722
Epoch 00630: val_loss improved from 2.28940 to 2.27225, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 631/5000
26/26 - 1s - loss: 1.8824 - val_loss: 2.2723
Epoch 632/5000
26/26 - 1s - loss: 1.8796 - val_loss: 2.2704
Epoch 633/5000
26/26 - 1s - loss: 1.8805 - val_loss: 2.2677
Epoch 634/5000
26/26 - 1s - loss: 1.8760 - val_loss: 2.2671
Epoch 635/5000
26/26 - 1s - loss: 1.8775 - val_loss: 2.2653
Epoch 636/5000
26/26 - 1s - loss: 1.8734 - val_loss: 2.2646
Epoch 637/5000
26/26 - 1s - loss: 1.8730 - val_loss: 2.2626
Epoch 638/5000
26/26 - 1s - loss: 1.8693 - val_loss: 2.2596
Epoch 639/5000
26/26 - 1s - loss: 1.8678 - val_loss: 2.2581
Epoch 640/5000
26/26 - 1s - loss: 1.8681 - val_loss: 2.2568
Epoch 00640: val_loss improved from 2.27225 to 2.25684, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 641/5000
26/26 - 1s - loss: 1.8647 - val_loss: 2.2569
Epoch 642/5000
26/26 - 1s - loss: 1.8637 - val_loss: 2.2543
Epoch 643/5000
26/26 - 1s - loss: 1.8610 - val_loss: 2.2536
Epoch 644/5000
26/26 - 1s - loss: 1.8606 - val_loss: 2.2513
Epoch 645/5000
26/26 - 1s - loss: 1.8591 - val_loss: 2.2486
Epoch 646/5000
26/26 - 1s - loss: 1.8573 - val_loss: 2.2474
Epoch 647/5000
26/26 - 1s - loss: 1.8555 - val_loss: 2.2446
Epoch 648/5000
26/26 - 1s - loss: 1.8528 - val_loss: 2.2426
Epoch 649/5000
26/26 - 1s - loss: 1.8513 - val_loss: 2.2412
Epoch 650/5000
26/26 - 1s - loss: 1.8521 - val_loss: 2.2402
Epoch 00650: val_loss improved from 2.25684 to 2.24024, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 651/5000
26/26 - 1s - loss: 1.8499 - val_loss: 2.2385
Epoch 652/5000
26/26 - 1s - loss: 1.8478 - val_loss: 2.2368
Epoch 653/5000
26/26 - 1s - loss: 1.8473 - val_loss: 2.2351
Epoch 654/5000
26/26 - 1s - loss: 1.8456 - val_loss: 2.2331
Epoch 655/5000
26/26 - 1s - loss: 1.8430 - val_loss: 2.2322
Epoch 656/5000
26/26 - 1s - loss: 1.8429 - val_loss: 2.2310
Epoch 657/5000
26/26 - 1s - loss: 1.8393 - val_loss: 2.2300
Epoch 658/5000
26/26 - 1s - loss: 1.8382 - val_loss: 2.2274
Epoch 659/5000
26/26 - 1s - loss: 1.8372 - val_loss: 2.2260
Epoch 660/5000
26/26 - 1s - loss: 1.8353 - val_loss: 2.2251
Epoch 00660: val_loss improved from 2.24024 to 2.22515, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 661/5000
26/26 - 1s - loss: 1.8341 - val_loss: 2.2228
Epoch 662/5000
26/26 - 1s - loss: 1.8319 - val_loss: 2.2214
Epoch 663/5000
26/26 - 1s - loss: 1.8308 - val_loss: 2.2205
Epoch 664/5000
26/26 - 1s - loss: 1.8271 - val_loss: 2.2183
Epoch 665/5000
26/26 - 1s - loss: 1.8261 - val_loss: 2.2161
Epoch 666/5000
26/26 - 1s - loss: 1.8239 - val_loss: 2.2143
Epoch 667/5000
26/26 - 1s - loss: 1.8221 - val_loss: 2.2134
Epoch 668/5000
26/26 - 1s - loss: 1.8222 - val_loss: 2.2095
Epoch 669/5000
26/26 - 1s - loss: 1.8190 - val_loss: 2.2106
Epoch 670/5000
26/26 - 1s - loss: 1.8165 - val_loss: 2.2091
Epoch 00670: val_loss improved from 2.22515 to 2.20910, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 671/5000
26/26 - 1s - loss: 1.8185 - val_loss: 2.2068
Epoch 672/5000
26/26 - 1s - loss: 1.8147 - val_loss: 2.2039
Epoch 673/5000
26/26 - 1s - loss: 1.8145 - val_loss: 2.2037
Epoch 674/5000
26/26 - 1s - loss: 1.8116 - val_loss: 2.2013
Epoch 675/5000
26/26 - 1s - loss: 1.8110 - val_loss: 2.2007
Epoch 676/5000
26/26 - 1s - loss: 1.8098 - val_loss: 2.1983
Epoch 677/5000
26/26 - 2s - loss: 1.8086 - val_loss: 2.1958
Epoch 678/5000
26/26 - 1s - loss: 1.8052 - val_loss: 2.1969
Epoch 679/5000
26/26 - 1s - loss: 1.8019 - val_loss: 2.1942
Epoch 680/5000
26/26 - 1s - loss: 1.8017 - val_loss: 2.1917
Epoch 00680: val_loss improved from 2.20910 to 2.19169, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 681/5000
26/26 - 1s - loss: 1.7996 - val_loss: 2.1915
Epoch 682/5000
26/26 - 1s - loss: 1.8005 - val_loss: 2.1899
Epoch 683/5000
26/26 - 1s - loss: 1.7974 - val_loss: 2.1875
Epoch 684/5000
26/26 - 1s - loss: 1.7972 - val_loss: 2.1857
Epoch 685/5000
26/26 - 1s - loss: 1.7949 - val_loss: 2.1842
Epoch 686/5000
26/26 - 1s - loss: 1.7924 - val_loss: 2.1841
Epoch 687/5000
26/26 - 1s - loss: 1.7918 - val_loss: 2.1825
Epoch 688/5000
26/26 - 1s - loss: 1.7884 - val_loss: 2.1801
Epoch 689/5000
26/26 - 1s - loss: 1.7887 - val_loss: 2.1797
Epoch 690/5000
26/26 - 1s - loss: 1.7852 - val_loss: 2.1784
Epoch 00690: val_loss improved from 2.19169 to 2.17843, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 691/5000
26/26 - 1s - loss: 1.7848 - val_loss: 2.1750
Epoch 692/5000
26/26 - 1s - loss: 1.7825 - val_loss: 2.1758
Epoch 693/5000
26/26 - 1s - loss: 1.7812 - val_loss: 2.1732
Epoch 694/5000
26/26 - 1s - loss: 1.7789 - val_loss: 2.1722
Epoch 695/5000
26/26 - 1s - loss: 1.7788 - val_loss: 2.1695
Epoch 696/5000
26/26 - 1s - loss: 1.7760 - val_loss: 2.1676
Epoch 697/5000
26/26 - 1s - loss: 1.7751 - val_loss: 2.1664
Epoch 698/5000
26/26 - 1s - loss: 1.7747 - val_loss: 2.1660
Epoch 699/5000
26/26 - 1s - loss: 1.7735 - val_loss: 2.1631
Epoch 700/5000
26/26 - 1s - loss: 1.7710 - val_loss: 2.1617
Epoch 00700: val_loss improved from 2.17843 to 2.16167, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 701/5000
26/26 - 1s - loss: 1.7692 - val_loss: 2.1613
Epoch 702/5000
26/26 - 1s - loss: 1.7679 - val_loss: 2.1593
Epoch 703/5000
26/26 - 1s - loss: 1.7677 - val_loss: 2.1579
Epoch 704/5000
26/26 - 1s - loss: 1.7638 - val_loss: 2.1558
Epoch 705/5000
26/26 - 1s - loss: 1.7640 - val_loss: 2.1553
Epoch 706/5000
26/26 - 1s - loss: 1.7630 - val_loss: 2.1532
Epoch 707/5000
26/26 - 1s - loss: 1.7615 - val_loss: 2.1521
Epoch 708/5000
26/26 - 1s - loss: 1.7586 - val_loss: 2.1496
Epoch 709/5000
26/26 - 1s - loss: 1.7574 - val_loss: 2.1477
Epoch 710/5000
26/26 - 1s - loss: 1.7530 - val_loss: 2.1465
Epoch 00710: val_loss improved from 2.16167 to 2.14653, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 711/5000
26/26 - 1s - loss: 1.7553 - val_loss: 2.1443
Epoch 712/5000
26/26 - 1s - loss: 1.7535 - val_loss: 2.1433
Epoch 713/5000
26/26 - 1s - loss: 1.7510 - val_loss: 2.1428
Epoch 714/5000
26/26 - 1s - loss: 1.7504 - val_loss: 2.1419
Epoch 715/5000
26/26 - 1s - loss: 1.7464 - val_loss: 2.1399
Epoch 716/5000
26/26 - 1s - loss: 1.7479 - val_loss: 2.1396
Epoch 717/5000
26/26 - 1s - loss: 1.7466 - val_loss: 2.1380
Epoch 718/5000
26/26 - 1s - loss: 1.7440 - val_loss: 2.1366
Epoch 719/5000
26/26 - 1s - loss: 1.7426 - val_loss: 2.1337
Epoch 720/5000
26/26 - 1s - loss: 1.7402 - val_loss: 2.1322
Epoch 00720: val_loss improved from 2.14653 to 2.13216, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 721/5000
26/26 - 1s - loss: 1.7399 - val_loss: 2.1314
Epoch 722/5000
26/26 - 1s - loss: 1.7393 - val_loss: 2.1315
Epoch 723/5000
26/26 - 1s - loss: 1.7361 - val_loss: 2.1277
Epoch 724/5000
26/26 - 1s - loss: 1.7354 - val_loss: 2.1264
Epoch 725/5000
26/26 - 1s - loss: 1.7350 - val_loss: 2.1259
Epoch 726/5000
26/26 - 1s - loss: 1.7320 - val_loss: 2.1247
Epoch 727/5000
26/26 - 1s - loss: 1.7314 - val_loss: 2.1224
Epoch 728/5000
26/26 - 1s - loss: 1.7272 - val_loss: 2.1210
Epoch 729/5000
26/26 - 1s - loss: 1.7279 - val_loss: 2.1195
Epoch 730/5000
26/26 - 1s - loss: 1.7260 - val_loss: 2.1177
Epoch 00730: val_loss improved from 2.13216 to 2.11773, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 731/5000
26/26 - 1s - loss: 1.7244 - val_loss: 2.1168
Epoch 732/5000
26/26 - 1s - loss: 1.7231 - val_loss: 2.1162
Epoch 733/5000
26/26 - 1s - loss: 1.7209 - val_loss: 2.1130
Epoch 734/5000
26/26 - 1s - loss: 1.7207 - val_loss: 2.1109
Epoch 735/5000
26/26 - 1s - loss: 1.7183 - val_loss: 2.1087
Epoch 736/5000
26/26 - 1s - loss: 1.7190 - val_loss: 2.1092
Epoch 737/5000
26/26 - 1s - loss: 1.7157 - val_loss: 2.1087
Epoch 738/5000
26/26 - 1s - loss: 1.7148 - val_loss: 2.1055
Epoch 739/5000
26/26 - 1s - loss: 1.7115 - val_loss: 2.1032
Epoch 740/5000
26/26 - 1s - loss: 1.7103 - val_loss: 2.1062
Epoch 00740: val_loss improved from 2.11773 to 2.10623, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 741/5000
26/26 - 1s - loss: 1.7095 - val_loss: 2.1028
Epoch 742/5000
26/26 - 1s - loss: 1.7085 - val_loss: 2.1016
Epoch 743/5000
26/26 - 1s - loss: 1.7070 - val_loss: 2.1007
Epoch 744/5000
26/26 - 1s - loss: 1.7066 - val_loss: 2.0986
Epoch 745/5000
26/26 - 1s - loss: 1.7048 - val_loss: 2.0974
Epoch 746/5000
26/26 - 1s - loss: 1.7028 - val_loss: 2.0948
Epoch 747/5000
26/26 - 1s - loss: 1.7024 - val_loss: 2.0958
Epoch 748/5000
26/26 - 1s - loss: 1.7024 - val_loss: 2.0930
Epoch 749/5000
26/26 - 1s - loss: 1.7001 - val_loss: 2.0916
Epoch 750/5000
26/26 - 1s - loss: 1.6981 - val_loss: 2.0912
Epoch 00750: val_loss improved from 2.10623 to 2.09119, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 751/5000
26/26 - 1s - loss: 1.6950 - val_loss: 2.0897
Epoch 752/5000
26/26 - 1s - loss: 1.6930 - val_loss: 2.0895
Epoch 753/5000
26/26 - 1s - loss: 1.6923 - val_loss: 2.0885
Epoch 754/5000
26/26 - 1s - loss: 1.6900 - val_loss: 2.0869
Epoch 755/5000
26/26 - 1s - loss: 1.6868 - val_loss: 2.0839
Epoch 756/5000
26/26 - 1s - loss: 1.6905 - val_loss: 2.0834
Epoch 757/5000
26/26 - 1s - loss: 1.6852 - val_loss: 2.0813
Epoch 758/5000
26/26 - 1s - loss: 1.6865 - val_loss: 2.0786
Epoch 759/5000
26/26 - 1s - loss: 1.6850 - val_loss: 2.0773
Epoch 760/5000
26/26 - 1s - loss: 1.6827 - val_loss: 2.0770
Epoch 00760: val_loss improved from 2.09119 to 2.07695, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 761/5000
26/26 - 1s - loss: 1.6813 - val_loss: 2.0773
Epoch 762/5000
26/26 - 1s - loss: 1.6793 - val_loss: 2.0751
Epoch 763/5000
26/26 - 1s - loss: 1.6790 - val_loss: 2.0716
Epoch 764/5000
26/26 - 1s - loss: 1.6770 - val_loss: 2.0709
Epoch 765/5000
26/26 - 1s - loss: 1.6758 - val_loss: 2.0691
Epoch 766/5000
26/26 - 1s - loss: 1.6751 - val_loss: 2.0672
Epoch 767/5000
26/26 - 1s - loss: 1.6728 - val_loss: 2.0659
Epoch 768/5000
26/26 - 1s - loss: 1.6728 - val_loss: 2.0654
Epoch 769/5000
26/26 - 1s - loss: 1.6712 - val_loss: 2.0647
Epoch 770/5000
26/26 - 1s - loss: 1.6702 - val_loss: 2.0624
Epoch 00770: val_loss improved from 2.07695 to 2.06245, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 771/5000
26/26 - 1s - loss: 1.6664 - val_loss: 2.0620
Epoch 772/5000
26/26 - 1s - loss: 1.6666 - val_loss: 2.0592
Epoch 773/5000
26/26 - 1s - loss: 1.6647 - val_loss: 2.0590
Epoch 774/5000
26/26 - 1s - loss: 1.6637 - val_loss: 2.0573
Epoch 775/5000
26/26 - 1s - loss: 1.6618 - val_loss: 2.0553
Epoch 776/5000
26/26 - 1s - loss: 1.6588 - val_loss: 2.0538
Epoch 777/5000
26/26 - 1s - loss: 1.6594 - val_loss: 2.0541
Epoch 778/5000
26/26 - 1s - loss: 1.6572 - val_loss: 2.0538
Epoch 779/5000
26/26 - 1s - loss: 1.6564 - val_loss: 2.0500
Epoch 780/5000
26/26 - 1s - loss: 1.6531 - val_loss: 2.0489
Epoch 00780: val_loss improved from 2.06245 to 2.04888, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 781/5000
26/26 - 1s - loss: 1.6525 - val_loss: 2.0472
Epoch 782/5000
26/26 - 1s - loss: 1.6517 - val_loss: 2.0455
Epoch 783/5000
26/26 - 1s - loss: 1.6504 - val_loss: 2.0445
Epoch 784/5000
26/26 - 1s - loss: 1.6502 - val_loss: 2.0429
Epoch 785/5000
26/26 - 1s - loss: 1.6483 - val_loss: 2.0408
Epoch 786/5000
26/26 - 1s - loss: 1.6448 - val_loss: 2.0404
Epoch 787/5000
26/26 - 1s - loss: 1.6487 - val_loss: 2.0396
Epoch 788/5000
26/26 - 1s - loss: 1.6455 - val_loss: 2.0381
Epoch 789/5000
26/26 - 1s - loss: 1.6431 - val_loss: 2.0362
Epoch 790/5000
26/26 - 1s - loss: 1.6409 - val_loss: 2.0355
Epoch 00790: val_loss improved from 2.04888 to 2.03549, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 791/5000
26/26 - 1s - loss: 1.6405 - val_loss: 2.0332
Epoch 792/5000
26/26 - 1s - loss: 1.6381 - val_loss: 2.0310
Epoch 793/5000
26/26 - 1s - loss: 1.6369 - val_loss: 2.0290
Epoch 794/5000
26/26 - 1s - loss: 1.6361 - val_loss: 2.0304
Epoch 795/5000
26/26 - 1s - loss: 1.6357 - val_loss: 2.0293
Epoch 796/5000
26/26 - 1s - loss: 1.6322 - val_loss: 2.0271
Epoch 797/5000
26/26 - 1s - loss: 1.6298 - val_loss: 2.0247
Epoch 798/5000
26/26 - 1s - loss: 1.6311 - val_loss: 2.0237
Epoch 799/5000
26/26 - 1s - loss: 1.6292 - val_loss: 2.0235
Epoch 800/5000
26/26 - 1s - loss: 1.6272 - val_loss: 2.0240
Epoch 00800: val_loss improved from 2.03549 to 2.02398, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 801/5000
26/26 - 1s - loss: 1.6275 - val_loss: 2.0232
Epoch 802/5000
26/26 - 1s - loss: 1.6250 - val_loss: 2.0201
Epoch 803/5000
26/26 - 1s - loss: 1.6213 - val_loss: 2.0176
Epoch 804/5000
26/26 - 1s - loss: 1.6211 - val_loss: 2.0176
Epoch 805/5000
26/26 - 1s - loss: 1.6215 - val_loss: 2.0152
Epoch 806/5000
26/26 - 1s - loss: 1.6207 - val_loss: 2.0134
Epoch 807/5000
26/26 - 1s - loss: 1.6163 - val_loss: 2.0123
Epoch 808/5000
26/26 - 1s - loss: 1.6172 - val_loss: 2.0127
Epoch 809/5000
26/26 - 1s - loss: 1.6163 - val_loss: 2.0107
Epoch 810/5000
26/26 - 1s - loss: 1.6139 - val_loss: 2.0092
Epoch 00810: val_loss improved from 2.02398 to 2.00917, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 811/5000
26/26 - 1s - loss: 1.6139 - val_loss: 2.0075
Epoch 812/5000
26/26 - 1s - loss: 1.6127 - val_loss: 2.0082
Epoch 813/5000
26/26 - 1s - loss: 1.6122 - val_loss: 2.0064
Epoch 814/5000
26/26 - 1s - loss: 1.6064 - val_loss: 2.0045
Epoch 815/5000
26/26 - 1s - loss: 1.6071 - val_loss: 2.0034
Epoch 816/5000
26/26 - 1s - loss: 1.6069 - val_loss: 2.0018
Epoch 817/5000
26/26 - 1s - loss: 1.6054 - val_loss: 2.0003
Epoch 818/5000
26/26 - 1s - loss: 1.6055 - val_loss: 2.0009
Epoch 819/5000
26/26 - 1s - loss: 1.6039 - val_loss: 1.9979
Epoch 820/5000
26/26 - 1s - loss: 1.6022 - val_loss: 1.9967
Epoch 00820: val_loss improved from 2.00917 to 1.99670, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 821/5000
26/26 - 1s - loss: 1.6014 - val_loss: 1.9963
Epoch 822/5000
26/26 - 1s - loss: 1.5974 - val_loss: 1.9932
Epoch 823/5000
26/26 - 1s - loss: 1.5956 - val_loss: 1.9928
Epoch 824/5000
26/26 - 1s - loss: 1.5977 - val_loss: 1.9929
Epoch 825/5000
26/26 - 1s - loss: 1.5947 - val_loss: 1.9902
Epoch 826/5000
26/26 - 1s - loss: 1.5951 - val_loss: 1.9891
Epoch 827/5000
26/26 - 1s - loss: 1.5915 - val_loss: 1.9888
Epoch 828/5000
26/26 - 1s - loss: 1.5904 - val_loss: 1.9861
Epoch 829/5000
26/26 - 1s - loss: 1.5875 - val_loss: 1.9866
Epoch 830/5000
26/26 - 1s - loss: 1.5886 - val_loss: 1.9846
Epoch 00830: val_loss improved from 1.99670 to 1.98463, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 831/5000
26/26 - 1s - loss: 1.5874 - val_loss: 1.9836
Epoch 832/5000
26/26 - 1s - loss: 1.5855 - val_loss: 1.9803
Epoch 833/5000
26/26 - 1s - loss: 1.5850 - val_loss: 1.9806
Epoch 834/5000
26/26 - 1s - loss: 1.5820 - val_loss: 1.9799
Epoch 835/5000
26/26 - 1s - loss: 1.5828 - val_loss: 1.9776
Epoch 836/5000
26/26 - 1s - loss: 1.5807 - val_loss: 1.9758
Epoch 837/5000
26/26 - 1s - loss: 1.5786 - val_loss: 1.9750
Epoch 838/5000
26/26 - 1s - loss: 1.5780 - val_loss: 1.9736
Epoch 839/5000
26/26 - 1s - loss: 1.5783 - val_loss: 1.9726
Epoch 840/5000
26/26 - 1s - loss: 1.5754 - val_loss: 1.9708
Epoch 00840: val_loss improved from 1.98463 to 1.97082, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 841/5000
26/26 - 1s - loss: 1.5735 - val_loss: 1.9689
Epoch 842/5000
26/26 - 2s - loss: 1.5717 - val_loss: 1.9675
Epoch 843/5000
26/26 - 1s - loss: 1.5720 - val_loss: 1.9673
Epoch 844/5000
26/26 - 1s - loss: 1.5703 - val_loss: 1.9658
Epoch 845/5000
26/26 - 1s - loss: 1.5684 - val_loss: 1.9633
Epoch 846/5000
26/26 - 1s - loss: 1.5681 - val_loss: 1.9635
Epoch 847/5000
26/26 - 1s - loss: 1.5671 - val_loss: 1.9636
Epoch 848/5000
26/26 - 2s - loss: 1.5662 - val_loss: 1.9617
Epoch 849/5000
26/26 - 1s - loss: 1.5642 - val_loss: 1.9603
Epoch 850/5000
26/26 - 1s - loss: 1.5633 - val_loss: 1.9593
Epoch 00850: val_loss improved from 1.97082 to 1.95931, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 851/5000
26/26 - 1s - loss: 1.5609 - val_loss: 1.9587
Epoch 852/5000
26/26 - 1s - loss: 1.5603 - val_loss: 1.9574
Epoch 853/5000
26/26 - 1s - loss: 1.5598 - val_loss: 1.9538
Epoch 854/5000
26/26 - 1s - loss: 1.5594 - val_loss: 1.9525
Epoch 855/5000
26/26 - 1s - loss: 1.5554 - val_loss: 1.9528
Epoch 856/5000
26/26 - 1s - loss: 1.5562 - val_loss: 1.9512
Epoch 857/5000
26/26 - 1s - loss: 1.5541 - val_loss: 1.9494
Epoch 858/5000
26/26 - 1s - loss: 1.5524 - val_loss: 1.9488
Epoch 859/5000
26/26 - 1s - loss: 1.5489 - val_loss: 1.9461
Epoch 860/5000
26/26 - 1s - loss: 1.5518 - val_loss: 1.9467
Epoch 00860: val_loss improved from 1.95931 to 1.94671, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 861/5000
26/26 - 1s - loss: 1.5472 - val_loss: 1.9470
Epoch 862/5000
26/26 - 1s - loss: 1.5498 - val_loss: 1.9429
Epoch 863/5000
26/26 - 1s - loss: 1.5486 - val_loss: 1.9428
Epoch 864/5000
26/26 - 1s - loss: 1.5440 - val_loss: 1.9390
Epoch 865/5000
26/26 - 1s - loss: 1.5434 - val_loss: 1.9392
Epoch 866/5000
26/26 - 1s - loss: 1.5440 - val_loss: 1.9388
Epoch 867/5000
26/26 - 1s - loss: 1.5407 - val_loss: 1.9385
Epoch 868/5000
26/26 - 1s - loss: 1.5404 - val_loss: 1.9389
Epoch 869/5000
26/26 - 1s - loss: 1.5392 - val_loss: 1.9374
Epoch 870/5000
26/26 - 1s - loss: 1.5406 - val_loss: 1.9368
Epoch 00870: val_loss improved from 1.94671 to 1.93675, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 871/5000
26/26 - 1s - loss: 1.5372 - val_loss: 1.9330
Epoch 872/5000
26/26 - 1s - loss: 1.5357 - val_loss: 1.9315
Epoch 873/5000
26/26 - 1s - loss: 1.5334 - val_loss: 1.9307
Epoch 874/5000
26/26 - 1s - loss: 1.5332 - val_loss: 1.9293
Epoch 875/5000
26/26 - 1s - loss: 1.5329 - val_loss: 1.9297
Epoch 876/5000
26/26 - 1s - loss: 1.5327 - val_loss: 1.9259
Epoch 877/5000
26/26 - 1s - loss: 1.5298 - val_loss: 1.9249
Epoch 878/5000
26/26 - 1s - loss: 1.5285 - val_loss: 1.9247
Epoch 879/5000
26/26 - 1s - loss: 1.5272 - val_loss: 1.9227
Epoch 880/5000
26/26 - 1s - loss: 1.5247 - val_loss: 1.9228
Epoch 00880: val_loss improved from 1.93675 to 1.92282, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 881/5000
26/26 - 1s - loss: 1.5243 - val_loss: 1.9222
Epoch 882/5000
26/26 - 1s - loss: 1.5241 - val_loss: 1.9195
Epoch 883/5000
26/26 - 1s - loss: 1.5216 - val_loss: 1.9180
Epoch 884/5000
26/26 - 2s - loss: 1.5189 - val_loss: 1.9186
Epoch 885/5000
26/26 - 1s - loss: 1.5194 - val_loss: 1.9157
Epoch 886/5000
26/26 - 1s - loss: 1.5183 - val_loss: 1.9158
Epoch 887/5000
26/26 - 1s - loss: 1.5182 - val_loss: 1.9142
Epoch 888/5000
26/26 - 1s - loss: 1.5157 - val_loss: 1.9125
Epoch 889/5000
26/26 - 1s - loss: 1.5141 - val_loss: 1.9110
Epoch 890/5000
26/26 - 1s - loss: 1.5139 - val_loss: 1.9104
Epoch 00890: val_loss improved from 1.92282 to 1.91035, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 891/5000
26/26 - 1s - loss: 1.5127 - val_loss: 1.9096
Epoch 892/5000
26/26 - 1s - loss: 1.5117 - val_loss: 1.9083
Epoch 893/5000
26/26 - 1s - loss: 1.5094 - val_loss: 1.9083
Epoch 894/5000
26/26 - 1s - loss: 1.5088 - val_loss: 1.9065
Epoch 895/5000
26/26 - 1s - loss: 1.5085 - val_loss: 1.9033
Epoch 896/5000
26/26 - 1s - loss: 1.5058 - val_loss: 1.9042
Epoch 897/5000
26/26 - 1s - loss: 1.5070 - val_loss: 1.9018
Epoch 898/5000
26/26 - 1s - loss: 1.5036 - val_loss: 1.9019
Epoch 899/5000
26/26 - 1s - loss: 1.5045 - val_loss: 1.9002
Epoch 900/5000
26/26 - 1s - loss: 1.5014 - val_loss: 1.8988
Epoch 00900: val_loss improved from 1.91035 to 1.89876, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 901/5000
26/26 - 1s - loss: 1.5004 - val_loss: 1.8982
Epoch 902/5000
26/26 - 1s - loss: 1.5001 - val_loss: 1.8963
Epoch 903/5000
26/26 - 1s - loss: 1.4965 - val_loss: 1.8966
Epoch 904/5000
26/26 - 1s - loss: 1.4972 - val_loss: 1.8941
Epoch 905/5000
26/26 - 1s - loss: 1.4964 - val_loss: 1.8922
Epoch 906/5000
26/26 - 1s - loss: 1.4947 - val_loss: 1.8910
Epoch 907/5000
26/26 - 1s - loss: 1.4941 - val_loss: 1.8928
Epoch 908/5000
26/26 - 1s - loss: 1.4937 - val_loss: 1.8891
Epoch 909/5000
26/26 - 1s - loss: 1.4910 - val_loss: 1.8880
Epoch 910/5000
26/26 - 1s - loss: 1.4891 - val_loss: 1.8863
Epoch 00910: val_loss improved from 1.89876 to 1.88628, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 911/5000
26/26 - 1s - loss: 1.4875 - val_loss: 1.8871
Epoch 912/5000
26/26 - 1s - loss: 1.4862 - val_loss: 1.8840
Epoch 913/5000
26/26 - 1s - loss: 1.4864 - val_loss: 1.8832
Epoch 914/5000
26/26 - 1s - loss: 1.4859 - val_loss: 1.8808
Epoch 915/5000
26/26 - 1s - loss: 1.4851 - val_loss: 1.8807
Epoch 916/5000
26/26 - 1s - loss: 1.4835 - val_loss: 1.8796
Epoch 917/5000
26/26 - 1s - loss: 1.4809 - val_loss: 1.8790
Epoch 918/5000
26/26 - 1s - loss: 1.4800 - val_loss: 1.8785
Epoch 919/5000
26/26 - 1s - loss: 1.4794 - val_loss: 1.8767
Epoch 920/5000
26/26 - 1s - loss: 1.4783 - val_loss: 1.8783
Epoch 00920: val_loss improved from 1.88628 to 1.87829, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 921/5000
26/26 - 1s - loss: 1.4778 - val_loss: 1.8763
Epoch 922/5000
26/26 - 1s - loss: 1.4772 - val_loss: 1.8744
Epoch 923/5000
26/26 - 1s - loss: 1.4752 - val_loss: 1.8734
Epoch 924/5000
26/26 - 1s - loss: 1.4730 - val_loss: 1.8730
Epoch 925/5000
26/26 - 1s - loss: 1.4712 - val_loss: 1.8720
Epoch 926/5000
26/26 - 1s - loss: 1.4724 - val_loss: 1.8702
Epoch 927/5000
26/26 - 1s - loss: 1.4690 - val_loss: 1.8690
Epoch 928/5000
26/26 - 1s - loss: 1.4697 - val_loss: 1.8666
Epoch 929/5000
26/26 - 1s - loss: 1.4684 - val_loss: 1.8668
Epoch 930/5000
26/26 - 1s - loss: 1.4661 - val_loss: 1.8643
Epoch 00930: val_loss improved from 1.87829 to 1.86432, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 931/5000
26/26 - 1s - loss: 1.4671 - val_loss: 1.8649
Epoch 932/5000
26/26 - 1s - loss: 1.4650 - val_loss: 1.8633
Epoch 933/5000
26/26 - 1s - loss: 1.4628 - val_loss: 1.8627
Epoch 934/5000
26/26 - 1s - loss: 1.4625 - val_loss: 1.8610
Epoch 935/5000
26/26 - 1s - loss: 1.4604 - val_loss: 1.8627
Epoch 936/5000
26/26 - 1s - loss: 1.4598 - val_loss: 1.8597
Epoch 937/5000
26/26 - 1s - loss: 1.4602 - val_loss: 1.8595
Epoch 938/5000
26/26 - 1s - loss: 1.4587 - val_loss: 1.8575
Epoch 939/5000
26/26 - 1s - loss: 1.4572 - val_loss: 1.8555
Epoch 940/5000
26/26 - 1s - loss: 1.4538 - val_loss: 1.8547
Epoch 00940: val_loss improved from 1.86432 to 1.85474, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 941/5000
26/26 - 1s - loss: 1.4541 - val_loss: 1.8544
Epoch 942/5000
26/26 - 1s - loss: 1.4543 - val_loss: 1.8521
Epoch 943/5000
26/26 - 1s - loss: 1.4535 - val_loss: 1.8501
Epoch 944/5000
26/26 - 1s - loss: 1.4497 - val_loss: 1.8495
Epoch 945/5000
26/26 - 1s - loss: 1.4517 - val_loss: 1.8491
Epoch 946/5000
26/26 - 1s - loss: 1.4484 - val_loss: 1.8470
Epoch 947/5000
26/26 - 1s - loss: 1.4478 - val_loss: 1.8453
Epoch 948/5000
26/26 - 1s - loss: 1.4467 - val_loss: 1.8448
Epoch 949/5000
26/26 - 1s - loss: 1.4458 - val_loss: 1.8447
Epoch 950/5000
26/26 - 1s - loss: 1.4434 - val_loss: 1.8436
Epoch 00950: val_loss improved from 1.85474 to 1.84357, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 951/5000
26/26 - 1s - loss: 1.4425 - val_loss: 1.8426
Epoch 952/5000
26/26 - 1s - loss: 1.4417 - val_loss: 1.8395
Epoch 953/5000
26/26 - 1s - loss: 1.4419 - val_loss: 1.8399
Epoch 954/5000
26/26 - 1s - loss: 1.4401 - val_loss: 1.8396
Epoch 955/5000
26/26 - 1s - loss: 1.4377 - val_loss: 1.8369
Epoch 956/5000
26/26 - 1s - loss: 1.4376 - val_loss: 1.8361
Epoch 957/5000
26/26 - 1s - loss: 1.4362 - val_loss: 1.8366
Epoch 958/5000
26/26 - 1s - loss: 1.4365 - val_loss: 1.8359
Epoch 959/5000
26/26 - 1s - loss: 1.4341 - val_loss: 1.8349
Epoch 960/5000
26/26 - 1s - loss: 1.4341 - val_loss: 1.8326
Epoch 00960: val_loss improved from 1.84357 to 1.83256, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 961/5000
26/26 - 1s - loss: 1.4309 - val_loss: 1.8328
Epoch 962/5000
26/26 - 1s - loss: 1.4316 - val_loss: 1.8299
Epoch 963/5000
26/26 - 1s - loss: 1.4284 - val_loss: 1.8278
Epoch 964/5000
26/26 - 1s - loss: 1.4277 - val_loss: 1.8281
Epoch 965/5000
26/26 - 1s - loss: 1.4276 - val_loss: 1.8280
Epoch 966/5000
26/26 - 1s - loss: 1.4281 - val_loss: 1.8256
Epoch 967/5000
26/26 - 2s - loss: 1.4238 - val_loss: 1.8243
Epoch 968/5000
26/26 - 1s - loss: 1.4228 - val_loss: 1.8230
Epoch 969/5000
26/26 - 1s - loss: 1.4242 - val_loss: 1.8235
Epoch 970/5000
26/26 - 1s - loss: 1.4250 - val_loss: 1.8214
Epoch 00970: val_loss improved from 1.83256 to 1.82136, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 971/5000
26/26 - 1s - loss: 1.4184 - val_loss: 1.8195
Epoch 972/5000
26/26 - 1s - loss: 1.4201 - val_loss: 1.8185
Epoch 973/5000
26/26 - 1s - loss: 1.4180 - val_loss: 1.8172
Epoch 974/5000
26/26 - 1s - loss: 1.4164 - val_loss: 1.8179
Epoch 975/5000
26/26 - 1s - loss: 1.4170 - val_loss: 1.8160
Epoch 976/5000
26/26 - 1s - loss: 1.4166 - val_loss: 1.8138
Epoch 977/5000
26/26 - 1s - loss: 1.4144 - val_loss: 1.8152
Epoch 978/5000
26/26 - 1s - loss: 1.4132 - val_loss: 1.8124
Epoch 979/5000
26/26 - 1s - loss: 1.4140 - val_loss: 1.8115
Epoch 980/5000
26/26 - 1s - loss: 1.4117 - val_loss: 1.8095
Epoch 00980: val_loss improved from 1.82136 to 1.80955, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 981/5000
26/26 - 1s - loss: 1.4119 - val_loss: 1.8103
Epoch 982/5000
26/26 - 1s - loss: 1.4072 - val_loss: 1.8079
Epoch 983/5000
26/26 - 1s - loss: 1.4097 - val_loss: 1.8045
Epoch 984/5000
26/26 - 1s - loss: 1.4061 - val_loss: 1.8044
Epoch 985/5000
26/26 - 1s - loss: 1.4054 - val_loss: 1.8045
Epoch 986/5000
26/26 - 1s - loss: 1.4060 - val_loss: 1.8026
Epoch 987/5000
26/26 - 1s - loss: 1.4051 - val_loss: 1.8025
Epoch 988/5000
26/26 - 1s - loss: 1.4035 - val_loss: 1.7999
Epoch 989/5000
26/26 - 1s - loss: 1.4006 - val_loss: 1.7995
Epoch 990/5000
26/26 - 1s - loss: 1.4013 - val_loss: 1.8004
Epoch 00990: val_loss improved from 1.80955 to 1.80040, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 991/5000
26/26 - 1s - loss: 1.3994 - val_loss: 1.8008
Epoch 992/5000
26/26 - 1s - loss: 1.3984 - val_loss: 1.7999
Epoch 993/5000
26/26 - 1s - loss: 1.3980 - val_loss: 1.7973
Epoch 994/5000
26/26 - 1s - loss: 1.3977 - val_loss: 1.7965
Epoch 995/5000
26/26 - 1s - loss: 1.3961 - val_loss: 1.7953
Epoch 996/5000
26/26 - 1s - loss: 1.3953 - val_loss: 1.7943
Epoch 997/5000
26/26 - 1s - loss: 1.3943 - val_loss: 1.7946
Epoch 998/5000
26/26 - 1s - loss: 1.3936 - val_loss: 1.7939
Epoch 999/5000
26/26 - 1s - loss: 1.3937 - val_loss: 1.7905
Epoch 1000/5000
26/26 - 1s - loss: 1.3900 - val_loss: 1.7922
Epoch 01000: val_loss improved from 1.80040 to 1.79217, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 1001/5000
26/26 - 1s - loss: 1.3895 - val_loss: 1.7898
Epoch 1002/5000
26/26 - 1s - loss: 1.3881 - val_loss: 1.7870
Epoch 1003/5000
26/26 - 1s - loss: 1.3874 - val_loss: 1.7868
Epoch 1004/5000
26/26 - 1s - loss: 1.3872 - val_loss: 1.7858
Epoch 1005/5000
26/26 - 1s - loss: 1.3849 - val_loss: 1.7830
Epoch 1006/5000
26/26 - 1s - loss: 1.3838 - val_loss: 1.7833
Epoch 1007/5000
26/26 - 1s - loss: 1.3827 - val_loss: 1.7825
Epoch 1008/5000
26/26 - 1s - loss: 1.3825 - val_loss: 1.7825
Epoch 1009/5000
26/26 - 1s - loss: 1.3830 - val_loss: 1.7817
Epoch 1010/5000
26/26 - 1s - loss: 1.3811 - val_loss: 1.7805
Epoch 01010: val_loss improved from 1.79217 to 1.78050, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 1011/5000
26/26 - 1s - loss: 1.3793 - val_loss: 1.7795
Epoch 1012/5000
26/26 - 1s - loss: 1.3777 - val_loss: 1.7780
Epoch 1013/5000
26/26 - 1s - loss: 1.3772 - val_loss: 1.7766
Epoch 1014/5000
26/26 - 1s - loss: 1.3736 - val_loss: 1.7749
Epoch 1015/5000
26/26 - 1s - loss: 1.3763 - val_loss: 1.7732
Epoch 1016/5000
26/26 - 1s - loss: 1.3754 - val_loss: 1.7716
Epoch 1017/5000
26/26 - 1s - loss: 1.3729 - val_loss: 1.7720
Epoch 1018/5000
26/26 - 1s - loss: 1.3719 - val_loss: 1.7709
Epoch 1019/5000
26/26 - 1s - loss: 1.3705 - val_loss: 1.7682
Epoch 1020/5000
26/26 - 1s - loss: 1.3716 - val_loss: 1.7680
Epoch 01020: val_loss improved from 1.78050 to 1.76801, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 1021/5000
26/26 - 1s - loss: 1.3682 - val_loss: 1.7688
Epoch 1022/5000
26/26 - 1s - loss: 1.3690 - val_loss: 1.7668
Epoch 1023/5000
26/26 - 1s - loss: 1.3663 - val_loss: 1.7653
Epoch 1024/5000
26/26 - 1s - loss: 1.3664 - val_loss: 1.7669
Epoch 1025/5000
26/26 - 1s - loss: 1.3651 - val_loss: 1.7648
Epoch 1026/5000
26/26 - 1s - loss: 1.3614 - val_loss: 1.7617
Epoch 1027/5000
26/26 - 1s - loss: 1.3622 - val_loss: 1.7619
Epoch 1028/5000
26/26 - 1s - loss: 1.3626 - val_loss: 1.7614
Epoch 1029/5000
26/26 - 1s - loss: 1.3609 - val_loss: 1.7593
Epoch 1030/5000
26/26 - 1s - loss: 1.3603 - val_loss: 1.7591
Epoch 01030: val_loss improved from 1.76801 to 1.75908, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 1031/5000
26/26 - 1s - loss: 1.3568 - val_loss: 1.7582
Epoch 1032/5000
26/26 - 1s - loss: 1.3568 - val_loss: 1.7569
Epoch 1033/5000
26/26 - 1s - loss: 1.3570 - val_loss: 1.7567
Epoch 1034/5000
26/26 - 1s - loss: 1.3547 - val_loss: 1.7549
Epoch 1035/5000
26/26 - 1s - loss: 1.3542 - val_loss: 1.7535
Epoch 1036/5000
26/26 - 1s - loss: 1.3540 - val_loss: 1.7529
Epoch 1037/5000
26/26 - 1s - loss: 1.3521 - val_loss: 1.7512
Epoch 1038/5000
26/26 - 1s - loss: 1.3531 - val_loss: 1.7526
Epoch 1039/5000
26/26 - 1s - loss: 1.3508 - val_loss: 1.7508
Epoch 1040/5000
26/26 - 1s - loss: 1.3498 - val_loss: 1.7478
Epoch 01040: val_loss improved from 1.75908 to 1.74775, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 1041/5000
26/26 - 1s - loss: 1.3499 - val_loss: 1.7497
Epoch 1042/5000
26/26 - 1s - loss: 1.3460 - val_loss: 1.7471
Epoch 1043/5000
26/26 - 1s - loss: 1.3479 - val_loss: 1.7470
Epoch 1044/5000
26/26 - 1s - loss: 1.3454 - val_loss: 1.7452
Epoch 1045/5000
26/26 - 1s - loss: 1.3428 - val_loss: 1.7444
Epoch 1046/5000
26/26 - 1s - loss: 1.3431 - val_loss: 1.7421
Epoch 1047/5000
26/26 - 1s - loss: 1.3424 - val_loss: 1.7421
Epoch 1048/5000
26/26 - 1s - loss: 1.3403 - val_loss: 1.7425
Epoch 1049/5000
26/26 - 1s - loss: 1.3425 - val_loss: 1.7401
Epoch 1050/5000
26/26 - 1s - loss: 1.3399 - val_loss: 1.7406
Epoch 01050: val_loss improved from 1.74775 to 1.74059, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 1051/5000
26/26 - 1s - loss: 1.3392 - val_loss: 1.7377
Epoch 1052/5000
26/26 - 1s - loss: 1.3361 - val_loss: 1.7397
Epoch 1053/5000
26/26 - 1s - loss: 1.3361 - val_loss: 1.7372
Epoch 1054/5000
26/26 - 1s - loss: 1.3359 - val_loss: 1.7360
Epoch 1055/5000
26/26 - 1s - loss: 1.3330 - val_loss: 1.7353
Epoch 1056/5000
26/26 - 1s - loss: 1.3346 - val_loss: 1.7334
Epoch 1057/5000
26/26 - 1s - loss: 1.3327 - val_loss: 1.7334
Epoch 1058/5000
26/26 - 2s - loss: 1.3333 - val_loss: 1.7335
Epoch 1059/5000
26/26 - 1s - loss: 1.3322 - val_loss: 1.7307
Epoch 1060/5000
26/26 - 1s - loss: 1.3292 - val_loss: 1.7295
Epoch 01060: val_loss improved from 1.74059 to 1.72953, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 1061/5000
26/26 - 1s - loss: 1.3276 - val_loss: 1.7289
Epoch 1062/5000
26/26 - 1s - loss: 1.3280 - val_loss: 1.7272
Epoch 1063/5000
26/26 - 1s - loss: 1.3256 - val_loss: 1.7269
Epoch 1064/5000
26/26 - 1s - loss: 1.3267 - val_loss: 1.7258
Epoch 1065/5000
26/26 - 1s - loss: 1.3274 - val_loss: 1.7250
Epoch 1066/5000
26/26 - 1s - loss: 1.3254 - val_loss: 1.7245
Epoch 1067/5000
26/26 - 1s - loss: 1.3228 - val_loss: 1.7237
Epoch 1068/5000
26/26 - 1s - loss: 1.3230 - val_loss: 1.7229
Epoch 1069/5000
26/26 - 1s - loss: 1.3214 - val_loss: 1.7214
Epoch 1070/5000
26/26 - 1s - loss: 1.3208 - val_loss: 1.7207
Epoch 01070: val_loss improved from 1.72953 to 1.72074, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 1071/5000
26/26 - 1s - loss: 1.3192 - val_loss: 1.7206
Epoch 1072/5000
26/26 - 1s - loss: 1.3189 - val_loss: 1.7190
Epoch 1073/5000
26/26 - 1s - loss: 1.3168 - val_loss: 1.7166
Epoch 1074/5000
26/26 - 1s - loss: 1.3162 - val_loss: 1.7161
Epoch 1075/5000
26/26 - 1s - loss: 1.3156 - val_loss: 1.7148
Epoch 1076/5000
26/26 - 1s - loss: 1.3138 - val_loss: 1.7146
Epoch 1077/5000
26/26 - 1s - loss: 1.3137 - val_loss: 1.7142
Epoch 1078/5000
26/26 - 1s - loss: 1.3116 - val_loss: 1.7132
Epoch 1079/5000
26/26 - 1s - loss: 1.3106 - val_loss: 1.7118
Epoch 1080/5000
26/26 - 1s - loss: 1.3112 - val_loss: 1.7119
Epoch 01080: val_loss improved from 1.72074 to 1.71186, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 1081/5000
26/26 - 1s - loss: 1.3103 - val_loss: 1.7104
Epoch 1082/5000
26/26 - 1s - loss: 1.3081 - val_loss: 1.7098
Epoch 1083/5000
26/26 - 1s - loss: 1.3067 - val_loss: 1.7094
Epoch 1084/5000
26/26 - 1s - loss: 1.3070 - val_loss: 1.7077
Epoch 1085/5000
26/26 - 1s - loss: 1.3061 - val_loss: 1.7046
Epoch 1086/5000
26/26 - 1s - loss: 1.3068 - val_loss: 1.7059
Epoch 1087/5000
26/26 - 1s - loss: 1.3046 - val_loss: 1.7054
Epoch 1088/5000
26/26 - 1s - loss: 1.3027 - val_loss: 1.7015
Epoch 1089/5000
26/26 - 1s - loss: 1.3022 - val_loss: 1.7024
Epoch 1090/5000
26/26 - 1s - loss: 1.3027 - val_loss: 1.7035
Epoch 01090: val_loss improved from 1.71186 to 1.70350, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 1091/5000
26/26 - 2s - loss: 1.3024 - val_loss: 1.7013
Epoch 1092/5000
26/26 - 2s - loss: 1.2990 - val_loss: 1.7020
Epoch 1093/5000
26/26 - 1s - loss: 1.2982 - val_loss: 1.6992
Epoch 1094/5000
26/26 - 1s - loss: 1.2992 - val_loss: 1.6960
Epoch 1095/5000
26/26 - 1s - loss: 1.2964 - val_loss: 1.6964
Epoch 1096/5000
26/26 - 1s - loss: 1.2965 - val_loss: 1.6953
Epoch 1097/5000
26/26 - 1s - loss: 1.2942 - val_loss: 1.6955
Epoch 1098/5000
26/26 - 1s - loss: 1.2940 - val_loss: 1.6952
Epoch 1099/5000
26/26 - 1s - loss: 1.2940 - val_loss: 1.6938
Epoch 1100/5000
26/26 - 1s - loss: 1.2922 - val_loss: 1.6917
Epoch 01100: val_loss improved from 1.70350 to 1.69171, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 1101/5000
26/26 - 1s - loss: 1.2927 - val_loss: 1.6915
Epoch 1102/5000
26/26 - 1s - loss: 1.2911 - val_loss: 1.6912
Epoch 1103/5000
26/26 - 1s - loss: 1.2902 - val_loss: 1.6902
Epoch 1104/5000
26/26 - 1s - loss: 1.2890 - val_loss: 1.6896
Epoch 1105/5000
26/26 - 1s - loss: 1.2869 - val_loss: 1.6870
Epoch 1106/5000
26/26 - 1s - loss: 1.2874 - val_loss: 1.6870
Epoch 1107/5000
26/26 - 1s - loss: 1.2864 - val_loss: 1.6857
Epoch 1108/5000
26/26 - 1s - loss: 1.2838 - val_loss: 1.6882
Epoch 1109/5000
26/26 - 1s - loss: 1.2850 - val_loss: 1.6846
Epoch 1110/5000
26/26 - 1s - loss: 1.2837 - val_loss: 1.6837
Epoch 01110: val_loss improved from 1.69171 to 1.68375, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 1111/5000
26/26 - 1s - loss: 1.2822 - val_loss: 1.6830
Epoch 1112/5000
26/26 - 1s - loss: 1.2808 - val_loss: 1.6840
Epoch 1113/5000
26/26 - 1s - loss: 1.2817 - val_loss: 1.6832
Epoch 1114/5000
26/26 - 1s - loss: 1.2786 - val_loss: 1.6814
Epoch 1115/5000
26/26 - 1s - loss: 1.2774 - val_loss: 1.6781
Epoch 1116/5000
26/26 - 1s - loss: 1.2778 - val_loss: 1.6794
Epoch 1117/5000
26/26 - 1s - loss: 1.2759 - val_loss: 1.6790
Epoch 1118/5000
26/26 - 1s - loss: 1.2761 - val_loss: 1.6747
Epoch 1119/5000
26/26 - 1s - loss: 1.2763 - val_loss: 1.6756
Epoch 1120/5000
26/26 - 1s - loss: 1.2745 - val_loss: 1.6759
Epoch 01120: val_loss improved from 1.68375 to 1.67588, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 1121/5000
26/26 - 1s - loss: 1.2711 - val_loss: 1.6729
Epoch 1122/5000
26/26 - 1s - loss: 1.2715 - val_loss: 1.6729
Epoch 1123/5000
26/26 - 1s - loss: 1.2716 - val_loss: 1.6707
Epoch 1124/5000
26/26 - 1s - loss: 1.2708 - val_loss: 1.6705
Epoch 1125/5000
26/26 - 1s - loss: 1.2716 - val_loss: 1.6702
Epoch 1126/5000
26/26 - 1s - loss: 1.2696 - val_loss: 1.6700
Epoch 1127/5000
26/26 - 1s - loss: 1.2673 - val_loss: 1.6705
Epoch 1128/5000
26/26 - 1s - loss: 1.2669 - val_loss: 1.6677
Epoch 1129/5000
26/26 - 1s - loss: 1.2662 - val_loss: 1.6682
Epoch 1130/5000
26/26 - 1s - loss: 1.2671 - val_loss: 1.6679
Epoch 01130: val_loss improved from 1.67588 to 1.66789, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 1131/5000
26/26 - 1s - loss: 1.2633 - val_loss: 1.6664
Epoch 1132/5000
26/26 - 1s - loss: 1.2620 - val_loss: 1.6652
Epoch 1133/5000
26/26 - 1s - loss: 1.2622 - val_loss: 1.6621
Epoch 1134/5000
26/26 - 1s - loss: 1.2603 - val_loss: 1.6640
Epoch 1135/5000
26/26 - 1s - loss: 1.2609 - val_loss: 1.6629
Epoch 1136/5000
26/26 - 1s - loss: 1.2592 - val_loss: 1.6611
Epoch 1137/5000
26/26 - 1s - loss: 1.2594 - val_loss: 1.6604
Epoch 1138/5000
26/26 - 1s - loss: 1.2577 - val_loss: 1.6589
Epoch 1139/5000
26/26 - 1s - loss: 1.2563 - val_loss: 1.6567
Epoch 1140/5000
26/26 - 1s - loss: 1.2558 - val_loss: 1.6569
Epoch 01140: val_loss improved from 1.66789 to 1.65688, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 1141/5000
26/26 - 1s - loss: 1.2547 - val_loss: 1.6560
Epoch 1142/5000
26/26 - 1s - loss: 1.2539 - val_loss: 1.6577
Epoch 1143/5000
26/26 - 1s - loss: 1.2540 - val_loss: 1.6535
Epoch 1144/5000
26/26 - 1s - loss: 1.2516 - val_loss: 1.6540
Epoch 1145/5000
26/26 - 1s - loss: 1.2510 - val_loss: 1.6529
Epoch 1146/5000
26/26 - 1s - loss: 1.2476 - val_loss: 1.6515
Epoch 1147/5000
26/26 - 1s - loss: 1.2501 - val_loss: 1.6511
Epoch 1148/5000
26/26 - 1s - loss: 1.2484 - val_loss: 1.6508
Epoch 1149/5000
26/26 - 1s - loss: 1.2484 - val_loss: 1.6469
Epoch 1150/5000
26/26 - 1s - loss: 1.2478 - val_loss: 1.6482
Epoch 01150: val_loss improved from 1.65688 to 1.64823, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 1151/5000
26/26 - 1s - loss: 1.2450 - val_loss: 1.6479
Epoch 1152/5000
26/26 - 1s - loss: 1.2460 - val_loss: 1.6472
Epoch 1153/5000
26/26 - 1s - loss: 1.2433 - val_loss: 1.6444
Epoch 1154/5000
26/26 - 1s - loss: 1.2421 - val_loss: 1.6454
Epoch 1155/5000
26/26 - 2s - loss: 1.2432 - val_loss: 1.6446
Epoch 1156/5000
26/26 - 1s - loss: 1.2428 - val_loss: 1.6459
Epoch 1157/5000
26/26 - 1s - loss: 1.2411 - val_loss: 1.6429
Epoch 1158/5000
26/26 - 1s - loss: 1.2403 - val_loss: 1.6418
Epoch 1159/5000
26/26 - 1s - loss: 1.2393 - val_loss: 1.6422
Epoch 1160/5000
26/26 - 1s - loss: 1.2383 - val_loss: 1.6399
Epoch 01160: val_loss improved from 1.64823 to 1.63993, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 1161/5000
26/26 - 1s - loss: 1.2396 - val_loss: 1.6381
Epoch 1162/5000
26/26 - 1s - loss: 1.2374 - val_loss: 1.6378
Epoch 1163/5000
26/26 - 1s - loss: 1.2349 - val_loss: 1.6370
Epoch 1164/5000
26/26 - 1s - loss: 1.2339 - val_loss: 1.6361
Epoch 1165/5000
26/26 - 1s - loss: 1.2336 - val_loss: 1.6343
Epoch 1166/5000
26/26 - 1s - loss: 1.2325 - val_loss: 1.6338
Epoch 1167/5000
26/26 - 1s - loss: 1.2317 - val_loss: 1.6348
Epoch 1168/5000
26/26 - 1s - loss: 1.2312 - val_loss: 1.6333
Epoch 1169/5000
26/26 - 1s - loss: 1.2311 - val_loss: 1.6314
Epoch 1170/5000
26/26 - 1s - loss: 1.2290 - val_loss: 1.6297
Epoch 01170: val_loss improved from 1.63993 to 1.62971, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 1171/5000
26/26 - 1s - loss: 1.2273 - val_loss: 1.6310
Epoch 1172/5000
26/26 - 1s - loss: 1.2284 - val_loss: 1.6304
Epoch 1173/5000
26/26 - 1s - loss: 1.2273 - val_loss: 1.6280
Epoch 1174/5000
26/26 - 1s - loss: 1.2244 - val_loss: 1.6281
Epoch 1175/5000
26/26 - 1s - loss: 1.2262 - val_loss: 1.6280
Epoch 1176/5000
26/26 - 1s - loss: 1.2245 - val_loss: 1.6264
Epoch 1177/5000
26/26 - 1s - loss: 1.2238 - val_loss: 1.6273
Epoch 1178/5000
26/26 - 1s - loss: 1.2231 - val_loss: 1.6249
Epoch 1179/5000
26/26 - 1s - loss: 1.2236 - val_loss: 1.6246
Epoch 1180/5000
26/26 - 1s - loss: 1.2209 - val_loss: 1.6224
Epoch 01180: val_loss improved from 1.62971 to 1.62243, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 1181/5000
26/26 - 1s - loss: 1.2211 - val_loss: 1.6237
Epoch 1182/5000
26/26 - 1s - loss: 1.2214 - val_loss: 1.6221
Epoch 1183/5000
26/26 - 1s - loss: 1.2190 - val_loss: 1.6195
Epoch 1184/5000
26/26 - 1s - loss: 1.2182 - val_loss: 1.6195
Epoch 1185/5000
26/26 - 1s - loss: 1.2170 - val_loss: 1.6186
Epoch 1186/5000
26/26 - 1s - loss: 1.2181 - val_loss: 1.6175
Epoch 1187/5000
26/26 - 1s - loss: 1.2180 - val_loss: 1.6163
Epoch 1188/5000
26/26 - 1s - loss: 1.2156 - val_loss: 1.6159
Epoch 1189/5000
26/26 - 1s - loss: 1.2143 - val_loss: 1.6150
Epoch 1190/5000
26/26 - 1s - loss: 1.2122 - val_loss: 1.6144
Epoch 01190: val_loss improved from 1.62243 to 1.61435, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 1191/5000
26/26 - 1s - loss: 1.2128 - val_loss: 1.6136
Epoch 1192/5000
26/26 - 1s - loss: 1.2113 - val_loss: 1.6129
Epoch 1193/5000
26/26 - 1s - loss: 1.2114 - val_loss: 1.6109
Epoch 1194/5000
26/26 - 1s - loss: 1.2106 - val_loss: 1.6109
Epoch 1195/5000
26/26 - 1s - loss: 1.2088 - val_loss: 1.6093
Epoch 1196/5000
26/26 - 1s - loss: 1.2072 - val_loss: 1.6106
Epoch 1197/5000
26/26 - 1s - loss: 1.2078 - val_loss: 1.6098
Epoch 1198/5000
26/26 - 1s - loss: 1.2052 - val_loss: 1.6095
Epoch 1199/5000
26/26 - 1s - loss: 1.2060 - val_loss: 1.6088
Epoch 1200/5000
26/26 - 1s - loss: 1.2043 - val_loss: 1.6069
Epoch 01200: val_loss improved from 1.61435 to 1.60686, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 1201/5000
26/26 - 1s - loss: 1.2041 - val_loss: 1.6052
Epoch 1202/5000
26/26 - 1s - loss: 1.2048 - val_loss: 1.6047
Epoch 1203/5000
26/26 - 1s - loss: 1.2015 - val_loss: 1.6039
Epoch 1204/5000
26/26 - 1s - loss: 1.2015 - val_loss: 1.6037
Epoch 1205/5000
26/26 - 2s - loss: 1.2010 - val_loss: 1.6012
Epoch 1206/5000
26/26 - 1s - loss: 1.2001 - val_loss: 1.6044
Epoch 1207/5000
26/26 - 1s - loss: 1.1997 - val_loss: 1.6017
Epoch 1208/5000
26/26 - 1s - loss: 1.1990 - val_loss: 1.6001
Epoch 1209/5000
26/26 - 1s - loss: 1.1972 - val_loss: 1.5985
Epoch 1210/5000
26/26 - 1s - loss: 1.1961 - val_loss: 1.5991
Epoch 01210: val_loss improved from 1.60686 to 1.59907, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 1211/5000
26/26 - 1s - loss: 1.1968 - val_loss: 1.5984
Epoch 1212/5000
26/26 - 1s - loss: 1.1948 - val_loss: 1.5948
Epoch 1213/5000
26/26 - 1s - loss: 1.1952 - val_loss: 1.5960
Epoch 1214/5000
26/26 - 1s - loss: 1.1934 - val_loss: 1.5963
Epoch 1215/5000
26/26 - 1s - loss: 1.1923 - val_loss: 1.5946
Epoch 1216/5000
26/26 - 2s - loss: 1.1934 - val_loss: 1.5951
Epoch 1217/5000
26/26 - 1s - loss: 1.1915 - val_loss: 1.5937
Epoch 1218/5000
26/26 - 1s - loss: 1.1914 - val_loss: 1.5926
Epoch 1219/5000
26/26 - 1s - loss: 1.1891 - val_loss: 1.5890
Epoch 1220/5000
26/26 - 1s - loss: 1.1893 - val_loss: 1.5904
Epoch 01220: val_loss improved from 1.59907 to 1.59042, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 1221/5000
26/26 - 1s - loss: 1.1878 - val_loss: 1.5883
Epoch 1222/5000
26/26 - 1s - loss: 1.1888 - val_loss: 1.5911
Epoch 1223/5000
26/26 - 1s - loss: 1.1847 - val_loss: 1.5885
Epoch 1224/5000
26/26 - 1s - loss: 1.1852 - val_loss: 1.5865
Epoch 1225/5000
26/26 - 1s - loss: 1.1836 - val_loss: 1.5862
Epoch 1226/5000
26/26 - 1s - loss: 1.1828 - val_loss: 1.5863
Epoch 1227/5000
26/26 - 1s - loss: 1.1831 - val_loss: 1.5847
Epoch 1228/5000
26/26 - 1s - loss: 1.1812 - val_loss: 1.5834
Epoch 1229/5000
26/26 - 1s - loss: 1.1833 - val_loss: 1.5833
Epoch 1230/5000
26/26 - 1s - loss: 1.1794 - val_loss: 1.5832
Epoch 01230: val_loss improved from 1.59042 to 1.58324, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 1231/5000
26/26 - 1s - loss: 1.1803 - val_loss: 1.5844
Epoch 1232/5000
26/26 - 1s - loss: 1.1794 - val_loss: 1.5825
Epoch 1233/5000
26/26 - 1s - loss: 1.1772 - val_loss: 1.5821
Epoch 1234/5000
26/26 - 1s - loss: 1.1778 - val_loss: 1.5823
Epoch 1235/5000
26/26 - 1s - loss: 1.1772 - val_loss: 1.5806
Epoch 1236/5000
26/26 - 1s - loss: 1.1763 - val_loss: 1.5760
Epoch 1237/5000
26/26 - 1s - loss: 1.1754 - val_loss: 1.5767
Epoch 1238/5000
26/26 - 1s - loss: 1.1745 - val_loss: 1.5762
Epoch 1239/5000
26/26 - 1s - loss: 1.1738 - val_loss: 1.5771
Epoch 1240/5000
26/26 - 1s - loss: 1.1712 - val_loss: 1.5756
Epoch 01240: val_loss improved from 1.58324 to 1.57560, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 1241/5000
26/26 - 1s - loss: 1.1716 - val_loss: 1.5739
Epoch 1242/5000
26/26 - 1s - loss: 1.1716 - val_loss: 1.5753
Epoch 1243/5000
26/26 - 1s - loss: 1.1699 - val_loss: 1.5739
Epoch 1244/5000
26/26 - 1s - loss: 1.1697 - val_loss: 1.5714
Epoch 1245/5000
26/26 - 1s - loss: 1.1680 - val_loss: 1.5723
Epoch 1246/5000
26/26 - 1s - loss: 1.1675 - val_loss: 1.5706
Epoch 1247/5000
26/26 - 1s - loss: 1.1681 - val_loss: 1.5697
Epoch 1248/5000
26/26 - 1s - loss: 1.1673 - val_loss: 1.5692
Epoch 1249/5000
26/26 - 1s - loss: 1.1662 - val_loss: 1.5677
Epoch 1250/5000
26/26 - 1s - loss: 1.1646 - val_loss: 1.5662
Epoch 01250: val_loss improved from 1.57560 to 1.56622, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 1251/5000
26/26 - 1s - loss: 1.1650 - val_loss: 1.5666
Epoch 1252/5000
26/26 - 1s - loss: 1.1625 - val_loss: 1.5643
Epoch 1253/5000
26/26 - 1s - loss: 1.1616 - val_loss: 1.5636
Epoch 1254/5000
26/26 - 1s - loss: 1.1625 - val_loss: 1.5647
Epoch 1255/5000
26/26 - 1s - loss: 1.1596 - val_loss: 1.5644
Epoch 1256/5000
26/26 - 1s - loss: 1.1612 - val_loss: 1.5629
Epoch 1257/5000
26/26 - 1s - loss: 1.1576 - val_loss: 1.5637
Epoch 1258/5000
26/26 - 1s - loss: 1.1577 - val_loss: 1.5613
Epoch 1259/5000
26/26 - 1s - loss: 1.1581 - val_loss: 1.5601
Epoch 1260/5000
26/26 - 1s - loss: 1.1570 - val_loss: 1.5617
Epoch 01260: val_loss improved from 1.56622 to 1.56173, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 1261/5000
26/26 - 1s - loss: 1.1572 - val_loss: 1.5595
Epoch 1262/5000
26/26 - 1s - loss: 1.1568 - val_loss: 1.5590
Epoch 1263/5000
26/26 - 1s - loss: 1.1546 - val_loss: 1.5576
Epoch 1264/5000
26/26 - 1s - loss: 1.1546 - val_loss: 1.5566
Epoch 1265/5000
26/26 - 1s - loss: 1.1524 - val_loss: 1.5560
Epoch 1266/5000
26/26 - 1s - loss: 1.1524 - val_loss: 1.5532
Epoch 1267/5000
26/26 - 1s - loss: 1.1524 - val_loss: 1.5552
Epoch 1268/5000
26/26 - 1s - loss: 1.1523 - val_loss: 1.5523
Epoch 1269/5000
26/26 - 1s - loss: 1.1497 - val_loss: 1.5525
Epoch 1270/5000
26/26 - 1s - loss: 1.1484 - val_loss: 1.5526
Epoch 01270: val_loss improved from 1.56173 to 1.55264, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 1271/5000
26/26 - 1s - loss: 1.1491 - val_loss: 1.5506
Epoch 1272/5000
26/26 - 1s - loss: 1.1475 - val_loss: 1.5498
Epoch 1273/5000
26/26 - 1s - loss: 1.1474 - val_loss: 1.5502
Epoch 1274/5000
26/26 - 1s - loss: 1.1468 - val_loss: 1.5482
Epoch 1275/5000
26/26 - 1s - loss: 1.1467 - val_loss: 1.5483
Epoch 1276/5000
26/26 - 1s - loss: 1.1435 - val_loss: 1.5485
Epoch 1277/5000
26/26 - 1s - loss: 1.1434 - val_loss: 1.5450
Epoch 1278/5000
26/26 - 1s - loss: 1.1453 - val_loss: 1.5461
Epoch 1279/5000
26/26 - 1s - loss: 1.1434 - val_loss: 1.5461
Epoch 1280/5000
26/26 - 1s - loss: 1.1421 - val_loss: 1.5439
Epoch 01280: val_loss improved from 1.55264 to 1.54389, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 1281/5000
26/26 - 1s - loss: 1.1419 - val_loss: 1.5443
Epoch 1282/5000
26/26 - 1s - loss: 1.1415 - val_loss: 1.5417
Epoch 1283/5000
26/26 - 1s - loss: 1.1391 - val_loss: 1.5418
Epoch 1284/5000
26/26 - 1s - loss: 1.1395 - val_loss: 1.5423
Epoch 1285/5000
26/26 - 1s - loss: 1.1350 - val_loss: 1.5412
Epoch 1286/5000
26/26 - 1s - loss: 1.1381 - val_loss: 1.5391
Epoch 1287/5000
26/26 - 1s - loss: 1.1383 - val_loss: 1.5393
Epoch 1288/5000
26/26 - 1s - loss: 1.1370 - val_loss: 1.5374
Epoch 1289/5000
26/26 - 1s - loss: 1.1355 - val_loss: 1.5372
Epoch 1290/5000
26/26 - 1s - loss: 1.1352 - val_loss: 1.5352
Epoch 01290: val_loss improved from 1.54389 to 1.53516, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 1291/5000
26/26 - 1s - loss: 1.1351 - val_loss: 1.5352
Epoch 1292/5000
26/26 - 1s - loss: 1.1332 - val_loss: 1.5356
Epoch 1293/5000
26/26 - 1s - loss: 1.1320 - val_loss: 1.5359
Epoch 1294/5000
26/26 - 1s - loss: 1.1314 - val_loss: 1.5333
Epoch 1295/5000
26/26 - 1s - loss: 1.1323 - val_loss: 1.5358
Epoch 1296/5000
26/26 - 1s - loss: 1.1303 - val_loss: 1.5343
Epoch 1297/5000
26/26 - 1s - loss: 1.1291 - val_loss: 1.5329
Epoch 1298/5000
26/26 - 1s - loss: 1.1289 - val_loss: 1.5323
Epoch 1299/5000
26/26 - 2s - loss: 1.1268 - val_loss: 1.5312
Epoch 1300/5000
26/26 - 1s - loss: 1.1279 - val_loss: 1.5302
Epoch 01300: val_loss improved from 1.53516 to 1.53023, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 1301/5000
26/26 - 1s - loss: 1.1260 - val_loss: 1.5311
Epoch 1302/5000
26/26 - 1s - loss: 1.1252 - val_loss: 1.5291
Epoch 1303/5000
26/26 - 1s - loss: 1.1242 - val_loss: 1.5280
Epoch 1304/5000
26/26 - 1s - loss: 1.1243 - val_loss: 1.5286
Epoch 1305/5000
26/26 - 1s - loss: 1.1239 - val_loss: 1.5265
Epoch 1306/5000
26/26 - 1s - loss: 1.1225 - val_loss: 1.5261
Epoch 1307/5000
26/26 - 1s - loss: 1.1227 - val_loss: 1.5266
Epoch 1308/5000
26/26 - 1s - loss: 1.1232 - val_loss: 1.5244
Epoch 1309/5000
26/26 - 1s - loss: 1.1201 - val_loss: 1.5249
Epoch 1310/5000
26/26 - 1s - loss: 1.1203 - val_loss: 1.5227
Epoch 01310: val_loss improved from 1.53023 to 1.52269, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 1311/5000
26/26 - 1s - loss: 1.1185 - val_loss: 1.5220
Epoch 1312/5000
26/26 - 1s - loss: 1.1180 - val_loss: 1.5247
Epoch 1313/5000
26/26 - 1s - loss: 1.1177 - val_loss: 1.5199
Epoch 1314/5000
26/26 - 1s - loss: 1.1158 - val_loss: 1.5217
Epoch 1315/5000
26/26 - 1s - loss: 1.1149 - val_loss: 1.5194
Epoch 1316/5000
26/26 - 1s - loss: 1.1152 - val_loss: 1.5177
Epoch 1317/5000
26/26 - 1s - loss: 1.1155 - val_loss: 1.5196
Epoch 1318/5000
26/26 - 1s - loss: 1.1135 - val_loss: 1.5166
Epoch 1319/5000
26/26 - 1s - loss: 1.1146 - val_loss: 1.5163
Epoch 1320/5000
26/26 - 1s - loss: 1.1121 - val_loss: 1.5157
Epoch 01320: val_loss improved from 1.52269 to 1.51575, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 1321/5000
26/26 - 1s - loss: 1.1128 - val_loss: 1.5153
Epoch 1322/5000
26/26 - 1s - loss: 1.1111 - val_loss: 1.5148
Epoch 1323/5000
26/26 - 1s - loss: 1.1103 - val_loss: 1.5144
Epoch 1324/5000
26/26 - 1s - loss: 1.1100 - val_loss: 1.5132
Epoch 1325/5000
26/26 - 1s - loss: 1.1097 - val_loss: 1.5128
Epoch 1326/5000
26/26 - 1s - loss: 1.1087 - val_loss: 1.5103
Epoch 1327/5000
26/26 - 1s - loss: 1.1079 - val_loss: 1.5100
Epoch 1328/5000
26/26 - 1s - loss: 1.1085 - val_loss: 1.5102
Epoch 1329/5000
26/26 - 1s - loss: 1.1069 - val_loss: 1.5103
Epoch 1330/5000
26/26 - 1s - loss: 1.1065 - val_loss: 1.5092
Epoch 01330: val_loss improved from 1.51575 to 1.50921, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 1331/5000
26/26 - 1s - loss: 1.1044 - val_loss: 1.5090
Epoch 1332/5000
26/26 - 1s - loss: 1.1044 - val_loss: 1.5077
Epoch 1333/5000
26/26 - 1s - loss: 1.1051 - val_loss: 1.5077
Epoch 1334/5000
26/26 - 1s - loss: 1.1041 - val_loss: 1.5068
Epoch 1335/5000
26/26 - 1s - loss: 1.1017 - val_loss: 1.5060
Epoch 1336/5000
26/26 - 1s - loss: 1.1004 - val_loss: 1.5067
Epoch 1337/5000
26/26 - 1s - loss: 1.1008 - val_loss: 1.5037
Epoch 1338/5000
26/26 - 1s - loss: 1.1001 - val_loss: 1.5054
Epoch 1339/5000
26/26 - 1s - loss: 1.0989 - val_loss: 1.5027
Epoch 1340/5000
26/26 - 1s - loss: 1.0974 - val_loss: 1.5034
Epoch 01340: val_loss improved from 1.50921 to 1.50341, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 1341/5000
26/26 - 1s - loss: 1.0971 - val_loss: 1.5011
Epoch 1342/5000
26/26 - 1s - loss: 1.0969 - val_loss: 1.5002
Epoch 1343/5000
26/26 - 1s - loss: 1.0958 - val_loss: 1.5000
Epoch 1344/5000
26/26 - 1s - loss: 1.0964 - val_loss: 1.5004
Epoch 1345/5000
26/26 - 1s - loss: 1.0958 - val_loss: 1.4999
Epoch 1346/5000
26/26 - 1s - loss: 1.0954 - val_loss: 1.4987
Epoch 1347/5000
26/26 - 1s - loss: 1.0948 - val_loss: 1.4985
Epoch 1348/5000
26/26 - 1s - loss: 1.0931 - val_loss: 1.4962
Epoch 1349/5000
26/26 - 1s - loss: 1.0923 - val_loss: 1.4967
Epoch 1350/5000
26/26 - 1s - loss: 1.0905 - val_loss: 1.4939
Epoch 01350: val_loss improved from 1.50341 to 1.49387, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 1351/5000
26/26 - 1s - loss: 1.0898 - val_loss: 1.4951
Epoch 1352/5000
26/26 - 1s - loss: 1.0890 - val_loss: 1.4946
Epoch 1353/5000
26/26 - 1s - loss: 1.0903 - val_loss: 1.4936
Epoch 1354/5000
26/26 - 1s - loss: 1.0879 - val_loss: 1.4933
Epoch 1355/5000
26/26 - 1s - loss: 1.0877 - val_loss: 1.4917
Epoch 1356/5000
26/26 - 2s - loss: 1.0888 - val_loss: 1.4922
Epoch 1357/5000
26/26 - 2s - loss: 1.0870 - val_loss: 1.4904
Epoch 1358/5000
26/26 - 1s - loss: 1.0872 - val_loss: 1.4895
Epoch 1359/5000
26/26 - 1s - loss: 1.0845 - val_loss: 1.4906
Epoch 1360/5000
26/26 - 1s - loss: 1.0845 - val_loss: 1.4891
Epoch 01360: val_loss improved from 1.49387 to 1.48909, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 1361/5000
26/26 - 1s - loss: 1.0846 - val_loss: 1.4874
Epoch 1362/5000
26/26 - 1s - loss: 1.0850 - val_loss: 1.4871
Epoch 1363/5000
26/26 - 1s - loss: 1.0819 - val_loss: 1.4862
Epoch 1364/5000
26/26 - 1s - loss: 1.0804 - val_loss: 1.4860
Epoch 1365/5000
26/26 - 1s - loss: 1.0816 - val_loss: 1.4850
Epoch 1366/5000
26/26 - 1s - loss: 1.0806 - val_loss: 1.4851
Epoch 1367/5000
26/26 - 1s - loss: 1.0798 - val_loss: 1.4855
Epoch 1368/5000
26/26 - 1s - loss: 1.0801 - val_loss: 1.4844
Epoch 1369/5000
26/26 - 1s - loss: 1.0778 - val_loss: 1.4844
Epoch 1370/5000
26/26 - 1s - loss: 1.0771 - val_loss: 1.4821
Epoch 01370: val_loss improved from 1.48909 to 1.48213, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 1371/5000
26/26 - 1s - loss: 1.0781 - val_loss: 1.4849
Epoch 1372/5000
26/26 - 1s - loss: 1.0791 - val_loss: 1.4799
Epoch 1373/5000
26/26 - 1s - loss: 1.0760 - val_loss: 1.4802
Epoch 1374/5000
26/26 - 1s - loss: 1.0750 - val_loss: 1.4778
Epoch 1375/5000
26/26 - 1s - loss: 1.0756 - val_loss: 1.4790
Epoch 1376/5000
26/26 - 1s - loss: 1.0741 - val_loss: 1.4817
Epoch 1377/5000
26/26 - 1s - loss: 1.0751 - val_loss: 1.4794
Epoch 1378/5000
26/26 - 1s - loss: 1.0717 - val_loss: 1.4776
Epoch 1379/5000
26/26 - 1s - loss: 1.0726 - val_loss: 1.4777
Epoch 1380/5000
26/26 - 1s - loss: 1.0710 - val_loss: 1.4763
Epoch 01380: val_loss improved from 1.48213 to 1.47631, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 1381/5000
26/26 - 1s - loss: 1.0701 - val_loss: 1.4749
Epoch 1382/5000
26/26 - 1s - loss: 1.0688 - val_loss: 1.4750
Epoch 1383/5000
26/26 - 1s - loss: 1.0691 - val_loss: 1.4734
Epoch 1384/5000
26/26 - 1s - loss: 1.0686 - val_loss: 1.4743
Epoch 1385/5000
26/26 - 1s - loss: 1.0673 - val_loss: 1.4726
Epoch 1386/5000
26/26 - 1s - loss: 1.0687 - val_loss: 1.4714
Epoch 1387/5000
26/26 - 1s - loss: 1.0676 - val_loss: 1.4725
Epoch 1388/5000
26/26 - 1s - loss: 1.0655 - val_loss: 1.4698
Epoch 1389/5000
26/26 - 1s - loss: 1.0655 - val_loss: 1.4701
Epoch 1390/5000
26/26 - 1s - loss: 1.0662 - val_loss: 1.4696
Epoch 01390: val_loss improved from 1.47631 to 1.46958, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 1391/5000
26/26 - 1s - loss: 1.0649 - val_loss: 1.4690
Epoch 1392/5000
26/26 - 1s - loss: 1.0644 - val_loss: 1.4700
Epoch 1393/5000
26/26 - 1s - loss: 1.0630 - val_loss: 1.4672
Epoch 1394/5000
26/26 - 1s - loss: 1.0623 - val_loss: 1.4667
Epoch 1395/5000
26/26 - 1s - loss: 1.0611 - val_loss: 1.4671
Epoch 1396/5000
26/26 - 1s - loss: 1.0610 - val_loss: 1.4676
Epoch 1397/5000
26/26 - 1s - loss: 1.0597 - val_loss: 1.4650
Epoch 1398/5000
26/26 - 1s - loss: 1.0592 - val_loss: 1.4639
Epoch 1399/5000
26/26 - 1s - loss: 1.0590 - val_loss: 1.4641
Epoch 1400/5000
26/26 - 1s - loss: 1.0572 - val_loss: 1.4631
Epoch 01400: val_loss improved from 1.46958 to 1.46308, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 1401/5000
26/26 - 1s - loss: 1.0575 - val_loss: 1.4630
Epoch 1402/5000
26/26 - 1s - loss: 1.0568 - val_loss: 1.4603
Epoch 1403/5000
26/26 - 1s - loss: 1.0552 - val_loss: 1.4606
Epoch 1404/5000
26/26 - 1s - loss: 1.0552 - val_loss: 1.4620
Epoch 1405/5000
26/26 - 1s - loss: 1.0546 - val_loss: 1.4590
Epoch 1406/5000
26/26 - 1s - loss: 1.0540 - val_loss: 1.4585
Epoch 1407/5000
26/26 - 1s - loss: 1.0520 - val_loss: 1.4593
Epoch 1408/5000
26/26 - 1s - loss: 1.0521 - val_loss: 1.4566
Epoch 1409/5000
26/26 - 1s - loss: 1.0512 - val_loss: 1.4575
Epoch 1410/5000
26/26 - 1s - loss: 1.0498 - val_loss: 1.4566
Epoch 01410: val_loss improved from 1.46308 to 1.45659, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 1411/5000
26/26 - 1s - loss: 1.0508 - val_loss: 1.4559
Epoch 1412/5000
26/26 - 1s - loss: 1.0508 - val_loss: 1.4559
Epoch 1413/5000
26/26 - 1s - loss: 1.0487 - val_loss: 1.4547
Epoch 1414/5000
26/26 - 1s - loss: 1.0481 - val_loss: 1.4533
Epoch 1415/5000
26/26 - 1s - loss: 1.0477 - val_loss: 1.4521
Epoch 1416/5000
26/26 - 1s - loss: 1.0478 - val_loss: 1.4537
Epoch 1417/5000
26/26 - 1s - loss: 1.0482 - val_loss: 1.4517
Epoch 1418/5000
26/26 - 1s - loss: 1.0481 - val_loss: 1.4514
Epoch 1419/5000
26/26 - 1s - loss: 1.0462 - val_loss: 1.4477
Epoch 1420/5000
26/26 - 1s - loss: 1.0453 - val_loss: 1.4495
Epoch 01420: val_loss improved from 1.45659 to 1.44955, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 1421/5000
26/26 - 1s - loss: 1.0453 - val_loss: 1.4478
Epoch 1422/5000
26/26 - 1s - loss: 1.0419 - val_loss: 1.4472
Epoch 1423/5000
26/26 - 2s - loss: 1.0443 - val_loss: 1.4489
Epoch 1424/5000
26/26 - 1s - loss: 1.0437 - val_loss: 1.4484
Epoch 1425/5000
26/26 - 1s - loss: 1.0412 - val_loss: 1.4475
Epoch 1426/5000
26/26 - 1s - loss: 1.0412 - val_loss: 1.4467
Epoch 1427/5000
26/26 - 1s - loss: 1.0418 - val_loss: 1.4454
Epoch 1428/5000
26/26 - 1s - loss: 1.0375 - val_loss: 1.4447
Epoch 1429/5000
26/26 - 1s - loss: 1.0382 - val_loss: 1.4459
Epoch 1430/5000
26/26 - 1s - loss: 1.0393 - val_loss: 1.4438
Epoch 01430: val_loss improved from 1.44955 to 1.44383, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 1431/5000
26/26 - 1s - loss: 1.0379 - val_loss: 1.4426
Epoch 1432/5000
26/26 - 1s - loss: 1.0363 - val_loss: 1.4422
Epoch 1433/5000
26/26 - 1s - loss: 1.0366 - val_loss: 1.4444
Epoch 1434/5000
26/26 - 1s - loss: 1.0374 - val_loss: 1.4406
Epoch 1435/5000
26/26 - 1s - loss: 1.0364 - val_loss: 1.4401
Epoch 1436/5000
26/26 - 1s - loss: 1.0344 - val_loss: 1.4391
Epoch 1437/5000
26/26 - 1s - loss: 1.0333 - val_loss: 1.4407
Epoch 1438/5000
26/26 - 1s - loss: 1.0348 - val_loss: 1.4399
Epoch 1439/5000
26/26 - 1s - loss: 1.0336 - val_loss: 1.4380
Epoch 1440/5000
26/26 - 1s - loss: 1.0336 - val_loss: 1.4367
Epoch 01440: val_loss improved from 1.44383 to 1.43669, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 1441/5000
26/26 - 1s - loss: 1.0324 - val_loss: 1.4358
Epoch 1442/5000
26/26 - 1s - loss: 1.0300 - val_loss: 1.4351
Epoch 1443/5000
26/26 - 1s - loss: 1.0312 - val_loss: 1.4344
Epoch 1444/5000
26/26 - 1s - loss: 1.0306 - val_loss: 1.4345
Epoch 1445/5000
26/26 - 1s - loss: 1.0295 - val_loss: 1.4331
Epoch 1446/5000
26/26 - 1s - loss: 1.0282 - val_loss: 1.4348
Epoch 1447/5000
26/26 - 1s - loss: 1.0290 - val_loss: 1.4331
Epoch 1448/5000
26/26 - 1s - loss: 1.0287 - val_loss: 1.4307
Epoch 1449/5000
26/26 - 1s - loss: 1.0271 - val_loss: 1.4312
Epoch 1450/5000
26/26 - 1s - loss: 1.0250 - val_loss: 1.4298
Epoch 01450: val_loss improved from 1.43669 to 1.42978, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 1451/5000
26/26 - 1s - loss: 1.0270 - val_loss: 1.4306
Epoch 1452/5000
26/26 - 1s - loss: 1.0252 - val_loss: 1.4281
Epoch 1453/5000
26/26 - 1s - loss: 1.0245 - val_loss: 1.4289
Epoch 1454/5000
26/26 - 1s - loss: 1.0230 - val_loss: 1.4281
Epoch 1455/5000
26/26 - 1s - loss: 1.0232 - val_loss: 1.4280
Epoch 1456/5000
26/26 - 1s - loss: 1.0232 - val_loss: 1.4272
Epoch 1457/5000
26/26 - 1s - loss: 1.0216 - val_loss: 1.4262
Epoch 1458/5000
26/26 - 1s - loss: 1.0208 - val_loss: 1.4264
Epoch 1459/5000
26/26 - 2s - loss: 1.0211 - val_loss: 1.4251
Epoch 1460/5000
26/26 - 1s - loss: 1.0191 - val_loss: 1.4261
Epoch 01460: val_loss improved from 1.42978 to 1.42614, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 1461/5000
26/26 - 1s - loss: 1.0191 - val_loss: 1.4249
Epoch 1462/5000
26/26 - 1s - loss: 1.0182 - val_loss: 1.4231
Epoch 1463/5000
26/26 - 1s - loss: 1.0177 - val_loss: 1.4238
Epoch 1464/5000
26/26 - 1s - loss: 1.0169 - val_loss: 1.4232
Epoch 1465/5000
26/26 - 1s - loss: 1.0165 - val_loss: 1.4214
Epoch 1466/5000
26/26 - 1s - loss: 1.0169 - val_loss: 1.4217
Epoch 1467/5000
26/26 - 1s - loss: 1.0150 - val_loss: 1.4203
Epoch 1468/5000
26/26 - 1s - loss: 1.0150 - val_loss: 1.4186
Epoch 1469/5000
26/26 - 1s - loss: 1.0140 - val_loss: 1.4196
Epoch 1470/5000
26/26 - 1s - loss: 1.0141 - val_loss: 1.4204
Epoch 01470: val_loss improved from 1.42614 to 1.42038, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 1471/5000
26/26 - 1s - loss: 1.0141 - val_loss: 1.4187
Epoch 1472/5000
26/26 - 1s - loss: 1.0125 - val_loss: 1.4177
Epoch 1473/5000
26/26 - 1s - loss: 1.0124 - val_loss: 1.4165
Epoch 1474/5000
26/26 - 1s - loss: 1.0129 - val_loss: 1.4171
Epoch 1475/5000
26/26 - 1s - loss: 1.0113 - val_loss: 1.4154
Epoch 1476/5000
26/26 - 1s - loss: 1.0110 - val_loss: 1.4142
Epoch 1477/5000
26/26 - 1s - loss: 1.0087 - val_loss: 1.4143
Epoch 1478/5000
26/26 - 1s - loss: 1.0087 - val_loss: 1.4139
Epoch 1479/5000
26/26 - 1s - loss: 1.0084 - val_loss: 1.4136
Epoch 1480/5000
26/26 - 1s - loss: 1.0086 - val_loss: 1.4131
Epoch 01480: val_loss improved from 1.42038 to 1.41312, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 1481/5000
26/26 - 1s - loss: 1.0079 - val_loss: 1.4124
Epoch 1482/5000
26/26 - 1s - loss: 1.0055 - val_loss: 1.4122
Epoch 1483/5000
26/26 - 1s - loss: 1.0074 - val_loss: 1.4140
Epoch 1484/5000
26/26 - 1s - loss: 1.0053 - val_loss: 1.4108
Epoch 1485/5000
26/26 - 1s - loss: 1.0049 - val_loss: 1.4109
Epoch 1486/5000
26/26 - 1s - loss: 1.0036 - val_loss: 1.4091
Epoch 1487/5000
26/26 - 1s - loss: 1.0035 - val_loss: 1.4098
Epoch 1488/5000
26/26 - 1s - loss: 1.0030 - val_loss: 1.4075
Epoch 1489/5000
26/26 - 1s - loss: 1.0006 - val_loss: 1.4076
Epoch 1490/5000
26/26 - 1s - loss: 1.0001 - val_loss: 1.4083
Epoch 01490: val_loss improved from 1.41312 to 1.40826, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 1491/5000
26/26 - 1s - loss: 1.0029 - val_loss: 1.4072
Epoch 1492/5000
26/26 - 1s - loss: 1.0006 - val_loss: 1.4056
Epoch 1493/5000
26/26 - 1s - loss: 1.0004 - val_loss: 1.4070
Epoch 1494/5000
26/26 - 1s - loss: 1.0009 - val_loss: 1.4046
Epoch 1495/5000
26/26 - 1s - loss: 0.9992 - val_loss: 1.4050
Epoch 1496/5000
26/26 - 1s - loss: 1.0011 - val_loss: 1.4036
Epoch 1497/5000
26/26 - 1s - loss: 0.9984 - val_loss: 1.4028
Epoch 1498/5000
26/26 - 1s - loss: 0.9970 - val_loss: 1.4033
Epoch 1499/5000
26/26 - 1s - loss: 0.9977 - val_loss: 1.4016
Epoch 1500/5000
26/26 - 1s - loss: 0.9951 - val_loss: 1.4036
Epoch 01500: val_loss improved from 1.40826 to 1.40358, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 1501/5000
26/26 - 1s - loss: 0.9958 - val_loss: 1.4021
Epoch 1502/5000
26/26 - 1s - loss: 0.9961 - val_loss: 1.4033
Epoch 1503/5000
26/26 - 1s - loss: 0.9949 - val_loss: 1.3993
Epoch 1504/5000
26/26 - 1s - loss: 0.9936 - val_loss: 1.4004
Epoch 1505/5000
26/26 - 1s - loss: 0.9940 - val_loss: 1.3986
Epoch 1506/5000
26/26 - 1s - loss: 0.9918 - val_loss: 1.3981
Epoch 1507/5000
26/26 - 1s - loss: 0.9924 - val_loss: 1.3966
Epoch 1508/5000
26/26 - 1s - loss: 0.9911 - val_loss: 1.3959
Epoch 1509/5000
26/26 - 1s - loss: 0.9936 - val_loss: 1.3955
Epoch 1510/5000
26/26 - 1s - loss: 0.9885 - val_loss: 1.3955
Epoch 01510: val_loss improved from 1.40358 to 1.39547, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 1511/5000
26/26 - 1s - loss: 0.9891 - val_loss: 1.3949
Epoch 1512/5000
26/26 - 1s - loss: 0.9894 - val_loss: 1.3953
Epoch 1513/5000
26/26 - 1s - loss: 0.9886 - val_loss: 1.3925
Epoch 1514/5000
26/26 - 1s - loss: 0.9883 - val_loss: 1.3933
Epoch 1515/5000
26/26 - 2s - loss: 0.9858 - val_loss: 1.3939
Epoch 1516/5000
26/26 - 1s - loss: 0.9875 - val_loss: 1.3927
Epoch 1517/5000
26/26 - 1s - loss: 0.9857 - val_loss: 1.3902
Epoch 1518/5000
26/26 - 1s - loss: 0.9844 - val_loss: 1.3914
Epoch 1519/5000
26/26 - 1s - loss: 0.9849 - val_loss: 1.3908
Epoch 1520/5000
26/26 - 1s - loss: 0.9837 - val_loss: 1.3918
Epoch 01520: val_loss improved from 1.39547 to 1.39175, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 1521/5000
26/26 - 1s - loss: 0.9836 - val_loss: 1.3917
Epoch 1522/5000
26/26 - 1s - loss: 0.9840 - val_loss: 1.3897
Epoch 1523/5000
26/26 - 1s - loss: 0.9822 - val_loss: 1.3894
Epoch 1524/5000
26/26 - 1s - loss: 0.9827 - val_loss: 1.3891
Epoch 1525/5000
26/26 - 1s - loss: 0.9807 - val_loss: 1.3893
Epoch 1526/5000
26/26 - 1s - loss: 0.9823 - val_loss: 1.3879
Epoch 1527/5000
26/26 - 1s - loss: 0.9812 - val_loss: 1.3868
Epoch 1528/5000
26/26 - 1s - loss: 0.9795 - val_loss: 1.3865
Epoch 1529/5000
26/26 - 2s - loss: 0.9787 - val_loss: 1.3861
Epoch 1530/5000
26/26 - 1s - loss: 0.9793 - val_loss: 1.3848
Epoch 01530: val_loss improved from 1.39175 to 1.38482, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 1531/5000
26/26 - 1s - loss: 0.9786 - val_loss: 1.3840
Epoch 1532/5000
26/26 - 1s - loss: 0.9764 - val_loss: 1.3861
Epoch 1533/5000
26/26 - 1s - loss: 0.9762 - val_loss: 1.3851
Epoch 1534/5000
26/26 - 1s - loss: 0.9774 - val_loss: 1.3831
Epoch 1535/5000
26/26 - 1s - loss: 0.9748 - val_loss: 1.3810
Epoch 1536/5000
26/26 - 1s - loss: 0.9749 - val_loss: 1.3823
Epoch 1537/5000
26/26 - 1s - loss: 0.9743 - val_loss: 1.3801
Epoch 1538/5000
26/26 - 1s - loss: 0.9745 - val_loss: 1.3806
Epoch 1539/5000
26/26 - 2s - loss: 0.9759 - val_loss: 1.3798
Epoch 1540/5000
26/26 - 1s - loss: 0.9730 - val_loss: 1.3794
Epoch 01540: val_loss improved from 1.38482 to 1.37940, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 1541/5000
26/26 - 1s - loss: 0.9712 - val_loss: 1.3784
Epoch 1542/5000
26/26 - 1s - loss: 0.9711 - val_loss: 1.3789
Epoch 1543/5000
26/26 - 1s - loss: 0.9717 - val_loss: 1.3773
Epoch 1544/5000
26/26 - 1s - loss: 0.9716 - val_loss: 1.3763
Epoch 1545/5000
26/26 - 1s - loss: 0.9697 - val_loss: 1.3775
Epoch 1546/5000
26/26 - 1s - loss: 0.9691 - val_loss: 1.3758
Epoch 1547/5000
26/26 - 2s - loss: 0.9685 - val_loss: 1.3745
Epoch 1548/5000
26/26 - 1s - loss: 0.9671 - val_loss: 1.3735
Epoch 1549/5000
26/26 - 1s - loss: 0.9684 - val_loss: 1.3727
Epoch 1550/5000
26/26 - 1s - loss: 0.9671 - val_loss: 1.3727
Epoch 01550: val_loss improved from 1.37940 to 1.37270, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 1551/5000
26/26 - 1s - loss: 0.9657 - val_loss: 1.3744
Epoch 1552/5000
26/26 - 1s - loss: 0.9664 - val_loss: 1.3746
Epoch 1553/5000
26/26 - 1s - loss: 0.9653 - val_loss: 1.3717
Epoch 1554/5000
26/26 - 1s - loss: 0.9659 - val_loss: 1.3717
Epoch 1555/5000
26/26 - 1s - loss: 0.9642 - val_loss: 1.3715
Epoch 1556/5000
26/26 - 1s - loss: 0.9643 - val_loss: 1.3717
Epoch 1557/5000
26/26 - 1s - loss: 0.9625 - val_loss: 1.3694
Epoch 1558/5000
26/26 - 1s - loss: 0.9628 - val_loss: 1.3710
Epoch 1559/5000
26/26 - 1s - loss: 0.9630 - val_loss: 1.3685
Epoch 1560/5000
26/26 - 1s - loss: 0.9618 - val_loss: 1.3681
Epoch 01560: val_loss improved from 1.37270 to 1.36814, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 1561/5000
26/26 - 1s - loss: 0.9599 - val_loss: 1.3670
Epoch 1562/5000
26/26 - 1s - loss: 0.9599 - val_loss: 1.3670
Epoch 1563/5000
26/26 - 1s - loss: 0.9601 - val_loss: 1.3688
Epoch 1564/5000
26/26 - 1s - loss: 0.9607 - val_loss: 1.3656
Epoch 1565/5000
26/26 - 1s - loss: 0.9602 - val_loss: 1.3667
Epoch 1566/5000
26/26 - 1s - loss: 0.9580 - val_loss: 1.3649
Epoch 1567/5000
26/26 - 1s - loss: 0.9582 - val_loss: 1.3643
Epoch 1568/5000
26/26 - 1s - loss: 0.9564 - val_loss: 1.3629
Epoch 1569/5000
26/26 - 1s - loss: 0.9566 - val_loss: 1.3631
Epoch 1570/5000
26/26 - 1s - loss: 0.9579 - val_loss: 1.3622
Epoch 01570: val_loss improved from 1.36814 to 1.36217, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 1571/5000
26/26 - 1s - loss: 0.9555 - val_loss: 1.3628
Epoch 1572/5000
26/26 - 1s - loss: 0.9570 - val_loss: 1.3624
Epoch 1573/5000
26/26 - 1s - loss: 0.9539 - val_loss: 1.3610
Epoch 1574/5000
26/26 - 1s - loss: 0.9538 - val_loss: 1.3614
Epoch 1575/5000
26/26 - 1s - loss: 0.9542 - val_loss: 1.3581
Epoch 1576/5000
26/26 - 1s - loss: 0.9523 - val_loss: 1.3599
Epoch 1577/5000
26/26 - 1s - loss: 0.9527 - val_loss: 1.3578
Epoch 1578/5000
26/26 - 1s - loss: 0.9522 - val_loss: 1.3591
Epoch 1579/5000
26/26 - 1s - loss: 0.9529 - val_loss: 1.3576
Epoch 1580/5000
26/26 - 1s - loss: 0.9500 - val_loss: 1.3576
Epoch 01580: val_loss improved from 1.36217 to 1.35756, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 1581/5000
26/26 - 1s - loss: 0.9517 - val_loss: 1.3583
Epoch 1582/5000
26/26 - 1s - loss: 0.9495 - val_loss: 1.3573
Epoch 1583/5000
26/26 - 1s - loss: 0.9510 - val_loss: 1.3565
Epoch 1584/5000
26/26 - 1s - loss: 0.9488 - val_loss: 1.3557
Epoch 1585/5000
26/26 - 1s - loss: 0.9482 - val_loss: 1.3531
Epoch 1586/5000
26/26 - 1s - loss: 0.9485 - val_loss: 1.3532
Epoch 1587/5000
26/26 - 1s - loss: 0.9485 - val_loss: 1.3518
Epoch 1588/5000
26/26 - 2s - loss: 0.9478 - val_loss: 1.3523
Epoch 1589/5000
26/26 - 1s - loss: 0.9470 - val_loss: 1.3535
Epoch 1590/5000
26/26 - 1s - loss: 0.9452 - val_loss: 1.3521
Epoch 01590: val_loss improved from 1.35756 to 1.35212, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 1591/5000
26/26 - 1s - loss: 0.9456 - val_loss: 1.3521
Epoch 1592/5000
26/26 - 1s - loss: 0.9436 - val_loss: 1.3509
Epoch 1593/5000
26/26 - 1s - loss: 0.9441 - val_loss: 1.3518
Epoch 1594/5000
26/26 - 1s - loss: 0.9437 - val_loss: 1.3487
Epoch 1595/5000
26/26 - 1s - loss: 0.9440 - val_loss: 1.3493
Epoch 1596/5000
26/26 - 1s - loss: 0.9442 - val_loss: 1.3509
Epoch 1597/5000
26/26 - 1s - loss: 0.9411 - val_loss: 1.3492
Epoch 1598/5000
26/26 - 1s - loss: 0.9404 - val_loss: 1.3485
Epoch 1599/5000
26/26 - 1s - loss: 0.9394 - val_loss: 1.3482
Epoch 1600/5000
26/26 - 1s - loss: 0.9400 - val_loss: 1.3456
Epoch 01600: val_loss improved from 1.35212 to 1.34564, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 1601/5000
26/26 - 1s - loss: 0.9401 - val_loss: 1.3464
Epoch 1602/5000
26/26 - 1s - loss: 0.9384 - val_loss: 1.3455
Epoch 1603/5000
26/26 - 1s - loss: 0.9396 - val_loss: 1.3461
Epoch 1604/5000
26/26 - 1s - loss: 0.9392 - val_loss: 1.3447
Epoch 1605/5000
26/26 - 1s - loss: 0.9374 - val_loss: 1.3447
Epoch 1606/5000
26/26 - 1s - loss: 0.9373 - val_loss: 1.3433
Epoch 1607/5000
26/26 - 1s - loss: 0.9357 - val_loss: 1.3432
Epoch 1608/5000
26/26 - 1s - loss: 0.9356 - val_loss: 1.3429
Epoch 1609/5000
26/26 - 1s - loss: 0.9360 - val_loss: 1.3435
Epoch 1610/5000
26/26 - 1s - loss: 0.9365 - val_loss: 1.3417
Epoch 01610: val_loss improved from 1.34564 to 1.34168, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 1611/5000
26/26 - 1s - loss: 0.9338 - val_loss: 1.3415
Epoch 1612/5000
26/26 - 1s - loss: 0.9336 - val_loss: 1.3408
Epoch 1613/5000
26/26 - 1s - loss: 0.9325 - val_loss: 1.3408
Epoch 1614/5000
26/26 - 1s - loss: 0.9322 - val_loss: 1.3386
Epoch 1615/5000
26/26 - 1s - loss: 0.9320 - val_loss: 1.3406
Epoch 1616/5000
26/26 - 1s - loss: 0.9319 - val_loss: 1.3422
Epoch 1617/5000
26/26 - 1s - loss: 0.9324 - val_loss: 1.3383
Epoch 1618/5000
26/26 - 1s - loss: 0.9305 - val_loss: 1.3371
Epoch 1619/5000
26/26 - 1s - loss: 0.9293 - val_loss: 1.3359
Epoch 1620/5000
26/26 - 1s - loss: 0.9294 - val_loss: 1.3375
Epoch 01620: val_loss improved from 1.34168 to 1.33750, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 1621/5000
26/26 - 1s - loss: 0.9290 - val_loss: 1.3373
Epoch 1622/5000
26/26 - 1s - loss: 0.9298 - val_loss: 1.3357
Epoch 1623/5000
26/26 - 1s - loss: 0.9275 - val_loss: 1.3342
Epoch 1624/5000
26/26 - 1s - loss: 0.9296 - val_loss: 1.3325
Epoch 1625/5000
26/26 - 1s - loss: 0.9263 - val_loss: 1.3338
Epoch 1626/5000
26/26 - 1s - loss: 0.9265 - val_loss: 1.3332
Epoch 1627/5000
26/26 - 1s - loss: 0.9274 - val_loss: 1.3320
Epoch 1628/5000
26/26 - 1s - loss: 0.9262 - val_loss: 1.3312
Epoch 1629/5000
26/26 - 2s - loss: 0.9245 - val_loss: 1.3307
Epoch 1630/5000
26/26 - 1s - loss: 0.9246 - val_loss: 1.3305
Epoch 01630: val_loss improved from 1.33750 to 1.33050, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 1631/5000
26/26 - 1s - loss: 0.9237 - val_loss: 1.3297
Epoch 1632/5000
26/26 - 1s - loss: 0.9237 - val_loss: 1.3296
Epoch 1633/5000
26/26 - 1s - loss: 0.9231 - val_loss: 1.3304
Epoch 1634/5000
26/26 - 1s - loss: 0.9230 - val_loss: 1.3282
Epoch 1635/5000
26/26 - 1s - loss: 0.9211 - val_loss: 1.3291
Epoch 1636/5000
26/26 - 1s - loss: 0.9213 - val_loss: 1.3306
Epoch 1637/5000
26/26 - 1s - loss: 0.9214 - val_loss: 1.3275
Epoch 1638/5000
26/26 - 1s - loss: 0.9212 - val_loss: 1.3271
Epoch 1639/5000
26/26 - 1s - loss: 0.9206 - val_loss: 1.3265
Epoch 1640/5000
26/26 - 1s - loss: 0.9187 - val_loss: 1.3250
Epoch 01640: val_loss improved from 1.33050 to 1.32500, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 1641/5000
26/26 - 1s - loss: 0.9192 - val_loss: 1.3247
Epoch 1642/5000
26/26 - 1s - loss: 0.9192 - val_loss: 1.3264
Epoch 1643/5000
26/26 - 1s - loss: 0.9178 - val_loss: 1.3251
Epoch 1644/5000
26/26 - 2s - loss: 0.9176 - val_loss: 1.3255
Epoch 1645/5000
26/26 - 1s - loss: 0.9171 - val_loss: 1.3241
Epoch 1646/5000
26/26 - 1s - loss: 0.9158 - val_loss: 1.3238
Epoch 1647/5000
26/26 - 1s - loss: 0.9173 - val_loss: 1.3229
Epoch 1648/5000
26/26 - 1s - loss: 0.9163 - val_loss: 1.3210
Epoch 1649/5000
26/26 - 1s - loss: 0.9135 - val_loss: 1.3209
Epoch 1650/5000
26/26 - 1s - loss: 0.9144 - val_loss: 1.3218
Epoch 01650: val_loss improved from 1.32500 to 1.32180, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 1651/5000
26/26 - 1s - loss: 0.9137 - val_loss: 1.3203
Epoch 1652/5000
26/26 - 1s - loss: 0.9130 - val_loss: 1.3193
Epoch 1653/5000
26/26 - 1s - loss: 0.9135 - val_loss: 1.3206
Epoch 1654/5000
26/26 - 1s - loss: 0.9118 - val_loss: 1.3180
Epoch 1655/5000
26/26 - 1s - loss: 0.9114 - val_loss: 1.3180
Epoch 1656/5000
26/26 - 1s - loss: 0.9114 - val_loss: 1.3179
Epoch 1657/5000
26/26 - 1s - loss: 0.9102 - val_loss: 1.3188
Epoch 1658/5000
26/26 - 1s - loss: 0.9099 - val_loss: 1.3184
Epoch 1659/5000
26/26 - 1s - loss: 0.9089 - val_loss: 1.3164
Epoch 1660/5000
26/26 - 1s - loss: 0.9100 - val_loss: 1.3170
Epoch 01660: val_loss improved from 1.32180 to 1.31702, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 1661/5000
26/26 - 1s - loss: 0.9088 - val_loss: 1.3153
Epoch 1662/5000
26/26 - 1s - loss: 0.9090 - val_loss: 1.3145
Epoch 1663/5000
26/26 - 1s - loss: 0.9086 - val_loss: 1.3126
Epoch 1664/5000
26/26 - 1s - loss: 0.9071 - val_loss: 1.3135
Epoch 1665/5000
26/26 - 1s - loss: 0.9077 - val_loss: 1.3120
Epoch 1666/5000
26/26 - 1s - loss: 0.9058 - val_loss: 1.3148
Epoch 1667/5000
26/26 - 1s - loss: 0.9060 - val_loss: 1.3133
Epoch 1668/5000
26/26 - 1s - loss: 0.9058 - val_loss: 1.3125
Epoch 1669/5000
26/26 - 1s - loss: 0.9053 - val_loss: 1.3133
Epoch 1670/5000
26/26 - 1s - loss: 0.9031 - val_loss: 1.3134
Epoch 01670: val_loss improved from 1.31702 to 1.31340, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 1671/5000
26/26 - 1s - loss: 0.9039 - val_loss: 1.3114
Epoch 1672/5000
26/26 - 1s - loss: 0.9032 - val_loss: 1.3107
Epoch 1673/5000
26/26 - 1s - loss: 0.9038 - val_loss: 1.3110
Epoch 1674/5000
26/26 - 1s - loss: 0.9026 - val_loss: 1.3084
Epoch 1675/5000
26/26 - 1s - loss: 0.9020 - val_loss: 1.3079
Epoch 1676/5000
26/26 - 1s - loss: 0.9001 - val_loss: 1.3074
Epoch 1677/5000
26/26 - 1s - loss: 0.8984 - val_loss: 1.3093
Epoch 1678/5000
26/26 - 1s - loss: 0.9013 - val_loss: 1.3088
Epoch 1679/5000
26/26 - 1s - loss: 0.9012 - val_loss: 1.3069
Epoch 1680/5000
26/26 - 1s - loss: 0.9002 - val_loss: 1.3076
Epoch 01680: val_loss improved from 1.31340 to 1.30764, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 1681/5000
26/26 - 1s - loss: 0.9001 - val_loss: 1.3075
Epoch 1682/5000
26/26 - 1s - loss: 0.8982 - val_loss: 1.3065
Epoch 1683/5000
26/26 - 1s - loss: 0.8981 - val_loss: 1.3059
Epoch 1684/5000
26/26 - 1s - loss: 0.8965 - val_loss: 1.3051
Epoch 1685/5000
26/26 - 1s - loss: 0.8965 - val_loss: 1.3043
Epoch 1686/5000
26/26 - 1s - loss: 0.8951 - val_loss: 1.3035
Epoch 1687/5000
26/26 - 1s - loss: 0.8967 - val_loss: 1.3034
Epoch 1688/5000
26/26 - 1s - loss: 0.8955 - val_loss: 1.3043
Epoch 1689/5000
26/26 - 1s - loss: 0.8951 - val_loss: 1.3025
Epoch 1690/5000
26/26 - 1s - loss: 0.8942 - val_loss: 1.3024
Epoch 01690: val_loss improved from 1.30764 to 1.30237, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 1691/5000
26/26 - 1s - loss: 0.8935 - val_loss: 1.3013
Epoch 1692/5000
26/26 - 1s - loss: 0.8936 - val_loss: 1.3025
Epoch 1693/5000
26/26 - 1s - loss: 0.8934 - val_loss: 1.3009
Epoch 1694/5000
26/26 - 1s - loss: 0.8921 - val_loss: 1.3001
Epoch 1695/5000
26/26 - 1s - loss: 0.8940 - val_loss: 1.2990
Epoch 1696/5000
26/26 - 1s - loss: 0.8911 - val_loss: 1.2994
Epoch 1697/5000
26/26 - 1s - loss: 0.8929 - val_loss: 1.2992
Epoch 1698/5000
26/26 - 1s - loss: 0.8903 - val_loss: 1.2970
Epoch 1699/5000
26/26 - 1s - loss: 0.8914 - val_loss: 1.2977
Epoch 1700/5000
26/26 - 1s - loss: 0.8910 - val_loss: 1.2984
Epoch 01700: val_loss improved from 1.30237 to 1.29842, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 1701/5000
26/26 - 1s - loss: 0.8882 - val_loss: 1.2983
Epoch 1702/5000
26/26 - 1s - loss: 0.8890 - val_loss: 1.2975
Epoch 1703/5000
26/26 - 1s - loss: 0.8888 - val_loss: 1.2947
Epoch 1704/5000
26/26 - 1s - loss: 0.8880 - val_loss: 1.2965
Epoch 1705/5000
26/26 - 1s - loss: 0.8883 - val_loss: 1.2936
Epoch 1706/5000
26/26 - 1s - loss: 0.8876 - val_loss: 1.2967
Epoch 1707/5000
26/26 - 1s - loss: 0.8855 - val_loss: 1.2920
Epoch 1708/5000
26/26 - 1s - loss: 0.8861 - val_loss: 1.2944
Epoch 1709/5000
26/26 - 1s - loss: 0.8866 - val_loss: 1.2932
Epoch 1710/5000
26/26 - 1s - loss: 0.8853 - val_loss: 1.2927
Epoch 01710: val_loss improved from 1.29842 to 1.29272, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 1711/5000
26/26 - 1s - loss: 0.8853 - val_loss: 1.2935
Epoch 1712/5000
26/26 - 2s - loss: 0.8830 - val_loss: 1.2930
Epoch 1713/5000
26/26 - 1s - loss: 0.8833 - val_loss: 1.2905
Epoch 1714/5000
26/26 - 1s - loss: 0.8844 - val_loss: 1.2925
Epoch 1715/5000
26/26 - 1s - loss: 0.8849 - val_loss: 1.2891
Epoch 1716/5000
26/26 - 1s - loss: 0.8840 - val_loss: 1.2885
Epoch 1717/5000
26/26 - 1s - loss: 0.8815 - val_loss: 1.2873
Epoch 1718/5000
26/26 - 1s - loss: 0.8816 - val_loss: 1.2909
Epoch 1719/5000
26/26 - 1s - loss: 0.8819 - val_loss: 1.2894
Epoch 1720/5000
26/26 - 1s - loss: 0.8814 - val_loss: 1.2865
Epoch 01720: val_loss improved from 1.29272 to 1.28651, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 1721/5000
26/26 - 1s - loss: 0.8812 - val_loss: 1.2867
Epoch 1722/5000
26/26 - 1s - loss: 0.8804 - val_loss: 1.2855
Epoch 1723/5000
26/26 - 1s - loss: 0.8792 - val_loss: 1.2868
Epoch 1724/5000
26/26 - 1s - loss: 0.8788 - val_loss: 1.2850
Epoch 1725/5000
26/26 - 1s - loss: 0.8779 - val_loss: 1.2860
Epoch 1726/5000
26/26 - 1s - loss: 0.8786 - val_loss: 1.2849
Epoch 1727/5000
26/26 - 1s - loss: 0.8777 - val_loss: 1.2855
Epoch 1728/5000
26/26 - 1s - loss: 0.8767 - val_loss: 1.2831
Epoch 1729/5000
26/26 - 1s - loss: 0.8751 - val_loss: 1.2840
Epoch 1730/5000
26/26 - 1s - loss: 0.8761 - val_loss: 1.2837
Epoch 01730: val_loss improved from 1.28651 to 1.28369, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 1731/5000
26/26 - 1s - loss: 0.8758 - val_loss: 1.2836
Epoch 1732/5000
26/26 - 1s - loss: 0.8744 - val_loss: 1.2823
Epoch 1733/5000
26/26 - 1s - loss: 0.8735 - val_loss: 1.2813
Epoch 1734/5000
26/26 - 1s - loss: 0.8733 - val_loss: 1.2829
Epoch 1735/5000
26/26 - 2s - loss: 0.8730 - val_loss: 1.2818
Epoch 1736/5000
26/26 - 1s - loss: 0.8725 - val_loss: 1.2821
Epoch 1737/5000
26/26 - 1s - loss: 0.8729 - val_loss: 1.2796
Epoch 1738/5000
26/26 - 1s - loss: 0.8721 - val_loss: 1.2798
Epoch 1739/5000
26/26 - 1s - loss: 0.8716 - val_loss: 1.2808
Epoch 1740/5000
26/26 - 1s - loss: 0.8704 - val_loss: 1.2806
Epoch 01740: val_loss improved from 1.28369 to 1.28062, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 1741/5000
26/26 - 1s - loss: 0.8720 - val_loss: 1.2805
Epoch 1742/5000
26/26 - 1s - loss: 0.8711 - val_loss: 1.2778
Epoch 1743/5000
26/26 - 1s - loss: 0.8701 - val_loss: 1.2764
Epoch 1744/5000
26/26 - 1s - loss: 0.8704 - val_loss: 1.2767
Epoch 1745/5000
26/26 - 1s - loss: 0.8687 - val_loss: 1.2761
Epoch 1746/5000
26/26 - 1s - loss: 0.8678 - val_loss: 1.2764
Epoch 1747/5000
26/26 - 1s - loss: 0.8685 - val_loss: 1.2759
Epoch 1748/5000
26/26 - 1s - loss: 0.8678 - val_loss: 1.2746
Epoch 1749/5000
26/26 - 1s - loss: 0.8670 - val_loss: 1.2761
Epoch 1750/5000
26/26 - 1s - loss: 0.8671 - val_loss: 1.2743
Epoch 01750: val_loss improved from 1.28062 to 1.27432, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 1751/5000
26/26 - 1s - loss: 0.8675 - val_loss: 1.2722
Epoch 1752/5000
26/26 - 1s - loss: 0.8662 - val_loss: 1.2729
Epoch 1753/5000
26/26 - 2s - loss: 0.8666 - val_loss: 1.2737
Epoch 1754/5000
26/26 - 1s - loss: 0.8641 - val_loss: 1.2720
Epoch 1755/5000
26/26 - 1s - loss: 0.8641 - val_loss: 1.2714
Epoch 1756/5000
26/26 - 1s - loss: 0.8638 - val_loss: 1.2718
Epoch 1757/5000
26/26 - 1s - loss: 0.8633 - val_loss: 1.2701
Epoch 1758/5000
26/26 - 1s - loss: 0.8622 - val_loss: 1.2713
Epoch 1759/5000
26/26 - 1s - loss: 0.8631 - val_loss: 1.2707
Epoch 1760/5000
26/26 - 1s - loss: 0.8622 - val_loss: 1.2686
Epoch 01760: val_loss improved from 1.27432 to 1.26864, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 1761/5000
26/26 - 1s - loss: 0.8615 - val_loss: 1.2691
Epoch 1762/5000
26/26 - 1s - loss: 0.8615 - val_loss: 1.2689
Epoch 1763/5000
26/26 - 1s - loss: 0.8628 - val_loss: 1.2686
Epoch 1764/5000
26/26 - 2s - loss: 0.8620 - val_loss: 1.2684
Epoch 1765/5000
26/26 - 1s - loss: 0.8603 - val_loss: 1.2682
Epoch 1766/5000
26/26 - 1s - loss: 0.8587 - val_loss: 1.2692
Epoch 1767/5000
26/26 - 1s - loss: 0.8594 - val_loss: 1.2682
Epoch 1768/5000
26/26 - 2s - loss: 0.8604 - val_loss: 1.2670
Epoch 1769/5000
26/26 - 1s - loss: 0.8577 - val_loss: 1.2662
Epoch 1770/5000
26/26 - 1s - loss: 0.8567 - val_loss: 1.2651
Epoch 01770: val_loss improved from 1.26864 to 1.26511, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 1771/5000
26/26 - 1s - loss: 0.8575 - val_loss: 1.2658
Epoch 1772/5000
26/26 - 1s - loss: 0.8567 - val_loss: 1.2638
Epoch 1773/5000
26/26 - 1s - loss: 0.8550 - val_loss: 1.2654
Epoch 1774/5000
26/26 - 1s - loss: 0.8563 - val_loss: 1.2642
Epoch 1775/5000
26/26 - 1s - loss: 0.8568 - val_loss: 1.2631
Epoch 1776/5000
26/26 - 1s - loss: 0.8544 - val_loss: 1.2636
Epoch 1777/5000
26/26 - 2s - loss: 0.8551 - val_loss: 1.2631
Epoch 1778/5000
26/26 - 1s - loss: 0.8534 - val_loss: 1.2657
Epoch 1779/5000
26/26 - 1s - loss: 0.8534 - val_loss: 1.2620
Epoch 1780/5000
26/26 - 1s - loss: 0.8525 - val_loss: 1.2633
Epoch 01780: val_loss improved from 1.26511 to 1.26326, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 1781/5000
26/26 - 1s - loss: 0.8536 - val_loss: 1.2612
Epoch 1782/5000
26/26 - 1s - loss: 0.8523 - val_loss: 1.2612
Epoch 1783/5000
26/26 - 1s - loss: 0.8530 - val_loss: 1.2611
Epoch 1784/5000
26/26 - 1s - loss: 0.8524 - val_loss: 1.2590
Epoch 1785/5000
26/26 - 1s - loss: 0.8513 - val_loss: 1.2604
Epoch 1786/5000
26/26 - 1s - loss: 0.8510 - val_loss: 1.2592
Epoch 1787/5000
26/26 - 1s - loss: 0.8502 - val_loss: 1.2573
Epoch 1788/5000
26/26 - 1s - loss: 0.8499 - val_loss: 1.2590
Epoch 1789/5000
26/26 - 1s - loss: 0.8492 - val_loss: 1.2579
Epoch 1790/5000
26/26 - 1s - loss: 0.8506 - val_loss: 1.2558
Epoch 01790: val_loss improved from 1.26326 to 1.25581, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 1791/5000
26/26 - 1s - loss: 0.8487 - val_loss: 1.2557
Epoch 1792/5000
26/26 - 1s - loss: 0.8472 - val_loss: 1.2574
Epoch 1793/5000
26/26 - 1s - loss: 0.8478 - val_loss: 1.2569
Epoch 1794/5000
26/26 - 2s - loss: 0.8486 - val_loss: 1.2565
Epoch 1795/5000
26/26 - 1s - loss: 0.8466 - val_loss: 1.2547
Epoch 1796/5000
26/26 - 1s - loss: 0.8459 - val_loss: 1.2547
Epoch 1797/5000
26/26 - 1s - loss: 0.8470 - val_loss: 1.2551
Epoch 1798/5000
26/26 - 1s - loss: 0.8451 - val_loss: 1.2532
Epoch 1799/5000
26/26 - 2s - loss: 0.8457 - val_loss: 1.2518
Epoch 1800/5000
26/26 - 1s - loss: 0.8457 - val_loss: 1.2520
Epoch 01800: val_loss improved from 1.25581 to 1.25204, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 1801/5000
26/26 - 1s - loss: 0.8448 - val_loss: 1.2518
Epoch 1802/5000
26/26 - 1s - loss: 0.8435 - val_loss: 1.2528
Epoch 1803/5000
26/26 - 1s - loss: 0.8442 - val_loss: 1.2514
Epoch 1804/5000
26/26 - 1s - loss: 0.8423 - val_loss: 1.2516
Epoch 1805/5000
26/26 - 1s - loss: 0.8439 - val_loss: 1.2535
Epoch 1806/5000
26/26 - 1s - loss: 0.8431 - val_loss: 1.2501
Epoch 1807/5000
26/26 - 1s - loss: 0.8405 - val_loss: 1.2501
Epoch 1808/5000
26/26 - 1s - loss: 0.8411 - val_loss: 1.2499
Epoch 1809/5000
26/26 - 1s - loss: 0.8413 - val_loss: 1.2488
Epoch 1810/5000
26/26 - 1s - loss: 0.8414 - val_loss: 1.2501
Epoch 01810: val_loss improved from 1.25204 to 1.25005, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 1811/5000
26/26 - 1s - loss: 0.8393 - val_loss: 1.2482
Epoch 1812/5000
26/26 - 1s - loss: 0.8400 - val_loss: 1.2482
Epoch 1813/5000
26/26 - 1s - loss: 0.8397 - val_loss: 1.2481
Epoch 1814/5000
26/26 - 1s - loss: 0.8400 - val_loss: 1.2461
Epoch 1815/5000
26/26 - 1s - loss: 0.8371 - val_loss: 1.2459
Epoch 1816/5000
26/26 - 1s - loss: 0.8376 - val_loss: 1.2452
Epoch 1817/5000
26/26 - 1s - loss: 0.8373 - val_loss: 1.2458
Epoch 1818/5000
26/26 - 1s - loss: 0.8377 - val_loss: 1.2457
Epoch 1819/5000
26/26 - 1s - loss: 0.8363 - val_loss: 1.2464
Epoch 1820/5000
26/26 - 1s - loss: 0.8363 - val_loss: 1.2453
Epoch 01820: val_loss improved from 1.25005 to 1.24527, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 1821/5000
26/26 - 1s - loss: 0.8371 - val_loss: 1.2452
Epoch 1822/5000
26/26 - 2s - loss: 0.8375 - val_loss: 1.2425
Epoch 1823/5000
26/26 - 1s - loss: 0.8354 - val_loss: 1.2425
Epoch 1824/5000
26/26 - 1s - loss: 0.8349 - val_loss: 1.2434
Epoch 1825/5000
26/26 - 1s - loss: 0.8340 - val_loss: 1.2431
Epoch 1826/5000
26/26 - 1s - loss: 0.8332 - val_loss: 1.2438
Epoch 1827/5000
26/26 - 1s - loss: 0.8342 - val_loss: 1.2424
Epoch 1828/5000
26/26 - 1s - loss: 0.8336 - val_loss: 1.2436
Epoch 1829/5000
26/26 - 1s - loss: 0.8345 - val_loss: 1.2413
Epoch 1830/5000
26/26 - 1s - loss: 0.8332 - val_loss: 1.2402
Epoch 01830: val_loss improved from 1.24527 to 1.24016, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 1831/5000
26/26 - 1s - loss: 0.8341 - val_loss: 1.2405
Epoch 1832/5000
26/26 - 1s - loss: 0.8308 - val_loss: 1.2417
Epoch 1833/5000
26/26 - 1s - loss: 0.8323 - val_loss: 1.2417
Epoch 1834/5000
26/26 - 1s - loss: 0.8313 - val_loss: 1.2373
Epoch 1835/5000
26/26 - 2s - loss: 0.8304 - val_loss: 1.2391
Epoch 1836/5000
26/26 - 1s - loss: 0.8303 - val_loss: 1.2395
Epoch 1837/5000
26/26 - 1s - loss: 0.8294 - val_loss: 1.2374
Epoch 1838/5000
26/26 - 1s - loss: 0.8304 - val_loss: 1.2377
Epoch 1839/5000
26/26 - 1s - loss: 0.8277 - val_loss: 1.2374
Epoch 1840/5000
26/26 - 1s - loss: 0.8291 - val_loss: 1.2343
Epoch 01840: val_loss improved from 1.24016 to 1.23429, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 1841/5000
26/26 - 1s - loss: 0.8280 - val_loss: 1.2344
Epoch 1842/5000
26/26 - 1s - loss: 0.8279 - val_loss: 1.2359
Epoch 1843/5000
26/26 - 1s - loss: 0.8268 - val_loss: 1.2353
Epoch 1844/5000
26/26 - 1s - loss: 0.8263 - val_loss: 1.2347
Epoch 1845/5000
26/26 - 1s - loss: 0.8261 - val_loss: 1.2343
Epoch 1846/5000
26/26 - 1s - loss: 0.8265 - val_loss: 1.2335
Epoch 1847/5000
26/26 - 1s - loss: 0.8253 - val_loss: 1.2343
Epoch 1848/5000
26/26 - 1s - loss: 0.8252 - val_loss: 1.2329
Epoch 1849/5000
26/26 - 1s - loss: 0.8247 - val_loss: 1.2321
Epoch 1850/5000
26/26 - 1s - loss: 0.8256 - val_loss: 1.2308
Epoch 01850: val_loss improved from 1.23429 to 1.23083, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 1851/5000
26/26 - 1s - loss: 0.8238 - val_loss: 1.2316
Epoch 1852/5000
26/26 - 1s - loss: 0.8226 - val_loss: 1.2309
Epoch 1853/5000
26/26 - 1s - loss: 0.8238 - val_loss: 1.2316
Epoch 1854/5000
26/26 - 1s - loss: 0.8224 - val_loss: 1.2309
Epoch 1855/5000
26/26 - 1s - loss: 0.8216 - val_loss: 1.2322
Epoch 1856/5000
26/26 - 1s - loss: 0.8213 - val_loss: 1.2305
Epoch 1857/5000
26/26 - 1s - loss: 0.8219 - val_loss: 1.2292
Epoch 1858/5000
26/26 - 1s - loss: 0.8215 - val_loss: 1.2300
Epoch 1859/5000
26/26 - 1s - loss: 0.8205 - val_loss: 1.2293
Epoch 1860/5000
26/26 - 1s - loss: 0.8203 - val_loss: 1.2292
Epoch 01860: val_loss improved from 1.23083 to 1.22924, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 1861/5000
26/26 - 1s - loss: 0.8196 - val_loss: 1.2302
Epoch 1862/5000
26/26 - 1s - loss: 0.8189 - val_loss: 1.2286
Epoch 1863/5000
26/26 - 1s - loss: 0.8210 - val_loss: 1.2277
Epoch 1864/5000
26/26 - 1s - loss: 0.8199 - val_loss: 1.2269
Epoch 1865/5000
26/26 - 1s - loss: 0.8193 - val_loss: 1.2280
Epoch 1866/5000
26/26 - 1s - loss: 0.8188 - val_loss: 1.2281
Epoch 1867/5000
26/26 - 1s - loss: 0.8191 - val_loss: 1.2262
Epoch 1868/5000
26/26 - 1s - loss: 0.8174 - val_loss: 1.2279
Epoch 1869/5000
26/26 - 1s - loss: 0.8165 - val_loss: 1.2258
Epoch 1870/5000
26/26 - 1s - loss: 0.8175 - val_loss: 1.2249
Epoch 01870: val_loss improved from 1.22924 to 1.22486, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 1871/5000
26/26 - 1s - loss: 0.8175 - val_loss: 1.2244
Epoch 1872/5000
26/26 - 1s - loss: 0.8157 - val_loss: 1.2235
Epoch 1873/5000
26/26 - 1s - loss: 0.8144 - val_loss: 1.2223
Epoch 1874/5000
26/26 - 1s - loss: 0.8140 - val_loss: 1.2211
Epoch 1875/5000
26/26 - 1s - loss: 0.8135 - val_loss: 1.2229
Epoch 1876/5000
26/26 - 2s - loss: 0.8135 - val_loss: 1.2242
Epoch 1877/5000
26/26 - 1s - loss: 0.8135 - val_loss: 1.2210
Epoch 1878/5000
26/26 - 1s - loss: 0.8139 - val_loss: 1.2225
Epoch 1879/5000
26/26 - 1s - loss: 0.8124 - val_loss: 1.2210
Epoch 1880/5000
26/26 - 1s - loss: 0.8137 - val_loss: 1.2206
Epoch 01880: val_loss improved from 1.22486 to 1.22057, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 1881/5000
26/26 - 1s - loss: 0.8131 - val_loss: 1.2213
Epoch 1882/5000
26/26 - 1s - loss: 0.8112 - val_loss: 1.2196
Epoch 1883/5000
26/26 - 1s - loss: 0.8102 - val_loss: 1.2183
Epoch 1884/5000
26/26 - 1s - loss: 0.8122 - val_loss: 1.2204
Epoch 1885/5000
26/26 - 1s - loss: 0.8095 - val_loss: 1.2193
Epoch 1886/5000
26/26 - 1s - loss: 0.8101 - val_loss: 1.2180
Epoch 1887/5000
26/26 - 1s - loss: 0.8094 - val_loss: 1.2197
Epoch 1888/5000
26/26 - 1s - loss: 0.8089 - val_loss: 1.2185
Epoch 1889/5000
26/26 - 1s - loss: 0.8083 - val_loss: 1.2195
Epoch 1890/5000
26/26 - 1s - loss: 0.8098 - val_loss: 1.2188
Epoch 01890: val_loss improved from 1.22057 to 1.21876, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 1891/5000
26/26 - 1s - loss: 0.8085 - val_loss: 1.2159
Epoch 1892/5000
26/26 - 1s - loss: 0.8088 - val_loss: 1.2175
Epoch 1893/5000
26/26 - 2s - loss: 0.8065 - val_loss: 1.2181
Epoch 1894/5000
26/26 - 1s - loss: 0.8075 - val_loss: 1.2171
Epoch 1895/5000
26/26 - 1s - loss: 0.8058 - val_loss: 1.2153
Epoch 1896/5000
26/26 - 1s - loss: 0.8068 - val_loss: 1.2169
Epoch 1897/5000
26/26 - 1s - loss: 0.8066 - val_loss: 1.2142
Epoch 1898/5000
26/26 - 1s - loss: 0.8042 - val_loss: 1.2160
Epoch 1899/5000
26/26 - 1s - loss: 0.8047 - val_loss: 1.2154
Epoch 1900/5000
26/26 - 1s - loss: 0.8043 - val_loss: 1.2138
Epoch 01900: val_loss improved from 1.21876 to 1.21378, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 1901/5000
26/26 - 1s - loss: 0.8048 - val_loss: 1.2145
Epoch 1902/5000
26/26 - 1s - loss: 0.8039 - val_loss: 1.2146
Epoch 1903/5000
26/26 - 1s - loss: 0.8034 - val_loss: 1.2142
Epoch 1904/5000
26/26 - 1s - loss: 0.8033 - val_loss: 1.2118
Epoch 1905/5000
26/26 - 1s - loss: 0.8034 - val_loss: 1.2110
Epoch 1906/5000
26/26 - 1s - loss: 0.8014 - val_loss: 1.2099
Epoch 1907/5000
26/26 - 1s - loss: 0.8020 - val_loss: 1.2084
Epoch 1908/5000
26/26 - 1s - loss: 0.8013 - val_loss: 1.2120
Epoch 1909/5000
26/26 - 1s - loss: 0.8009 - val_loss: 1.2112
Epoch 1910/5000
26/26 - 1s - loss: 0.8019 - val_loss: 1.2112
Epoch 01910: val_loss improved from 1.21378 to 1.21122, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 1911/5000
26/26 - 1s - loss: 0.7994 - val_loss: 1.2091
Epoch 1912/5000
26/26 - 1s - loss: 0.7998 - val_loss: 1.2096
Epoch 1913/5000
26/26 - 1s - loss: 0.7993 - val_loss: 1.2111
Epoch 1914/5000
26/26 - 1s - loss: 0.8013 - val_loss: 1.2092
Epoch 1915/5000
26/26 - 1s - loss: 0.7989 - val_loss: 1.2088
Epoch 1916/5000
26/26 - 1s - loss: 0.7982 - val_loss: 1.2083
Epoch 1917/5000
26/26 - 1s - loss: 0.7986 - val_loss: 1.2074
Epoch 1918/5000
26/26 - 1s - loss: 0.7968 - val_loss: 1.2087
Epoch 1919/5000
26/26 - 1s - loss: 0.7961 - val_loss: 1.2075
Epoch 1920/5000
26/26 - 1s - loss: 0.7967 - val_loss: 1.2054
Epoch 01920: val_loss improved from 1.21122 to 1.20544, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 1921/5000
26/26 - 1s - loss: 0.7956 - val_loss: 1.2063
Epoch 1922/5000
26/26 - 1s - loss: 0.7953 - val_loss: 1.2059
Epoch 1923/5000
26/26 - 1s - loss: 0.7966 - val_loss: 1.2058
Epoch 1924/5000
26/26 - 1s - loss: 0.7969 - val_loss: 1.2040
Epoch 1925/5000
26/26 - 1s - loss: 0.7947 - val_loss: 1.2034
Epoch 1926/5000
26/26 - 1s - loss: 0.7950 - val_loss: 1.2022
Epoch 1927/5000
26/26 - 1s - loss: 0.7949 - val_loss: 1.2030
Epoch 1928/5000
26/26 - 1s - loss: 0.7935 - val_loss: 1.2029
Epoch 1929/5000
26/26 - 1s - loss: 0.7936 - val_loss: 1.2017
Epoch 1930/5000
26/26 - 1s - loss: 0.7933 - val_loss: 1.2007
Epoch 01930: val_loss improved from 1.20544 to 1.20070, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 1931/5000
26/26 - 1s - loss: 0.7922 - val_loss: 1.2019
Epoch 1932/5000
26/26 - 1s - loss: 0.7932 - val_loss: 1.2006
Epoch 1933/5000
26/26 - 2s - loss: 0.7924 - val_loss: 1.1985
Epoch 1934/5000
26/26 - 1s - loss: 0.7914 - val_loss: 1.2001
Epoch 1935/5000
26/26 - 1s - loss: 0.7907 - val_loss: 1.2001
Epoch 1936/5000
26/26 - 1s - loss: 0.7908 - val_loss: 1.1994
Epoch 1937/5000
26/26 - 1s - loss: 0.7903 - val_loss: 1.1985
Epoch 1938/5000
26/26 - 2s - loss: 0.7900 - val_loss: 1.1985
Epoch 1939/5000
26/26 - 1s - loss: 0.7908 - val_loss: 1.1978
Epoch 1940/5000
26/26 - 1s - loss: 0.7898 - val_loss: 1.1972
Epoch 01940: val_loss improved from 1.20070 to 1.19722, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 1941/5000
26/26 - 1s - loss: 0.7902 - val_loss: 1.1966
Epoch 1942/5000
26/26 - 1s - loss: 0.7885 - val_loss: 1.1980
Epoch 1943/5000
26/26 - 1s - loss: 0.7884 - val_loss: 1.1945
Epoch 1944/5000
26/26 - 1s - loss: 0.7882 - val_loss: 1.1962
Epoch 1945/5000
26/26 - 1s - loss: 0.7879 - val_loss: 1.1946
Epoch 1946/5000
26/26 - 1s - loss: 0.7875 - val_loss: 1.1963
Epoch 1947/5000
26/26 - 1s - loss: 0.7860 - val_loss: 1.1956
Epoch 1948/5000
26/26 - 1s - loss: 0.7871 - val_loss: 1.1949
Epoch 1949/5000
26/26 - 2s - loss: 0.7862 - val_loss: 1.1941
Epoch 1950/5000
26/26 - 1s - loss: 0.7856 - val_loss: 1.1939
Epoch 01950: val_loss improved from 1.19722 to 1.19388, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 1951/5000
26/26 - 1s - loss: 0.7843 - val_loss: 1.1935
Epoch 1952/5000
26/26 - 1s - loss: 0.7830 - val_loss: 1.1930
Epoch 1953/5000
26/26 - 1s - loss: 0.7858 - val_loss: 1.1944
Epoch 1954/5000
26/26 - 1s - loss: 0.7834 - val_loss: 1.1936
Epoch 1955/5000
26/26 - 1s - loss: 0.7836 - val_loss: 1.1930
Epoch 1956/5000
26/26 - 1s - loss: 0.7815 - val_loss: 1.1920
Epoch 1957/5000
26/26 - 1s - loss: 0.7835 - val_loss: 1.1931
Epoch 1958/5000
26/26 - 2s - loss: 0.7822 - val_loss: 1.1916
Epoch 1959/5000
26/26 - 1s - loss: 0.7835 - val_loss: 1.1909
Epoch 1960/5000
26/26 - 1s - loss: 0.7819 - val_loss: 1.1906
Epoch 01960: val_loss improved from 1.19388 to 1.19056, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 1961/5000
26/26 - 1s - loss: 0.7802 - val_loss: 1.1898
Epoch 1962/5000
26/26 - 1s - loss: 0.7814 - val_loss: 1.1907
Epoch 1963/5000
26/26 - 1s - loss: 0.7800 - val_loss: 1.1922
Epoch 1964/5000
26/26 - 1s - loss: 0.7802 - val_loss: 1.1891
Epoch 1965/5000
26/26 - 1s - loss: 0.7795 - val_loss: 1.1892
Epoch 1966/5000
26/26 - 1s - loss: 0.7803 - val_loss: 1.1875
Epoch 1967/5000
26/26 - 1s - loss: 0.7799 - val_loss: 1.1892
Epoch 1968/5000
26/26 - 1s - loss: 0.7787 - val_loss: 1.1881
Epoch 1969/5000
26/26 - 1s - loss: 0.7789 - val_loss: 1.1885
Epoch 1970/5000
26/26 - 1s - loss: 0.7785 - val_loss: 1.1876
Epoch 01970: val_loss improved from 1.19056 to 1.18760, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 1971/5000
26/26 - 1s - loss: 0.7792 - val_loss: 1.1860
Epoch 1972/5000
26/26 - 1s - loss: 0.7776 - val_loss: 1.1861
Epoch 1973/5000
26/26 - 1s - loss: 0.7772 - val_loss: 1.1872
Epoch 1974/5000
26/26 - 2s - loss: 0.7786 - val_loss: 1.1845
Epoch 1975/5000
26/26 - 1s - loss: 0.7777 - val_loss: 1.1860
Epoch 1976/5000
26/26 - 1s - loss: 0.7760 - val_loss: 1.1864
Epoch 1977/5000
26/26 - 1s - loss: 0.7763 - val_loss: 1.1857
Epoch 1978/5000
26/26 - 2s - loss: 0.7756 - val_loss: 1.1856
Epoch 1979/5000
26/26 - 1s - loss: 0.7751 - val_loss: 1.1838
Epoch 1980/5000
26/26 - 1s - loss: 0.7757 - val_loss: 1.1816
Epoch 01980: val_loss improved from 1.18760 to 1.18156, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 1981/5000
26/26 - 1s - loss: 0.7753 - val_loss: 1.1829
Epoch 1982/5000
26/26 - 1s - loss: 0.7730 - val_loss: 1.1851
Epoch 1983/5000
26/26 - 2s - loss: 0.7738 - val_loss: 1.1828
Epoch 1984/5000
26/26 - 1s - loss: 0.7732 - val_loss: 1.1828
Epoch 1985/5000
26/26 - 1s - loss: 0.7726 - val_loss: 1.1822
Epoch 1986/5000
26/26 - 2s - loss: 0.7731 - val_loss: 1.1825
Epoch 1987/5000
26/26 - 1s - loss: 0.7718 - val_loss: 1.1828
Epoch 1988/5000
26/26 - 1s - loss: 0.7738 - val_loss: 1.1804
Epoch 1989/5000
26/26 - 1s - loss: 0.7712 - val_loss: 1.1829
Epoch 1990/5000
26/26 - 1s - loss: 0.7708 - val_loss: 1.1825
Epoch 01990: val_loss did not improve from 1.18156
Epoch 1991/5000
26/26 - 1s - loss: 0.7700 - val_loss: 1.1816
Epoch 1992/5000
26/26 - 1s - loss: 0.7712 - val_loss: 1.1803
Epoch 1993/5000
26/26 - 1s - loss: 0.7708 - val_loss: 1.1782
Epoch 1994/5000
26/26 - 1s - loss: 0.7704 - val_loss: 1.1807
Epoch 1995/5000
26/26 - 1s - loss: 0.7710 - val_loss: 1.1798
Epoch 1996/5000
26/26 - 1s - loss: 0.7691 - val_loss: 1.1779
Epoch 1997/5000
26/26 - 1s - loss: 0.7681 - val_loss: 1.1778
Epoch 1998/5000
26/26 - 1s - loss: 0.7680 - val_loss: 1.1764
Epoch 1999/5000
26/26 - 1s - loss: 0.7682 - val_loss: 1.1769
Epoch 2000/5000
26/26 - 1s - loss: 0.7692 - val_loss: 1.1766
Epoch 02000: val_loss improved from 1.18156 to 1.17664, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 2001/5000
26/26 - 1s - loss: 0.7670 - val_loss: 1.1774
Epoch 2002/5000
26/26 - 1s - loss: 0.7672 - val_loss: 1.1772
Epoch 2003/5000
26/26 - 1s - loss: 0.7671 - val_loss: 1.1770
Epoch 2004/5000
26/26 - 1s - loss: 0.7658 - val_loss: 1.1764
Epoch 2005/5000
26/26 - 1s - loss: 0.7663 - val_loss: 1.1777
Epoch 2006/5000
26/26 - 1s - loss: 0.7667 - val_loss: 1.1753
Epoch 2007/5000
26/26 - 1s - loss: 0.7649 - val_loss: 1.1767
Epoch 2008/5000
26/26 - 1s - loss: 0.7646 - val_loss: 1.1750
Epoch 2009/5000
26/26 - 1s - loss: 0.7653 - val_loss: 1.1745
Epoch 2010/5000
26/26 - 1s - loss: 0.7651 - val_loss: 1.1748
Epoch 02010: val_loss improved from 1.17664 to 1.17476, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 2011/5000
26/26 - 1s - loss: 0.7648 - val_loss: 1.1736
Epoch 2012/5000
26/26 - 1s - loss: 0.7646 - val_loss: 1.1726
Epoch 2013/5000
26/26 - 1s - loss: 0.7622 - val_loss: 1.1735
Epoch 2014/5000
26/26 - 1s - loss: 0.7633 - val_loss: 1.1729
Epoch 2015/5000
26/26 - 1s - loss: 0.7624 - val_loss: 1.1714
Epoch 2016/5000
26/26 - 1s - loss: 0.7625 - val_loss: 1.1709
Epoch 2017/5000
26/26 - 1s - loss: 0.7625 - val_loss: 1.1719
Epoch 2018/5000
26/26 - 1s - loss: 0.7602 - val_loss: 1.1705
Epoch 2019/5000
26/26 - 1s - loss: 0.7614 - val_loss: 1.1707
Epoch 2020/5000
26/26 - 1s - loss: 0.7602 - val_loss: 1.1702
Epoch 02020: val_loss improved from 1.17476 to 1.17021, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 2021/5000
26/26 - 1s - loss: 0.7609 - val_loss: 1.1705
Epoch 2022/5000
26/26 - 1s - loss: 0.7601 - val_loss: 1.1695
Epoch 2023/5000
26/26 - 1s - loss: 0.7591 - val_loss: 1.1704
Epoch 2024/5000
26/26 - 1s - loss: 0.7604 - val_loss: 1.1681
Epoch 2025/5000
26/26 - 1s - loss: 0.7585 - val_loss: 1.1694
Epoch 2026/5000
26/26 - 1s - loss: 0.7595 - val_loss: 1.1694
Epoch 2027/5000
26/26 - 1s - loss: 0.7591 - val_loss: 1.1673
Epoch 2028/5000
26/26 - 1s - loss: 0.7578 - val_loss: 1.1687
Epoch 2029/5000
26/26 - 1s - loss: 0.7580 - val_loss: 1.1679
Epoch 2030/5000
26/26 - 1s - loss: 0.7568 - val_loss: 1.1685
Epoch 02030: val_loss improved from 1.17021 to 1.16846, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 2031/5000
26/26 - 1s - loss: 0.7564 - val_loss: 1.1674
Epoch 2032/5000
26/26 - 2s - loss: 0.7566 - val_loss: 1.1672
Epoch 2033/5000
26/26 - 1s - loss: 0.7584 - val_loss: 1.1673
Epoch 2034/5000
26/26 - 1s - loss: 0.7564 - val_loss: 1.1659
Epoch 2035/5000
26/26 - 1s - loss: 0.7558 - val_loss: 1.1664
Epoch 2036/5000
26/26 - 1s - loss: 0.7547 - val_loss: 1.1665
Epoch 2037/5000
26/26 - 1s - loss: 0.7562 - val_loss: 1.1637
Epoch 2038/5000
26/26 - 1s - loss: 0.7533 - val_loss: 1.1648
Epoch 2039/5000
26/26 - 1s - loss: 0.7545 - val_loss: 1.1624
Epoch 2040/5000
26/26 - 1s - loss: 0.7539 - val_loss: 1.1629
Epoch 02040: val_loss improved from 1.16846 to 1.16295, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 2041/5000
26/26 - 1s - loss: 0.7549 - val_loss: 1.1619
Epoch 2042/5000
26/26 - 1s - loss: 0.7533 - val_loss: 1.1645
Epoch 2043/5000
26/26 - 1s - loss: 0.7523 - val_loss: 1.1629
Epoch 2044/5000
26/26 - 1s - loss: 0.7530 - val_loss: 1.1607
Epoch 2045/5000
26/26 - 1s - loss: 0.7528 - val_loss: 1.1603
Epoch 2046/5000
26/26 - 1s - loss: 0.7519 - val_loss: 1.1619
Epoch 2047/5000
26/26 - 1s - loss: 0.7512 - val_loss: 1.1597
Epoch 2048/5000
26/26 - 1s - loss: 0.7495 - val_loss: 1.1625
Epoch 2049/5000
26/26 - 1s - loss: 0.7517 - val_loss: 1.1601
Epoch 2050/5000
26/26 - 1s - loss: 0.7512 - val_loss: 1.1620
Epoch 02050: val_loss improved from 1.16295 to 1.16202, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 2051/5000
26/26 - 1s - loss: 0.7509 - val_loss: 1.1619
Epoch 2052/5000
26/26 - 1s - loss: 0.7498 - val_loss: 1.1606
Epoch 2053/5000
26/26 - 1s - loss: 0.7503 - val_loss: 1.1592
Epoch 2054/5000
26/26 - 1s - loss: 0.7501 - val_loss: 1.1607
Epoch 2055/5000
26/26 - 1s - loss: 0.7489 - val_loss: 1.1578
Epoch 2056/5000
26/26 - 1s - loss: 0.7482 - val_loss: 1.1599
Epoch 2057/5000
26/26 - 1s - loss: 0.7489 - val_loss: 1.1585
Epoch 2058/5000
26/26 - 1s - loss: 0.7480 - val_loss: 1.1574
Epoch 2059/5000
26/26 - 1s - loss: 0.7480 - val_loss: 1.1571
Epoch 2060/5000
26/26 - 1s - loss: 0.7473 - val_loss: 1.1578
Epoch 02060: val_loss improved from 1.16202 to 1.15777, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 2061/5000
26/26 - 1s - loss: 0.7482 - val_loss: 1.1587
Epoch 2062/5000
26/26 - 1s - loss: 0.7479 - val_loss: 1.1554
Epoch 2063/5000
26/26 - 2s - loss: 0.7477 - val_loss: 1.1543
Epoch 2064/5000
26/26 - 1s - loss: 0.7473 - val_loss: 1.1556
Epoch 2065/5000
26/26 - 1s - loss: 0.7466 - val_loss: 1.1544
Epoch 2066/5000
26/26 - 1s - loss: 0.7451 - val_loss: 1.1544
Epoch 2067/5000
26/26 - 1s - loss: 0.7450 - val_loss: 1.1543
Epoch 2068/5000
26/26 - 1s - loss: 0.7446 - val_loss: 1.1529
Epoch 2069/5000
26/26 - 1s - loss: 0.7446 - val_loss: 1.1535
Epoch 2070/5000
26/26 - 1s - loss: 0.7435 - val_loss: 1.1537
Epoch 02070: val_loss improved from 1.15777 to 1.15370, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 2071/5000
26/26 - 1s - loss: 0.7438 - val_loss: 1.1545
Epoch 2072/5000
26/26 - 2s - loss: 0.7438 - val_loss: 1.1539
Epoch 2073/5000
26/26 - 1s - loss: 0.7436 - val_loss: 1.1516
Epoch 2074/5000
26/26 - 1s - loss: 0.7437 - val_loss: 1.1521
Epoch 2075/5000
26/26 - 1s - loss: 0.7427 - val_loss: 1.1532
Epoch 2076/5000
26/26 - 1s - loss: 0.7432 - val_loss: 1.1514
Epoch 2077/5000
26/26 - 1s - loss: 0.7421 - val_loss: 1.1509
Epoch 2078/5000
26/26 - 1s - loss: 0.7404 - val_loss: 1.1507
Epoch 2079/5000
26/26 - 1s - loss: 0.7423 - val_loss: 1.1515
Epoch 2080/5000
26/26 - 1s - loss: 0.7431 - val_loss: 1.1506
Epoch 02080: val_loss improved from 1.15370 to 1.15058, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 2081/5000
26/26 - 1s - loss: 0.7415 - val_loss: 1.1508
Epoch 2082/5000
26/26 - 2s - loss: 0.7396 - val_loss: 1.1498
Epoch 2083/5000
26/26 - 1s - loss: 0.7410 - val_loss: 1.1497
Epoch 2084/5000
26/26 - 1s - loss: 0.7407 - val_loss: 1.1499
Epoch 2085/5000
26/26 - 1s - loss: 0.7394 - val_loss: 1.1481
Epoch 2086/5000
26/26 - 1s - loss: 0.7395 - val_loss: 1.1496
Epoch 2087/5000
26/26 - 1s - loss: 0.7405 - val_loss: 1.1482
Epoch 2088/5000
26/26 - 1s - loss: 0.7392 - val_loss: 1.1474
Epoch 2089/5000
26/26 - 1s - loss: 0.7374 - val_loss: 1.1496
Epoch 2090/5000
26/26 - 1s - loss: 0.7378 - val_loss: 1.1473
Epoch 02090: val_loss improved from 1.15058 to 1.14726, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 2091/5000
26/26 - 1s - loss: 0.7385 - val_loss: 1.1467
Epoch 2092/5000
26/26 - 1s - loss: 0.7373 - val_loss: 1.1474
Epoch 2093/5000
26/26 - 1s - loss: 0.7375 - val_loss: 1.1454
Epoch 2094/5000
26/26 - 1s - loss: 0.7357 - val_loss: 1.1476
Epoch 2095/5000
26/26 - 1s - loss: 0.7367 - val_loss: 1.1460
Epoch 2096/5000
26/26 - 1s - loss: 0.7364 - val_loss: 1.1450
Epoch 2097/5000
26/26 - 1s - loss: 0.7363 - val_loss: 1.1442
Epoch 2098/5000
26/26 - 1s - loss: 0.7353 - val_loss: 1.1447
Epoch 2099/5000
26/26 - 1s - loss: 0.7332 - val_loss: 1.1433
Epoch 2100/5000
26/26 - 1s - loss: 0.7339 - val_loss: 1.1463
Epoch 02100: val_loss improved from 1.14726 to 1.14633, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 2101/5000
26/26 - 1s - loss: 0.7349 - val_loss: 1.1442
Epoch 2102/5000
26/26 - 1s - loss: 0.7349 - val_loss: 1.1443
Epoch 2103/5000
26/26 - 2s - loss: 0.7337 - val_loss: 1.1441
Epoch 2104/5000
26/26 - 1s - loss: 0.7326 - val_loss: 1.1435
Epoch 2105/5000
26/26 - 1s - loss: 0.7337 - val_loss: 1.1410
Epoch 2106/5000
26/26 - 1s - loss: 0.7322 - val_loss: 1.1420
Epoch 2107/5000
26/26 - 2s - loss: 0.7308 - val_loss: 1.1412
Epoch 2108/5000
26/26 - 1s - loss: 0.7309 - val_loss: 1.1410
Epoch 2109/5000
26/26 - 1s - loss: 0.7313 - val_loss: 1.1424
Epoch 2110/5000
26/26 - 1s - loss: 0.7315 - val_loss: 1.1404
Epoch 02110: val_loss improved from 1.14633 to 1.14043, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 2111/5000
26/26 - 1s - loss: 0.7296 - val_loss: 1.1407
Epoch 2112/5000
26/26 - 1s - loss: 0.7293 - val_loss: 1.1394
Epoch 2113/5000
26/26 - 1s - loss: 0.7305 - val_loss: 1.1396
Epoch 2114/5000
26/26 - 1s - loss: 0.7309 - val_loss: 1.1393
Epoch 2115/5000
26/26 - 1s - loss: 0.7305 - val_loss: 1.1410
Epoch 2116/5000
26/26 - 1s - loss: 0.7303 - val_loss: 1.1389
Epoch 2117/5000
26/26 - 1s - loss: 0.7308 - val_loss: 1.1386
Epoch 2118/5000
26/26 - 1s - loss: 0.7298 - val_loss: 1.1396
Epoch 2119/5000
26/26 - 1s - loss: 0.7279 - val_loss: 1.1375
Epoch 2120/5000
26/26 - 1s - loss: 0.7278 - val_loss: 1.1366
Epoch 02120: val_loss improved from 1.14043 to 1.13662, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 2121/5000
26/26 - 1s - loss: 0.7277 - val_loss: 1.1381
Epoch 2122/5000
26/26 - 1s - loss: 0.7273 - val_loss: 1.1374
Epoch 2123/5000
26/26 - 1s - loss: 0.7267 - val_loss: 1.1395
Epoch 2124/5000
26/26 - 2s - loss: 0.7263 - val_loss: 1.1352
Epoch 2125/5000
26/26 - 1s - loss: 0.7271 - val_loss: 1.1371
Epoch 2126/5000
26/26 - 1s - loss: 0.7264 - val_loss: 1.1370
Epoch 2127/5000
26/26 - 1s - loss: 0.7276 - val_loss: 1.1367
Epoch 2128/5000
26/26 - 1s - loss: 0.7269 - val_loss: 1.1359
Epoch 2129/5000
26/26 - 1s - loss: 0.7256 - val_loss: 1.1345
Epoch 2130/5000
26/26 - 2s - loss: 0.7251 - val_loss: 1.1348
Epoch 02130: val_loss improved from 1.13662 to 1.13476, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 2131/5000
26/26 - 1s - loss: 0.7258 - val_loss: 1.1350
Epoch 2132/5000
26/26 - 1s - loss: 0.7235 - val_loss: 1.1335
Epoch 2133/5000
26/26 - 1s - loss: 0.7245 - val_loss: 1.1341
Epoch 2134/5000
26/26 - 1s - loss: 0.7246 - val_loss: 1.1326
Epoch 2135/5000
26/26 - 1s - loss: 0.7241 - val_loss: 1.1332
Epoch 2136/5000
26/26 - 1s - loss: 0.7225 - val_loss: 1.1346
Epoch 2137/5000
26/26 - 1s - loss: 0.7230 - val_loss: 1.1322
Epoch 2138/5000
26/26 - 1s - loss: 0.7236 - val_loss: 1.1314
Epoch 2139/5000
26/26 - 1s - loss: 0.7223 - val_loss: 1.1326
Epoch 2140/5000
26/26 - 1s - loss: 0.7231 - val_loss: 1.1312
Epoch 02140: val_loss improved from 1.13476 to 1.13121, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 2141/5000
26/26 - 1s - loss: 0.7231 - val_loss: 1.1318
Epoch 2142/5000
26/26 - 1s - loss: 0.7215 - val_loss: 1.1315
Epoch 2143/5000
26/26 - 1s - loss: 0.7203 - val_loss: 1.1302
Epoch 2144/5000
26/26 - 1s - loss: 0.7215 - val_loss: 1.1320
Epoch 2145/5000
26/26 - 1s - loss: 0.7205 - val_loss: 1.1289
Epoch 2146/5000
26/26 - 1s - loss: 0.7212 - val_loss: 1.1310
Epoch 2147/5000
26/26 - 1s - loss: 0.7203 - val_loss: 1.1314
Epoch 2148/5000
26/26 - 1s - loss: 0.7188 - val_loss: 1.1281
Epoch 2149/5000
26/26 - 1s - loss: 0.7205 - val_loss: 1.1295
Epoch 2150/5000
26/26 - 2s - loss: 0.7201 - val_loss: 1.1284
Epoch 02150: val_loss improved from 1.13121 to 1.12845, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 2151/5000
26/26 - 1s - loss: 0.7205 - val_loss: 1.1309
Epoch 2152/5000
26/26 - 1s - loss: 0.7178 - val_loss: 1.1281
Epoch 2153/5000
26/26 - 1s - loss: 0.7186 - val_loss: 1.1272
Epoch 2154/5000
26/26 - 1s - loss: 0.7167 - val_loss: 1.1269
Epoch 2155/5000
26/26 - 1s - loss: 0.7166 - val_loss: 1.1274
Epoch 2156/5000
26/26 - 1s - loss: 0.7168 - val_loss: 1.1274
Epoch 2157/5000
26/26 - 1s - loss: 0.7159 - val_loss: 1.1282
Epoch 2158/5000
26/26 - 2s - loss: 0.7171 - val_loss: 1.1271
Epoch 2159/5000
26/26 - 1s - loss: 0.7163 - val_loss: 1.1246
Epoch 2160/5000
26/26 - 1s - loss: 0.7154 - val_loss: 1.1266
Epoch 02160: val_loss improved from 1.12845 to 1.12657, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 2161/5000
26/26 - 1s - loss: 0.7145 - val_loss: 1.1252
Epoch 2162/5000
26/26 - 1s - loss: 0.7145 - val_loss: 1.1247
Epoch 2163/5000
26/26 - 1s - loss: 0.7160 - val_loss: 1.1250
Epoch 2164/5000
26/26 - 1s - loss: 0.7145 - val_loss: 1.1250
Epoch 2165/5000
26/26 - 1s - loss: 0.7155 - val_loss: 1.1233
Epoch 2166/5000
26/26 - 1s - loss: 0.7143 - val_loss: 1.1260
Epoch 2167/5000
26/26 - 1s - loss: 0.7160 - val_loss: 1.1235
Epoch 2168/5000
26/26 - 1s - loss: 0.7144 - val_loss: 1.1261
Epoch 2169/5000
26/26 - 1s - loss: 0.7144 - val_loss: 1.1218
Epoch 2170/5000
26/26 - 1s - loss: 0.7127 - val_loss: 1.1229
Epoch 02170: val_loss improved from 1.12657 to 1.12295, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 2171/5000
26/26 - 1s - loss: 0.7131 - val_loss: 1.1225
Epoch 2172/5000
26/26 - 1s - loss: 0.7116 - val_loss: 1.1221
Epoch 2173/5000
26/26 - 1s - loss: 0.7128 - val_loss: 1.1222
Epoch 2174/5000
26/26 - 1s - loss: 0.7112 - val_loss: 1.1227
Epoch 2175/5000
26/26 - 1s - loss: 0.7110 - val_loss: 1.1216
Epoch 2176/5000
26/26 - 1s - loss: 0.7116 - val_loss: 1.1208
Epoch 2177/5000
26/26 - 1s - loss: 0.7119 - val_loss: 1.1219
Epoch 2178/5000
26/26 - 1s - loss: 0.7113 - val_loss: 1.1208
Epoch 2179/5000
26/26 - 1s - loss: 0.7092 - val_loss: 1.1192
Epoch 2180/5000
26/26 - 2s - loss: 0.7099 - val_loss: 1.1198
Epoch 02180: val_loss improved from 1.12295 to 1.11976, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 2181/5000
26/26 - 1s - loss: 0.7088 - val_loss: 1.1194
Epoch 2182/5000
26/26 - 1s - loss: 0.7095 - val_loss: 1.1207
Epoch 2183/5000
26/26 - 1s - loss: 0.7083 - val_loss: 1.1196
Epoch 2184/5000
26/26 - 1s - loss: 0.7095 - val_loss: 1.1203
Epoch 2185/5000
26/26 - 1s - loss: 0.7079 - val_loss: 1.1179
Epoch 2186/5000
26/26 - 1s - loss: 0.7083 - val_loss: 1.1183
Epoch 2187/5000
26/26 - 1s - loss: 0.7075 - val_loss: 1.1195
Epoch 2188/5000
26/26 - 1s - loss: 0.7077 - val_loss: 1.1172
Epoch 2189/5000
26/26 - 1s - loss: 0.7076 - val_loss: 1.1180
Epoch 2190/5000
26/26 - 1s - loss: 0.7075 - val_loss: 1.1169
Epoch 02190: val_loss improved from 1.11976 to 1.11685, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 2191/5000
26/26 - 1s - loss: 0.7073 - val_loss: 1.1200
Epoch 2192/5000
26/26 - 1s - loss: 0.7092 - val_loss: 1.1158
Epoch 2193/5000
26/26 - 1s - loss: 0.7069 - val_loss: 1.1158
Epoch 2194/5000
26/26 - 1s - loss: 0.7056 - val_loss: 1.1158
Epoch 2195/5000
26/26 - 1s - loss: 0.7059 - val_loss: 1.1182
Epoch 2196/5000
26/26 - 1s - loss: 0.7053 - val_loss: 1.1161
Epoch 2197/5000
26/26 - 2s - loss: 0.7055 - val_loss: 1.1166
Epoch 2198/5000
26/26 - 2s - loss: 0.7040 - val_loss: 1.1145
Epoch 2199/5000
26/26 - 1s - loss: 0.7048 - val_loss: 1.1146
Epoch 2200/5000
26/26 - 1s - loss: 0.7052 - val_loss: 1.1130
Epoch 02200: val_loss improved from 1.11685 to 1.11302, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 2201/5000
26/26 - 1s - loss: 0.7042 - val_loss: 1.1143
Epoch 2202/5000
26/26 - 1s - loss: 0.7050 - val_loss: 1.1132
Epoch 2203/5000
26/26 - 1s - loss: 0.7034 - val_loss: 1.1133
Epoch 2204/5000
26/26 - 1s - loss: 0.7034 - val_loss: 1.1133
Epoch 2205/5000
26/26 - 1s - loss: 0.7041 - val_loss: 1.1128
Epoch 2206/5000
26/26 - 1s - loss: 0.7026 - val_loss: 1.1129
Epoch 2207/5000
26/26 - 1s - loss: 0.7019 - val_loss: 1.1117
Epoch 2208/5000
26/26 - 1s - loss: 0.7014 - val_loss: 1.1116
Epoch 2209/5000
26/26 - 1s - loss: 0.7013 - val_loss: 1.1110
Epoch 2210/5000
26/26 - 1s - loss: 0.7021 - val_loss: 1.1110
Epoch 02210: val_loss improved from 1.11302 to 1.11104, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 2211/5000
26/26 - 1s - loss: 0.6998 - val_loss: 1.1112
Epoch 2212/5000
26/26 - 1s - loss: 0.7011 - val_loss: 1.1107
Epoch 2213/5000
26/26 - 1s - loss: 0.7000 - val_loss: 1.1123
Epoch 2214/5000
26/26 - 1s - loss: 0.7016 - val_loss: 1.1113
Epoch 2215/5000
26/26 - 1s - loss: 0.7004 - val_loss: 1.1095
Epoch 2216/5000
26/26 - 1s - loss: 0.7000 - val_loss: 1.1106
Epoch 2217/5000
26/26 - 1s - loss: 0.6999 - val_loss: 1.1096
Epoch 2218/5000
26/26 - 1s - loss: 0.6994 - val_loss: 1.1082
Epoch 2219/5000
26/26 - 1s - loss: 0.6997 - val_loss: 1.1097
Epoch 2220/5000
26/26 - 1s - loss: 0.6986 - val_loss: 1.1073
Epoch 02220: val_loss improved from 1.11104 to 1.10728, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 2221/5000
26/26 - 1s - loss: 0.7000 - val_loss: 1.1091
Epoch 2222/5000
26/26 - 2s - loss: 0.6979 - val_loss: 1.1091
Epoch 2223/5000
26/26 - 1s - loss: 0.6990 - val_loss: 1.1079
Epoch 2224/5000
26/26 - 1s - loss: 0.6976 - val_loss: 1.1089
Epoch 2225/5000
26/26 - 1s - loss: 0.6976 - val_loss: 1.1087
Epoch 2226/5000
26/26 - 1s - loss: 0.6978 - val_loss: 1.1087
Epoch 2227/5000
26/26 - 1s - loss: 0.6962 - val_loss: 1.1073
Epoch 2228/5000
26/26 - 1s - loss: 0.6963 - val_loss: 1.1065
Epoch 2229/5000
26/26 - 1s - loss: 0.6958 - val_loss: 1.1072
Epoch 2230/5000
26/26 - 1s - loss: 0.6956 - val_loss: 1.1078
Epoch 02230: val_loss did not improve from 1.10728
Epoch 2231/5000
26/26 - 1s - loss: 0.6960 - val_loss: 1.1059
Epoch 2232/5000
26/26 - 1s - loss: 0.6961 - val_loss: 1.1054
Epoch 2233/5000
26/26 - 1s - loss: 0.6942 - val_loss: 1.1059
Epoch 2234/5000
26/26 - 1s - loss: 0.6965 - val_loss: 1.1060
Epoch 2235/5000
26/26 - 1s - loss: 0.6949 - val_loss: 1.1040
Epoch 2236/5000
26/26 - 1s - loss: 0.6951 - val_loss: 1.1036
Epoch 2237/5000
26/26 - 1s - loss: 0.6949 - val_loss: 1.1037
Epoch 2238/5000
26/26 - 1s - loss: 0.6929 - val_loss: 1.1026
Epoch 2239/5000
26/26 - 1s - loss: 0.6950 - val_loss: 1.1030
Epoch 2240/5000
26/26 - 1s - loss: 0.6935 - val_loss: 1.1024
Epoch 02240: val_loss improved from 1.10728 to 1.10237, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 2241/5000
26/26 - 1s - loss: 0.6928 - val_loss: 1.1033
Epoch 2242/5000
26/26 - 2s - loss: 0.6932 - val_loss: 1.1046
Epoch 2243/5000
26/26 - 1s - loss: 0.6921 - val_loss: 1.1026
Epoch 2244/5000
26/26 - 1s - loss: 0.6915 - val_loss: 1.1037
Epoch 2245/5000
26/26 - 1s - loss: 0.6921 - val_loss: 1.1031
Epoch 2246/5000
26/26 - 1s - loss: 0.6905 - val_loss: 1.1021
Epoch 2247/5000
26/26 - 1s - loss: 0.6910 - val_loss: 1.1028
Epoch 2248/5000
26/26 - 1s - loss: 0.6917 - val_loss: 1.1011
Epoch 2249/5000
26/26 - 1s - loss: 0.6916 - val_loss: 1.1001
Epoch 2250/5000
26/26 - 1s - loss: 0.6907 - val_loss: 1.0990
Epoch 02250: val_loss improved from 1.10237 to 1.09895, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 2251/5000
26/26 - 1s - loss: 0.6908 - val_loss: 1.1002
Epoch 2252/5000
26/26 - 1s - loss: 0.6903 - val_loss: 1.0997
Epoch 2253/5000
26/26 - 1s - loss: 0.6898 - val_loss: 1.1016
Epoch 2254/5000
26/26 - 1s - loss: 0.6889 - val_loss: 1.0999
Epoch 2255/5000
26/26 - 1s - loss: 0.6892 - val_loss: 1.0981
Epoch 2256/5000
26/26 - 1s - loss: 0.6906 - val_loss: 1.0999
Epoch 2257/5000
26/26 - 1s - loss: 0.6892 - val_loss: 1.0982
Epoch 2258/5000
26/26 - 1s - loss: 0.6891 - val_loss: 1.0978
Epoch 2259/5000
26/26 - 1s - loss: 0.6880 - val_loss: 1.0984
Epoch 2260/5000
26/26 - 1s - loss: 0.6879 - val_loss: 1.0975
Epoch 02260: val_loss improved from 1.09895 to 1.09747, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 2261/5000
26/26 - 1s - loss: 0.6875 - val_loss: 1.0976
Epoch 2262/5000
26/26 - 1s - loss: 0.6861 - val_loss: 1.0967
Epoch 2263/5000
26/26 - 1s - loss: 0.6876 - val_loss: 1.0981
Epoch 2264/5000
26/26 - 1s - loss: 0.6873 - val_loss: 1.0964
Epoch 2265/5000
26/26 - 1s - loss: 0.6868 - val_loss: 1.0972
Epoch 2266/5000
26/26 - 1s - loss: 0.6867 - val_loss: 1.0970
Epoch 2267/5000
26/26 - 2s - loss: 0.6849 - val_loss: 1.0960
Epoch 2268/5000
26/26 - 1s - loss: 0.6865 - val_loss: 1.0960
Epoch 2269/5000
26/26 - 1s - loss: 0.6852 - val_loss: 1.0951
Epoch 2270/5000
26/26 - 1s - loss: 0.6865 - val_loss: 1.0959
Epoch 02270: val_loss improved from 1.09747 to 1.09589, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 2271/5000
26/26 - 1s - loss: 0.6846 - val_loss: 1.0959
Epoch 2272/5000
26/26 - 1s - loss: 0.6844 - val_loss: 1.0948
Epoch 2273/5000
26/26 - 1s - loss: 0.6838 - val_loss: 1.0972
Epoch 2274/5000
26/26 - 1s - loss: 0.6839 - val_loss: 1.0960
Epoch 2275/5000
26/26 - 1s - loss: 0.6848 - val_loss: 1.0939
Epoch 2276/5000
26/26 - 1s - loss: 0.6842 - val_loss: 1.0931
Epoch 2277/5000
26/26 - 1s - loss: 0.6835 - val_loss: 1.0933
Epoch 2278/5000
26/26 - 1s - loss: 0.6819 - val_loss: 1.0926
Epoch 2279/5000
26/26 - 1s - loss: 0.6826 - val_loss: 1.0943
Epoch 2280/5000
26/26 - 1s - loss: 0.6827 - val_loss: 1.0926
Epoch 02280: val_loss improved from 1.09589 to 1.09256, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 2281/5000
26/26 - 1s - loss: 0.6805 - val_loss: 1.0932
Epoch 2282/5000
26/26 - 1s - loss: 0.6822 - val_loss: 1.0934
Epoch 2283/5000
26/26 - 1s - loss: 0.6811 - val_loss: 1.0917
Epoch 2284/5000
26/26 - 1s - loss: 0.6804 - val_loss: 1.0896
Epoch 2285/5000
26/26 - 1s - loss: 0.6806 - val_loss: 1.0923
Epoch 2286/5000
26/26 - 1s - loss: 0.6812 - val_loss: 1.0922
Epoch 2287/5000
26/26 - 1s - loss: 0.6820 - val_loss: 1.0913
Epoch 2288/5000
26/26 - 1s - loss: 0.6804 - val_loss: 1.0900
Epoch 2289/5000
26/26 - 1s - loss: 0.6798 - val_loss: 1.0895
Epoch 2290/5000
26/26 - 2s - loss: 0.6790 - val_loss: 1.0904
Epoch 02290: val_loss improved from 1.09256 to 1.09045, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 2291/5000
26/26 - 1s - loss: 0.6807 - val_loss: 1.0909
Epoch 2292/5000
26/26 - 2s - loss: 0.6785 - val_loss: 1.0895
Epoch 2293/5000
26/26 - 1s - loss: 0.6788 - val_loss: 1.0884
Epoch 2294/5000
26/26 - 1s - loss: 0.6795 - val_loss: 1.0900
Epoch 2295/5000
26/26 - 1s - loss: 0.6780 - val_loss: 1.0893
Epoch 2296/5000
26/26 - 1s - loss: 0.6792 - val_loss: 1.0872
Epoch 2297/5000
26/26 - 1s - loss: 0.6775 - val_loss: 1.0878
Epoch 2298/5000
26/26 - 1s - loss: 0.6768 - val_loss: 1.0900
Epoch 2299/5000
26/26 - 1s - loss: 0.6760 - val_loss: 1.0878
Epoch 2300/5000
26/26 - 1s - loss: 0.6767 - val_loss: 1.0860
Epoch 02300: val_loss improved from 1.09045 to 1.08596, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 2301/5000
26/26 - 1s - loss: 0.6772 - val_loss: 1.0858
Epoch 2302/5000
26/26 - 1s - loss: 0.6775 - val_loss: 1.0866
Epoch 2303/5000
26/26 - 1s - loss: 0.6757 - val_loss: 1.0875
Epoch 2304/5000
26/26 - 1s - loss: 0.6759 - val_loss: 1.0872
Epoch 2305/5000
26/26 - 1s - loss: 0.6766 - val_loss: 1.0870
Epoch 2306/5000
26/26 - 1s - loss: 0.6750 - val_loss: 1.0840
Epoch 2307/5000
26/26 - 1s - loss: 0.6756 - val_loss: 1.0840
Epoch 2308/5000
26/26 - 1s - loss: 0.6760 - val_loss: 1.0853
Epoch 2309/5000
26/26 - 1s - loss: 0.6747 - val_loss: 1.0852
Epoch 2310/5000
26/26 - 1s - loss: 0.6748 - val_loss: 1.0855
Epoch 02310: val_loss improved from 1.08596 to 1.08549, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 2311/5000
26/26 - 1s - loss: 0.6753 - val_loss: 1.0856
Epoch 2312/5000
26/26 - 1s - loss: 0.6734 - val_loss: 1.0851
Epoch 2313/5000
26/26 - 1s - loss: 0.6737 - val_loss: 1.0858
Epoch 2314/5000
26/26 - 1s - loss: 0.6747 - val_loss: 1.0842
Epoch 2315/5000
26/26 - 2s - loss: 0.6733 - val_loss: 1.0835
Epoch 2316/5000
26/26 - 1s - loss: 0.6735 - val_loss: 1.0833
Epoch 2317/5000
26/26 - 1s - loss: 0.6726 - val_loss: 1.0845
Epoch 2318/5000
26/26 - 1s - loss: 0.6724 - val_loss: 1.0834
Epoch 2319/5000
26/26 - 1s - loss: 0.6727 - val_loss: 1.0841
Epoch 2320/5000
26/26 - 1s - loss: 0.6732 - val_loss: 1.0838
Epoch 02320: val_loss improved from 1.08549 to 1.08380, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 2321/5000
26/26 - 1s - loss: 0.6733 - val_loss: 1.0817
Epoch 2322/5000
26/26 - 1s - loss: 0.6717 - val_loss: 1.0821
Epoch 2323/5000
26/26 - 1s - loss: 0.6723 - val_loss: 1.0797
Epoch 2324/5000
26/26 - 1s - loss: 0.6694 - val_loss: 1.0816
Epoch 2325/5000
26/26 - 1s - loss: 0.6709 - val_loss: 1.0797
Epoch 2326/5000
26/26 - 1s - loss: 0.6709 - val_loss: 1.0799
Epoch 2327/5000
26/26 - 1s - loss: 0.6709 - val_loss: 1.0800
Epoch 2328/5000
26/26 - 1s - loss: 0.6705 - val_loss: 1.0798
Epoch 2329/5000
26/26 - 1s - loss: 0.6689 - val_loss: 1.0815
Epoch 2330/5000
26/26 - 1s - loss: 0.6714 - val_loss: 1.0814
Epoch 02330: val_loss improved from 1.08380 to 1.08136, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 2331/5000
26/26 - 1s - loss: 0.6702 - val_loss: 1.0778
Epoch 2332/5000
26/26 - 2s - loss: 0.6700 - val_loss: 1.0799
Epoch 2333/5000
26/26 - 1s - loss: 0.6689 - val_loss: 1.0791
Epoch 2334/5000
26/26 - 1s - loss: 0.6679 - val_loss: 1.0785
Epoch 2335/5000
26/26 - 1s - loss: 0.6686 - val_loss: 1.0769
Epoch 2336/5000
26/26 - 1s - loss: 0.6695 - val_loss: 1.0781
Epoch 2337/5000
26/26 - 1s - loss: 0.6671 - val_loss: 1.0788
Epoch 2338/5000
26/26 - 1s - loss: 0.6688 - val_loss: 1.0775
Epoch 2339/5000
26/26 - 1s - loss: 0.6672 - val_loss: 1.0763
Epoch 2340/5000
26/26 - 1s - loss: 0.6684 - val_loss: 1.0776
Epoch 02340: val_loss improved from 1.08136 to 1.07764, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 2341/5000
26/26 - 1s - loss: 0.6685 - val_loss: 1.0760
Epoch 2342/5000
26/26 - 1s - loss: 0.6675 - val_loss: 1.0773
Epoch 2343/5000
26/26 - 1s - loss: 0.6670 - val_loss: 1.0763
Epoch 2344/5000
26/26 - 1s - loss: 0.6656 - val_loss: 1.0760
Epoch 2345/5000
26/26 - 1s - loss: 0.6673 - val_loss: 1.0763
Epoch 2346/5000
26/26 - 1s - loss: 0.6666 - val_loss: 1.0771
Epoch 2347/5000
26/26 - 1s - loss: 0.6657 - val_loss: 1.0734
Epoch 2348/5000
26/26 - 1s - loss: 0.6667 - val_loss: 1.0739
Epoch 2349/5000
26/26 - 1s - loss: 0.6663 - val_loss: 1.0757
Epoch 2350/5000
26/26 - 1s - loss: 0.6644 - val_loss: 1.0744
Epoch 02350: val_loss improved from 1.07764 to 1.07441, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 2351/5000
26/26 - 1s - loss: 0.6656 - val_loss: 1.0757
Epoch 2352/5000
26/26 - 1s - loss: 0.6645 - val_loss: 1.0734
Epoch 2353/5000
26/26 - 1s - loss: 0.6649 - val_loss: 1.0753
Epoch 2354/5000
26/26 - 1s - loss: 0.6630 - val_loss: 1.0730
Epoch 2355/5000
26/26 - 1s - loss: 0.6632 - val_loss: 1.0735
Epoch 2356/5000
26/26 - 1s - loss: 0.6634 - val_loss: 1.0728
Epoch 2357/5000
26/26 - 1s - loss: 0.6640 - val_loss: 1.0732
Epoch 2358/5000
26/26 - 1s - loss: 0.6633 - val_loss: 1.0729
Epoch 2359/5000
26/26 - 1s - loss: 0.6622 - val_loss: 1.0715
Epoch 2360/5000
26/26 - 1s - loss: 0.6628 - val_loss: 1.0728
Epoch 02360: val_loss improved from 1.07441 to 1.07280, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 2361/5000
26/26 - 1s - loss: 0.6627 - val_loss: 1.0719
Epoch 2362/5000
26/26 - 1s - loss: 0.6643 - val_loss: 1.0713
Epoch 2363/5000
26/26 - 1s - loss: 0.6619 - val_loss: 1.0714
Epoch 2364/5000
26/26 - 1s - loss: 0.6604 - val_loss: 1.0703
Epoch 2365/5000
26/26 - 1s - loss: 0.6616 - val_loss: 1.0697
Epoch 2366/5000
26/26 - 1s - loss: 0.6612 - val_loss: 1.0685
Epoch 2367/5000
26/26 - 1s - loss: 0.6618 - val_loss: 1.0704
Epoch 2368/5000
26/26 - 1s - loss: 0.6604 - val_loss: 1.0705
Epoch 2369/5000
26/26 - 1s - loss: 0.6591 - val_loss: 1.0689
Epoch 2370/5000
26/26 - 1s - loss: 0.6590 - val_loss: 1.0677
Epoch 02370: val_loss improved from 1.07280 to 1.06770, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 2371/5000
26/26 - 1s - loss: 0.6602 - val_loss: 1.0687
Epoch 2372/5000
26/26 - 1s - loss: 0.6596 - val_loss: 1.0702
Epoch 2373/5000
26/26 - 2s - loss: 0.6597 - val_loss: 1.0687
Epoch 2374/5000
26/26 - 1s - loss: 0.6600 - val_loss: 1.0699
Epoch 2375/5000
26/26 - 1s - loss: 0.6592 - val_loss: 1.0687
Epoch 2376/5000
26/26 - 1s - loss: 0.6594 - val_loss: 1.0682
Epoch 2377/5000
26/26 - 1s - loss: 0.6576 - val_loss: 1.0704
Epoch 2378/5000
26/26 - 1s - loss: 0.6579 - val_loss: 1.0697
Epoch 2379/5000
26/26 - 2s - loss: 0.6580 - val_loss: 1.0691
Epoch 2380/5000
26/26 - 1s - loss: 0.6584 - val_loss: 1.0678
Epoch 02380: val_loss did not improve from 1.06770
Epoch 2381/5000
26/26 - 1s - loss: 0.6571 - val_loss: 1.0669
Epoch 2382/5000
26/26 - 1s - loss: 0.6565 - val_loss: 1.0674
Epoch 2383/5000
26/26 - 1s - loss: 0.6578 - val_loss: 1.0660
Epoch 2384/5000
26/26 - 1s - loss: 0.6549 - val_loss: 1.0656
Epoch 2385/5000
26/26 - 1s - loss: 0.6548 - val_loss: 1.0669
Epoch 2386/5000
26/26 - 1s - loss: 0.6552 - val_loss: 1.0661
Epoch 2387/5000
26/26 - 1s - loss: 0.6562 - val_loss: 1.0663
Epoch 2388/5000
26/26 - 1s - loss: 0.6568 - val_loss: 1.0665
Epoch 2389/5000
26/26 - 1s - loss: 0.6544 - val_loss: 1.0645
Epoch 2390/5000
26/26 - 1s - loss: 0.6544 - val_loss: 1.0670
Epoch 02390: val_loss improved from 1.06770 to 1.06700, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 2391/5000
26/26 - 1s - loss: 0.6555 - val_loss: 1.0665
Epoch 2392/5000
26/26 - 1s - loss: 0.6539 - val_loss: 1.0651
Epoch 2393/5000
26/26 - 1s - loss: 0.6535 - val_loss: 1.0655
Epoch 2394/5000
26/26 - 1s - loss: 0.6543 - val_loss: 1.0651
Epoch 2395/5000
26/26 - 1s - loss: 0.6541 - val_loss: 1.0633
Epoch 2396/5000
26/26 - 1s - loss: 0.6530 - val_loss: 1.0633
Epoch 2397/5000
26/26 - 1s - loss: 0.6546 - val_loss: 1.0645
Epoch 2398/5000
26/26 - 1s - loss: 0.6532 - val_loss: 1.0637
Epoch 2399/5000
26/26 - 1s - loss: 0.6539 - val_loss: 1.0638
Epoch 2400/5000
26/26 - 2s - loss: 0.6526 - val_loss: 1.0616
Epoch 02400: val_loss improved from 1.06700 to 1.06164, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 2401/5000
26/26 - 1s - loss: 0.6520 - val_loss: 1.0633
Epoch 2402/5000
26/26 - 1s - loss: 0.6516 - val_loss: 1.0631
Epoch 2403/5000
26/26 - 1s - loss: 0.6520 - val_loss: 1.0617
Epoch 2404/5000
26/26 - 1s - loss: 0.6515 - val_loss: 1.0615
Epoch 2405/5000
26/26 - 1s - loss: 0.6511 - val_loss: 1.0621
Epoch 2406/5000
26/26 - 1s - loss: 0.6524 - val_loss: 1.0621
Epoch 2407/5000
26/26 - 1s - loss: 0.6508 - val_loss: 1.0612
Epoch 2408/5000
26/26 - 1s - loss: 0.6516 - val_loss: 1.0612
Epoch 2409/5000
26/26 - 1s - loss: 0.6505 - val_loss: 1.0602
Epoch 2410/5000
26/26 - 1s - loss: 0.6500 - val_loss: 1.0598
Epoch 02410: val_loss improved from 1.06164 to 1.05978, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 2411/5000
26/26 - 1s - loss: 0.6515 - val_loss: 1.0600
Epoch 2412/5000
26/26 - 1s - loss: 0.6495 - val_loss: 1.0594
Epoch 2413/5000
26/26 - 1s - loss: 0.6494 - val_loss: 1.0602
Epoch 2414/5000
26/26 - 2s - loss: 0.6505 - val_loss: 1.0600
Epoch 2415/5000
26/26 - 2s - loss: 0.6484 - val_loss: 1.0602
Epoch 2416/5000
26/26 - 1s - loss: 0.6484 - val_loss: 1.0584
Epoch 2417/5000
26/26 - 1s - loss: 0.6497 - val_loss: 1.0594
Epoch 2418/5000
26/26 - 2s - loss: 0.6488 - val_loss: 1.0591
Epoch 2419/5000
26/26 - 2s - loss: 0.6480 - val_loss: 1.0586
Epoch 2420/5000
26/26 - 1s - loss: 0.6480 - val_loss: 1.0593
Epoch 02420: val_loss improved from 1.05978 to 1.05933, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 2421/5000
26/26 - 1s - loss: 0.6471 - val_loss: 1.0581
Epoch 2422/5000
26/26 - 1s - loss: 0.6482 - val_loss: 1.0568
Epoch 2423/5000
26/26 - 1s - loss: 0.6472 - val_loss: 1.0581
Epoch 2424/5000
26/26 - 1s - loss: 0.6468 - val_loss: 1.0583
Epoch 2425/5000
26/26 - 2s - loss: 0.6463 - val_loss: 1.0584
Epoch 2426/5000
26/26 - 1s - loss: 0.6460 - val_loss: 1.0563
Epoch 2427/5000
26/26 - 1s - loss: 0.6462 - val_loss: 1.0577
Epoch 2428/5000
26/26 - 1s - loss: 0.6475 - val_loss: 1.0560
Epoch 2429/5000
26/26 - 1s - loss: 0.6462 - val_loss: 1.0560
Epoch 2430/5000
26/26 - 1s - loss: 0.6461 - val_loss: 1.0556
Epoch 02430: val_loss improved from 1.05933 to 1.05559, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 2431/5000
26/26 - 1s - loss: 0.6459 - val_loss: 1.0552
Epoch 2432/5000
26/26 - 1s - loss: 0.6450 - val_loss: 1.0566
Epoch 2433/5000
26/26 - 1s - loss: 0.6462 - val_loss: 1.0544
Epoch 2434/5000
26/26 - 1s - loss: 0.6445 - val_loss: 1.0549
Epoch 2435/5000
26/26 - 1s - loss: 0.6441 - val_loss: 1.0548
Epoch 2436/5000
26/26 - 1s - loss: 0.6442 - val_loss: 1.0540
Epoch 2437/5000
26/26 - 1s - loss: 0.6431 - val_loss: 1.0547
Epoch 2438/5000
26/26 - 2s - loss: 0.6432 - val_loss: 1.0530
Epoch 2439/5000
26/26 - 1s - loss: 0.6449 - val_loss: 1.0548
Epoch 2440/5000
26/26 - 1s - loss: 0.6438 - val_loss: 1.0525
Epoch 02440: val_loss improved from 1.05559 to 1.05252, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 2441/5000
26/26 - 1s - loss: 0.6422 - val_loss: 1.0540
Epoch 2442/5000
26/26 - 1s - loss: 0.6419 - val_loss: 1.0522
Epoch 2443/5000
26/26 - 1s - loss: 0.6430 - val_loss: 1.0529
Epoch 2444/5000
26/26 - 1s - loss: 0.6430 - val_loss: 1.0522
Epoch 2445/5000
26/26 - 1s - loss: 0.6431 - val_loss: 1.0516
Epoch 2446/5000
26/26 - 1s - loss: 0.6418 - val_loss: 1.0516
Epoch 2447/5000
26/26 - 1s - loss: 0.6419 - val_loss: 1.0516
Epoch 2448/5000
26/26 - 1s - loss: 0.6406 - val_loss: 1.0499
Epoch 2449/5000
26/26 - 1s - loss: 0.6421 - val_loss: 1.0512
Epoch 2450/5000
26/26 - 1s - loss: 0.6403 - val_loss: 1.0506
Epoch 02450: val_loss improved from 1.05252 to 1.05059, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 2451/5000
26/26 - 1s - loss: 0.6416 - val_loss: 1.0530
Epoch 2452/5000
26/26 - 2s - loss: 0.6396 - val_loss: 1.0498
Epoch 2453/5000
26/26 - 1s - loss: 0.6407 - val_loss: 1.0503
Epoch 2454/5000
26/26 - 1s - loss: 0.6402 - val_loss: 1.0500
Epoch 2455/5000
26/26 - 1s - loss: 0.6410 - val_loss: 1.0508
Epoch 2456/5000
26/26 - 1s - loss: 0.6403 - val_loss: 1.0506
Epoch 2457/5000
26/26 - 1s - loss: 0.6397 - val_loss: 1.0497
Epoch 2458/5000
26/26 - 1s - loss: 0.6395 - val_loss: 1.0490
Epoch 2459/5000
26/26 - 1s - loss: 0.6402 - val_loss: 1.0502
Epoch 2460/5000
26/26 - 1s - loss: 0.6388 - val_loss: 1.0489
Epoch 02460: val_loss improved from 1.05059 to 1.04886, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 2461/5000
26/26 - 1s - loss: 0.6380 - val_loss: 1.0494
Epoch 2462/5000
26/26 - 1s - loss: 0.6387 - val_loss: 1.0500
Epoch 2463/5000
26/26 - 1s - loss: 0.6386 - val_loss: 1.0476
Epoch 2464/5000
26/26 - 1s - loss: 0.6374 - val_loss: 1.0477
Epoch 2465/5000
26/26 - 1s - loss: 0.6369 - val_loss: 1.0483
Epoch 2466/5000
26/26 - 1s - loss: 0.6370 - val_loss: 1.0470
Epoch 2467/5000
26/26 - 1s - loss: 0.6361 - val_loss: 1.0478
Epoch 2468/5000
26/26 - 1s - loss: 0.6378 - val_loss: 1.0475
Epoch 2469/5000
26/26 - 1s - loss: 0.6376 - val_loss: 1.0477
Epoch 2470/5000
26/26 - 1s - loss: 0.6361 - val_loss: 1.0488
Epoch 02470: val_loss improved from 1.04886 to 1.04880, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 2471/5000
26/26 - 1s - loss: 0.6358 - val_loss: 1.0470
Epoch 2472/5000
26/26 - 1s - loss: 0.6369 - val_loss: 1.0490
Epoch 2473/5000
26/26 - 2s - loss: 0.6355 - val_loss: 1.0459
Epoch 2474/5000
26/26 - 1s - loss: 0.6370 - val_loss: 1.0450
Epoch 2475/5000
26/26 - 1s - loss: 0.6355 - val_loss: 1.0455
Epoch 2476/5000
26/26 - 1s - loss: 0.6360 - val_loss: 1.0465
Epoch 2477/5000
26/26 - 1s - loss: 0.6361 - val_loss: 1.0444
Epoch 2478/5000
26/26 - 1s - loss: 0.6349 - val_loss: 1.0436
Epoch 2479/5000
26/26 - 1s - loss: 0.6337 - val_loss: 1.0434
Epoch 2480/5000
26/26 - 1s - loss: 0.6353 - val_loss: 1.0454
Epoch 02480: val_loss improved from 1.04880 to 1.04544, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 2481/5000
26/26 - 1s - loss: 0.6351 - val_loss: 1.0441
Epoch 2482/5000
26/26 - 1s - loss: 0.6342 - val_loss: 1.0452
Epoch 2483/5000
26/26 - 1s - loss: 0.6343 - val_loss: 1.0437
Epoch 2484/5000
26/26 - 1s - loss: 0.6335 - val_loss: 1.0441
Epoch 2485/5000
26/26 - 1s - loss: 0.6334 - val_loss: 1.0416
Epoch 2486/5000
26/26 - 1s - loss: 0.6324 - val_loss: 1.0431
Epoch 2487/5000
26/26 - 1s - loss: 0.6330 - val_loss: 1.0432
Epoch 2488/5000
26/26 - 1s - loss: 0.6331 - val_loss: 1.0437
Epoch 2489/5000
26/26 - 1s - loss: 0.6333 - val_loss: 1.0438
Epoch 2490/5000
26/26 - 1s - loss: 0.6336 - val_loss: 1.0411
Epoch 02490: val_loss improved from 1.04544 to 1.04106, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 2491/5000
26/26 - 1s - loss: 0.6322 - val_loss: 1.0421
Epoch 2492/5000
26/26 - 1s - loss: 0.6320 - val_loss: 1.0413
Epoch 2493/5000
26/26 - 1s - loss: 0.6320 - val_loss: 1.0413
Epoch 2494/5000
26/26 - 1s - loss: 0.6321 - val_loss: 1.0397
Epoch 2495/5000
26/26 - 1s - loss: 0.6324 - val_loss: 1.0406
Epoch 2496/5000
26/26 - 1s - loss: 0.6309 - val_loss: 1.0406
Epoch 2497/5000
26/26 - 1s - loss: 0.6301 - val_loss: 1.0390
Epoch 2498/5000
26/26 - 2s - loss: 0.6314 - val_loss: 1.0403
Epoch 2499/5000
26/26 - 1s - loss: 0.6291 - val_loss: 1.0408
Epoch 2500/5000
26/26 - 1s - loss: 0.6304 - val_loss: 1.0392
Epoch 02500: val_loss improved from 1.04106 to 1.03919, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 2501/5000
26/26 - 1s - loss: 0.6295 - val_loss: 1.0393
Epoch 2502/5000
26/26 - 1s - loss: 0.6301 - val_loss: 1.0403
Epoch 2503/5000
26/26 - 1s - loss: 0.6293 - val_loss: 1.0395
Epoch 2504/5000
26/26 - 1s - loss: 0.6301 - val_loss: 1.0397
Epoch 2505/5000
26/26 - 1s - loss: 0.6290 - val_loss: 1.0404
Epoch 2506/5000
26/26 - 1s - loss: 0.6291 - val_loss: 1.0391
Epoch 2507/5000
26/26 - 1s - loss: 0.6286 - val_loss: 1.0400
Epoch 2508/5000
26/26 - 1s - loss: 0.6280 - val_loss: 1.0377
Epoch 2509/5000
26/26 - 1s - loss: 0.6267 - val_loss: 1.0390
Epoch 2510/5000
26/26 - 1s - loss: 0.6301 - val_loss: 1.0397
Epoch 02510: val_loss did not improve from 1.03919
Epoch 2511/5000
26/26 - 1s - loss: 0.6299 - val_loss: 1.0386
Epoch 2512/5000
26/26 - 1s - loss: 0.6290 - val_loss: 1.0380
Epoch 2513/5000
26/26 - 1s - loss: 0.6276 - val_loss: 1.0387
Epoch 2514/5000
26/26 - 1s - loss: 0.6280 - val_loss: 1.0359
Epoch 2515/5000
26/26 - 1s - loss: 0.6269 - val_loss: 1.0363
Epoch 2516/5000
26/26 - 1s - loss: 0.6254 - val_loss: 1.0386
Epoch 2517/5000
26/26 - 1s - loss: 0.6271 - val_loss: 1.0381
Epoch 2518/5000
26/26 - 1s - loss: 0.6253 - val_loss: 1.0353
Epoch 2519/5000
26/26 - 1s - loss: 0.6251 - val_loss: 1.0379
Epoch 2520/5000
26/26 - 1s - loss: 0.6260 - val_loss: 1.0362
Epoch 02520: val_loss improved from 1.03919 to 1.03616, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 2521/5000
26/26 - 1s - loss: 0.6261 - val_loss: 1.0352
Epoch 2522/5000
26/26 - 1s - loss: 0.6251 - val_loss: 1.0365
Epoch 2523/5000
26/26 - 1s - loss: 0.6243 - val_loss: 1.0360
Epoch 2524/5000
26/26 - 1s - loss: 0.6247 - val_loss: 1.0351
Epoch 2525/5000
26/26 - 1s - loss: 0.6251 - val_loss: 1.0352
Epoch 2526/5000
26/26 - 1s - loss: 0.6243 - val_loss: 1.0340
Epoch 2527/5000
26/26 - 2s - loss: 0.6252 - val_loss: 1.0347
Epoch 2528/5000
26/26 - 2s - loss: 0.6230 - val_loss: 1.0352
Epoch 2529/5000
26/26 - 1s - loss: 0.6239 - val_loss: 1.0350
Epoch 2530/5000
26/26 - 1s - loss: 0.6223 - val_loss: 1.0348
Epoch 02530: val_loss improved from 1.03616 to 1.03476, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 2531/5000
26/26 - 1s - loss: 0.6242 - val_loss: 1.0324
Epoch 2532/5000
26/26 - 1s - loss: 0.6232 - val_loss: 1.0318
Epoch 2533/5000
26/26 - 1s - loss: 0.6228 - val_loss: 1.0340
Epoch 2534/5000
26/26 - 1s - loss: 0.6225 - val_loss: 1.0331
Epoch 2535/5000
26/26 - 1s - loss: 0.6233 - val_loss: 1.0334
Epoch 2536/5000
26/26 - 1s - loss: 0.6234 - val_loss: 1.0321
Epoch 2537/5000
26/26 - 1s - loss: 0.6208 - val_loss: 1.0321
Epoch 2538/5000
26/26 - 1s - loss: 0.6222 - val_loss: 1.0328
Epoch 2539/5000
26/26 - 1s - loss: 0.6201 - val_loss: 1.0316
Epoch 2540/5000
26/26 - 1s - loss: 0.6214 - val_loss: 1.0313
Epoch 02540: val_loss improved from 1.03476 to 1.03128, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 2541/5000
26/26 - 1s - loss: 0.6201 - val_loss: 1.0309
Epoch 2542/5000
26/26 - 1s - loss: 0.6205 - val_loss: 1.0318
Epoch 2543/5000
26/26 - 1s - loss: 0.6212 - val_loss: 1.0311
Epoch 2544/5000
26/26 - 1s - loss: 0.6223 - val_loss: 1.0333
Epoch 2545/5000
26/26 - 1s - loss: 0.6210 - val_loss: 1.0322
Epoch 2546/5000
26/26 - 1s - loss: 0.6200 - val_loss: 1.0321
Epoch 2547/5000
26/26 - 1s - loss: 0.6206 - val_loss: 1.0310
Epoch 2548/5000
26/26 - 2s - loss: 0.6194 - val_loss: 1.0309
Epoch 2549/5000
26/26 - 2s - loss: 0.6188 - val_loss: 1.0296
Epoch 2550/5000
26/26 - 1s - loss: 0.6196 - val_loss: 1.0314
Epoch 02550: val_loss did not improve from 1.03128
Epoch 2551/5000
26/26 - 2s - loss: 0.6195 - val_loss: 1.0303
Epoch 2552/5000
26/26 - 1s - loss: 0.6191 - val_loss: 1.0292
Epoch 2553/5000
26/26 - 1s - loss: 0.6194 - val_loss: 1.0302
Epoch 2554/5000
26/26 - 1s - loss: 0.6181 - val_loss: 1.0288
Epoch 2555/5000
26/26 - 1s - loss: 0.6185 - val_loss: 1.0288
Epoch 2556/5000
26/26 - 1s - loss: 0.6186 - val_loss: 1.0296
Epoch 2557/5000
26/26 - 1s - loss: 0.6180 - val_loss: 1.0281
Epoch 2558/5000
26/26 - 1s - loss: 0.6176 - val_loss: 1.0278
Epoch 2559/5000
26/26 - 1s - loss: 0.6189 - val_loss: 1.0315
Epoch 2560/5000
26/26 - 1s - loss: 0.6167 - val_loss: 1.0287
Epoch 02560: val_loss improved from 1.03128 to 1.02869, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 2561/5000
26/26 - 1s - loss: 0.6180 - val_loss: 1.0284
Epoch 2562/5000
26/26 - 1s - loss: 0.6174 - val_loss: 1.0280
Epoch 2563/5000
26/26 - 1s - loss: 0.6175 - val_loss: 1.0266
Epoch 2564/5000
26/26 - 1s - loss: 0.6167 - val_loss: 1.0269
Epoch 2565/5000
26/26 - 2s - loss: 0.6170 - val_loss: 1.0281
Epoch 2566/5000
26/26 - 1s - loss: 0.6162 - val_loss: 1.0266
Epoch 2567/5000
26/26 - 1s - loss: 0.6161 - val_loss: 1.0250
Epoch 2568/5000
26/26 - 1s - loss: 0.6151 - val_loss: 1.0253
Epoch 2569/5000
26/26 - 1s - loss: 0.6159 - val_loss: 1.0272
Epoch 2570/5000
26/26 - 1s - loss: 0.6146 - val_loss: 1.0255
Epoch 02570: val_loss improved from 1.02869 to 1.02551, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 2571/5000
26/26 - 1s - loss: 0.6158 - val_loss: 1.0263
Epoch 2572/5000
26/26 - 1s - loss: 0.6145 - val_loss: 1.0264
Epoch 2573/5000
26/26 - 1s - loss: 0.6154 - val_loss: 1.0246
Epoch 2574/5000
26/26 - 1s - loss: 0.6150 - val_loss: 1.0250
Epoch 2575/5000
26/26 - 2s - loss: 0.6154 - val_loss: 1.0251
Epoch 2576/5000
26/26 - 1s - loss: 0.6140 - val_loss: 1.0246
Epoch 2577/5000
26/26 - 1s - loss: 0.6141 - val_loss: 1.0245
Epoch 2578/5000
26/26 - 1s - loss: 0.6136 - val_loss: 1.0260
Epoch 2579/5000
26/26 - 1s - loss: 0.6133 - val_loss: 1.0217
Epoch 2580/5000
26/26 - 1s - loss: 0.6129 - val_loss: 1.0245
Epoch 02580: val_loss improved from 1.02551 to 1.02450, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 2581/5000
26/26 - 2s - loss: 0.6117 - val_loss: 1.0229
Epoch 2582/5000
26/26 - 1s - loss: 0.6128 - val_loss: 1.0251
Epoch 2583/5000
26/26 - 1s - loss: 0.6123 - val_loss: 1.0230
Epoch 2584/5000
26/26 - 1s - loss: 0.6130 - val_loss: 1.0222
Epoch 2585/5000
26/26 - 1s - loss: 0.6122 - val_loss: 1.0231
Epoch 2586/5000
26/26 - 1s - loss: 0.6114 - val_loss: 1.0235
Epoch 2587/5000
26/26 - 1s - loss: 0.6135 - val_loss: 1.0228
Epoch 2588/5000
26/26 - 1s - loss: 0.6123 - val_loss: 1.0219
Epoch 2589/5000
26/26 - 1s - loss: 0.6118 - val_loss: 1.0209
Epoch 2590/5000
26/26 - 1s - loss: 0.6120 - val_loss: 1.0219
Epoch 02590: val_loss improved from 1.02450 to 1.02194, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 2591/5000
26/26 - 1s - loss: 0.6110 - val_loss: 1.0218
Epoch 2592/5000
26/26 - 1s - loss: 0.6121 - val_loss: 1.0220
Epoch 2593/5000
26/26 - 1s - loss: 0.6099 - val_loss: 1.0203
Epoch 2594/5000
26/26 - 1s - loss: 0.6112 - val_loss: 1.0206
Epoch 2595/5000
26/26 - 1s - loss: 0.6105 - val_loss: 1.0218
Epoch 2596/5000
26/26 - 1s - loss: 0.6102 - val_loss: 1.0207
Epoch 2597/5000
26/26 - 1s - loss: 0.6102 - val_loss: 1.0210
Epoch 2598/5000
26/26 - 1s - loss: 0.6098 - val_loss: 1.0204
Epoch 2599/5000
26/26 - 1s - loss: 0.6094 - val_loss: 1.0213
Epoch 2600/5000
26/26 - 1s - loss: 0.6097 - val_loss: 1.0196
Epoch 02600: val_loss improved from 1.02194 to 1.01956, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 2601/5000
26/26 - 1s - loss: 0.6087 - val_loss: 1.0192
Epoch 2602/5000
26/26 - 1s - loss: 0.6099 - val_loss: 1.0210
Epoch 2603/5000
26/26 - 1s - loss: 0.6098 - val_loss: 1.0213
Epoch 2604/5000
26/26 - 1s - loss: 0.6089 - val_loss: 1.0191
Epoch 2605/5000
26/26 - 1s - loss: 0.6089 - val_loss: 1.0207
Epoch 2606/5000
26/26 - 1s - loss: 0.6094 - val_loss: 1.0193
Epoch 2607/5000
26/26 - 1s - loss: 0.6082 - val_loss: 1.0187
Epoch 2608/5000
26/26 - 1s - loss: 0.6079 - val_loss: 1.0181
Epoch 2609/5000
26/26 - 1s - loss: 0.6072 - val_loss: 1.0188
Epoch 2610/5000
26/26 - 1s - loss: 0.6083 - val_loss: 1.0161
Epoch 02610: val_loss improved from 1.01956 to 1.01605, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 2611/5000
26/26 - 1s - loss: 0.6078 - val_loss: 1.0166
Epoch 2612/5000
26/26 - 1s - loss: 0.6082 - val_loss: 1.0179
Epoch 2613/5000
26/26 - 1s - loss: 0.6067 - val_loss: 1.0171
Epoch 2614/5000
26/26 - 1s - loss: 0.6067 - val_loss: 1.0161
Epoch 2615/5000
26/26 - 1s - loss: 0.6070 - val_loss: 1.0169
Epoch 2616/5000
26/26 - 1s - loss: 0.6069 - val_loss: 1.0165
Epoch 2617/5000
26/26 - 1s - loss: 0.6060 - val_loss: 1.0168
Epoch 2618/5000
26/26 - 1s - loss: 0.6070 - val_loss: 1.0165
Epoch 2619/5000
26/26 - 1s - loss: 0.6070 - val_loss: 1.0166
Epoch 2620/5000
26/26 - 1s - loss: 0.6051 - val_loss: 1.0160
Epoch 02620: val_loss improved from 1.01605 to 1.01600, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 2621/5000
26/26 - 1s - loss: 0.6051 - val_loss: 1.0153
Epoch 2622/5000
26/26 - 1s - loss: 0.6049 - val_loss: 1.0159
Epoch 2623/5000
26/26 - 2s - loss: 0.6053 - val_loss: 1.0150
Epoch 2624/5000
26/26 - 1s - loss: 0.6062 - val_loss: 1.0153
Epoch 2625/5000
26/26 - 1s - loss: 0.6049 - val_loss: 1.0167
Epoch 2626/5000
26/26 - 1s - loss: 0.6054 - val_loss: 1.0146
Epoch 2627/5000
26/26 - 1s - loss: 0.6053 - val_loss: 1.0159
Epoch 2628/5000
26/26 - 1s - loss: 0.6038 - val_loss: 1.0144
Epoch 2629/5000
26/26 - 1s - loss: 0.6039 - val_loss: 1.0132
Epoch 2630/5000
26/26 - 1s - loss: 0.6043 - val_loss: 1.0152
Epoch 02630: val_loss improved from 1.01600 to 1.01519, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 2631/5000
26/26 - 1s - loss: 0.6028 - val_loss: 1.0148
Epoch 2632/5000
26/26 - 1s - loss: 0.6047 - val_loss: 1.0136
Epoch 2633/5000
26/26 - 1s - loss: 0.6028 - val_loss: 1.0132
Epoch 2634/5000
26/26 - 2s - loss: 0.6038 - val_loss: 1.0138
Epoch 2635/5000
26/26 - 1s - loss: 0.6030 - val_loss: 1.0139
Epoch 2636/5000
26/26 - 1s - loss: 0.6028 - val_loss: 1.0131
Epoch 2637/5000
26/26 - 1s - loss: 0.6022 - val_loss: 1.0125
Epoch 2638/5000
26/26 - 1s - loss: 0.6035 - val_loss: 1.0143
Epoch 2639/5000
26/26 - 1s - loss: 0.6028 - val_loss: 1.0128
Epoch 2640/5000
26/26 - 1s - loss: 0.6012 - val_loss: 1.0143
Epoch 02640: val_loss improved from 1.01519 to 1.01429, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 2641/5000
26/26 - 1s - loss: 0.6016 - val_loss: 1.0122
Epoch 2642/5000
26/26 - 1s - loss: 0.6007 - val_loss: 1.0130
Epoch 2643/5000
26/26 - 1s - loss: 0.6021 - val_loss: 1.0141
Epoch 2644/5000
26/26 - 1s - loss: 0.6006 - val_loss: 1.0121
Epoch 2645/5000
26/26 - 2s - loss: 0.6001 - val_loss: 1.0119
Epoch 2646/5000
26/26 - 1s - loss: 0.5995 - val_loss: 1.0108
Epoch 2647/5000
26/26 - 1s - loss: 0.6002 - val_loss: 1.0121
Epoch 2648/5000
26/26 - 1s - loss: 0.6005 - val_loss: 1.0099
Epoch 2649/5000
26/26 - 1s - loss: 0.6011 - val_loss: 1.0095
Epoch 2650/5000
26/26 - 1s - loss: 0.6004 - val_loss: 1.0104
Epoch 02650: val_loss improved from 1.01429 to 1.01037, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 2651/5000
26/26 - 1s - loss: 0.5998 - val_loss: 1.0119
Epoch 2652/5000
26/26 - 1s - loss: 0.5996 - val_loss: 1.0110
Epoch 2653/5000
26/26 - 1s - loss: 0.5995 - val_loss: 1.0099
Epoch 2654/5000
26/26 - 1s - loss: 0.5991 - val_loss: 1.0099
Epoch 2655/5000
26/26 - 1s - loss: 0.5999 - val_loss: 1.0114
Epoch 2656/5000
26/26 - 2s - loss: 0.5974 - val_loss: 1.0083
Epoch 2657/5000
26/26 - 1s - loss: 0.5970 - val_loss: 1.0096
Epoch 2658/5000
26/26 - 1s - loss: 0.5984 - val_loss: 1.0097
Epoch 2659/5000
26/26 - 1s - loss: 0.5975 - val_loss: 1.0066
Epoch 2660/5000
26/26 - 1s - loss: 0.5982 - val_loss: 1.0080
Epoch 02660: val_loss improved from 1.01037 to 1.00796, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 2661/5000
26/26 - 1s - loss: 0.5998 - val_loss: 1.0085
Epoch 2662/5000
26/26 - 1s - loss: 0.5977 - val_loss: 1.0086
Epoch 2663/5000
26/26 - 1s - loss: 0.5991 - val_loss: 1.0068
Epoch 2664/5000
26/26 - 2s - loss: 0.5977 - val_loss: 1.0084
Epoch 2665/5000
26/26 - 1s - loss: 0.5977 - val_loss: 1.0083
Epoch 2666/5000
26/26 - 1s - loss: 0.5972 - val_loss: 1.0057
Epoch 2667/5000
26/26 - 1s - loss: 0.5961 - val_loss: 1.0069
Epoch 2668/5000
26/26 - 1s - loss: 0.5962 - val_loss: 1.0074
Epoch 2669/5000
26/26 - 1s - loss: 0.5965 - val_loss: 1.0083
Epoch 2670/5000
26/26 - 1s - loss: 0.5969 - val_loss: 1.0069
Epoch 02670: val_loss improved from 1.00796 to 1.00693, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 2671/5000
26/26 - 1s - loss: 0.5970 - val_loss: 1.0042
Epoch 2672/5000
26/26 - 1s - loss: 0.5960 - val_loss: 1.0059
Epoch 2673/5000
26/26 - 1s - loss: 0.5937 - val_loss: 1.0095
Epoch 2674/5000
26/26 - 1s - loss: 0.5952 - val_loss: 1.0069
Epoch 2675/5000
26/26 - 1s - loss: 0.5952 - val_loss: 1.0063
Epoch 2676/5000
26/26 - 1s - loss: 0.5946 - val_loss: 1.0060
Epoch 2677/5000
26/26 - 1s - loss: 0.5950 - val_loss: 1.0054
Epoch 2678/5000
26/26 - 1s - loss: 0.5938 - val_loss: 1.0056
Epoch 2679/5000
26/26 - 1s - loss: 0.5954 - val_loss: 1.0074
Epoch 2680/5000
26/26 - 1s - loss: 0.5949 - val_loss: 1.0061
Epoch 02680: val_loss improved from 1.00693 to 1.00615, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 2681/5000
26/26 - 1s - loss: 0.5935 - val_loss: 1.0046
Epoch 2682/5000
26/26 - 2s - loss: 0.5942 - val_loss: 1.0062
Epoch 2683/5000
26/26 - 1s - loss: 0.5944 - val_loss: 1.0046
Epoch 2684/5000
26/26 - 1s - loss: 0.5938 - val_loss: 1.0066
Epoch 2685/5000
26/26 - 1s - loss: 0.5926 - val_loss: 1.0053
Epoch 2686/5000
26/26 - 1s - loss: 0.5935 - val_loss: 1.0052
Epoch 2687/5000
26/26 - 1s - loss: 0.5919 - val_loss: 1.0037
Epoch 2688/5000
26/26 - 1s - loss: 0.5930 - val_loss: 1.0041
Epoch 2689/5000
26/26 - 1s - loss: 0.5929 - val_loss: 1.0030
Epoch 2690/5000
26/26 - 1s - loss: 0.5923 - val_loss: 1.0044
Epoch 02690: val_loss improved from 1.00615 to 1.00435, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 2691/5000
26/26 - 1s - loss: 0.5927 - val_loss: 1.0031
Epoch 2692/5000
26/26 - 1s - loss: 0.5919 - val_loss: 1.0020
Epoch 2693/5000
26/26 - 1s - loss: 0.5928 - val_loss: 1.0031
Epoch 2694/5000
26/26 - 1s - loss: 0.5910 - val_loss: 1.0032
Epoch 2695/5000
26/26 - 1s - loss: 0.5918 - val_loss: 1.0027
Epoch 2696/5000
26/26 - 1s - loss: 0.5917 - val_loss: 1.0033
Epoch 2697/5000
26/26 - 1s - loss: 0.5917 - val_loss: 1.0020
Epoch 2698/5000
26/26 - 1s - loss: 0.5903 - val_loss: 1.0034
Epoch 2699/5000
26/26 - 2s - loss: 0.5926 - val_loss: 1.0028
Epoch 2700/5000
26/26 - 1s - loss: 0.5918 - val_loss: 1.0022
Epoch 02700: val_loss improved from 1.00435 to 1.00219, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 2701/5000
26/26 - 1s - loss: 0.5910 - val_loss: 1.0025
Epoch 2702/5000
26/26 - 1s - loss: 0.5907 - val_loss: 1.0015
Epoch 2703/5000
26/26 - 1s - loss: 0.5892 - val_loss: 1.0005
Epoch 2704/5000
26/26 - 1s - loss: 0.5917 - val_loss: 1.0020
Epoch 2705/5000
26/26 - 2s - loss: 0.5896 - val_loss: 1.0011
Epoch 2706/5000
26/26 - 1s - loss: 0.5879 - val_loss: 1.0001
Epoch 2707/5000
26/26 - 1s - loss: 0.5893 - val_loss: 0.9998
Epoch 2708/5000
26/26 - 1s - loss: 0.5886 - val_loss: 1.0011
Epoch 2709/5000
26/26 - 1s - loss: 0.5897 - val_loss: 1.0011
Epoch 2710/5000
26/26 - 1s - loss: 0.5893 - val_loss: 1.0003
Epoch 02710: val_loss improved from 1.00219 to 1.00027, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 2711/5000
26/26 - 1s - loss: 0.5901 - val_loss: 0.9999
Epoch 2712/5000
26/26 - 1s - loss: 0.5884 - val_loss: 0.9982
Epoch 2713/5000
26/26 - 1s - loss: 0.5895 - val_loss: 0.9988
Epoch 2714/5000
26/26 - 1s - loss: 0.5880 - val_loss: 0.9987
Epoch 2715/5000
26/26 - 1s - loss: 0.5875 - val_loss: 0.9987
Epoch 2716/5000
26/26 - 1s - loss: 0.5878 - val_loss: 0.9993
Epoch 2717/5000
26/26 - 1s - loss: 0.5878 - val_loss: 0.9985
Epoch 2718/5000
26/26 - 1s - loss: 0.5874 - val_loss: 0.9988
Epoch 2719/5000
26/26 - 1s - loss: 0.5868 - val_loss: 0.9975
Epoch 2720/5000
26/26 - 1s - loss: 0.5871 - val_loss: 0.9987
Epoch 02720: val_loss improved from 1.00027 to 0.99873, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 2721/5000
26/26 - 1s - loss: 0.5871 - val_loss: 0.9974
Epoch 2722/5000
26/26 - 1s - loss: 0.5874 - val_loss: 0.9976
Epoch 2723/5000
26/26 - 1s - loss: 0.5869 - val_loss: 0.9986
Epoch 2724/5000
26/26 - 1s - loss: 0.5869 - val_loss: 0.9959
Epoch 2725/5000
26/26 - 1s - loss: 0.5854 - val_loss: 0.9976
Epoch 2726/5000
26/26 - 1s - loss: 0.5851 - val_loss: 0.9976
Epoch 2727/5000
26/26 - 1s - loss: 0.5849 - val_loss: 0.9972
Epoch 2728/5000
26/26 - 1s - loss: 0.5850 - val_loss: 0.9977
Epoch 2729/5000
26/26 - 1s - loss: 0.5856 - val_loss: 0.9969
Epoch 2730/5000
26/26 - 1s - loss: 0.5843 - val_loss: 0.9976
Epoch 02730: val_loss improved from 0.99873 to 0.99760, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 2731/5000
26/26 - 1s - loss: 0.5849 - val_loss: 0.9960
Epoch 2732/5000
26/26 - 1s - loss: 0.5858 - val_loss: 0.9952
Epoch 2733/5000
26/26 - 1s - loss: 0.5859 - val_loss: 0.9966
Epoch 2734/5000
26/26 - 1s - loss: 0.5838 - val_loss: 0.9948
Epoch 2735/5000
26/26 - 1s - loss: 0.5844 - val_loss: 0.9938
Epoch 2736/5000
26/26 - 1s - loss: 0.5839 - val_loss: 0.9953
Epoch 2737/5000
26/26 - 1s - loss: 0.5838 - val_loss: 0.9944
Epoch 2738/5000
26/26 - 1s - loss: 0.5834 - val_loss: 0.9934
Epoch 2739/5000
26/26 - 1s - loss: 0.5836 - val_loss: 0.9953
Epoch 2740/5000
26/26 - 2s - loss: 0.5826 - val_loss: 0.9937
Epoch 02740: val_loss improved from 0.99760 to 0.99371, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 2741/5000
26/26 - 1s - loss: 0.5826 - val_loss: 0.9944
Epoch 2742/5000
26/26 - 1s - loss: 0.5831 - val_loss: 0.9946
Epoch 2743/5000
26/26 - 1s - loss: 0.5837 - val_loss: 0.9922
Epoch 2744/5000
26/26 - 1s - loss: 0.5824 - val_loss: 0.9932
Epoch 2745/5000
26/26 - 1s - loss: 0.5826 - val_loss: 0.9930
Epoch 2746/5000
26/26 - 1s - loss: 0.5828 - val_loss: 0.9931
Epoch 2747/5000
26/26 - 1s - loss: 0.5828 - val_loss: 0.9924
Epoch 2748/5000
26/26 - 1s - loss: 0.5831 - val_loss: 0.9933
Epoch 2749/5000
26/26 - 1s - loss: 0.5830 - val_loss: 0.9933
Epoch 2750/5000
26/26 - 1s - loss: 0.5826 - val_loss: 0.9930
Epoch 02750: val_loss improved from 0.99371 to 0.99301, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 2751/5000
26/26 - 1s - loss: 0.5819 - val_loss: 0.9917
Epoch 2752/5000
26/26 - 1s - loss: 0.5825 - val_loss: 0.9918
Epoch 2753/5000
26/26 - 1s - loss: 0.5826 - val_loss: 0.9931
Epoch 2754/5000
26/26 - 1s - loss: 0.5825 - val_loss: 0.9919
Epoch 2755/5000
26/26 - 1s - loss: 0.5808 - val_loss: 0.9927
Epoch 2756/5000
26/26 - 1s - loss: 0.5807 - val_loss: 0.9914
Epoch 2757/5000
26/26 - 1s - loss: 0.5808 - val_loss: 0.9902
Epoch 2758/5000
26/26 - 1s - loss: 0.5812 - val_loss: 0.9918
Epoch 2759/5000
26/26 - 1s - loss: 0.5805 - val_loss: 0.9920
Epoch 2760/5000
26/26 - 1s - loss: 0.5792 - val_loss: 0.9918
Epoch 02760: val_loss improved from 0.99301 to 0.99177, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 2761/5000
26/26 - 1s - loss: 0.5803 - val_loss: 0.9913
Epoch 2762/5000
26/26 - 1s - loss: 0.5799 - val_loss: 0.9895
Epoch 2763/5000
26/26 - 1s - loss: 0.5793 - val_loss: 0.9916
Epoch 2764/5000
26/26 - 1s - loss: 0.5791 - val_loss: 0.9889
Epoch 2765/5000
26/26 - 1s - loss: 0.5793 - val_loss: 0.9915
Epoch 2766/5000
26/26 - 1s - loss: 0.5791 - val_loss: 0.9896
Epoch 2767/5000
26/26 - 1s - loss: 0.5802 - val_loss: 0.9921
Epoch 2768/5000
26/26 - 1s - loss: 0.5800 - val_loss: 0.9893
Epoch 2769/5000
26/26 - 1s - loss: 0.5789 - val_loss: 0.9912
Epoch 2770/5000
26/26 - 1s - loss: 0.5781 - val_loss: 0.9908
Epoch 02770: val_loss improved from 0.99177 to 0.99082, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 2771/5000
26/26 - 1s - loss: 0.5771 - val_loss: 0.9906
Epoch 2772/5000
26/26 - 1s - loss: 0.5775 - val_loss: 0.9903
Epoch 2773/5000
26/26 - 1s - loss: 0.5781 - val_loss: 0.9899
Epoch 2774/5000
26/26 - 1s - loss: 0.5771 - val_loss: 0.9889
Epoch 2775/5000
26/26 - 1s - loss: 0.5790 - val_loss: 0.9900
Epoch 2776/5000
26/26 - 1s - loss: 0.5791 - val_loss: 0.9893
Epoch 2777/5000
26/26 - 1s - loss: 0.5771 - val_loss: 0.9913
Epoch 2778/5000
26/26 - 1s - loss: 0.5768 - val_loss: 0.9871
Epoch 2779/5000
26/26 - 1s - loss: 0.5765 - val_loss: 0.9898
Epoch 2780/5000
26/26 - 1s - loss: 0.5771 - val_loss: 0.9899
Epoch 02780: val_loss improved from 0.99082 to 0.98987, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 2781/5000
26/26 - 1s - loss: 0.5790 - val_loss: 0.9883
Epoch 2782/5000
26/26 - 1s - loss: 0.5774 - val_loss: 0.9876
Epoch 2783/5000
26/26 - 1s - loss: 0.5766 - val_loss: 0.9879
Epoch 2784/5000
26/26 - 1s - loss: 0.5760 - val_loss: 0.9874
Epoch 2785/5000
26/26 - 1s - loss: 0.5758 - val_loss: 0.9872
Epoch 2786/5000
26/26 - 1s - loss: 0.5767 - val_loss: 0.9860
Epoch 2787/5000
26/26 - 2s - loss: 0.5757 - val_loss: 0.9868
Epoch 2788/5000
26/26 - 2s - loss: 0.5748 - val_loss: 0.9854
Epoch 2789/5000
26/26 - 1s - loss: 0.5749 - val_loss: 0.9876
Epoch 2790/5000
26/26 - 1s - loss: 0.5750 - val_loss: 0.9872
Epoch 02790: val_loss improved from 0.98987 to 0.98721, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 2791/5000
26/26 - 1s - loss: 0.5750 - val_loss: 0.9866
Epoch 2792/5000
26/26 - 1s - loss: 0.5746 - val_loss: 0.9871
Epoch 2793/5000
26/26 - 1s - loss: 0.5739 - val_loss: 0.9881
Epoch 2794/5000
26/26 - 2s - loss: 0.5745 - val_loss: 0.9877
Epoch 2795/5000
26/26 - 1s - loss: 0.5735 - val_loss: 0.9854
Epoch 2796/5000
26/26 - 2s - loss: 0.5750 - val_loss: 0.9855
Epoch 2797/5000
26/26 - 1s - loss: 0.5733 - val_loss: 0.9845
Epoch 2798/5000
26/26 - 1s - loss: 0.5733 - val_loss: 0.9853
Epoch 2799/5000
26/26 - 1s - loss: 0.5736 - val_loss: 0.9838
Epoch 2800/5000
26/26 - 1s - loss: 0.5736 - val_loss: 0.9861
Epoch 02800: val_loss improved from 0.98721 to 0.98613, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 2801/5000
26/26 - 1s - loss: 0.5735 - val_loss: 0.9850
Epoch 2802/5000
26/26 - 1s - loss: 0.5721 - val_loss: 0.9850
Epoch 2803/5000
26/26 - 1s - loss: 0.5717 - val_loss: 0.9839
Epoch 2804/5000
26/26 - 1s - loss: 0.5722 - val_loss: 0.9849
Epoch 2805/5000
26/26 - 1s - loss: 0.5732 - val_loss: 0.9828
Epoch 2806/5000
26/26 - 1s - loss: 0.5724 - val_loss: 0.9843
Epoch 2807/5000
26/26 - 1s - loss: 0.5722 - val_loss: 0.9829
Epoch 2808/5000
26/26 - 2s - loss: 0.5729 - val_loss: 0.9842
Epoch 2809/5000
26/26 - 1s - loss: 0.5720 - val_loss: 0.9836
Epoch 2810/5000
26/26 - 1s - loss: 0.5718 - val_loss: 0.9827
Epoch 02810: val_loss improved from 0.98613 to 0.98269, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 2811/5000
26/26 - 1s - loss: 0.5709 - val_loss: 0.9823
Epoch 2812/5000
26/26 - 1s - loss: 0.5710 - val_loss: 0.9824
Epoch 2813/5000
26/26 - 1s - loss: 0.5703 - val_loss: 0.9812
Epoch 2814/5000
26/26 - 1s - loss: 0.5710 - val_loss: 0.9812
Epoch 2815/5000
26/26 - 1s - loss: 0.5713 - val_loss: 0.9819
Epoch 2816/5000
26/26 - 1s - loss: 0.5710 - val_loss: 0.9798
Epoch 2817/5000
26/26 - 1s - loss: 0.5696 - val_loss: 0.9823
Epoch 2818/5000
26/26 - 1s - loss: 0.5703 - val_loss: 0.9817
Epoch 2819/5000
26/26 - 1s - loss: 0.5716 - val_loss: 0.9825
Epoch 2820/5000
26/26 - 1s - loss: 0.5715 - val_loss: 0.9818
Epoch 02820: val_loss improved from 0.98269 to 0.98179, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 2821/5000
26/26 - 1s - loss: 0.5697 - val_loss: 0.9813
Epoch 2822/5000
26/26 - 1s - loss: 0.5692 - val_loss: 0.9817
Epoch 2823/5000
26/26 - 1s - loss: 0.5703 - val_loss: 0.9808
Epoch 2824/5000
26/26 - 1s - loss: 0.5687 - val_loss: 0.9819
Epoch 2825/5000
26/26 - 1s - loss: 0.5686 - val_loss: 0.9800
Epoch 2826/5000
26/26 - 1s - loss: 0.5700 - val_loss: 0.9812
Epoch 2827/5000
26/26 - 1s - loss: 0.5691 - val_loss: 0.9812
Epoch 2828/5000
26/26 - 1s - loss: 0.5699 - val_loss: 0.9819
Epoch 2829/5000
26/26 - 2s - loss: 0.5692 - val_loss: 0.9809
Epoch 2830/5000
26/26 - 1s - loss: 0.5688 - val_loss: 0.9815
Epoch 02830: val_loss improved from 0.98179 to 0.98150, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 2831/5000
26/26 - 1s - loss: 0.5682 - val_loss: 0.9798
Epoch 2832/5000
26/26 - 1s - loss: 0.5692 - val_loss: 0.9795
Epoch 2833/5000
26/26 - 1s - loss: 0.5677 - val_loss: 0.9788
Epoch 2834/5000
26/26 - 1s - loss: 0.5681 - val_loss: 0.9807
Epoch 2835/5000
26/26 - 1s - loss: 0.5673 - val_loss: 0.9792
Epoch 2836/5000
26/26 - 2s - loss: 0.5672 - val_loss: 0.9795
Epoch 2837/5000
26/26 - 1s - loss: 0.5676 - val_loss: 0.9794
Epoch 2838/5000
26/26 - 1s - loss: 0.5671 - val_loss: 0.9820
Epoch 2839/5000
26/26 - 1s - loss: 0.5672 - val_loss: 0.9792
Epoch 2840/5000
26/26 - 1s - loss: 0.5662 - val_loss: 0.9796
Epoch 02840: val_loss improved from 0.98150 to 0.97963, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 2841/5000
26/26 - 1s - loss: 0.5674 - val_loss: 0.9788
Epoch 2842/5000
26/26 - 1s - loss: 0.5661 - val_loss: 0.9790
Epoch 2843/5000
26/26 - 1s - loss: 0.5659 - val_loss: 0.9784
Epoch 2844/5000
26/26 - 1s - loss: 0.5666 - val_loss: 0.9785
Epoch 2845/5000
26/26 - 1s - loss: 0.5653 - val_loss: 0.9769
Epoch 2846/5000
26/26 - 1s - loss: 0.5664 - val_loss: 0.9777
Epoch 2847/5000
26/26 - 1s - loss: 0.5662 - val_loss: 0.9788
Epoch 2848/5000
26/26 - 1s - loss: 0.5651 - val_loss: 0.9767
Epoch 2849/5000
26/26 - 1s - loss: 0.5661 - val_loss: 0.9763
Epoch 2850/5000
26/26 - 1s - loss: 0.5653 - val_loss: 0.9779
Epoch 02850: val_loss improved from 0.97963 to 0.97790, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 2851/5000
26/26 - 1s - loss: 0.5648 - val_loss: 0.9765
Epoch 2852/5000
26/26 - 1s - loss: 0.5654 - val_loss: 0.9749
Epoch 2853/5000
26/26 - 1s - loss: 0.5658 - val_loss: 0.9760
Epoch 2854/5000
26/26 - 1s - loss: 0.5647 - val_loss: 0.9769
Epoch 2855/5000
26/26 - 1s - loss: 0.5655 - val_loss: 0.9763
Epoch 2856/5000
26/26 - 1s - loss: 0.5647 - val_loss: 0.9770
Epoch 2857/5000
26/26 - 1s - loss: 0.5639 - val_loss: 0.9744
Epoch 2858/5000
26/26 - 1s - loss: 0.5641 - val_loss: 0.9764
Epoch 2859/5000
26/26 - 1s - loss: 0.5648 - val_loss: 0.9757
Epoch 2860/5000
26/26 - 2s - loss: 0.5641 - val_loss: 0.9772
Epoch 02860: val_loss improved from 0.97790 to 0.97721, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 2861/5000
26/26 - 1s - loss: 0.5627 - val_loss: 0.9767
Epoch 2862/5000
26/26 - 1s - loss: 0.5642 - val_loss: 0.9752
Epoch 2863/5000
26/26 - 1s - loss: 0.5633 - val_loss: 0.9743
Epoch 2864/5000
26/26 - 1s - loss: 0.5656 - val_loss: 0.9749
Epoch 2865/5000
26/26 - 1s - loss: 0.5634 - val_loss: 0.9755
Epoch 2866/5000
26/26 - 1s - loss: 0.5618 - val_loss: 0.9756
Epoch 2867/5000
26/26 - 1s - loss: 0.5621 - val_loss: 0.9746
Epoch 2868/5000
26/26 - 1s - loss: 0.5618 - val_loss: 0.9748
Epoch 2869/5000
26/26 - 1s - loss: 0.5633 - val_loss: 0.9745
Epoch 2870/5000
26/26 - 2s - loss: 0.5617 - val_loss: 0.9744
Epoch 02870: val_loss improved from 0.97721 to 0.97443, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 2871/5000
26/26 - 1s - loss: 0.5613 - val_loss: 0.9739
Epoch 2872/5000
26/26 - 1s - loss: 0.5626 - val_loss: 0.9741
Epoch 2873/5000
26/26 - 1s - loss: 0.5613 - val_loss: 0.9744
Epoch 2874/5000
26/26 - 2s - loss: 0.5606 - val_loss: 0.9745
Epoch 2875/5000
26/26 - 1s - loss: 0.5619 - val_loss: 0.9729
Epoch 2876/5000
26/26 - 1s - loss: 0.5610 - val_loss: 0.9715
Epoch 2877/5000
26/26 - 1s - loss: 0.5614 - val_loss: 0.9739
Epoch 2878/5000
26/26 - 1s - loss: 0.5611 - val_loss: 0.9728
Epoch 2879/5000
26/26 - 1s - loss: 0.5624 - val_loss: 0.9720
Epoch 2880/5000
26/26 - 1s - loss: 0.5607 - val_loss: 0.9725
Epoch 02880: val_loss improved from 0.97443 to 0.97248, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 2881/5000
26/26 - 1s - loss: 0.5596 - val_loss: 0.9734
Epoch 2882/5000
26/26 - 1s - loss: 0.5605 - val_loss: 0.9705
Epoch 2883/5000
26/26 - 1s - loss: 0.5598 - val_loss: 0.9713
Epoch 2884/5000
26/26 - 1s - loss: 0.5610 - val_loss: 0.9740
Epoch 2885/5000
26/26 - 1s - loss: 0.5595 - val_loss: 0.9726
Epoch 2886/5000
26/26 - 1s - loss: 0.5596 - val_loss: 0.9707
Epoch 2887/5000
26/26 - 1s - loss: 0.5602 - val_loss: 0.9712
Epoch 2888/5000
26/26 - 1s - loss: 0.5584 - val_loss: 0.9733
Epoch 2889/5000
26/26 - 1s - loss: 0.5593 - val_loss: 0.9709
Epoch 2890/5000
26/26 - 1s - loss: 0.5599 - val_loss: 0.9699
Epoch 02890: val_loss improved from 0.97248 to 0.96989, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 2891/5000
26/26 - 1s - loss: 0.5601 - val_loss: 0.9713
Epoch 2892/5000
26/26 - 1s - loss: 0.5571 - val_loss: 0.9699
Epoch 2893/5000
26/26 - 2s - loss: 0.5584 - val_loss: 0.9697
Epoch 2894/5000
26/26 - 1s - loss: 0.5587 - val_loss: 0.9697
Epoch 2895/5000
26/26 - 1s - loss: 0.5587 - val_loss: 0.9694
Epoch 2896/5000
26/26 - 1s - loss: 0.5577 - val_loss: 0.9686
Epoch 2897/5000
26/26 - 1s - loss: 0.5575 - val_loss: 0.9696
Epoch 2898/5000
26/26 - 1s - loss: 0.5579 - val_loss: 0.9700
Epoch 2899/5000
26/26 - 1s - loss: 0.5570 - val_loss: 0.9694
Epoch 2900/5000
26/26 - 1s - loss: 0.5572 - val_loss: 0.9705
Epoch 02900: val_loss did not improve from 0.96989
Epoch 2901/5000
26/26 - 2s - loss: 0.5576 - val_loss: 0.9700
Epoch 2902/5000
26/26 - 1s - loss: 0.5572 - val_loss: 0.9702
Epoch 2903/5000
26/26 - 1s - loss: 0.5574 - val_loss: 0.9705
Epoch 2904/5000
26/26 - 1s - loss: 0.5562 - val_loss: 0.9695
Epoch 2905/5000
26/26 - 1s - loss: 0.5565 - val_loss: 0.9702
Epoch 2906/5000
26/26 - 1s - loss: 0.5561 - val_loss: 0.9684
Epoch 2907/5000
26/26 - 1s - loss: 0.5567 - val_loss: 0.9677
Epoch 2908/5000
26/26 - 1s - loss: 0.5572 - val_loss: 0.9686
Epoch 2909/5000
26/26 - 1s - loss: 0.5559 - val_loss: 0.9684
Epoch 2910/5000
26/26 - 1s - loss: 0.5559 - val_loss: 0.9692
Epoch 02910: val_loss improved from 0.96989 to 0.96915, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 2911/5000
26/26 - 1s - loss: 0.5553 - val_loss: 0.9680
Epoch 2912/5000
26/26 - 2s - loss: 0.5553 - val_loss: 0.9669
Epoch 2913/5000
26/26 - 1s - loss: 0.5545 - val_loss: 0.9674
Epoch 2914/5000
26/26 - 1s - loss: 0.5545 - val_loss: 0.9668
Epoch 2915/5000
26/26 - 1s - loss: 0.5556 - val_loss: 0.9659
Epoch 2916/5000
26/26 - 1s - loss: 0.5543 - val_loss: 0.9672
Epoch 2917/5000
26/26 - 1s - loss: 0.5541 - val_loss: 0.9683
Epoch 2918/5000
26/26 - 2s - loss: 0.5552 - val_loss: 0.9665
Epoch 2919/5000
26/26 - 1s - loss: 0.5543 - val_loss: 0.9670
Epoch 2920/5000
26/26 - 1s - loss: 0.5547 - val_loss: 0.9648
Epoch 02920: val_loss improved from 0.96915 to 0.96480, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 2921/5000
26/26 - 1s - loss: 0.5542 - val_loss: 0.9658
Epoch 2922/5000
26/26 - 2s - loss: 0.5539 - val_loss: 0.9658
Epoch 2923/5000
26/26 - 1s - loss: 0.5530 - val_loss: 0.9653
Epoch 2924/5000
26/26 - 1s - loss: 0.5539 - val_loss: 0.9661
Epoch 2925/5000
26/26 - 1s - loss: 0.5529 - val_loss: 0.9652
Epoch 2926/5000
26/26 - 1s - loss: 0.5535 - val_loss: 0.9663
Epoch 2927/5000
26/26 - 1s - loss: 0.5540 - val_loss: 0.9662
Epoch 2928/5000
26/26 - 1s - loss: 0.5538 - val_loss: 0.9645
Epoch 2929/5000
26/26 - 1s - loss: 0.5526 - val_loss: 0.9654
Epoch 2930/5000
26/26 - 1s - loss: 0.5528 - val_loss: 0.9649
Epoch 02930: val_loss did not improve from 0.96480
Epoch 2931/5000
26/26 - 1s - loss: 0.5536 - val_loss: 0.9661
Epoch 2932/5000
26/26 - 1s - loss: 0.5527 - val_loss: 0.9653
Epoch 2933/5000
26/26 - 1s - loss: 0.5528 - val_loss: 0.9660
Epoch 2934/5000
26/26 - 1s - loss: 0.5532 - val_loss: 0.9640
Epoch 2935/5000
26/26 - 1s - loss: 0.5529 - val_loss: 0.9644
Epoch 2936/5000
26/26 - 1s - loss: 0.5523 - val_loss: 0.9669
Epoch 2937/5000
26/26 - 1s - loss: 0.5525 - val_loss: 0.9661
Epoch 2938/5000
26/26 - 2s - loss: 0.5534 - val_loss: 0.9647
Epoch 2939/5000
26/26 - 1s - loss: 0.5509 - val_loss: 0.9664
Epoch 2940/5000
26/26 - 1s - loss: 0.5518 - val_loss: 0.9646
Epoch 02940: val_loss improved from 0.96480 to 0.96461, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 2941/5000
26/26 - 1s - loss: 0.5511 - val_loss: 0.9628
Epoch 2942/5000
26/26 - 1s - loss: 0.5517 - val_loss: 0.9636
Epoch 2943/5000
26/26 - 1s - loss: 0.5513 - val_loss: 0.9629
Epoch 2944/5000
26/26 - 1s - loss: 0.5517 - val_loss: 0.9626
Epoch 2945/5000
26/26 - 1s - loss: 0.5508 - val_loss: 0.9630
Epoch 2946/5000
26/26 - 1s - loss: 0.5490 - val_loss: 0.9635
Epoch 2947/5000
26/26 - 2s - loss: 0.5524 - val_loss: 0.9628
Epoch 2948/5000
26/26 - 1s - loss: 0.5507 - val_loss: 0.9616
Epoch 2949/5000
26/26 - 1s - loss: 0.5500 - val_loss: 0.9618
Epoch 2950/5000
26/26 - 1s - loss: 0.5508 - val_loss: 0.9612
Epoch 02950: val_loss improved from 0.96461 to 0.96116, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 2951/5000
26/26 - 1s - loss: 0.5516 - val_loss: 0.9622
Epoch 2952/5000
26/26 - 1s - loss: 0.5502 - val_loss: 0.9629
Epoch 2953/5000
26/26 - 2s - loss: 0.5495 - val_loss: 0.9601
Epoch 2954/5000
26/26 - 1s - loss: 0.5493 - val_loss: 0.9619
Epoch 2955/5000
26/26 - 1s - loss: 0.5499 - val_loss: 0.9616
Epoch 2956/5000
26/26 - 1s - loss: 0.5500 - val_loss: 0.9612
Epoch 2957/5000
26/26 - 1s - loss: 0.5488 - val_loss: 0.9612
Epoch 2958/5000
26/26 - 1s - loss: 0.5487 - val_loss: 0.9631
Epoch 2959/5000
26/26 - 1s - loss: 0.5488 - val_loss: 0.9624
Epoch 2960/5000
26/26 - 1s - loss: 0.5490 - val_loss: 0.9628
Epoch 02960: val_loss did not improve from 0.96116
Epoch 2961/5000
26/26 - 1s - loss: 0.5479 - val_loss: 0.9622
Epoch 2962/5000
26/26 - 1s - loss: 0.5476 - val_loss: 0.9611
Epoch 2963/5000
26/26 - 1s - loss: 0.5476 - val_loss: 0.9615
Epoch 2964/5000
26/26 - 1s - loss: 0.5483 - val_loss: 0.9597
Epoch 2965/5000
26/26 - 1s - loss: 0.5482 - val_loss: 0.9605
Epoch 2966/5000
26/26 - 1s - loss: 0.5478 - val_loss: 0.9594
Epoch 2967/5000
26/26 - 1s - loss: 0.5492 - val_loss: 0.9604
Epoch 2968/5000
26/26 - 1s - loss: 0.5472 - val_loss: 0.9604
Epoch 2969/5000
26/26 - 1s - loss: 0.5481 - val_loss: 0.9589
Epoch 2970/5000
26/26 - 1s - loss: 0.5465 - val_loss: 0.9608
Epoch 02970: val_loss improved from 0.96116 to 0.96082, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 2971/5000
26/26 - 1s - loss: 0.5470 - val_loss: 0.9601
Epoch 2972/5000
26/26 - 1s - loss: 0.5476 - val_loss: 0.9592
Epoch 2973/5000
26/26 - 1s - loss: 0.5468 - val_loss: 0.9590
Epoch 2974/5000
26/26 - 1s - loss: 0.5463 - val_loss: 0.9600
Epoch 2975/5000
26/26 - 2s - loss: 0.5481 - val_loss: 0.9566
Epoch 2976/5000
26/26 - 1s - loss: 0.5466 - val_loss: 0.9588
Epoch 2977/5000
26/26 - 1s - loss: 0.5461 - val_loss: 0.9578
Epoch 2978/5000
26/26 - 1s - loss: 0.5470 - val_loss: 0.9572
Epoch 2979/5000
26/26 - 1s - loss: 0.5463 - val_loss: 0.9568
Epoch 2980/5000
26/26 - 1s - loss: 0.5464 - val_loss: 0.9575
Epoch 02980: val_loss improved from 0.96082 to 0.95746, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 2981/5000
26/26 - 1s - loss: 0.5458 - val_loss: 0.9578
Epoch 2982/5000
26/26 - 1s - loss: 0.5457 - val_loss: 0.9568
Epoch 2983/5000
26/26 - 1s - loss: 0.5450 - val_loss: 0.9597
Epoch 2984/5000
26/26 - 1s - loss: 0.5454 - val_loss: 0.9575
Epoch 2985/5000
26/26 - 1s - loss: 0.5450 - val_loss: 0.9583
Epoch 2986/5000
26/26 - 1s - loss: 0.5442 - val_loss: 0.9578
Epoch 2987/5000
26/26 - 1s - loss: 0.5452 - val_loss: 0.9592
Epoch 2988/5000
26/26 - 1s - loss: 0.5448 - val_loss: 0.9577
Epoch 2989/5000
26/26 - 1s - loss: 0.5454 - val_loss: 0.9580
Epoch 2990/5000
26/26 - 1s - loss: 0.5441 - val_loss: 0.9569
Epoch 02990: val_loss improved from 0.95746 to 0.95687, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 2991/5000
26/26 - 1s - loss: 0.5449 - val_loss: 0.9561
Epoch 2992/5000
26/26 - 1s - loss: 0.5448 - val_loss: 0.9559
Epoch 2993/5000
26/26 - 1s - loss: 0.5432 - val_loss: 0.9569
Epoch 2994/5000
26/26 - 1s - loss: 0.5436 - val_loss: 0.9575
Epoch 2995/5000
26/26 - 2s - loss: 0.5434 - val_loss: 0.9560
Epoch 2996/5000
26/26 - 1s - loss: 0.5444 - val_loss: 0.9556
Epoch 2997/5000
26/26 - 1s - loss: 0.5446 - val_loss: 0.9552
Epoch 2998/5000
26/26 - 1s - loss: 0.5440 - val_loss: 0.9574
Epoch 2999/5000
26/26 - 2s - loss: 0.5441 - val_loss: 0.9570
Epoch 3000/5000
26/26 - 1s - loss: 0.5430 - val_loss: 0.9568
Epoch 03000: val_loss improved from 0.95687 to 0.95679, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 3001/5000
26/26 - 1s - loss: 0.5444 - val_loss: 0.9548
Epoch 3002/5000
26/26 - 1s - loss: 0.5441 - val_loss: 0.9561
Epoch 3003/5000
26/26 - 1s - loss: 0.5424 - val_loss: 0.9548
Epoch 3004/5000
26/26 - 1s - loss: 0.5419 - val_loss: 0.9561
Epoch 3005/5000
26/26 - 1s - loss: 0.5435 - val_loss: 0.9558
Epoch 3006/5000
26/26 - 1s - loss: 0.5417 - val_loss: 0.9554
Epoch 3007/5000
26/26 - 1s - loss: 0.5416 - val_loss: 0.9551
Epoch 3008/5000
26/26 - 1s - loss: 0.5416 - val_loss: 0.9537
Epoch 3009/5000
26/26 - 1s - loss: 0.5424 - val_loss: 0.9541
Epoch 3010/5000
26/26 - 1s - loss: 0.5413 - val_loss: 0.9554
Epoch 03010: val_loss improved from 0.95679 to 0.95544, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 3011/5000
26/26 - 1s - loss: 0.5423 - val_loss: 0.9544
Epoch 3012/5000
26/26 - 1s - loss: 0.5415 - val_loss: 0.9541
Epoch 3013/5000
26/26 - 1s - loss: 0.5415 - val_loss: 0.9547
Epoch 3014/5000
26/26 - 1s - loss: 0.5416 - val_loss: 0.9553
Epoch 3015/5000
26/26 - 1s - loss: 0.5413 - val_loss: 0.9550
Epoch 3016/5000
26/26 - 1s - loss: 0.5403 - val_loss: 0.9553
Epoch 3017/5000
26/26 - 1s - loss: 0.5412 - val_loss: 0.9554
Epoch 3018/5000
26/26 - 1s - loss: 0.5406 - val_loss: 0.9549
Epoch 3019/5000
26/26 - 1s - loss: 0.5412 - val_loss: 0.9546
Epoch 3020/5000
26/26 - 1s - loss: 0.5399 - val_loss: 0.9525
Epoch 03020: val_loss improved from 0.95544 to 0.95247, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 3021/5000
26/26 - 1s - loss: 0.5386 - val_loss: 0.9525
Epoch 3022/5000
26/26 - 1s - loss: 0.5399 - val_loss: 0.9537
Epoch 3023/5000
26/26 - 1s - loss: 0.5387 - val_loss: 0.9532
Epoch 3024/5000
26/26 - 1s - loss: 0.5399 - val_loss: 0.9524
Epoch 3025/5000
26/26 - 1s - loss: 0.5391 - val_loss: 0.9519
Epoch 3026/5000
26/26 - 1s - loss: 0.5392 - val_loss: 0.9533
Epoch 3027/5000
26/26 - 1s - loss: 0.5391 - val_loss: 0.9535
Epoch 3028/5000
26/26 - 1s - loss: 0.5400 - val_loss: 0.9523
Epoch 3029/5000
26/26 - 1s - loss: 0.5405 - val_loss: 0.9519
Epoch 3030/5000
26/26 - 1s - loss: 0.5389 - val_loss: 0.9519
Epoch 03030: val_loss improved from 0.95247 to 0.95190, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 3031/5000
26/26 - 1s - loss: 0.5394 - val_loss: 0.9515
Epoch 3032/5000
26/26 - 1s - loss: 0.5378 - val_loss: 0.9513
Epoch 3033/5000
26/26 - 1s - loss: 0.5374 - val_loss: 0.9498
Epoch 3034/5000
26/26 - 1s - loss: 0.5386 - val_loss: 0.9505
Epoch 3035/5000
26/26 - 1s - loss: 0.5399 - val_loss: 0.9515
Epoch 3036/5000
26/26 - 1s - loss: 0.5380 - val_loss: 0.9515
Epoch 3037/5000
26/26 - 2s - loss: 0.5382 - val_loss: 0.9481
Epoch 3038/5000
26/26 - 1s - loss: 0.5388 - val_loss: 0.9501
Epoch 3039/5000
26/26 - 1s - loss: 0.5375 - val_loss: 0.9496
Epoch 3040/5000
26/26 - 1s - loss: 0.5376 - val_loss: 0.9509
Epoch 03040: val_loss improved from 0.95190 to 0.95093, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 3041/5000
26/26 - 1s - loss: 0.5377 - val_loss: 0.9511
Epoch 3042/5000
26/26 - 1s - loss: 0.5370 - val_loss: 0.9500
Epoch 3043/5000
26/26 - 1s - loss: 0.5368 - val_loss: 0.9488
Epoch 3044/5000
26/26 - 1s - loss: 0.5379 - val_loss: 0.9493
Epoch 3045/5000
26/26 - 1s - loss: 0.5349 - val_loss: 0.9494
Epoch 3046/5000
26/26 - 1s - loss: 0.5376 - val_loss: 0.9496
Epoch 3047/5000
26/26 - 1s - loss: 0.5356 - val_loss: 0.9497
Epoch 3048/5000
26/26 - 1s - loss: 0.5367 - val_loss: 0.9477
Epoch 3049/5000
26/26 - 1s - loss: 0.5362 - val_loss: 0.9481
Epoch 3050/5000
26/26 - 1s - loss: 0.5360 - val_loss: 0.9490
Epoch 03050: val_loss improved from 0.95093 to 0.94905, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 3051/5000
26/26 - 1s - loss: 0.5362 - val_loss: 0.9489
Epoch 3052/5000
26/26 - 1s - loss: 0.5366 - val_loss: 0.9500
Epoch 3053/5000
26/26 - 1s - loss: 0.5352 - val_loss: 0.9469
Epoch 3054/5000
26/26 - 1s - loss: 0.5365 - val_loss: 0.9481
Epoch 3055/5000
26/26 - 1s - loss: 0.5350 - val_loss: 0.9477
Epoch 3056/5000
26/26 - 1s - loss: 0.5351 - val_loss: 0.9475
Epoch 3057/5000
26/26 - 1s - loss: 0.5364 - val_loss: 0.9465
Epoch 3058/5000
26/26 - 1s - loss: 0.5358 - val_loss: 0.9469
Epoch 3059/5000
26/26 - 1s - loss: 0.5353 - val_loss: 0.9479
Epoch 3060/5000
26/26 - 1s - loss: 0.5344 - val_loss: 0.9466
Epoch 03060: val_loss improved from 0.94905 to 0.94665, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 3061/5000
26/26 - 1s - loss: 0.5347 - val_loss: 0.9486
Epoch 3062/5000
26/26 - 1s - loss: 0.5343 - val_loss: 0.9471
Epoch 3063/5000
26/26 - 1s - loss: 0.5349 - val_loss: 0.9457
Epoch 3064/5000
26/26 - 1s - loss: 0.5351 - val_loss: 0.9463
Epoch 3065/5000
26/26 - 1s - loss: 0.5336 - val_loss: 0.9455
Epoch 3066/5000
26/26 - 1s - loss: 0.5339 - val_loss: 0.9473
Epoch 3067/5000
26/26 - 1s - loss: 0.5351 - val_loss: 0.9457
Epoch 3068/5000
26/26 - 1s - loss: 0.5338 - val_loss: 0.9454
Epoch 3069/5000
26/26 - 1s - loss: 0.5341 - val_loss: 0.9452
Epoch 3070/5000
26/26 - 1s - loss: 0.5341 - val_loss: 0.9458
Epoch 03070: val_loss improved from 0.94665 to 0.94582, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 3071/5000
26/26 - 1s - loss: 0.5344 - val_loss: 0.9462
Epoch 3072/5000
26/26 - 1s - loss: 0.5331 - val_loss: 0.9450
Epoch 3073/5000
26/26 - 1s - loss: 0.5338 - val_loss: 0.9465
Epoch 3074/5000
26/26 - 1s - loss: 0.5340 - val_loss: 0.9457
Epoch 3075/5000
26/26 - 1s - loss: 0.5334 - val_loss: 0.9468
Epoch 3076/5000
26/26 - 1s - loss: 0.5334 - val_loss: 0.9435
Epoch 3077/5000
26/26 - 1s - loss: 0.5327 - val_loss: 0.9448
Epoch 3078/5000
26/26 - 1s - loss: 0.5321 - val_loss: 0.9443
Epoch 3079/5000
26/26 - 1s - loss: 0.5322 - val_loss: 0.9445
Epoch 3080/5000
26/26 - 1s - loss: 0.5328 - val_loss: 0.9446
Epoch 03080: val_loss improved from 0.94582 to 0.94463, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 3081/5000
26/26 - 1s - loss: 0.5315 - val_loss: 0.9444
Epoch 3082/5000
26/26 - 1s - loss: 0.5318 - val_loss: 0.9445
Epoch 3083/5000
26/26 - 1s - loss: 0.5317 - val_loss: 0.9447
Epoch 3084/5000
26/26 - 1s - loss: 0.5320 - val_loss: 0.9442
Epoch 3085/5000
26/26 - 1s - loss: 0.5315 - val_loss: 0.9429
Epoch 3086/5000
26/26 - 1s - loss: 0.5327 - val_loss: 0.9450
Epoch 3087/5000
26/26 - 1s - loss: 0.5318 - val_loss: 0.9442
Epoch 3088/5000
26/26 - 1s - loss: 0.5304 - val_loss: 0.9448
Epoch 3089/5000
26/26 - 1s - loss: 0.5296 - val_loss: 0.9420
Epoch 3090/5000
26/26 - 1s - loss: 0.5314 - val_loss: 0.9429
Epoch 03090: val_loss improved from 0.94463 to 0.94286, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 3091/5000
26/26 - 1s - loss: 0.5317 - val_loss: 0.9440
Epoch 3092/5000
26/26 - 1s - loss: 0.5317 - val_loss: 0.9423
Epoch 3093/5000
26/26 - 1s - loss: 0.5304 - val_loss: 0.9432
Epoch 3094/5000
26/26 - 1s - loss: 0.5303 - val_loss: 0.9418
Epoch 3095/5000
26/26 - 1s - loss: 0.5301 - val_loss: 0.9415
Epoch 3096/5000
26/26 - 1s - loss: 0.5315 - val_loss: 0.9422
Epoch 3097/5000
26/26 - 1s - loss: 0.5293 - val_loss: 0.9410
Epoch 3098/5000
26/26 - 1s - loss: 0.5302 - val_loss: 0.9409
Epoch 3099/5000
26/26 - 1s - loss: 0.5295 - val_loss: 0.9425
Epoch 3100/5000
26/26 - 1s - loss: 0.5287 - val_loss: 0.9434
Epoch 03100: val_loss did not improve from 0.94286
Epoch 3101/5000
26/26 - 1s - loss: 0.5295 - val_loss: 0.9423
Epoch 3102/5000
26/26 - 1s - loss: 0.5293 - val_loss: 0.9409
Epoch 3103/5000
26/26 - 1s - loss: 0.5297 - val_loss: 0.9419
Epoch 3104/5000
26/26 - 1s - loss: 0.5302 - val_loss: 0.9413
Epoch 3105/5000
26/26 - 1s - loss: 0.5299 - val_loss: 0.9396
Epoch 3106/5000
26/26 - 1s - loss: 0.5290 - val_loss: 0.9415
Epoch 3107/5000
26/26 - 1s - loss: 0.5286 - val_loss: 0.9406
Epoch 3108/5000
26/26 - 1s - loss: 0.5288 - val_loss: 0.9407
Epoch 3109/5000
26/26 - 1s - loss: 0.5288 - val_loss: 0.9402
Epoch 3110/5000
26/26 - 1s - loss: 0.5281 - val_loss: 0.9419
Epoch 03110: val_loss improved from 0.94286 to 0.94190, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 3111/5000
26/26 - 1s - loss: 0.5288 - val_loss: 0.9392
Epoch 3112/5000
26/26 - 1s - loss: 0.5293 - val_loss: 0.9394
Epoch 3113/5000
26/26 - 1s - loss: 0.5282 - val_loss: 0.9396
Epoch 3114/5000
26/26 - 1s - loss: 0.5291 - val_loss: 0.9405
Epoch 3115/5000
26/26 - 1s - loss: 0.5273 - val_loss: 0.9408
Epoch 3116/5000
26/26 - 1s - loss: 0.5272 - val_loss: 0.9389
Epoch 3117/5000
26/26 - 1s - loss: 0.5276 - val_loss: 0.9403
Epoch 3118/5000
26/26 - 1s - loss: 0.5267 - val_loss: 0.9386
Epoch 3119/5000
26/26 - 1s - loss: 0.5274 - val_loss: 0.9388
Epoch 3120/5000
26/26 - 2s - loss: 0.5256 - val_loss: 0.9409
Epoch 03120: val_loss improved from 0.94190 to 0.94092, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 3121/5000
26/26 - 1s - loss: 0.5273 - val_loss: 0.9399
Epoch 3122/5000
26/26 - 1s - loss: 0.5279 - val_loss: 0.9407
Epoch 3123/5000
26/26 - 1s - loss: 0.5273 - val_loss: 0.9380
Epoch 3124/5000
26/26 - 1s - loss: 0.5269 - val_loss: 0.9416
Epoch 3125/5000
26/26 - 1s - loss: 0.5268 - val_loss: 0.9396
Epoch 3126/5000
26/26 - 1s - loss: 0.5263 - val_loss: 0.9383
Epoch 3127/5000
26/26 - 1s - loss: 0.5274 - val_loss: 0.9390
Epoch 3128/5000
26/26 - 1s - loss: 0.5259 - val_loss: 0.9386
Epoch 3129/5000
26/26 - 1s - loss: 0.5266 - val_loss: 0.9396
Epoch 3130/5000
26/26 - 1s - loss: 0.5258 - val_loss: 0.9382
Epoch 03130: val_loss improved from 0.94092 to 0.93820, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 3131/5000
26/26 - 1s - loss: 0.5264 - val_loss: 0.9367
Epoch 3132/5000
26/26 - 1s - loss: 0.5266 - val_loss: 0.9372
Epoch 3133/5000
26/26 - 1s - loss: 0.5248 - val_loss: 0.9358
Epoch 3134/5000
26/26 - 1s - loss: 0.5257 - val_loss: 0.9386
Epoch 3135/5000
26/26 - 1s - loss: 0.5245 - val_loss: 0.9373
Epoch 3136/5000
26/26 - 1s - loss: 0.5248 - val_loss: 0.9366
Epoch 3137/5000
26/26 - 1s - loss: 0.5255 - val_loss: 0.9361
Epoch 3138/5000
26/26 - 1s - loss: 0.5253 - val_loss: 0.9357
Epoch 3139/5000
26/26 - 1s - loss: 0.5251 - val_loss: 0.9372
Epoch 3140/5000
26/26 - 1s - loss: 0.5254 - val_loss: 0.9356
Epoch 03140: val_loss improved from 0.93820 to 0.93558, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 3141/5000
26/26 - 1s - loss: 0.5240 - val_loss: 0.9365
Epoch 3142/5000
26/26 - 1s - loss: 0.5244 - val_loss: 0.9368
Epoch 3143/5000
26/26 - 1s - loss: 0.5245 - val_loss: 0.9355
Epoch 3144/5000
26/26 - 1s - loss: 0.5244 - val_loss: 0.9356
Epoch 3145/5000
26/26 - 1s - loss: 0.5240 - val_loss: 0.9382
Epoch 3146/5000
26/26 - 1s - loss: 0.5246 - val_loss: 0.9376
Epoch 3147/5000
26/26 - 1s - loss: 0.5261 - val_loss: 0.9358
Epoch 3148/5000
26/26 - 1s - loss: 0.5238 - val_loss: 0.9375
Epoch 3149/5000
26/26 - 1s - loss: 0.5238 - val_loss: 0.9351
Epoch 3150/5000
26/26 - 1s - loss: 0.5224 - val_loss: 0.9359
Epoch 03150: val_loss did not improve from 0.93558
Epoch 3151/5000
26/26 - 1s - loss: 0.5227 - val_loss: 0.9372
Epoch 3152/5000
26/26 - 1s - loss: 0.5231 - val_loss: 0.9359
Epoch 3153/5000
26/26 - 1s - loss: 0.5235 - val_loss: 0.9351
Epoch 3154/5000
26/26 - 1s - loss: 0.5232 - val_loss: 0.9358
Epoch 3155/5000
26/26 - 1s - loss: 0.5233 - val_loss: 0.9358
Epoch 3156/5000
26/26 - 1s - loss: 0.5241 - val_loss: 0.9357
Epoch 3157/5000
26/26 - 1s - loss: 0.5230 - val_loss: 0.9369
Epoch 3158/5000
26/26 - 1s - loss: 0.5222 - val_loss: 0.9334
Epoch 3159/5000
26/26 - 1s - loss: 0.5225 - val_loss: 0.9363
Epoch 3160/5000
26/26 - 1s - loss: 0.5218 - val_loss: 0.9361
Epoch 03160: val_loss did not improve from 0.93558
Epoch 3161/5000
26/26 - 1s - loss: 0.5212 - val_loss: 0.9356
Epoch 3162/5000
26/26 - 2s - loss: 0.5220 - val_loss: 0.9347
Epoch 3163/5000
26/26 - 2s - loss: 0.5218 - val_loss: 0.9358
Epoch 3164/5000
26/26 - 1s - loss: 0.5221 - val_loss: 0.9331
Epoch 3165/5000
26/26 - 1s - loss: 0.5219 - val_loss: 0.9348
Epoch 3166/5000
26/26 - 1s - loss: 0.5206 - val_loss: 0.9362
Epoch 3167/5000
26/26 - 1s - loss: 0.5212 - val_loss: 0.9354
Epoch 3168/5000
26/26 - 1s - loss: 0.5209 - val_loss: 0.9335
Epoch 3169/5000
26/26 - 1s - loss: 0.5201 - val_loss: 0.9332
Epoch 3170/5000
26/26 - 1s - loss: 0.5217 - val_loss: 0.9335
Epoch 03170: val_loss improved from 0.93558 to 0.93349, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 3171/5000
26/26 - 1s - loss: 0.5202 - val_loss: 0.9336
Epoch 3172/5000
26/26 - 1s - loss: 0.5207 - val_loss: 0.9330
Epoch 3173/5000
26/26 - 1s - loss: 0.5209 - val_loss: 0.9344
Epoch 3174/5000
26/26 - 1s - loss: 0.5213 - val_loss: 0.9314
Epoch 3175/5000
26/26 - 1s - loss: 0.5211 - val_loss: 0.9320
Epoch 3176/5000
26/26 - 1s - loss: 0.5215 - val_loss: 0.9334
Epoch 3177/5000
26/26 - 1s - loss: 0.5207 - val_loss: 0.9329
Epoch 3178/5000
26/26 - 1s - loss: 0.5198 - val_loss: 0.9335
Epoch 3179/5000
26/26 - 1s - loss: 0.5196 - val_loss: 0.9326
Epoch 3180/5000
26/26 - 1s - loss: 0.5203 - val_loss: 0.9339
Epoch 03180: val_loss did not improve from 0.93349
Epoch 3181/5000
26/26 - 1s - loss: 0.5203 - val_loss: 0.9322
Epoch 3182/5000
26/26 - 1s - loss: 0.5205 - val_loss: 0.9324
Epoch 3183/5000
26/26 - 1s - loss: 0.5191 - val_loss: 0.9315
Epoch 3184/5000
26/26 - 1s - loss: 0.5191 - val_loss: 0.9321
Epoch 3185/5000
26/26 - 1s - loss: 0.5188 - val_loss: 0.9312
Epoch 3186/5000
26/26 - 1s - loss: 0.5192 - val_loss: 0.9308
Epoch 3187/5000
26/26 - 1s - loss: 0.5194 - val_loss: 0.9315
Epoch 3188/5000
26/26 - 1s - loss: 0.5187 - val_loss: 0.9318
Epoch 3189/5000
26/26 - 1s - loss: 0.5187 - val_loss: 0.9312
Epoch 3190/5000
26/26 - 1s - loss: 0.5190 - val_loss: 0.9324
Epoch 03190: val_loss improved from 0.93349 to 0.93237, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 3191/5000
26/26 - 1s - loss: 0.5192 - val_loss: 0.9299
Epoch 3192/5000
26/26 - 1s - loss: 0.5183 - val_loss: 0.9303
Epoch 3193/5000
26/26 - 1s - loss: 0.5197 - val_loss: 0.9302
Epoch 3194/5000
26/26 - 1s - loss: 0.5188 - val_loss: 0.9316
Epoch 3195/5000
26/26 - 1s - loss: 0.5176 - val_loss: 0.9313
Epoch 3196/5000
26/26 - 1s - loss: 0.5174 - val_loss: 0.9313
Epoch 3197/5000
26/26 - 1s - loss: 0.5189 - val_loss: 0.9309
Epoch 3198/5000
26/26 - 1s - loss: 0.5189 - val_loss: 0.9318
Epoch 3199/5000
26/26 - 1s - loss: 0.5167 - val_loss: 0.9309
Epoch 3200/5000
26/26 - 1s - loss: 0.5188 - val_loss: 0.9299
Epoch 03200: val_loss improved from 0.93237 to 0.92989, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 3201/5000
26/26 - 1s - loss: 0.5178 - val_loss: 0.9296
Epoch 3202/5000
26/26 - 1s - loss: 0.5174 - val_loss: 0.9304
Epoch 3203/5000
26/26 - 1s - loss: 0.5177 - val_loss: 0.9298
Epoch 3204/5000
26/26 - 1s - loss: 0.5175 - val_loss: 0.9310
Epoch 3205/5000
26/26 - 2s - loss: 0.5166 - val_loss: 0.9299
Epoch 3206/5000
26/26 - 1s - loss: 0.5169 - val_loss: 0.9302
Epoch 3207/5000
26/26 - 1s - loss: 0.5162 - val_loss: 0.9305
Epoch 3208/5000
26/26 - 1s - loss: 0.5172 - val_loss: 0.9294
Epoch 3209/5000
26/26 - 1s - loss: 0.5149 - val_loss: 0.9294
Epoch 3210/5000
26/26 - 1s - loss: 0.5170 - val_loss: 0.9295
Epoch 03210: val_loss improved from 0.92989 to 0.92951, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 3211/5000
26/26 - 1s - loss: 0.5176 - val_loss: 0.9290
Epoch 3212/5000
26/26 - 1s - loss: 0.5162 - val_loss: 0.9284
Epoch 3213/5000
26/26 - 1s - loss: 0.5163 - val_loss: 0.9277
Epoch 3214/5000
26/26 - 1s - loss: 0.5155 - val_loss: 0.9284
Epoch 3215/5000
26/26 - 1s - loss: 0.5169 - val_loss: 0.9293
Epoch 3216/5000
26/26 - 1s - loss: 0.5157 - val_loss: 0.9286
Epoch 3217/5000
26/26 - 1s - loss: 0.5153 - val_loss: 0.9274
Epoch 3218/5000
26/26 - 1s - loss: 0.5144 - val_loss: 0.9277
Epoch 3219/5000
26/26 - 1s - loss: 0.5144 - val_loss: 0.9274
Epoch 3220/5000
26/26 - 1s - loss: 0.5143 - val_loss: 0.9262
Epoch 03220: val_loss improved from 0.92951 to 0.92615, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 3221/5000
26/26 - 1s - loss: 0.5159 - val_loss: 0.9282
Epoch 3222/5000
26/26 - 1s - loss: 0.5149 - val_loss: 0.9275
Epoch 3223/5000
26/26 - 1s - loss: 0.5164 - val_loss: 0.9278
Epoch 3224/5000
26/26 - 1s - loss: 0.5149 - val_loss: 0.9264
Epoch 3225/5000
26/26 - 1s - loss: 0.5145 - val_loss: 0.9252
Epoch 3226/5000
26/26 - 1s - loss: 0.5150 - val_loss: 0.9259
Epoch 3227/5000
26/26 - 1s - loss: 0.5152 - val_loss: 0.9269
Epoch 3228/5000
26/26 - 1s - loss: 0.5142 - val_loss: 0.9263
Epoch 3229/5000
26/26 - 1s - loss: 0.5147 - val_loss: 0.9275
Epoch 3230/5000
26/26 - 1s - loss: 0.5139 - val_loss: 0.9255
Epoch 03230: val_loss improved from 0.92615 to 0.92550, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 3231/5000
26/26 - 1s - loss: 0.5142 - val_loss: 0.9269
Epoch 3232/5000
26/26 - 1s - loss: 0.5121 - val_loss: 0.9253
Epoch 3233/5000
26/26 - 1s - loss: 0.5154 - val_loss: 0.9262
Epoch 3234/5000
26/26 - 1s - loss: 0.5138 - val_loss: 0.9261
Epoch 3235/5000
26/26 - 1s - loss: 0.5131 - val_loss: 0.9259
Epoch 3236/5000
26/26 - 1s - loss: 0.5132 - val_loss: 0.9250
Epoch 3237/5000
26/26 - 1s - loss: 0.5133 - val_loss: 0.9261
Epoch 3238/5000
26/26 - 1s - loss: 0.5130 - val_loss: 0.9260
Epoch 3239/5000
26/26 - 1s - loss: 0.5132 - val_loss: 0.9263
Epoch 3240/5000
26/26 - 1s - loss: 0.5133 - val_loss: 0.9274
Epoch 03240: val_loss did not improve from 0.92550
Epoch 3241/5000
26/26 - 1s - loss: 0.5128 - val_loss: 0.9251
Epoch 3242/5000
26/26 - 1s - loss: 0.5133 - val_loss: 0.9250
Epoch 3243/5000
26/26 - 1s - loss: 0.5113 - val_loss: 0.9263
Epoch 3244/5000
26/26 - 1s - loss: 0.5127 - val_loss: 0.9248
Epoch 3245/5000
26/26 - 1s - loss: 0.5117 - val_loss: 0.9249
Epoch 3246/5000
26/26 - 1s - loss: 0.5113 - val_loss: 0.9257
Epoch 3247/5000
26/26 - 1s - loss: 0.5116 - val_loss: 0.9248
Epoch 3248/5000
26/26 - 1s - loss: 0.5121 - val_loss: 0.9243
Epoch 3249/5000
26/26 - 1s - loss: 0.5118 - val_loss: 0.9242
Epoch 3250/5000
26/26 - 1s - loss: 0.5116 - val_loss: 0.9265
Epoch 03250: val_loss did not improve from 0.92550
Epoch 3251/5000
26/26 - 1s - loss: 0.5121 - val_loss: 0.9249
Epoch 3252/5000
26/26 - 1s - loss: 0.5109 - val_loss: 0.9249
Epoch 3253/5000
26/26 - 1s - loss: 0.5111 - val_loss: 0.9224
Epoch 3254/5000
26/26 - 1s - loss: 0.5113 - val_loss: 0.9248
Epoch 3255/5000
26/26 - 1s - loss: 0.5115 - val_loss: 0.9207
Epoch 3256/5000
26/26 - 1s - loss: 0.5111 - val_loss: 0.9230
Epoch 3257/5000
26/26 - 1s - loss: 0.5110 - val_loss: 0.9234
Epoch 3258/5000
26/26 - 1s - loss: 0.5100 - val_loss: 0.9242
Epoch 3259/5000
26/26 - 1s - loss: 0.5112 - val_loss: 0.9234
Epoch 3260/5000
26/26 - 1s - loss: 0.5094 - val_loss: 0.9230
Epoch 03260: val_loss improved from 0.92550 to 0.92298, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 3261/5000
26/26 - 1s - loss: 0.5109 - val_loss: 0.9231
Epoch 3262/5000
26/26 - 1s - loss: 0.5108 - val_loss: 0.9233
Epoch 3263/5000
26/26 - 1s - loss: 0.5106 - val_loss: 0.9225
Epoch 3264/5000
26/26 - 1s - loss: 0.5092 - val_loss: 0.9243
Epoch 3265/5000
26/26 - 1s - loss: 0.5103 - val_loss: 0.9218
Epoch 3266/5000
26/26 - 1s - loss: 0.5095 - val_loss: 0.9213
Epoch 3267/5000
26/26 - 2s - loss: 0.5085 - val_loss: 0.9201
Epoch 3268/5000
26/26 - 1s - loss: 0.5099 - val_loss: 0.9226
Epoch 3269/5000
26/26 - 1s - loss: 0.5089 - val_loss: 0.9230
Epoch 3270/5000
26/26 - 1s - loss: 0.5098 - val_loss: 0.9228
Epoch 03270: val_loss improved from 0.92298 to 0.92275, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 3271/5000
26/26 - 1s - loss: 0.5093 - val_loss: 0.9224
Epoch 3272/5000
26/26 - 1s - loss: 0.5092 - val_loss: 0.9206
Epoch 3273/5000
26/26 - 1s - loss: 0.5100 - val_loss: 0.9211
Epoch 3274/5000
26/26 - 1s - loss: 0.5087 - val_loss: 0.9220
Epoch 3275/5000
26/26 - 1s - loss: 0.5088 - val_loss: 0.9226
Epoch 3276/5000
26/26 - 1s - loss: 0.5090 - val_loss: 0.9198
Epoch 3277/5000
26/26 - 1s - loss: 0.5084 - val_loss: 0.9208
Epoch 3278/5000
26/26 - 1s - loss: 0.5085 - val_loss: 0.9212
Epoch 3279/5000
26/26 - 1s - loss: 0.5078 - val_loss: 0.9201
Epoch 3280/5000
26/26 - 1s - loss: 0.5075 - val_loss: 0.9207
Epoch 03280: val_loss improved from 0.92275 to 0.92070, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 3281/5000
26/26 - 1s - loss: 0.5089 - val_loss: 0.9212
Epoch 3282/5000
26/26 - 1s - loss: 0.5087 - val_loss: 0.9215
Epoch 3283/5000
26/26 - 1s - loss: 0.5075 - val_loss: 0.9206
Epoch 3284/5000
26/26 - 1s - loss: 0.5080 - val_loss: 0.9230
Epoch 3285/5000
26/26 - 1s - loss: 0.5079 - val_loss: 0.9227
Epoch 3286/5000
26/26 - 1s - loss: 0.5082 - val_loss: 0.9190
Epoch 3287/5000
26/26 - 1s - loss: 0.5076 - val_loss: 0.9216
Epoch 3288/5000
26/26 - 1s - loss: 0.5074 - val_loss: 0.9195
Epoch 3289/5000
26/26 - 2s - loss: 0.5071 - val_loss: 0.9191
Epoch 3290/5000
26/26 - 1s - loss: 0.5074 - val_loss: 0.9193
Epoch 03290: val_loss improved from 0.92070 to 0.91930, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 3291/5000
26/26 - 1s - loss: 0.5071 - val_loss: 0.9215
Epoch 3292/5000
26/26 - 1s - loss: 0.5065 - val_loss: 0.9193
Epoch 3293/5000
26/26 - 1s - loss: 0.5094 - val_loss: 0.9193
Epoch 3294/5000
26/26 - 1s - loss: 0.5066 - val_loss: 0.9200
Epoch 3295/5000
26/26 - 1s - loss: 0.5061 - val_loss: 0.9194
Epoch 3296/5000
26/26 - 1s - loss: 0.5067 - val_loss: 0.9202
Epoch 3297/5000
26/26 - 1s - loss: 0.5076 - val_loss: 0.9193
Epoch 3298/5000
26/26 - 1s - loss: 0.5057 - val_loss: 0.9182
Epoch 3299/5000
26/26 - 1s - loss: 0.5062 - val_loss: 0.9184
Epoch 3300/5000
26/26 - 1s - loss: 0.5064 - val_loss: 0.9184
Epoch 03300: val_loss improved from 0.91930 to 0.91839, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 3301/5000
26/26 - 1s - loss: 0.5063 - val_loss: 0.9179
Epoch 3302/5000
26/26 - 1s - loss: 0.5060 - val_loss: 0.9179
Epoch 3303/5000
26/26 - 1s - loss: 0.5065 - val_loss: 0.9188
Epoch 3304/5000
26/26 - 1s - loss: 0.5060 - val_loss: 0.9177
Epoch 3305/5000
26/26 - 1s - loss: 0.5062 - val_loss: 0.9183
Epoch 3306/5000
26/26 - 1s - loss: 0.5055 - val_loss: 0.9179
Epoch 3307/5000
26/26 - 1s - loss: 0.5054 - val_loss: 0.9158
Epoch 3308/5000
26/26 - 1s - loss: 0.5069 - val_loss: 0.9178
Epoch 3309/5000
26/26 - 1s - loss: 0.5066 - val_loss: 0.9193
Epoch 3310/5000
26/26 - 1s - loss: 0.5065 - val_loss: 0.9190
Epoch 03310: val_loss did not improve from 0.91839
Epoch 3311/5000
26/26 - 1s - loss: 0.5057 - val_loss: 0.9175
Epoch 3312/5000
26/26 - 1s - loss: 0.5055 - val_loss: 0.9163
Epoch 3313/5000
26/26 - 1s - loss: 0.5051 - val_loss: 0.9172
Epoch 3314/5000
26/26 - 1s - loss: 0.5045 - val_loss: 0.9164
Epoch 3315/5000
26/26 - 1s - loss: 0.5052 - val_loss: 0.9179
Epoch 3316/5000
26/26 - 1s - loss: 0.5047 - val_loss: 0.9181
Epoch 3317/5000
26/26 - 1s - loss: 0.5048 - val_loss: 0.9183
Epoch 3318/5000
26/26 - 1s - loss: 0.5037 - val_loss: 0.9176
Epoch 3319/5000
26/26 - 1s - loss: 0.5036 - val_loss: 0.9160
Epoch 3320/5000
26/26 - 1s - loss: 0.5047 - val_loss: 0.9177
Epoch 03320: val_loss improved from 0.91839 to 0.91771, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 3321/5000
26/26 - 1s - loss: 0.5046 - val_loss: 0.9163
Epoch 3322/5000
26/26 - 1s - loss: 0.5046 - val_loss: 0.9153
Epoch 3323/5000
26/26 - 1s - loss: 0.5043 - val_loss: 0.9152
Epoch 3324/5000
26/26 - 1s - loss: 0.5039 - val_loss: 0.9160
Epoch 3325/5000
26/26 - 1s - loss: 0.5035 - val_loss: 0.9160
Epoch 3326/5000
26/26 - 1s - loss: 0.5047 - val_loss: 0.9147
Epoch 3327/5000
26/26 - 1s - loss: 0.5038 - val_loss: 0.9166
Epoch 3328/5000
26/26 - 1s - loss: 0.5040 - val_loss: 0.9159
Epoch 3329/5000
26/26 - 1s - loss: 0.5030 - val_loss: 0.9161
Epoch 3330/5000
26/26 - 1s - loss: 0.5023 - val_loss: 0.9148
Epoch 03330: val_loss improved from 0.91771 to 0.91477, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 3331/5000
26/26 - 1s - loss: 0.5030 - val_loss: 0.9152
Epoch 3332/5000
26/26 - 1s - loss: 0.5036 - val_loss: 0.9146
Epoch 3333/5000
26/26 - 1s - loss: 0.5025 - val_loss: 0.9156
Epoch 3334/5000
26/26 - 1s - loss: 0.5023 - val_loss: 0.9167
Epoch 3335/5000
26/26 - 1s - loss: 0.5022 - val_loss: 0.9144
Epoch 3336/5000
26/26 - 1s - loss: 0.5022 - val_loss: 0.9145
Epoch 3337/5000
26/26 - 1s - loss: 0.5027 - val_loss: 0.9148
Epoch 3338/5000
26/26 - 1s - loss: 0.5018 - val_loss: 0.9169
Epoch 3339/5000
26/26 - 1s - loss: 0.5020 - val_loss: 0.9146
Epoch 3340/5000
26/26 - 1s - loss: 0.5022 - val_loss: 0.9138
Epoch 03340: val_loss improved from 0.91477 to 0.91380, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 3341/5000
26/26 - 1s - loss: 0.5023 - val_loss: 0.9154
Epoch 3342/5000
26/26 - 1s - loss: 0.5017 - val_loss: 0.9163
Epoch 3343/5000
26/26 - 1s - loss: 0.5030 - val_loss: 0.9137
Epoch 3344/5000
26/26 - 1s - loss: 0.5017 - val_loss: 0.9163
Epoch 3345/5000
26/26 - 1s - loss: 0.5029 - val_loss: 0.9124
Epoch 3346/5000
26/26 - 1s - loss: 0.5024 - val_loss: 0.9129
Epoch 3347/5000
26/26 - 1s - loss: 0.5009 - val_loss: 0.9146
Epoch 3348/5000
26/26 - 1s - loss: 0.5025 - val_loss: 0.9158
Epoch 3349/5000
26/26 - 1s - loss: 0.5012 - val_loss: 0.9145
Epoch 3350/5000
26/26 - 1s - loss: 0.5003 - val_loss: 0.9148
Epoch 03350: val_loss did not improve from 0.91380
Epoch 3351/5000
26/26 - 1s - loss: 0.5018 - val_loss: 0.9161
Epoch 3352/5000
26/26 - 1s - loss: 0.4999 - val_loss: 0.9145
Epoch 3353/5000
26/26 - 1s - loss: 0.5009 - val_loss: 0.9128
Epoch 3354/5000
26/26 - 1s - loss: 0.5008 - val_loss: 0.9134
Epoch 3355/5000
26/26 - 1s - loss: 0.5009 - val_loss: 0.9144
Epoch 3356/5000
26/26 - 1s - loss: 0.5013 - val_loss: 0.9143
Epoch 3357/5000
26/26 - 1s - loss: 0.5001 - val_loss: 0.9142
Epoch 3358/5000
26/26 - 1s - loss: 0.4992 - val_loss: 0.9123
Epoch 3359/5000
26/26 - 1s - loss: 0.5006 - val_loss: 0.9124
Epoch 3360/5000
26/26 - 1s - loss: 0.4999 - val_loss: 0.9126
Epoch 03360: val_loss improved from 0.91380 to 0.91258, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 3361/5000
26/26 - 1s - loss: 0.4996 - val_loss: 0.9139
Epoch 3362/5000
26/26 - 1s - loss: 0.4997 - val_loss: 0.9133
Epoch 3363/5000
26/26 - 1s - loss: 0.5003 - val_loss: 0.9137
Epoch 3364/5000
26/26 - 1s - loss: 0.4994 - val_loss: 0.9136
Epoch 3365/5000
26/26 - 1s - loss: 0.4993 - val_loss: 0.9103
Epoch 3366/5000
26/26 - 1s - loss: 0.4990 - val_loss: 0.9128
Epoch 3367/5000
26/26 - 1s - loss: 0.4996 - val_loss: 0.9129
Epoch 3368/5000
26/26 - 1s - loss: 0.4992 - val_loss: 0.9138
Epoch 3369/5000
26/26 - 1s - loss: 0.4996 - val_loss: 0.9121
Epoch 3370/5000
26/26 - 1s - loss: 0.4986 - val_loss: 0.9121
Epoch 03370: val_loss improved from 0.91258 to 0.91210, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 3371/5000
26/26 - 1s - loss: 0.4990 - val_loss: 0.9111
Epoch 3372/5000
26/26 - 2s - loss: 0.4984 - val_loss: 0.9108
Epoch 3373/5000
26/26 - 1s - loss: 0.4993 - val_loss: 0.9111
Epoch 3374/5000
26/26 - 1s - loss: 0.4983 - val_loss: 0.9119
Epoch 3375/5000
26/26 - 1s - loss: 0.4982 - val_loss: 0.9130
Epoch 3376/5000
26/26 - 1s - loss: 0.4988 - val_loss: 0.9102
Epoch 3377/5000
26/26 - 1s - loss: 0.4982 - val_loss: 0.9127
Epoch 3378/5000
26/26 - 1s - loss: 0.4981 - val_loss: 0.9118
Epoch 3379/5000
26/26 - 1s - loss: 0.4981 - val_loss: 0.9113
Epoch 3380/5000
26/26 - 1s - loss: 0.4974 - val_loss: 0.9108
Epoch 03380: val_loss improved from 0.91210 to 0.91078, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 3381/5000
26/26 - 1s - loss: 0.4985 - val_loss: 0.9122
Epoch 3382/5000
26/26 - 1s - loss: 0.4980 - val_loss: 0.9111
Epoch 3383/5000
26/26 - 1s - loss: 0.4974 - val_loss: 0.9099
Epoch 3384/5000
26/26 - 1s - loss: 0.4982 - val_loss: 0.9113
Epoch 3385/5000
26/26 - 1s - loss: 0.4987 - val_loss: 0.9084
Epoch 3386/5000
26/26 - 1s - loss: 0.4976 - val_loss: 0.9087
Epoch 3387/5000
26/26 - 1s - loss: 0.4980 - val_loss: 0.9094
Epoch 3388/5000
26/26 - 1s - loss: 0.4972 - val_loss: 0.9095
Epoch 3389/5000
26/26 - 1s - loss: 0.4982 - val_loss: 0.9092
Epoch 3390/5000
26/26 - 1s - loss: 0.4977 - val_loss: 0.9115
Epoch 03390: val_loss did not improve from 0.91078
Epoch 3391/5000
26/26 - 1s - loss: 0.4966 - val_loss: 0.9102
Epoch 3392/5000
26/26 - 1s - loss: 0.4973 - val_loss: 0.9086
Epoch 3393/5000
26/26 - 1s - loss: 0.4965 - val_loss: 0.9099
Epoch 3394/5000
26/26 - 1s - loss: 0.4964 - val_loss: 0.9105
Epoch 3395/5000
26/26 - 1s - loss: 0.4959 - val_loss: 0.9084
Epoch 3396/5000
26/26 - 1s - loss: 0.4966 - val_loss: 0.9095
Epoch 3397/5000
26/26 - 1s - loss: 0.4960 - val_loss: 0.9104
Epoch 3398/5000
26/26 - 1s - loss: 0.4953 - val_loss: 0.9078
Epoch 3399/5000
26/26 - 1s - loss: 0.4961 - val_loss: 0.9095
Epoch 3400/5000
26/26 - 1s - loss: 0.4956 - val_loss: 0.9101
Epoch 03400: val_loss improved from 0.91078 to 0.91015, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 3401/5000
26/26 - 1s - loss: 0.4967 - val_loss: 0.9094
Epoch 3402/5000
26/26 - 1s - loss: 0.4959 - val_loss: 0.9090
Epoch 3403/5000
26/26 - 1s - loss: 0.4960 - val_loss: 0.9088
Epoch 3404/5000
26/26 - 1s - loss: 0.4955 - val_loss: 0.9082
Epoch 3405/5000
26/26 - 1s - loss: 0.4971 - val_loss: 0.9080
Epoch 3406/5000
26/26 - 1s - loss: 0.4944 - val_loss: 0.9075
Epoch 3407/5000
26/26 - 1s - loss: 0.4964 - val_loss: 0.9062
Epoch 3408/5000
26/26 - 1s - loss: 0.4954 - val_loss: 0.9070
Epoch 3409/5000
26/26 - 1s - loss: 0.4956 - val_loss: 0.9071
Epoch 3410/5000
26/26 - 1s - loss: 0.4946 - val_loss: 0.9083
Epoch 03410: val_loss improved from 0.91015 to 0.90833, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 3411/5000
26/26 - 1s - loss: 0.4942 - val_loss: 0.9097
Epoch 3412/5000
26/26 - 1s - loss: 0.4953 - val_loss: 0.9078
Epoch 3413/5000
26/26 - 1s - loss: 0.4943 - val_loss: 0.9094
Epoch 3414/5000
26/26 - 2s - loss: 0.4943 - val_loss: 0.9066
Epoch 3415/5000
26/26 - 2s - loss: 0.4935 - val_loss: 0.9080
Epoch 3416/5000
26/26 - 1s - loss: 0.4948 - val_loss: 0.9076
Epoch 3417/5000
26/26 - 1s - loss: 0.4951 - val_loss: 0.9073
Epoch 3418/5000
26/26 - 1s - loss: 0.4940 - val_loss: 0.9082
Epoch 3419/5000
26/26 - 1s - loss: 0.4943 - val_loss: 0.9075
Epoch 3420/5000
26/26 - 1s - loss: 0.4939 - val_loss: 0.9069
Epoch 03420: val_loss improved from 0.90833 to 0.90686, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 3421/5000
26/26 - 1s - loss: 0.4948 - val_loss: 0.9078
Epoch 3422/5000
26/26 - 1s - loss: 0.4946 - val_loss: 0.9053
Epoch 3423/5000
26/26 - 1s - loss: 0.4932 - val_loss: 0.9059
Epoch 3424/5000
26/26 - 1s - loss: 0.4936 - val_loss: 0.9060
Epoch 3425/5000
26/26 - 1s - loss: 0.4937 - val_loss: 0.9060
Epoch 3426/5000
26/26 - 1s - loss: 0.4936 - val_loss: 0.9045
Epoch 3427/5000
26/26 - 1s - loss: 0.4922 - val_loss: 0.9061
Epoch 3428/5000
26/26 - 1s - loss: 0.4929 - val_loss: 0.9052
Epoch 3429/5000
26/26 - 1s - loss: 0.4919 - val_loss: 0.9055
Epoch 3430/5000
26/26 - 1s - loss: 0.4931 - val_loss: 0.9062
Epoch 03430: val_loss improved from 0.90686 to 0.90621, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 3431/5000
26/26 - 1s - loss: 0.4924 - val_loss: 0.9062
Epoch 3432/5000
26/26 - 1s - loss: 0.4929 - val_loss: 0.9046
Epoch 3433/5000
26/26 - 1s - loss: 0.4934 - val_loss: 0.9050
Epoch 3434/5000
26/26 - 1s - loss: 0.4926 - val_loss: 0.9061
Epoch 3435/5000
26/26 - 1s - loss: 0.4927 - val_loss: 0.9062
Epoch 3436/5000
26/26 - 1s - loss: 0.4927 - val_loss: 0.9058
Epoch 3437/5000
26/26 - 1s - loss: 0.4919 - val_loss: 0.9050
Epoch 3438/5000
26/26 - 1s - loss: 0.4913 - val_loss: 0.9046
Epoch 3439/5000
26/26 - 1s - loss: 0.4927 - val_loss: 0.9046
Epoch 3440/5000
26/26 - 1s - loss: 0.4916 - val_loss: 0.9052
Epoch 03440: val_loss improved from 0.90621 to 0.90522, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 3441/5000
26/26 - 1s - loss: 0.4915 - val_loss: 0.9030
Epoch 3442/5000
26/26 - 1s - loss: 0.4912 - val_loss: 0.9034
Epoch 3443/5000
26/26 - 1s - loss: 0.4904 - val_loss: 0.9049
Epoch 3444/5000
26/26 - 1s - loss: 0.4914 - val_loss: 0.9061
Epoch 3445/5000
26/26 - 1s - loss: 0.4915 - val_loss: 0.9061
Epoch 3446/5000
26/26 - 1s - loss: 0.4909 - val_loss: 0.9043
Epoch 3447/5000
26/26 - 1s - loss: 0.4909 - val_loss: 0.9029
Epoch 3448/5000
26/26 - 1s - loss: 0.4913 - val_loss: 0.9041
Epoch 3449/5000
26/26 - 1s - loss: 0.4911 - val_loss: 0.9054
Epoch 3450/5000
26/26 - 1s - loss: 0.4910 - val_loss: 0.9056
Epoch 03450: val_loss did not improve from 0.90522
Epoch 3451/5000
26/26 - 1s - loss: 0.4902 - val_loss: 0.9058
Epoch 3452/5000
26/26 - 1s - loss: 0.4921 - val_loss: 0.9042
Epoch 3453/5000
26/26 - 1s - loss: 0.4909 - val_loss: 0.9055
Epoch 3454/5000
26/26 - 2s - loss: 0.4895 - val_loss: 0.9031
Epoch 3455/5000
26/26 - 2s - loss: 0.4898 - val_loss: 0.9054
Epoch 3456/5000
26/26 - 1s - loss: 0.4910 - val_loss: 0.9040
Epoch 3457/5000
26/26 - 1s - loss: 0.4901 - val_loss: 0.9042
Epoch 3458/5000
26/26 - 1s - loss: 0.4900 - val_loss: 0.9032
Epoch 3459/5000
26/26 - 1s - loss: 0.4889 - val_loss: 0.9045
Epoch 3460/5000
26/26 - 1s - loss: 0.4906 - val_loss: 0.9034
Epoch 03460: val_loss improved from 0.90522 to 0.90337, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-4.model.weights.hdf5
Epoch 3461/5000
26/26 - 1s - loss: 0.4898 - val_loss: 0.9049
Restoring model weights from the end of the best epoch.
Epoch 03461: early stopping
INFO     Computation time for training the single-label model for AR: 84.07 min
INFO     Network type: REG
INFO     Model: "sequential"
INFO     _________________________________________________________________
INFO     Layer (type)                 Output Shape              Param #
INFO     =================================================================
INFO     dense (Dense)                (None, 1024)              2098176
INFO     _________________________________________________________________
INFO     dropout (Dropout)            (None, 1024)              0
INFO     _________________________________________________________________
INFO     dense_1 (Dense)              (None, 512)               524800
INFO     _________________________________________________________________
INFO     dropout_1 (Dropout)          (None, 512)               0
INFO     _________________________________________________________________
INFO     dense_2 (Dense)              (None, 256)               131328
INFO     _________________________________________________________________
INFO     dropout_2 (Dropout)          (None, 256)               0
INFO     _________________________________________________________________
INFO     dense_3 (Dense)              (None, 128)               32896
INFO     _________________________________________________________________
INFO     dropout_3 (Dropout)          (None, 128)               0
INFO     _________________________________________________________________
INFO     dense_4 (Dense)              (None, 1)                 129
INFO     =================================================================
INFO     Total params: 2,787,329
INFO     Trainable params: 2,787,329
INFO     Non-trainable params: 0
INFO     _________________________________________________________________
INFO     Evaluating trained model 'AR single-labeled Fold-4' on test data
INFO     Training of fold number: 5
INFO     Training sample distribution: train data: {-1.2016366720199585: 7, -1.201635479927063: 4, -1.20163094997406: 4, -1.2016324996948242: 4, -1.2016353607177734: 4, -1.2016384601593018: 3, -1.2016327381134033: 3, -1.201636552810669: 3, -1.2016363143920898: 3, -1.2016304731369019: 3, -1.2016383409500122: 3, -1.2016377449035645: 3, -1.201637625694275: 3, -1.2016375064849854: 2, -1.201636791229248: 2, -1.2016351222991943: 2, -1.2016191482543945: 2, -1.2016041278839111: 2, -1.2016342878341675: 2, -1.201622486114502: 2, -1.2016302347183228: 2, -1.2016369104385376: 2, -1.2016339302062988: 2, -1.201621651649475: 2, -1.2016355991363525: 2, -1.2016347646713257: 2, -1.2016253471374512: 2, -1.201635718345642: 2, -1.2016288042068481: 2, -1.2016159296035767: 2, -0.37317758798599243: 1, 1.6089627742767334: 1, 0.8664619326591492: 1, 0.32629087567329407: 1, -1.0201867818832397: 1, -0.5348793864250183: 1, -0.3040960133075714: 1, 1.415509581565857: 1, 0.663663923740387: 1, 1.4251669645309448: 1, 0.9998847246170044: 1, 1.2753889560699463: 1, -0.18418414890766144: 1, 1.3083291053771973: 1, 0.8396581411361694: 1, 1.6950387954711914: 1, 0.21748410165309906: 1, 0.2609155476093292: 1, -0.17784874141216278: 1, -0.30124133825302124: 1, 0.1385408341884613: 1, 0.04832748696208: 1, 1.595760464668274: 1, 0.8440163731575012: 1, -0.5706648826599121: 1, 0.17204155027866364: 1, -0.4307803809642792: 1, 0.20332399010658264: 1, 0.36003923416137695: 1, 0.06883639097213745: 1, 1.605971336364746: 1, 0.2661615014076233: 1, 0.22961212694644928: 1, 0.2954026460647583: 1, 1.5088152885437012: 1, 0.26908448338508606: 1, -0.6546655297279358: 1, -0.9010018706321716: 1, -1.1963087320327759: 1, -0.16027171909809113: 1, -0.425843745470047: 1, 0.5299685597419739: 1, 0.2713952362537384: 1, 1.4458893537521362: 1, 0.06667295098304749: 1, -0.4970245659351349: 1, 0.3489625155925751: 1, -0.3774075210094452: 1, -0.16336557269096375: 1, -0.31709083914756775: 1, 0.08119866997003555: 1, -0.35442060232162476: 1, 0.22109845280647278: 1, 0.8313724994659424: 1, -0.41311657428741455: 1, -1.201627492904663: 1, 0.28807583451271057: 1, 0.6573249697685242: 1, 0.4933412969112396: 1, 0.3488617241382599: 1, -1.2016195058822632: 1, 0.31096193194389343: 1, 0.5601941347122192: 1, 0.5732383131980896: 1, 1.6106586456298828: 1, 1.168498158454895: 1, -1.0873279571533203: 1, -0.2516374886035919: 1, 0.10525540262460709: 1, 1.4158756732940674: 1, 0.3664189279079437: 1, -0.4459679424762726: 1, 1.1870161294937134: 1, 0.5256680846214294: 1, 1.4836735725402832: 1, 0.6158161163330078: 1, 1.3986883163452148: 1, 0.831580638885498: 1, 1.4535586833953857: 1, 1.4237861633300781: 1, -0.6966411471366882: 1, 0.6442342400550842: 1, 1.2952117919921875: 1, 0.6829988956451416: 1, 1.5720155239105225: 1, 1.571059226989746: 1, -0.026365874335169792: 1, 0.643775999546051: 1, 0.20390741527080536: 1, 0.7471779584884644: 1, -1.115770697593689: 1, 1.090896725654602: 1, 1.1508636474609375: 1, 1.0995265245437622: 1, 0.34950539469718933: 1, 0.9149655103683472: 1, -0.040320102125406265: 1, -1.1746125221252441: 1, 0.3443826735019684: 1, 1.286658525466919: 1, 1.5370275974273682: 1, 0.7171676754951477: 1, 0.7614589929580688: 1, 1.4500402212142944: 1, -0.24653010070323944: 1, 1.088638186454773: 1, 1.5124294757843018: 1, -0.22351212799549103: 1, 0.9285130500793457: 1, 0.5170465111732483: 1, 0.912930965423584: 1, -0.21738800406455994: 1, 0.8274267315864563: 1, 1.0551327466964722: 1, 0.394859254360199: 1, -0.004194003064185381: 1, -1.201627254486084: 1, 0.23328566551208496: 1, -0.030014334246516228: 1, -0.4297850430011749: 1, 1.6003168821334839: 1, -0.20539666712284088: 1, 0.7950616478919983: 1, -0.059458885341882706: 1, -0.22176611423492432: 1, 0.13578376173973083: 1, -0.4253336787223816: 1, 0.2726168930530548: 1, -0.02133699133992195: 1, 0.03207547590136528: 1, 1.5805106163024902: 1, 0.4048600494861603: 1, 0.637361466884613: 1, 1.3996703624725342: 1, 1.7721431255340576: 1, -1.1701372861862183: 1, 1.457643747329712: 1, -0.10739652067422867: 1, 1.4923255443572998: 1, 0.10213274508714676: 1, -0.17338967323303223: 1, 0.9650787115097046: 1, -0.9669303297996521: 1, 1.1683435440063477: 1, 0.31522658467292786: 1, 0.695344090461731: 1, 0.21625953912734985: 1, 0.2743425965309143: 1, 0.2500914931297302: 1, 1.9054079055786133: 1, -0.31671836972236633: 1, 0.29451489448547363: 1, 0.36155056953430176: 1, 0.25783771276474: 1, -0.4993174076080322: 1, 0.7279888391494751: 1, 0.7412875294685364: 1, 0.015656888484954834: 1, -0.3344474732875824: 1, 1.5035943984985352: 1, 0.5403873920440674: 1, 1.2994223833084106: 1, 0.3245508372783661: 1, 1.2997812032699585: 1, -0.48075738549232483: 1, -0.13643299043178558: 1, 0.49536043405532837: 1, 1.2379846572875977: 1, 0.23716896772384644: 1, -0.10035426914691925: 1, 0.0010552277090027928: 1, -1.189130187034607: 1, 1.4424492120742798: 1, 0.31334978342056274: 1, -0.20045150816440582: 1, 0.8569538593292236: 1, -0.2393776774406433: 1, 0.8028292059898376: 1, 0.6465133428573608: 1, 0.7402693033218384: 1, -1.0873202085494995: 1, 0.540539026260376: 1, 0.10738043487071991: 1, 1.4755654335021973: 1, 1.166581153869629: 1, -0.6442912220954895: 1, 1.5410983562469482: 1, -0.9223102331161499: 1, 0.5436715483665466: 1, -1.1639471054077148: 1, -1.2007761001586914: 1, -1.045175313949585: 1, -1.197718858718872: 1, -0.7745821475982666: 1, -0.9927355051040649: 1, -1.196489930152893: 1, -1.1964974403381348: 1, -1.1945018768310547: 1, -1.1871466636657715: 1, -1.197901725769043: 1, -1.1866750717163086: 1, -1.1664562225341797: 1, -1.1972938776016235: 1, 0.005114047322422266: 1, -1.1962217092514038: 1, -1.1939657926559448: 1, -1.1927686929702759: 1, -1.2016290426254272: 1, -1.2008297443389893: 1, -1.1980621814727783: 1, -1.2007057666778564: 1, -1.1476235389709473: 1, -1.1962871551513672: 1, -1.2000701427459717: 1, -1.2015609741210938: 1, -0.2831217646598816: 1, 0.7523799538612366: 1, -1.201596975326538: 1, -1.201615810394287: 1, -1.186226725578308: 1, -1.2014594078063965: 1, -0.27123570442199707: 1, 0.5888639092445374: 1, -1.1996524333953857: 1, 0.39954620599746704: 1, -1.1674489974975586: 1, -1.201310396194458: 1, 1.14208984375: 1, -0.9657180905342102: 1, -1.1987498998641968: 1, 0.4001854956150055: 1, -0.512914776802063: 1, 0.034827083349227905: 1, 1.520749807357788: 1, -1.1323087215423584: 1, -1.198028802871704: 1, -1.201625108718872: 1, -1.1205617189407349: 1, 0.5479292273521423: 1, -0.27864202857017517: 1, -0.5042855143547058: 1, -0.32787415385246277: 1, 0.2115481197834015: 1, -1.177890419960022: 1, -0.8235211968421936: 1, 0.2787120044231415: 1, -0.683555543422699: 1, -0.30772721767425537: 1, 0.5139665007591248: 1, -0.3334684669971466: 1, -0.9769929647445679: 1, 0.7675377726554871: 1, -0.4147615134716034: 1, 1.6281145811080933: 1, -0.883184015750885: 1, 0.0290671493858099: 1, -0.6085047125816345: 1, 1.0926192998886108: 1, 0.2407364845275879: 1, 0.21325090527534485: 1, -0.5222632884979248: 1, -0.29486238956451416: 1, -1.1962995529174805: 1, -0.39423879981040955: 1, -1.1956098079681396: 1, -1.1269394159317017: 1, -0.7543148398399353: 1, -0.15980762243270874: 1, -0.8842195272445679: 1, -0.7196366786956787: 1, -0.2302703857421875: 1, -0.5718616843223572: 1, 1.7539664506912231: 1, -0.005905755329877138: 1, 1.1079081296920776: 1, -1.1402051448822021: 1, -0.07565759867429733: 1, -0.26343750953674316: 1, 1.4295574426651: 1, -0.00951747503131628: 1, 1.4342314004898071: 1, -0.09802207350730896: 1, 0.9686956405639648: 1, 0.28549933433532715: 1, -0.5639210939407349: 1, -0.06465810537338257: 1, -0.4154701232910156: 1, 1.5978947877883911: 1, 1.1153340339660645: 1, -0.1421377956867218: 1, 1.4907145500183105: 1, 1.6047093868255615: 1, 0.007752139586955309: 1, -1.1937233209609985: 1, -1.2012096643447876: 1, -1.1647279262542725: 1, -1.2015366554260254: 1, 0.37413451075553894: 1, 0.16982176899909973: 1, -0.8657602667808533: 1, -0.6204319000244141: 1, -1.2013626098632812: 1, -1.201349139213562: 1, -0.46576231718063354: 1, 0.339458167552948: 1, -1.137963891029358: 1, -0.9888867139816284: 1, -1.092740535736084: 1, -1.2016162872314453: 1, -0.3419588804244995: 1, 0.44327667355537415: 1, 0.9703378081321716: 1, 0.6741006970405579: 1, 0.32263097167015076: 1, -0.3203604817390442: 1, 1.1970174312591553: 1, -0.09881063550710678: 1, 0.2669691741466522: 1, -0.29657527804374695: 1, 0.699573814868927: 1, 0.21768306195735931: 1, 1.3818247318267822: 1, 0.21878471970558167: 1, 1.3501269817352295: 1, 0.34516385197639465: 1, 0.11128426343202591: 1, -0.5620038509368896: 1, 0.007953275926411152: 1, -0.5027830004692078: 1, -0.24468590319156647: 1, 1.478103518486023: 1, -0.399366557598114: 1, -1.1840753555297852: 1, 1.3625078201293945: 1, 0.12603989243507385: 1, -1.1730265617370605: 1, 0.9611315131187439: 1, 0.1561044156551361: 1, -1.1959139108657837: 1, -1.2015589475631714: 1, -1.2016221284866333: 1, 0.812082827091217: 1, -1.201629400253296: 1, -0.21383443474769592: 1, -1.1999688148498535: 1, -1.2006030082702637: 1, -1.201606273651123: 1, 0.6647039651870728: 1, 0.46835482120513916: 1, -1.2016280889511108: 1, -1.201348900794983: 1, -1.1382324695587158: 1, -1.1506352424621582: 1, -0.8027163147926331: 1, -1.201614260673523: 1, -1.2002341747283936: 1, -1.2014697790145874: 1, -1.2014739513397217: 1, -0.7784246206283569: 1, -1.0070439577102661: 1, -1.2015974521636963: 1, -1.201509714126587: 1, 0.031881630420684814: 1, -1.2004907131195068: 1, -1.2016046047210693: 1, -0.4260459542274475: 1, -0.506174623966217: 1, -0.3525408208370209: 1, -1.180529236793518: 1, -1.2015936374664307: 1, 0.8243502378463745: 1, -1.1526696681976318: 1, -1.2014310359954834: 1, -1.0348321199417114: 1, 0.603252649307251: 1, -0.4781683385372162: 1, -1.2015955448150635: 1, -1.2016127109527588: 1, -1.201597809791565: 1, 0.10008653253316879: 1, -0.9153497219085693: 1, 0.1308048814535141: 1, -0.1883729249238968: 1, -1.055851936340332: 1, -0.5025617480278015: 1, 0.3411409258842468: 1, 0.4633817672729492: 1, 1.4536134004592896: 1, 1.8447388410568237: 1, 0.7908697128295898: 1, 0.10187211632728577: 1, -0.9047161340713501: 1, -1.1412583589553833: 1, -1.0422825813293457: 1, -0.6316778063774109: 1, 1.4887964725494385: 1, -0.8920286297798157: 1, 1.788353681564331: 1, 0.43905025720596313: 1, -1.1907743215560913: 1, -0.18136551976203918: 1, 0.6965684294700623: 1, -1.1941906213760376: 1, -0.26542410254478455: 1, -1.0395952463150024: 1, -0.3497014045715332: 1, -0.9432769417762756: 1, -1.0090559720993042: 1, 1.3791615962982178: 1, 1.577986240386963: 1, 1.3899286985397339: 1, -0.5884833335876465: 1, -0.5754680633544922: 1, -1.1041064262390137: 1, -1.197619080543518: 1, -0.2147151529788971: 1, -0.9940837025642395: 1, -1.0393953323364258: 1, 1.3862724304199219: 1, 1.6304298639297485: 1, -0.8935588002204895: 1, -1.0519613027572632: 1, -1.18907630443573: 1, -1.0768063068389893: 1, -0.7096079587936401: 1, -0.2589719295501709: 1, -1.1560314893722534: 1, -1.2003082036972046: 1, -0.4400210678577423: 1, -1.1370965242385864: 1, -0.8680421113967896: 1, -1.193503737449646: 1, -0.45158419013023376: 1, -1.1848548650741577: 1, -0.6980454921722412: 1, -1.188598394393921: 1, 1.192413568496704: 1, -1.1573199033737183: 1, -1.098894715309143: 1, -0.24214474856853485: 1, 0.9790754318237305: 1, -1.1999518871307373: 1, 0.06941241025924683: 1, 1.9090871810913086: 1, 1.5657020807266235: 1, -1.1800106763839722: 1, -1.1358855962753296: 1, -0.5620161294937134: 1, -1.18631112575531: 1, -0.3605387806892395: 1, 0.292474627494812: 1, 0.20924636721611023: 1, -0.3594827950000763: 1, -0.5120261311531067: 1, -0.43377333879470825: 1, 0.6492028832435608: 1, 0.5685755014419556: 1, 0.46832725405693054: 1, 0.20812031626701355: 1, -1.0893759727478027: 1, 0.8691079020500183: 1, -1.1004241704940796: 1, 0.5786248445510864: 1, -1.0407896041870117: 1, -0.12463472783565521: 1, 0.19889964163303375: 1, 1.7476170063018799: 1, -1.1312958002090454: 1, -1.2013144493103027: 1, -0.41735291481018066: 1, 1.5167564153671265: 1, -0.2154475301504135: 1, -0.8993450403213501: 1, -1.1072322130203247: 1, -1.1329883337020874: 1, 1.8358889818191528: 1, -0.8241826295852661: 1, -1.0799649953842163: 1, -1.1919200420379639: 1, -0.07864277809858322: 1, -0.04164140671491623: 1, -0.8634552955627441: 1, -0.9751180410385132: 1, -1.0622771978378296: 1, -1.1442792415618896: 1, 0.2268732339143753: 1, -0.5990430116653442: 1, 0.15106460452079773: 1, -1.2012841701507568: 1, -0.9642779231071472: 1, 0.35480692982673645: 1, 0.5211433172225952: 1, -0.08619289845228195: 1, -0.9803085327148438: 1, 0.5575955510139465: 1, 0.4876076281070709: 1, 1.0163378715515137: 1, -0.9919153451919556: 1, -0.233070969581604: 1, -1.1343045234680176: 1, -0.8108161091804504: 1, 1.4876383543014526: 1, -0.9951556921005249: 1, 1.4807751178741455: 1, -1.0858170986175537: 1, -0.90742427110672: 1, -1.024053692817688: 1, 0.6749281883239746: 1, -0.7701697945594788: 1, -0.5982488989830017: 1, -0.3465023636817932: 1, -0.3573164939880371: 1, 1.3890480995178223: 1, -1.0539400577545166: 1, -1.0776159763336182: 1, -0.8936908841133118: 1, -1.194373369216919: 1, 1.4680920839309692: 1, 0.31239357590675354: 1, 0.3124358654022217: 1, -0.20395949482917786: 1, -0.04237562045454979: 1, 0.9760450720787048: 1, -0.13872799277305603: 1, 0.3074534237384796: 1, -1.1716605424880981: 1, -1.1579349040985107: 1, 0.46070119738578796: 1, -0.9134752750396729: 1, 0.49092090129852295: 1, -0.49170398712158203: 1, -0.8270519375801086: 1, -0.04211708903312683: 1, -0.974824070930481: 1, -0.9460977911949158: 1, -0.7166287899017334: 1, -1.19014310836792: 1, -0.67027348279953: 1, 1.8432141542434692: 1, -0.8671839237213135: 1, -1.1605626344680786: 1, -0.8580590486526489: 1, -0.7244925498962402: 1, -1.2001420259475708: 1, -0.8962797522544861: 1, -1.199107050895691: 1, 0.30057549476623535: 1, 0.9532740116119385: 1, 0.8790757060050964: 1, -0.9627416729927063: 1, -0.608140766620636: 1, -1.0076677799224854: 1, 1.0116826295852661: 1, -0.9982122778892517: 1, -1.1845873594284058: 1, -1.1898142099380493: 1, -1.2009141445159912: 1, -1.0507465600967407: 1, -1.1325860023498535: 1, 1.8212419748306274: 1, -0.36594024300575256: 1, -0.9299888610839844: 1, -1.0714704990386963: 1, -0.4354245066642761: 1, -1.1712636947631836: 1, -1.1646332740783691: 1, -0.09378552436828613: 1, 0.025435535237193108: 1, -1.1946264505386353: 1, 0.3200169503688812: 1, -0.8890491724014282: 1, 0.6293842196464539: 1, -0.8917420506477356: 1, 1.1045739650726318: 1, 0.04404761642217636: 1, -1.1212905645370483: 1, 1.4690353870391846: 1, 1.4842547178268433: 1, -1.1791499853134155: 1, 0.2662027180194855: 1, 1.536348581314087: 1, 1.8480533361434937: 1, 1.0025136470794678: 1, 1.6904795169830322: 1, -1.0221163034439087: 1, -0.1391475349664688: 1, -1.094030499458313: 1, 0.5474697947502136: 1, -0.7628646492958069: 1, -0.8923998475074768: 1, 1.0554637908935547: 1, -0.26597675681114197: 1, -1.0213873386383057: 1, 1.281778335571289: 1, 0.8741052150726318: 1, 1.3615977764129639: 1, -0.9881011247634888: 1, -1.2016340494155884: 1, 0.8268551230430603: 1, -0.7180438041687012: 1, 1.0402084589004517: 1, -0.6611031889915466: 1, -0.7170498371124268: 1, 0.6606081128120422: 1, 0.8417104482650757: 1, 0.8892757296562195: 1, 0.02830575592815876: 1, 1.0541952848434448: 1, 1.2882025241851807: 1, -0.01062939316034317: 1, 1.0303751230239868: 1, 1.5870535373687744: 1, 0.11351441591978073: 1, 1.0355088710784912: 1, -1.107023000717163: 1, -0.16252407431602478: 1, 0.9657272696495056: 1, 0.6718612909317017: 1, 0.9499619007110596: 1, -1.1798655986785889: 1, 0.10751932114362717: 1, -0.539240837097168: 1, 0.2634084224700928: 1, -0.021469445899128914: 1, -0.589444637298584: 1, 1.053705096244812: 1, 1.001185417175293: 1, 0.6039040088653564: 1, 0.3286954462528229: 1, 0.46373531222343445: 1, -0.016411839053034782: 1, -0.5022063255310059: 1, 0.16298139095306396: 1, 0.31799715757369995: 1, 0.1881372481584549: 1, 0.16384904086589813: 1, 0.003300704760476947: 1, 0.2808535397052765: 1, 0.2640135586261749: 1, 1.3437533378601074: 1, -0.26631277799606323: 1, 0.9666041731834412: 1, 1.4656307697296143: 1, 1.2950940132141113: 1, 1.210314393043518: 1, 0.5456136465072632: 1, -0.8510425090789795: 1, 0.36215391755104065: 1, -0.5825971961021423: 1, -0.16857680678367615: 1, 0.7533062696456909: 1, 1.5823100805282593: 1, 1.3024239540100098: 1, 0.9470359086990356: 1, 1.2427196502685547: 1, -0.5881722569465637: 1, 0.71575528383255: 1, -0.930620014667511: 1, -0.16512484848499298: 1, 1.0202414989471436: 1, 1.1998642683029175: 1, 0.22981515526771545: 1, 1.3784377574920654: 1, 0.7611046433448792: 1, -1.113705039024353: 1, 0.7825468182563782: 1, 1.4694101810455322: 1, 0.6321052312850952: 1, 1.427628755569458: 1, -0.06996402144432068: 1, -0.08030800521373749: 1, 0.4723914861679077: 1, 0.8504006266593933: 1, 0.1011820137500763: 1, 0.6780659556388855: 1, 1.495969295501709: 1, 1.3494455814361572: 1, 1.0170906782150269: 1, 0.6696186065673828: 1, -0.6162902116775513: 1, 1.3002121448516846: 1, 1.4811328649520874: 1, -0.15746326744556427: 1, -0.25767362117767334: 1, 0.12876805663108826: 1, 1.9331234693527222: 1, 0.465168297290802: 1, 0.2403424084186554: 1, -0.5288078188896179: 1, 1.224327802658081: 1, -0.9218448400497437: 1, 1.1008855104446411: 1, 1.4938230514526367: 1, 1.1383615732192993: 1, 0.3698660731315613: 1, -0.18478137254714966: 1, 0.9005318284034729: 1, 1.7851945161819458: 1, -0.8747767210006714: 1, -0.731924295425415: 1, -0.05734042823314667: 1, -0.45086827874183655: 1, -1.0084080696105957: 1, -0.9050359129905701: 1, -0.5784834027290344: 1, -0.9131625890731812: 1, 1.1575602293014526: 1, 0.40835040807724: 1, -0.8622487187385559: 1, -0.48030614852905273: 1, 1.2659807205200195: 1, -0.8956496715545654: 1, -0.8754668831825256: 1, -0.2101057469844818: 1, -0.38419657945632935: 1, -0.6876721978187561: 1, -0.27164560556411743: 1, 1.6284458637237549: 1, -0.09434226900339127: 1, -0.0960833728313446: 1, -1.1873440742492676: 1, 0.9346560835838318: 1, -0.5242934823036194: 1, -1.199570655822754: 1, -0.4367877244949341: 1, -0.8407037854194641: 1, -1.1983946561813354: 1, -0.5144169330596924: 1, -0.7376867532730103: 1, -1.1570571660995483: 1, -0.700495719909668: 1, -0.9755853414535522: 1, -0.5850008726119995: 1, -1.1196062564849854: 1, -1.17500901222229: 1, -1.1970343589782715: 1, -0.9793020486831665: 1, 1.324471354484558: 1, -1.2006902694702148: 1, -1.0523524284362793: 1, -0.5724524259567261: 1, -0.7811231017112732: 1, 0.784543514251709: 1, -0.748826265335083: 1, 0.22341269254684448: 1, 1.1112544536590576: 1, 1.9067574739456177: 1, 0.3255096673965454: 1, -0.18094922602176666: 1, 0.002877143444493413: 1, 1.4062000513076782: 1, -0.2782626748085022: 1, -0.047685928642749786: 1, -0.19720852375030518: 1, 0.3313300311565399: 1, 0.7724436521530151: 1, 1.3262649774551392: 1, -0.11414719372987747: 1, 0.3607413172721863: 1, -0.10225572437047958: 1, 0.7951827049255371: 1, 1.366266131401062: 1, 0.9568199515342712: 1, -0.16261765360832214: 1, 0.1897212415933609: 1, 0.900846004486084: 1, 0.32160520553588867: 1, -0.8486149311065674: 1, 0.12815426290035248: 1, 1.7614071369171143: 1, -0.16177639365196228: 1, 0.03332117572426796: 1, 0.739153265953064: 1, 0.7527873516082764: 1, 0.2964378595352173: 1, 0.07571198046207428: 1, -0.8877851366996765: 1, 1.6006011962890625: 1, -0.9158876538276672: 1, -1.1895103454589844: 1, -1.1913617849349976: 1, -1.0719594955444336: 1, 0.35307395458221436: 1, -0.8234555125236511: 1, -1.1507015228271484: 1, -0.9339156150817871: 1, -1.1607003211975098: 1, 1.9238320589065552: 1, 1.1217374801635742: 1, -1.1969212293624878: 1, -0.13197508454322815: 1, -1.07969331741333: 1, 0.1708119511604309: 1, -0.23679472506046295: 1, -0.14438967406749725: 1, -0.4185396134853363: 1, 0.6560998558998108: 1, -0.116549052298069: 1, 1.7107861042022705: 1, -1.1264077425003052: 1, -0.962668240070343: 1, -0.26848453283309937: 1, 0.5131713151931763: 1, 1.76934015750885: 1, -1.2016271352767944: 1, 0.9834553003311157: 1, -1.2006548643112183: 1, -1.0099024772644043: 1, 1.5100682973861694: 1, -1.2016189098358154: 1, 1.0540943145751953: 1, -1.2013123035430908: 1, -0.2686515748500824: 1, 1.8881990909576416: 1, -1.061113953590393: 1, 1.7208062410354614: 1, 0.11243647336959839: 1, -1.2014974355697632: 1, 1.2202951908111572: 1, -0.4105451703071594: 1, 1.280470848083496: 1, -0.807515025138855: 1, -0.4567924439907074: 1, 1.7558945417404175: 1, -0.7435175180435181: 1, 1.4320223331451416: 1, 0.5438616275787354: 1, 1.5315228700637817: 1, 0.591416597366333: 1, 0.27008119225502014: 1, 0.24442099034786224: 1, 0.7021965980529785: 1, 0.27857378125190735: 1, 1.1086541414260864: 1, -0.025178229436278343: 1, 1.7298698425292969: 1, 1.4182209968566895: 1, -0.2232077419757843: 1, 0.014584558084607124: 1, 1.891126036643982: 1, 0.19680047035217285: 1, 0.8425688147544861: 1, -1.0590986013412476: 1, 1.9098440408706665: 1, 1.6939563751220703: 1, 0.2675780653953552: 1, 1.6441566944122314: 1, 0.28526046872138977: 1, -1.2016263008117676: 1, 0.18436919152736664: 1, -0.253825306892395: 1, -1.088794231414795: 1, -0.7490406036376953: 1, 0.10260368138551712: 1, 1.4280599355697632: 1, -0.2810556888580322: 1, -0.5811882019042969: 1, 1.2709132432937622: 1, 0.804813027381897: 1, 0.6003371477127075: 1, 0.9758270978927612: 1, 1.8529928922653198: 1, 1.3301595449447632: 1, -0.44384631514549255: 1, 1.7166484594345093: 1, 0.5419895052909851: 1, -1.0994056463241577: 1, 1.0374422073364258: 1, 1.901644229888916: 1, 0.5713223814964294: 1, 1.3496158123016357: 1, 1.9196945428848267: 1, 1.9020731449127197: 1, 0.5880253314971924: 1, -0.8662946820259094: 1, 1.5722275972366333: 1, -1.2002416849136353: 1, 1.6493014097213745: 1, -0.9635469913482666: 1, -0.7447215914726257: 1, -0.35984915494918823: 1, 1.1998889446258545: 1, -0.9704957008361816: 1, -1.102870225906372: 1, 1.5992790460586548: 1, -0.9806917309761047: 1, -1.2016212940216064: 1, 0.454349547624588: 1, -0.5300003886222839: 1, 1.2889325618743896: 1, 0.35712626576423645: 1, -0.864342451095581: 1, 1.9024626016616821: 1, 1.6134487390518188: 1, 1.5396013259887695: 1, -1.1658029556274414: 1, -1.2016136646270752: 1, -1.0555497407913208: 1, 0.9952530264854431: 1, 0.26564425230026245: 1, 1.8953936100006104: 1, 0.03225273638963699: 1, 0.43645578622817993: 1, -0.2772659659385681: 1, 1.6143009662628174: 1, 0.04922454059123993: 1, 0.2889866232872009: 1, 0.766596257686615: 1, -0.22210338711738586: 1, 0.9173278212547302: 1, 0.14515815675258636: 1, -0.03429622948169708: 1, 1.545008897781372: 1, -0.4220041036605835: 1, 1.590468406677246: 1, 1.3033519983291626: 1, 0.5755087733268738: 1, 0.19334031641483307: 1, 1.8893579244613647: 1, -1.0097246170043945: 1, 0.39559683203697205: 1, -0.1754150390625: 1, -0.03317539766430855: 1, 1.7539278268814087: 1, 0.26557838916778564: 1, 1.1236602067947388: 1, -0.7293344736099243: 1, 0.81379234790802: 1, -0.3100615441799164: 1, -0.22727444767951965: 1, 1.692647099494934: 1, 1.8799982070922852: 1, 1.2042120695114136: 1, -0.15924076735973358: 1, 1.5299123525619507: 1, 1.9158005714416504: 1, 1.7392624616622925: 1, 0.4917255938053131: 1, 1.6179100275039673: 1, 0.11675674468278885: 1, 0.16001862287521362: 1, -0.22299611568450928: 1, 1.6735926866531372: 1, 0.0023630079813301563: 1, 0.8739101886749268: 1, 0.24924464523792267: 1, -0.7589280009269714: 1, -0.03557446971535683: 1, 0.7833142876625061: 1, 1.0263571739196777: 1, -0.4791189730167389: 1, 0.8505056500434875: 1, 0.1083393469452858: 1, 1.7069745063781738: 1, 1.7395148277282715: 1, 1.0135881900787354: 1, -0.31705647706985474: 1, 0.9889004230499268: 1, 0.8423707485198975: 1, 0.5330069065093994: 1, 1.0885628461837769: 1, -0.19816404581069946: 1, 1.612230896949768: 1, -0.3399538993835449: 1, 0.9761800765991211: 1, 0.723430871963501: 1, 0.9554869532585144: 1, 0.7741647958755493: 1, 0.6886505484580994: 1, 0.5059344172477722: 1, 1.8263726234436035: 1, -0.04758370667695999: 1, 1.7850080728530884: 1, -0.2531147599220276: 1, 0.9327585697174072: 1, 1.865704894065857: 1, 0.949316143989563: 1, 1.3716888427734375: 1, -0.0515512116253376: 1, 0.29674577713012695: 1, 0.7860816717147827: 1, 0.48093563318252563: 1, -0.2709258496761322: 1, 1.0254307985305786: 1, -0.12864308059215546: 1, -0.180640310049057: 1, -0.6814844608306885: 1, 0.4076603651046753: 1, 1.6906383037567139: 1, 1.5815212726593018: 1, 1.7223035097122192: 1, -0.22097758948802948: 1, 0.34220972657203674: 1, 0.6320856809616089: 1, 1.9327584505081177: 1, 0.6905925273895264: 1, 1.4221618175506592: 1, 1.4504951238632202: 1, -1.054260492324829: 1, -0.10395042598247528: 1, -1.2016111612319946: 1, -1.0791629552841187: 1, -0.5369133949279785: 1, 0.38025134801864624: 1, -1.201594352722168: 1, 0.5681759119033813: 1, -0.43114355206489563: 1, -0.7838892936706543: 1, -0.0861414223909378: 1, 1.6581584215164185: 1, 0.8071705102920532: 1, -1.2015928030014038: 1, -1.2015763521194458: 1, 1.3225599527359009: 1, 1.4854387044906616: 1, 0.7080846428871155: 1, 1.7376213073730469: 1, 1.9389169216156006: 1, 1.0590068101882935: 1, 1.2539737224578857: 1, -1.0813920497894287: 1, -1.0571757555007935: 1, -0.9450681209564209: 1, 0.10180643945932388: 1, 1.2595564126968384: 1, -0.13542917370796204: 1, 1.2727433443069458: 1, 0.2591017186641693: 1, 1.0987391471862793: 1, -0.7696746587753296: 1, 1.5291143655776978: 1, 1.7461694478988647: 1, 1.8303353786468506: 1, 1.8749902248382568: 1, -1.201613187789917: 1, 0.3816104531288147: 1, -1.002498984336853: 1, -0.20698606967926025: 1, 0.20394694805145264: 1, 0.780617356300354: 1, 1.8291547298431396: 1, 1.6759966611862183: 1, 1.0316096544265747: 1, 1.7389863729476929: 1, 1.896134853363037: 1, -1.040601134300232: 1, -0.8136187195777893: 1, 1.3174397945404053: 1, 1.16111421585083: 1, -1.085170865058899: 1, 0.8646785616874695: 1, 0.6801310777664185: 1, -1.2015223503112793: 1, -0.6532332301139832: 1, -0.20700129866600037: 1, 0.4620521366596222: 1, -1.0611162185668945: 1, -0.34224218130111694: 1, -0.06900987029075623: 1, 1.8261455297470093: 1, -0.3587249517440796: 1, -0.40272125601768494: 1, -1.130736231803894: 1, -0.24564200639724731: 1, -1.2007230520248413: 1, 0.9158507585525513: 1, 0.9156394600868225: 1, -0.6896651983261108: 1, 1.3129916191101074: 1, 1.5496872663497925: 1, -1.2016278505325317: 1, -1.1936933994293213: 1, 0.4616207182407379: 1, -0.7448302507400513: 1, 1.8480027914047241: 1, -0.7542659640312195: 1, -1.1320221424102783: 1, 0.5805153250694275: 1, -0.9954119920730591: 1, 1.7776927947998047: 1, -0.8793452382087708: 1, -1.2016057968139648: 1, 0.4253986179828644: 1, 1.5920872688293457: 1, 1.7425659894943237: 1, 1.0934252738952637: 1, -1.14544677734375: 1, -0.6306192278862: 1, 0.9750880599021912: 1, 1.6034055948257446: 1, 0.1718989461660385: 1, -1.200726866722107: 1, -1.2011404037475586: 1, 1.339892864227295: 1, -0.13886263966560364: 1, -0.24342182278633118: 1, -0.7379482984542847: 1, -0.5476839542388916: 1, 1.1129239797592163: 1, -1.1302224397659302: 1, 1.3335411548614502: 1, 1.110692024230957: 1, -0.655949056148529: 1, -0.24759358167648315: 1, 0.12914451956748962: 1, 1.4475048780441284: 1, 0.9216548800468445: 1, 0.25442907214164734: 1, 1.636014699935913: 1, 1.6676998138427734: 1, -1.2014168500900269: 1, 0.9160022139549255: 1, -0.9175146222114563: 1, 0.10331395268440247: 1, 0.9006003737449646: 1, 1.0343732833862305: 1, 1.6221997737884521: 1, 1.8067305088043213: 1, 1.9172451496124268: 1, 0.43482455611228943: 1, 1.8706549406051636: 1, 1.446770191192627: 1, -0.2059621661901474: 1, 1.760259985923767: 1, -0.11584310233592987: 1, -0.9069592952728271: 1, 1.8416829109191895: 1, 1.7511584758758545: 1, 0.6508381366729736: 1, -1.2014458179473877: 1, 1.4848576784133911: 1, 0.04224063828587532: 1, 0.9807740449905396: 1, 1.1523829698562622: 1, 1.796111822128296: 1, 1.6013163328170776: 1, -0.94952791929245: 1, 0.3571586012840271: 1, 1.8800233602523804: 1, -0.7931265234947205: 1, -0.5870760679244995: 1, -0.4422439932823181: 1, 0.5422061085700989: 1, 1.5719680786132812: 1, -1.2016080617904663: 1, -0.7923315763473511: 1, 0.8521577715873718: 1, 1.114802360534668: 1, -0.7990305423736572: 1, -1.201361894607544: 1, -0.8557604551315308: 1, -0.08804576843976974: 1, 0.8335886001586914: 1, 1.788615107536316: 1, 0.08071509003639221: 1, 0.8788592219352722: 1, 0.019096076488494873: 1, 1.7602899074554443: 1, 1.8266348838806152: 1, 1.058468222618103: 1, 1.7661612033843994: 1, -0.7876200675964355: 1, 0.13984030485153198: 1, 1.9223994016647339: 1, -0.11489463597536087: 1, 1.0331618785858154: 1, -1.156775951385498: 1, -0.4194781482219696: 1, 0.1478143036365509: 1, 0.35647323727607727: 1, 1.2849032878875732: 1, 1.5142070055007935: 1, -0.5466570258140564: 1, 1.0356202125549316: 1, 1.8354456424713135: 1, 1.8180814981460571: 1, 1.4667671918869019: 1, -0.21755054593086243: 1, 1.8225407600402832: 1, 1.0119178295135498: 1, 1.5984208583831787: 1, 1.0276689529418945: 1, 0.63859623670578: 1, 1.7614209651947021: 1, -0.010684509761631489: 1, 1.4813575744628906: 1, -0.907404899597168: 1, 1.2843722105026245: 1, 1.7114263772964478: 1, 1.473099708557129: 1, 0.9352988600730896: 1, 1.597861409187317: 1, 0.9059203267097473: 1, 1.0176738500595093: 1, 1.703546404838562: 1, 0.9797016382217407: 1, 0.9968640804290771: 1, 1.5016316175460815: 1, 1.6772441864013672: 1, 0.7037346363067627: 1, 1.7267848253250122: 1, -0.5458275079727173: 1, -0.7692103385925293: 1, 1.231010913848877: 1, -0.4275071322917938: 1, 0.7566526532173157: 1, -0.3422335684299469: 1, 0.006228437647223473: 1, -0.8751227855682373: 1, 1.542358636856079: 1, -0.46590861678123474: 1, -0.6440175771713257: 1, -0.7915438413619995: 1, -0.5345551371574402: 1, -0.4592617452144623: 1, -0.9877877235412598: 1, -1.0387167930603027: 1, -0.9143604040145874: 1, -1.1957098245620728: 1, -0.4729682505130768: 1, -0.8232591152191162: 1, -0.032225385308265686: 1, 1.2115764617919922: 1, 0.9276106357574463: 1, -0.47733497619628906: 1, -0.014680324122309685: 1, -0.24040797352790833: 1, -0.6007823944091797: 1, -0.7505127787590027: 1, -0.3081098794937134: 1, -0.16766004264354706: 1, 1.8227369785308838: 1, -1.093284010887146: 1, -0.014453819021582603: 1, -0.8769359588623047: 1, 0.7159720659255981: 1, -0.0832226425409317: 1, 0.012689988128840923: 1, -0.3753896951675415: 1, 1.5722923278808594: 1, -0.0211151335388422: 1, 0.19413244724273682: 1, -0.06991042196750641: 1, 0.2551988363265991: 1, 1.483720302581787: 1, 1.052090048789978: 1, 1.5233625173568726: 1, 1.3913298845291138: 1, -0.466978520154953: 1, -0.9003047943115234: 1, 1.569981575012207: 1, 0.9121710062026978: 1, -0.34284013509750366: 1, -0.47152507305145264: 1, 0.017721591517329216: 1, 1.3759360313415527: 1, 1.1691573858261108: 1, -0.9762966632843018: 1, 0.15484686195850372: 1, -0.5857658982276917: 1, -1.1479105949401855: 1, -0.5629591941833496: 1, -0.24207068979740143: 1, -0.46113380789756775: 1, 1.7606215476989746: 1, 0.6444322466850281: 1, -0.3169466555118561: 1, 1.3363116979599: 1, -0.8456059098243713: 1, 0.3244344890117645: 1, -0.8443267941474915: 1, -1.0356733798980713: 1, -1.1333893537521362: 1, 0.22667939960956573: 1, 0.43118155002593994: 1, 1.6555308103561401: 1, 0.6444940567016602: 1, -0.2761753499507904: 1, -0.0690179392695427: 1, -0.06596078723669052: 1, -0.44544142484664917: 1, 1.3399251699447632: 1, -1.1891201734542847: 1, -0.31910526752471924: 1, 1.1735796928405762: 1, -0.4622204601764679: 1, -0.7031784653663635: 1, -0.37577909231185913: 1, -0.03591597080230713: 1, -0.14395083487033844: 1, 0.5354000329971313: 1, 0.171223446726799: 1, -0.34326422214508057: 1, 1.4956955909729004: 1, -0.33291712403297424: 1, -0.6786049008369446: 1, -0.4369107186794281: 1, -0.48933231830596924: 1, -0.9299617409706116: 1, 1.8707987070083618: 1, 0.6785101294517517: 1, 0.29917436838150024: 1, -1.185569167137146: 1, 1.6845048666000366: 1, -1.0817426443099976: 1, -0.6234576106071472: 1, -0.023513898253440857: 1, 0.025362450629472733: 1, -0.30685290694236755: 1, 0.15043634176254272: 1, -0.5880576372146606: 1, -0.7417653203010559: 1, 0.4303920269012451: 1, -0.13842415809631348: 1, -0.3907565474510193: 1, -0.17455770075321198: 1, -1.1868388652801514: 1, -0.9987406134605408: 1, 1.4534815549850464: 1, -0.1967451572418213: 1, -0.8708242177963257: 1, 0.9746946096420288: 1, 0.8612715601921082: 1, -0.5582360029220581: 1, -0.5378177762031555: 1, -0.960394024848938: 1, -0.200442373752594: 1, -1.1109116077423096: 1, 1.7616217136383057: 1, -0.4605187475681305: 1, 0.06790906190872192: 1, -0.2937408983707428: 1, -0.3424704372882843: 1, 1.6493046283721924: 1, -0.5413272380828857: 1, -0.30134135484695435: 1, 0.7517246007919312: 1, 1.3204209804534912: 1, -0.4007585644721985: 1, 1.5550175905227661: 1, -0.6757361888885498: 1, -0.47023993730545044: 1, 0.7365891337394714: 1, 1.0860453844070435: 1, 1.477781891822815: 1, -0.03796708956360817: 1, -0.6538054943084717: 1, 1.5324621200561523: 1, -0.46823152899742126: 1, -0.07945768535137177: 1, -0.8472578525543213: 1, 1.5701237916946411: 1, -1.1767117977142334: 1, 0.6789939403533936: 1, 1.0024393796920776: 1, -0.1308683305978775: 1, 3.323413610458374: 1, 0.3248298168182373: 1, 1.4998488426208496: 1, -0.6655375957489014: 1, 0.5128249526023865: 1, 0.04851381108164787: 1, -0.8232764601707458: 1, -0.5174428224563599: 1, -0.21868036687374115: 1, 1.6888245344161987: 1, 0.6790488958358765: 1, 1.2261123657226562: 1, -1.0713788270950317: 1, -0.5210259556770325: 1, -0.49563685059547424: 1, -0.2992294430732727: 1, -0.8521113991737366: 1, -1.179208755493164: 1, 1.3037792444229126: 1, 0.17652635276317596: 1, 1.6146701574325562: 1, -0.36514076590538025: 1, 0.31867286562919617: 1, 1.7650138139724731: 1, -0.29448574781417847: 1, 0.3796524703502655: 1, 0.5300026535987854: 1, 1.9010276794433594: 1, -0.5381679534912109: 1, -0.879592776298523: 1, -0.288830429315567: 1, -0.8833669424057007: 1, -0.007546336855739355: 1, 0.6150393486022949: 1, 1.187366247177124: 1, -0.766767144203186: 1, -0.238590806722641: 1, -0.5338919758796692: 1, -0.4835919141769409: 1, -0.13618627190589905: 1, -0.5030357241630554: 1, 0.672914445400238: 1, 0.273947536945343: 1, -1.1766016483306885: 1, 1.376474142074585: 1, 1.5933094024658203: 1, 1.8017815351486206: 1, 0.9130324721336365: 1, 0.7212937474250793: 1, 0.48826223611831665: 1, 1.058951735496521: 1, 1.4193427562713623: 1, -0.23398016393184662: 1, -0.5802597403526306: 1, -1.1006834506988525: 1, 0.2422785758972168: 1, -0.21730461716651917: 1, -0.03495830297470093: 1, 0.3640252351760864: 1, -0.9720814824104309: 1, -0.18857857584953308: 1, 1.5428107976913452: 1, 0.3213244378566742: 1, 0.5874748826026917: 1, -1.0406304597854614: 1, -0.25207433104515076: 1, -0.9834045767784119: 1, -0.1501469612121582: 1, -0.3135724663734436: 1, -1.1350090503692627: 1, -0.6055639982223511: 1, 0.12007596343755722: 1, -0.4845651388168335: 1, 0.5530001521110535: 1, 1.6552691459655762: 1, 1.0318180322647095: 1, -1.0319366455078125: 1, 0.2126206010580063: 1, -0.5899453163146973: 1, 1.7796648740768433: 1, -0.392406165599823: 1, -0.8350648880004883: 1, -1.103760838508606: 1, -1.188301682472229: 1, -0.8279891610145569: 1, -0.2970311641693115: 1, -0.7790790796279907: 1, -1.0625981092453003: 1, -0.6904935240745544: 1, -1.1205850839614868: 1, 1.9273109436035156: 1, 0.3162490129470825: 1, 1.552185297012329: 1, -1.1448076963424683: 1, -0.9971945881843567: 1, 0.2097160518169403: 1, -0.04817575961351395: 1, 1.2531977891921997: 1, -0.46826034784317017: 1, 2.0270323753356934: 1, 0.04068145155906677: 1, 1.0491001605987549: 1, 0.6704513430595398: 1, 1.8792171478271484: 1, 0.46222802996635437: 1, 1.6683679819107056: 1, -0.09393322467803955: 1, -0.8678878545761108: 1, 0.5500550270080566: 1, 0.11937177926301956: 1, -0.5307965278625488: 1, -0.2874172031879425: 1, 1.4624748229980469: 1, 0.8171444535255432: 1, 1.8530430793762207: 1, 0.1769435554742813: 1, 0.2092854529619217: 1, 1.2401331663131714: 1, -1.2016234397888184: 1, 0.7334970831871033: 1, -0.2089931219816208: 1, -0.2558027505874634: 1, -0.4171464443206787: 1, -0.024080123752355576: 1, -0.4433523118495941: 1, -0.6431723237037659: 1, -0.6276612877845764: 1, 0.6405824422836304: 1, 1.7097703218460083: 1, -1.2016232013702393: 1, -1.2016154527664185: 1, 1.8505216836929321: 1, 0.3744315505027771: 1, -0.24634599685668945: 1, 1.852098822593689: 1, 0.3915187418460846: 1, 1.5747997760772705: 1, 0.49887171387672424: 1, 0.7721536755561829: 1, -0.01187801081687212: 1, 0.10666077584028244: 1, -1.2015986442565918: 1, -0.9726563692092896: 1, 1.6697852611541748: 1, 0.7304840087890625: 1, 0.21643884479999542: 1, -0.3255411982536316: 1, -0.061833277344703674: 1, -0.26246213912963867: 1, 0.21555371582508087: 1, 0.3141234815120697: 1, -0.22874142229557037: 1, -1.1232612133026123: 1, 0.5790113210678101: 1, -1.155177116394043: 1, -0.30786851048469543: 1, -0.3431845009326935: 1, -0.563433825969696: 1, 1.896323323249817: 1, -0.3123464286327362: 1, 0.18035785853862762: 1, -0.4653182625770569: 1, -1.1008306741714478: 1, -0.37186357378959656: 1, -0.8952922821044922: 1, -0.9930511116981506: 1, -0.27475300431251526: 1, -1.1336802244186401: 1, -0.364163339138031: 1, -0.3189155161380768: 1, -0.7983001470565796: 1, -0.7234905958175659: 1, -0.25122812390327454: 1, -0.9730278849601746: 1, -1.2009780406951904: 1, 1.5011951923370361: 1, 1.571593999862671: 1, -1.1689732074737549: 1, -0.24450848996639252: 1, 1.5116616487503052: 1, -1.061142921447754: 1, -1.1617506742477417: 1, -0.42767956852912903: 1, -0.0695866122841835: 1, 1.93668532371521: 1, 0.3824501037597656: 1, 0.7185215353965759: 1, -0.608808159828186: 1, -0.3237372636795044: 1, 0.13262629508972168: 1, 1.629822015762329: 1, -0.8671026229858398: 1, 1.8596769571304321: 1, 1.2151012420654297: 1, -0.43086937069892883: 1, 1.7544053792953491: 1, -0.5209143161773682: 1, -0.4168057143688202: 1, -0.2183121144771576: 1, -0.906280517578125: 1, -0.22242674231529236: 1, 1.7963730096817017: 1, -0.9981749057769775: 1, -0.44949105381965637: 1, -0.26545509696006775: 1, -0.9465891718864441: 1, -1.2015472650527954: 1, 4.112330913543701: 1, -1.2015608549118042: 1, -1.201536774635315: 1, -1.1448140144348145: 1, 4.5118513107299805: 1, 0.8056516647338867: 1, -1.201555848121643: 1, -1.1515345573425293: 1, -0.13113076984882355: 1, -1.1207038164138794: 1, -1.1624175310134888: 1, -1.1994960308074951: 1, -1.2015869617462158: 1, -0.2223142683506012: 1, 0.7078472375869751: 1, -0.5361114740371704: 1, -1.2013746500015259: 1, -0.3992172181606293: 1, -1.1680830717086792: 1, -0.5809049010276794: 1, -1.1971925497055054: 1, -1.1829675436019897: 1, -1.1953339576721191: 1, -1.1831696033477783: 1, -1.1635701656341553: 1, -0.6922621726989746: 1, -0.26783594489097595: 1, -0.8816695213317871: 1, -0.3383774757385254: 1, -0.29477736353874207: 1, -0.8523722290992737: 1, -0.7296833395957947: 1, 0.7230984568595886: 1, -1.1925454139709473: 1, -1.0483030080795288: 1, -1.2002716064453125: 1, 0.5143935680389404: 1, -1.1498054265975952: 1, -1.110012173652649: 1, -0.6403390765190125: 1, -1.1406184434890747: 1, -0.9465845823287964: 1, -1.1945738792419434: 1, -1.1494790315628052: 1, -1.201560139656067: 1, -1.2015953063964844: 1, 1.4938876628875732: 1, 0.7922017574310303: 1, 1.2598285675048828: 1, 0.264765202999115: 1, -1.1379677057266235: 1, -0.17077305912971497: 1, 0.8018452525138855: 1, 0.1461368203163147: 1, 1.2778337001800537: 1, -1.1420694589614868: 1, -1.1992239952087402: 1, 0.02189079485833645: 1, -0.3552163541316986: 1, -0.9202075004577637: 1, -0.8984062075614929: 1, -1.09720778465271: 1, -1.2008600234985352: 1, -0.11124473065137863: 1, 0.7242860198020935: 1, -1.2016023397445679: 1, 1.2057219743728638: 1, 0.5111210942268372: 1, -0.7557051777839661: 1, 0.8666924238204956: 1, -1.1981743574142456: 1, -0.16389766335487366: 1, -0.136034294962883: 1, 0.8696494698524475: 1, -1.187011957168579: 1, -0.861803412437439: 1, -1.201612949371338: 1, -1.0178923606872559: 1, -1.083876609802246: 1, -1.1375747919082642: 1, -0.19534148275852203: 1, -1.201613426208496: 1, -0.3075839579105377: 1, -0.32589226961135864: 1, -1.0825824737548828: 1, -1.1735186576843262: 1, 0.7587617635726929: 1, -0.16215069591999054: 1, -1.2009185552597046: 1, 0.043348729610443115: 1, -1.192414402961731: 1, -0.4258005619049072: 1, 0.7787987589836121: 1, 0.7848809361457825: 1, -0.6031686067581177: 1, 1.25115966796875: 1, 1.143233299255371: 1, 1.488932728767395: 1, 0.8550933599472046: 1, -0.05180063098669052: 1, -0.3311309218406677: 1, -1.1774768829345703: 1, -1.2016228437423706: 1, 0.3949566185474396: 1, -1.1074978113174438: 1, 1.2888545989990234: 1, -1.1091188192367554: 1, 0.8944936394691467: 1, -1.1906999349594116: 1, -0.012192374095320702: 1, 0.6273993253707886: 1, -0.04764068126678467: 1, -1.1882494688034058: 1, -0.6981508731842041: 1, -0.9249427914619446: 1, -0.17132940888404846: 1, -0.0996609777212143: 1, -1.20045006275177: 1, -0.14265145361423492: 1, -1.1139642000198364: 1, 0.944926917552948: 1, -0.03975825384259224: 1, -0.4824763238430023: 1, -1.0657732486724854: 1, -1.2001534700393677: 1, 0.4001394212245941: 1, -0.6715388298034668: 1, -0.6006320714950562: 1, -0.12521179020404816: 1, -0.5657321810722351: 1, -1.051085352897644: 1, -0.5369265675544739: 1, -0.39080610871315: 1, -0.03033183142542839: 1, -0.10903797298669815: 1, -0.7482910752296448: 1, -0.07614605128765106: 1, -1.15325927734375: 1, 0.9220188856124878: 1, 1.3031054735183716: 1, -1.150406837463379: 1, -0.1598413586616516: 1, -1.1756606101989746: 1, 0.032617583870887756: 1, -1.1632294654846191: 1, -1.1993433237075806: 1, -0.20172715187072754: 1, -0.06760650873184204: 1, -0.7514510154724121: 1, 1.2156920433044434: 1, 1.282943606376648: 1, -1.195853590965271: 1, 0.821479320526123: 1, -1.1690752506256104: 1, -0.5976389050483704: 1, 1.0670119524002075: 1, -0.7073397040367126: 1, 0.6907184720039368: 1, -0.4774172008037567: 1, 0.6389100551605225: 1, -1.0666824579238892: 1, -0.3503449857234955: 1, -0.7184330821037292: 1, -1.2002339363098145: 1, -0.2299073189496994: 1, 0.9924389719963074: 1, 0.9692907333374023: 1, -0.21156159043312073: 1, -1.144519329071045: 1, -0.39197561144828796: 1, -1.1276121139526367: 1, 0.004596139770001173: 1, -1.0416687726974487: 1, -0.576421856880188: 1, -1.2016373872756958: 1, -1.0180860757827759: 1, -0.4656817615032196: 1, 0.6975425481796265: 1, 0.9127581715583801: 1, -0.35478705167770386: 1, -0.44190311431884766: 1, -1.1707801818847656: 1, 0.9517895579338074: 1, 1.229008674621582: 1, 0.5329916477203369: 1, 0.880368709564209: 1, -1.084214448928833: 1, -0.8594576716423035: 1, -0.35862404108047485: 1, 0.605282723903656: 1, -0.8211961388587952: 1, 0.369517058134079: 1, 1.2405850887298584: 1, 0.842113196849823: 1, -0.9499993324279785: 1, 0.48456287384033203: 1, -1.2016358375549316: 1, -0.5739816427230835: 1, -0.6867349743843079: 1, -0.5342384576797485: 1, -1.2015101909637451: 1, 0.3778545558452606: 1, 0.4334481656551361: 1, -0.13237591087818146: 1, 0.6366953253746033: 1, -1.20154869556427: 1, 1.1094199419021606: 1, 1.0952398777008057: 1, 1.258391261100769: 1, -0.07435453683137894: 1, 1.1807068586349487: 1, -0.15606260299682617: 1, -0.7601802349090576: 1, -0.5920382142066956: 1, 0.8677871227264404: 1, 0.669349730014801: 1, 0.5067287683486938: 1, 0.5890273451805115: 1, 0.8621184229850769: 1, -0.8395327925682068: 1, -0.5897277593612671: 1, 0.050834186375141144: 1, -0.08644621819257736: 1, 1.1708914041519165: 1, 0.9452794194221497: 1, -0.09844925999641418: 1, 1.0215860605239868: 1, -0.5935716032981873: 1, 1.0410280227661133: 1, -1.1992988586425781: 1, -1.1941806077957153: 1, -0.3055903911590576: 1, -1.1760457754135132: 1, 1.2313082218170166: 1, -0.3122004270553589: 1, -0.8366453051567078: 1, -0.3642917275428772: 1, -0.5142630338668823: 1, -0.005674127489328384: 1, 0.7907249331474304: 1, -0.13443662226200104: 1, -0.002722974168136716: 1, -1.1028809547424316: 1, -1.1580485105514526: 1, 0.8628989458084106: 1, -0.24963954091072083: 1, -0.5910298824310303: 1, -0.5807573795318604: 1, -1.2010724544525146: 1, -1.1809542179107666: 1, 0.013616573065519333: 1, -0.003650385420769453: 1, -0.048064157366752625: 1, -0.42906203866004944: 1, -0.024461327120661736: 1, -0.32544630765914917: 1, -0.2519146203994751: 1, 1.30207359790802: 1, 0.6744810342788696: 1, 0.9348665475845337: 1, -1.0131398439407349: 1, 0.01674770377576351: 1, -0.3348834216594696: 1, 0.00948107335716486: 1, -0.06836508214473724: 1, -0.15388239920139313: 1, 0.05971863865852356: 1, -0.12709279358386993: 1, 1.1462621688842773: 1, -0.7992537021636963: 1, -1.1966403722763062: 1, -0.13060888648033142: 1, -0.8245752453804016: 1, -0.5774872899055481: 1, -0.24131079018115997: 1, -0.15946216881275177: 1, -1.193916916847229: 1, -0.6155209541320801: 1, 1.1092147827148438: 1, 0.6174435615539551: 1, -0.7010860443115234: 1, -0.01721060648560524: 1, 1.043671727180481: 1, -0.15634003281593323: 1, -0.3034926950931549: 1, -0.34825557470321655: 1, -0.06601618975400925: 1, -0.21185417473316193: 1, 0.7746310830116272: 1, -1.1586220264434814: 1, -1.2016310691833496: 1, -0.3917173445224762: 1, -0.02432066947221756: 1, 0.2524677515029907: 1, 0.2549906373023987: 1, 0.7206960916519165: 1, 0.6609118580818176: 1, -0.6903147101402283: 1, 1.00320303440094: 1, -1.198868989944458: 1, 1.1278204917907715: 1, 0.2748369872570038: 1, 0.9495259523391724: 1, 0.15262554585933685: 1, 0.6296795010566711: 1, -1.1068792343139648: 1, -0.6869497895240784: 1, 0.07662157714366913: 1, -0.09314227104187012: 1, 1.196900725364685: 1, -0.09158548712730408: 1, -0.1383906453847885: 1, -0.17342792451381683: 1, 1.2882169485092163: 1, -1.1826090812683105: 1, 0.7786849141120911: 1, -0.19042982161045074: 1, -1.200330376625061: 1, 1.0692790746688843: 1, -0.257107138633728: 1, 0.606712281703949: 1, -1.182140827178955: 1, 0.028185075148940086: 1, -0.27063482999801636: 1, 0.41225412487983704: 1, 0.9105262160301208: 1, 1.1562855243682861: 1, -1.2016348838806152: 1, -0.21517714858055115: 1, 0.9726781249046326: 1, -0.12876513600349426: 1, -1.1697360277175903: 1, 0.008013189770281315: 1, -0.5649014711380005: 1, -0.21699510514736176: 1, -0.1227283924818039: 1, -0.7380070686340332: 1, 0.8076177835464478: 1, -0.0404001921415329: 1, 0.6649335622787476: 1, -0.028692159801721573: 1, -0.11206144839525223: 1, -0.11562295258045197: 1, 1.1690500974655151: 1, -0.618209183216095: 1, 1.0350605249404907: 1, 1.1997345685958862: 1, -0.26567548513412476: 1, 0.5923774242401123: 1, 0.7346874475479126: 1, -0.016240552067756653: 1, -0.6019781231880188: 1, -1.1322532892227173: 1, 0.49005118012428284: 1, -0.7893494963645935: 1, -1.192280888557434: 1, -1.2016328573226929: 1, 0.5382747054100037: 1, 1.0084635019302368: 1, 0.810942530632019: 1, -0.0674244612455368: 1, 0.6161129474639893: 1, -1.006115436553955: 1, -0.27813780307769775: 1, -0.3076569437980652: 1, -0.9430715441703796: 1, -1.1914066076278687: 1, -1.0579359531402588: 1, -0.16474246978759766: 1, 1.1363762617111206: 1, -1.2016372680664062: 1, 0.8519235253334045: 1, 1.3037681579589844: 1, -0.03992554917931557: 1, -0.8901136517524719: 1, -0.03235474228858948: 1, -0.07969757169485092: 1, -0.04230939969420433: 1, -1.1908975839614868: 1, 0.4138732850551605: 1, -0.8764730095863342: 1, -0.24605818092823029: 1, -0.171332448720932: 1, -0.8429122567176819: 1, 1.0700626373291016: 1, 1.257509708404541: 1, 0.9156388640403748: 1, -0.7673166990280151: 1, -0.15097910165786743: 1, -0.14771254360675812: 1, -0.7642948627471924: 1, -0.9743238091468811: 1, -0.10418325662612915: 1, 0.68455970287323: 1, -0.05808640271425247: 1, -0.881252646446228: 1, -0.5050346255302429: 1, -1.200080394744873: 1, 0.17733901739120483: 1, 1.0595874786376953: 1, -1.0076838731765747: 1, -0.07029284536838531: 1, 0.762050449848175: 1, 1.030333161354065: 1, 0.5583640336990356: 1, 0.9881972074508667: 1, -1.109034538269043: 1, 0.5070648193359375: 1, -0.019765598699450493: 1, -0.29025569558143616: 1, -0.07907160371541977: 1, -0.0026253368705511093: 1, -1.0697368383407593: 1, 0.12906195223331451: 1, -0.08915898948907852: 1, 0.004652172327041626: 1, -0.5897085666656494: 1, -0.23161835968494415: 1, 0.9278587102890015: 1, 1.0157313346862793: 1, 1.1558300256729126: 1, -1.1260875463485718: 1, 0.048983871936798096: 1, -1.0963338613510132: 1, -0.23146884143352509: 1, 0.9011586904525757: 1, -0.2553582787513733: 1, 1.0766181945800781: 1, -0.2878003716468811: 1, -1.155191421508789: 1, -1.1991511583328247: 1, 1.023429274559021: 1, -0.7420557737350464: 1, -0.3634156584739685: 1, 1.1214849948883057: 1, -1.0054869651794434: 1, -1.1934514045715332: 1, -0.002401667181402445: 1, 1.2993144989013672: 1, -1.1338447332382202: 1, -0.6266202926635742: 1, -0.3548189401626587: 1, 1.0808086395263672: 1, -0.455705851316452: 1, -0.9566242694854736: 1, 0.033837273716926575: 1, -0.999508798122406: 1, -0.05626079440116882: 1, -0.5931430459022522: 1, -0.8736492395401001: 1, -0.5885310769081116: 1, 0.666642963886261: 1, -1.0668615102767944: 1, -0.5772318840026855: 1, -1.194190263748169: 1, -0.19914157688617706: 1, -1.17979097366333: 1, -1.1744723320007324: 1, -1.1722731590270996: 1, -0.4475323557853699: 1, -1.0135008096694946: 1, -1.1582452058792114: 1, -1.191677212715149: 1, -0.63347989320755: 1, -1.1062737703323364: 1, -1.0160771608352661: 1, -1.1520825624465942: 1, -1.167817234992981: 1, -0.9714952111244202: 1, -1.0810678005218506: 1, -0.6395750641822815: 1, -1.1800310611724854: 1, -1.194927453994751: 1, -1.1721937656402588: 1, -1.0433918237686157: 1, -0.6635865569114685: 1, -1.201562523841858: 1, -0.8834558129310608: 1, -1.1989647150039673: 1, -0.7228063344955444: 1, -0.9656466245651245: 1, -1.1463063955307007: 1, -0.694696843624115: 1, -0.6411266922950745: 1, -0.4774998426437378: 1, -1.1344302892684937: 1, -0.2650972902774811: 1, -1.026742935180664: 1, -0.4338090121746063: 1, -1.1509727239608765: 1, -0.5115338563919067: 1, -0.9300684928894043: 1, -1.193282127380371: 1, -1.157931923866272: 1, -0.6325321793556213: 1, -0.8867827653884888: 1, -1.1729844808578491: 1, -1.010536789894104: 1, -1.0467379093170166: 1, -1.0309724807739258: 1, -1.162119746208191: 1, -1.2006478309631348: 1, -1.1339654922485352: 1, -1.198327660560608: 1, -1.091170310974121: 1, -1.175873041152954: 1, -1.1691778898239136: 1, -1.2014809846878052: 1, -1.1855055093765259: 1, -1.1832531690597534: 1, -1.2012261152267456: 1, -0.6185922622680664: 1, -0.8899372816085815: 1, -0.9966712594032288: 1, -0.7500604391098022: 1, -0.6249680519104004: 1, -1.1552096605300903: 1, -1.1936330795288086: 1, -1.170555830001831: 1, -0.957834780216217: 1, -1.0740134716033936: 1, -1.1905823945999146: 1, -1.2015275955200195: 1, -1.1300313472747803: 1, -1.1405699253082275: 1, -0.40758535265922546: 1, -1.1857632398605347: 1, -0.8829452991485596: 1, -0.5632508397102356: 1, -1.1954984664916992: 1, -1.1685314178466797: 1, -1.0831377506256104: 1, -1.1470420360565186: 1, -1.161582589149475: 1, -1.0068711042404175: 1, -0.7897263169288635: 1, -1.126016616821289: 1, -1.1558103561401367: 1, -1.2008585929870605: 1, -0.5804309248924255: 1, -0.9933805465698242: 1, -0.832706868648529: 1, -1.1563661098480225: 1, -0.9732658267021179: 1, -1.1349726915359497: 1, -1.1859160661697388: 1, -1.1969208717346191: 1, -1.1333073377609253: 1, -1.1816785335540771: 1, -1.1568188667297363: 1, -0.8366922736167908: 1, -0.41631850600242615: 1, 1.0577486753463745: 1, -1.1981457471847534: 1, 1.7253838777542114: 1, -0.4444347023963928: 1, -1.0461647510528564: 1, -0.8267379403114319: 1, -1.0589720010757446: 1, -1.1486388444900513: 1, -1.1036909818649292: 1, -0.25539615750312805: 1, -1.1933567523956299: 1, -0.86516273021698: 1, -0.9225814342498779: 1, -0.7736660242080688: 1, -1.200010895729065: 1, -1.087907314300537: 1, -0.3156169652938843: 1, -1.2006280422210693: 1, -1.1891672611236572: 1, -1.1977088451385498: 1, -1.1703253984451294: 1, -1.144382119178772: 1, -0.9726890325546265: 1, -0.9449849724769592: 1, 5.037554740905762: 1, -1.1754673719406128: 1, 5.012721538543701: 1, -0.6635236740112305: 1, 0.057195477187633514: 1, -0.9046985507011414: 1, -0.622269868850708: 1, 5.001932144165039: 1, 4.900933742523193: 1, 4.722275257110596: 1, 0.939198911190033: 1, -0.7939543128013611: 1, 4.302996635437012: 1, 4.58308219909668: 1, -1.1364892721176147: 1, -1.1480247974395752: 1, 4.875144004821777: 1, 5.072229385375977: 1, 0.4937030076980591: 1, 5.05051851272583: 1, 3.830434560775757: 1, -1.1785725355148315: 1, -1.1136521100997925: 1, 5.006033897399902: 1, -0.4824954569339752: 1, -1.1986464262008667: 1, -1.0933367013931274: 1, -1.0186684131622314: 1, -1.1946135759353638: 1, -0.5497206449508667: 1, -1.1790684461593628: 1, -0.6818874478340149: 1, -1.0620733499526978: 1, -0.8758900761604309: 1, -1.1150031089782715: 1, 0.18425099551677704: 1, -0.75212162733078: 1, -0.7968862652778625: 1, -1.145255446434021: 1, -1.1212965250015259: 1, -1.2005667686462402: 1, -0.9272821545600891: 1, -1.02364981174469: 1, -1.0699774026870728: 1, -0.5409148931503296: 1, -1.0538722276687622: 1, -0.9141108393669128: 1, -0.8772833347320557: 1, -0.5520062446594238: 1, -0.9129625558853149: 1, -0.9539603590965271: 1, -1.0416630506515503: 1, -1.1181161403656006: 1, -0.6967374682426453: 1, -1.1434556245803833: 1, -1.1468044519424438: 1, -0.4437340795993805: 1, -0.967963457107544: 1, -1.0246316194534302: 1, -0.1397843062877655: 1, -0.47614291310310364: 1, -0.8666650652885437: 1, -0.8599510192871094: 1, -0.9453350901603699: 1, -1.0074002742767334: 1, -1.1503796577453613: 1, -0.9824236631393433: 1, -1.189503788948059: 1, -0.5638068318367004: 1, -0.4648326635360718: 1, -1.0434380769729614: 1, -1.00275719165802: 1, -0.6867276430130005: 1, -0.669135332107544: 1, -1.2014120817184448: 1, -1.0521938800811768: 1, -1.1961623430252075: 1, -1.1871360540390015: 1, 0.6101611256599426: 1, 0.7108749151229858: 1, -0.726171612739563: 1, 0.7067909836769104: 1, 0.8852934837341309: 1, 1.1564749479293823: 1, 1.094826102256775: 1, 0.7936035394668579: 1, -0.9092623591423035: 1, 1.593440055847168: 1, 1.0043728351593018: 1, 1.6096405982971191: 1, -0.6205415725708008: 1, 0.9798484444618225: 1, -0.6111128330230713: 1, -1.0877141952514648: 1, 0.4647965729236603: 1, 0.9053057432174683: 1, 1.6532078981399536: 1, -0.8635501265525818: 1, 0.5944019556045532: 1, 1.8255465030670166: 1, -0.44575440883636475: 1, 0.8697875142097473: 1, -0.16204535961151123: 1, 0.8136993050575256: 1, 1.6066374778747559: 1, -0.17054051160812378: 1, -1.1964715719223022: 1, 0.3594037592411041: 1, -1.2015913724899292: 1, -0.4477367699146271: 1, -1.016434669494629: 1, -0.31994664669036865: 1, 0.18603742122650146: 1, -0.9458178281784058: 1, 0.6880602836608887: 1, 0.6052579283714294: 1, -0.39169713854789734: 1, 0.8645955324172974: 1, 0.7324214577674866: 1, -1.201418161392212: 1, -0.2950673997402191: 1, -0.15851348638534546: 1, -1.1721446514129639: 1, -0.23552395403385162: 1, -0.7970835566520691: 1, 0.5490202307701111: 1, 1.2443398237228394: 1, 0.2436400204896927: 1, 1.036679744720459: 1, -1.199397087097168: 1, -1.1621276140213013: 1, 1.123262643814087: 1, -0.411021888256073: 1, 0.6509128212928772: 1, 0.8030492067337036: 1, -1.2016305923461914: 1, -1.2016229629516602: 1, 0.5780434608459473: 1, -0.7776852250099182: 1, -0.32108673453330994: 1, -1.1028145551681519: 1, 0.6591589450836182: 1, -0.1892606019973755: 1, -0.8244988322257996: 1, -0.5816406607627869: 1, -0.6496835947036743: 1, -0.7252834439277649: 1, -0.7019102573394775: 1, -1.1959316730499268: 1, -0.9352316856384277: 1, -0.5334532856941223: 1, -1.171905517578125: 1, -1.201583743095398: 1, 0.2817704975605011: 1, -1.154268741607666: 1, 1.2748090028762817: 1, -0.750208854675293: 1, 1.7609484195709229: 1, -1.201631784439087: 1, -0.7198786735534668: 1, -0.1808653175830841: 1, -0.17489729821681976: 1, 0.05797763168811798: 1, 1.0823159217834473: 1, 0.5601547360420227: 1, 1.0962333679199219: 1, 0.6412140727043152: 1, 1.3093386888504028: 1, 1.5706232786178589: 1, -0.3561877906322479: 1, -0.8054187893867493: 1, 0.5863446593284607: 1, -0.34684932231903076: 1, 1.585684895515442: 1, 1.1463862657546997: 1, -1.2015637159347534: 1, -1.1971783638000488: 1, -1.188680648803711: 1, -1.1990872621536255: 1, -0.6870049834251404: 1, -0.6259949207305908: 1, -1.186979055404663: 1, -1.1963162422180176: 1, -1.201351284980774: 1, -1.1594713926315308: 1, -0.9010990262031555: 1, -0.5026354193687439: 1, -1.1365838050842285: 1, -1.196357011795044: 1, -1.1999870538711548: 1, 5.645717620849609: 1, -1.1910632848739624: 1, -1.195172905921936: 1, -0.459964781999588: 1, -1.1054575443267822: 1, 0.741217315196991: 1, 1.8886953592300415: 1, 0.19575154781341553: 1, -1.1993547677993774: 1, -1.164048194885254: 1, -1.1929975748062134: 1, -1.1676386594772339: 1, -1.188031554222107: 1, -1.2014538049697876: 1, -1.1610828638076782: 1, -1.0014375448226929: 1, -1.2015451192855835: 1, -1.1066040992736816: 1, -0.7456731200218201: 1, -1.1595861911773682: 1, -1.1821529865264893: 1, -1.076475739479065: 1, -1.2009553909301758: 1, -1.0944702625274658: 1, -1.1019858121871948: 1, -1.1624016761779785: 1, -1.1929867267608643: 1, -1.18364417552948: 1, -1.1482324600219727: 1, -1.1857080459594727: 1, -1.0951330661773682: 1, -1.1485586166381836: 1, -1.0865159034729004: 1, -1.1643073558807373: 1, -1.1970044374465942: 1, -0.40432149171829224: 1, 0.3444092869758606: 1, 0.7184975147247314: 1, -1.201563835144043: 1, -1.1862393617630005: 1, -0.9986592531204224: 1, 1.7719837427139282: 1, -1.2008846998214722: 1, -1.0349972248077393: 1, -0.9958106875419617: 1, -1.0542218685150146: 1, 0.4647902548313141: 1, -1.1924408674240112: 1, -1.1728081703186035: 1, -0.6460915207862854: 1, -1.1948115825653076: 1, -1.1685289144515991: 1, -1.1994209289550781: 1, -1.173497200012207: 1, -0.638097882270813: 1, -0.6645460724830627: 1, -1.2015430927276611: 1, 0.23683039844036102: 1, 0.05473056063055992: 1, -1.0081939697265625: 1, -1.1948130130767822: 1, -1.1950762271881104: 1, -1.1612766981124878: 1, -1.157366394996643: 1, -1.1819733381271362: 1, -1.194710373878479: 1, 0.6086026430130005: 1, -1.175083041191101: 1, 0.275499552488327: 1, -0.23024950921535492: 1, -1.171040415763855: 1, -1.1539435386657715: 1, -1.1912822723388672: 1, -1.1865227222442627: 1, -0.4641222357749939: 1, -1.1811505556106567: 1, 1.6846264600753784: 1, -0.7029581665992737: 1, -0.6273359060287476: 1, -1.0972120761871338: 1, -0.40274444222450256: 1, -1.1986582279205322: 1, -1.2010711431503296: 1, -1.1966667175292969: 1, -1.198758602142334: 1, -1.196737289428711: 1, 0.5745441317558289: 1, 0.5615567564964294: 1, -1.1851680278778076: 1, 0.4515918791294098: 1, 0.30889958143234253: 1, -0.632728099822998: 1, 0.3140702247619629: 1, -0.10326965153217316: 1, -0.11514480412006378: 1, 1.6200270652770996: 1, 0.12041562795639038: 1, 0.7718502283096313: 1, 0.05425111949443817: 1, 0.3626178801059723: 1, 0.20913533866405487: 1, 1.4364150762557983: 1, 1.622856616973877: 1, -0.40100592374801636: 1, -0.20442236959934235: 1, -0.10369612276554108: 1, 0.7984791398048401: 1, 1.481839656829834: 1, 1.3110328912734985: 1, 0.5713070034980774: 1, 1.5503476858139038: 1, 1.561331033706665: 1, 1.431997537612915: 1, 0.6983891129493713: 1, 0.2561040222644806: 1, 1.914624810218811: 1, 1.3110779523849487: 1, 1.55558443069458: 1, 0.18870316445827484: 1, 0.858607292175293: 1, 1.282376766204834: 1, 0.7782679200172424: 1, 1.0659617185592651: 1, 0.987504780292511: 1, 0.22794750332832336: 1, 0.9434877038002014: 1, -0.1031346470117569: 1, -0.22873257100582123: 1, 1.379611611366272: 1, 0.22853031754493713: 1, -0.6209532618522644: 1, -0.21601253747940063: 1, 1.5148382186889648: 1, 0.33931559324264526: 1, 0.11681299656629562: 1, 0.06557659059762955: 1, 0.3338538706302643: 1, 0.1928955614566803: 1, 0.23876602947711945: 1, 0.23767612874507904: 1, 1.260263442993164: 1, 0.7438257932662964: 1, 0.18295887112617493: 1, 0.2222539335489273: 1, 0.276736855506897: 1, 0.35236239433288574: 1, 1.6643083095550537: 1, -0.10448039323091507: 1, -0.19414900243282318: 1, -1.1556631326675415: 1, 0.07517638802528381: 1, 0.2801852524280548: 1, -0.48821160197257996: 1, 1.4599251747131348: 1, 1.5357881784439087: 1, -0.05793027952313423: 1, 1.3920340538024902: 1, 0.44933900237083435: 1, -0.5775644183158875: 1, 1.233654499053955: 1, 1.215183138847351: 1, 1.3900312185287476: 1, 1.4145740270614624: 1, 1.1246896982192993: 1, -0.13019442558288574: 1, 1.1509000062942505: 1, 0.14948004484176636: 1, -0.1196289211511612: 1, 0.11391282081604004: 1, -0.44012218713760376: 1, 1.4899855852127075: 1, -0.1709771454334259: 1, -0.812441349029541: 1, 0.6854439377784729: 1, 1.5044559240341187: 1, -0.6113037467002869: 1, 1.1449819803237915: 1, -0.9236016869544983: 1, 0.7352478504180908: 1, 1.0590577125549316: 1, -0.31073465943336487: 1, 0.21012525260448456: 1, 0.592692494392395: 1, -0.6031805276870728: 1, 0.6647680401802063: 1, -0.9130086302757263: 1, -0.10372047126293182: 1, 1.4786081314086914: 1, 0.002965901279821992: 1, -0.5475073456764221: 1, -0.16844011843204498: 1, 0.9252344369888306: 1, 0.10832516103982925: 1, 0.31476086378097534: 1, 0.4845307767391205: 1, 0.021630888804793358: 1, 0.05546194687485695: 1, 1.189407229423523: 1, -1.0867854356765747: 1, 0.9222752451896667: 1, 1.0225938558578491: 1, -0.22581757605075836: 1, 0.753506600856781: 1, -0.4890022277832031: 1, 0.8458276391029358: 1, 0.29691436886787415: 1, 1.0407052040100098: 1, -1.1736985445022583: 1, 0.8794386982917786: 1, 0.26258811354637146: 1, -0.20083148777484894: 1, 1.244690179824829: 1, -0.22985504567623138: 1, -0.8935246467590332: 1, 0.3153885304927826: 1, -1.2015849351882935: 1, -0.5888482928276062: 1, -1.1383882761001587: 1, -1.2012838125228882: 1, -1.2016196250915527: 1, -1.2016048431396484: 1, -1.2015061378479004: 1, -1.2015372514724731: 1, -0.416104793548584: 1, -0.11705746501684189: 1, -1.201634168624878: 1, -1.1903178691864014: 1, -1.171350359916687: 1, -1.1623584032058716: 1, -1.201610803604126: 1, -1.1100589036941528: 1, 0.3900337219238281: 1, -0.9999420642852783: 1, -1.2016303539276123: 1, -1.2014681100845337: 1, -0.06736226379871368: 1, -1.2015819549560547: 1, 0.34270647168159485: 1, 1.5059622526168823: 1, 1.059990644454956: 1, 0.021963730454444885: 1, 1.5651975870132446: 1, -0.11230547726154327: 1, 0.01178812701255083: 1, -0.5517370104789734: 1, -0.5199218392372131: 1, 0.21266861259937286: 1, -0.03138621151447296: 1, 1.466456651687622: 1, 0.5634517669677734: 1, 0.9224774241447449: 1, 1.256608247756958: 1, 1.533963680267334: 1, 1.3359390497207642: 1, -0.03743843734264374: 1, 0.31844547390937805: 1, -0.1758507341146469: 1, 0.7778939008712769: 1, 1.5112932920455933: 1, -0.1960129290819168: 1, -1.1865193843841553: 1, 0.9693878293037415: 1, 0.35881760716438293: 1, 0.7781831622123718: 1, 0.11682117730379105: 1, 0.3591007590293884: 1, 1.0209075212478638: 1, -0.0920228511095047: 1, -0.06609977036714554: 1, 1.466652274131775: 1, 0.11165464669466019: 1, -0.014552570879459381: 1, 1.3268651962280273: 1, 0.853833794593811: 1, -0.3219013214111328: 1, 0.8655160665512085: 1, 0.8642333745956421: 1, -0.5888111591339111: 1, 1.7802213430404663: 1, 0.2615222632884979: 1, 0.5367603302001953: 1, 1.3780083656311035: 1, 1.1870133876800537: 1, 1.126368761062622: 1, 1.2197668552398682: 1, 0.7675086855888367: 1, 0.004675476811826229: 1, 0.7520656585693359: 1, 1.3589857816696167: 1, 0.9935461282730103: 1, 0.2826254069805145: 1, 0.09744428098201752: 1, 1.2778698205947876: 1, 1.3494865894317627: 1, 1.0432312488555908: 1, 1.4069277048110962: 1, -1.1632437705993652: 1, 1.5597952604293823: 1, 1.5033998489379883: 1, 0.2389289289712906: 1, 1.1072840690612793: 1, 0.22126778960227966: 1, -0.8828660249710083: 1, -0.3440307080745697: 1, 1.5512200593948364: 1, -0.19494549930095673: 1, 0.29415011405944824: 1, 0.05755604803562164: 1, 0.8618729114532471: 1, 0.9195832014083862: 1, 1.5557059049606323: 1, -0.7541334629058838: 1, 0.2642950117588043: 1, 0.8805737495422363: 1, 0.11520501971244812: 1, -0.07051629573106766: 1, 1.4821670055389404: 1, 1.471611738204956: 1, 1.302850365638733: 1, 1.5040947198867798: 1, 0.7159395813941956: 1, 1.0842758417129517: 1, 0.9064841866493225: 1, 1.3597513437271118: 1, 0.8933335542678833: 1, 0.0863155722618103: 1, 1.5239653587341309: 1, 0.5069756507873535: 1, 1.5916389226913452: 1, 1.3772739171981812: 1, 0.19180983304977417: 1, -0.43688738346099854: 1, 0.8997297883033752: 1, 0.9983642101287842: 1, 0.6865427494049072: 1, 0.8533394932746887: 1, 1.133412480354309: 1, 0.8847638368606567: 1, 1.3578754663467407: 1, -0.16789886355400085: 1, -0.17545948922634125: 1, 0.263934850692749: 1, 1.1847658157348633: 1, 1.1921144723892212: 1, 0.7092652916908264: 1, 0.0384686179459095: 1, 0.7332795858383179: 1, -0.5194405913352966: 1, 0.009009350091218948: 1, 1.4580349922180176: 1, 1.4103199243545532: 1, 0.8865175843238831: 1, -1.193596363067627: 1, 1.5027039051055908: 1, 0.6448225975036621: 1, 0.2692132890224457: 1, 1.4735376834869385: 1, -0.870884895324707: 1, 0.008224710822105408: 1, 0.48444247245788574: 1, -0.30212122201919556: 1, -0.19083698093891144: 1, -0.009973025880753994: 1, 0.3276282548904419: 1, -0.56696617603302: 1, -0.004799619782716036: 1, 0.6183062195777893: 1, -1.1040070056915283: 1, 1.3463716506958008: 1, 1.3844947814941406: 1, 0.7684015035629272: 1, 0.22540217638015747: 1, -0.9639919996261597: 1, -0.025239035487174988: 1, 0.041503969579935074: 1, 1.4127790927886963: 1, 0.8287729024887085: 1, -0.11932859569787979: 1, -1.1504056453704834: 1, 1.5179330110549927: 1, -0.20943275094032288: 1, 0.3431006968021393: 1, 0.35639217495918274: 1, 1.405086636543274: 1, 1.5550216436386108: 1, 0.80833899974823: 1, 2.0702569484710693: 1, 0.9344565272331238: 1, -0.2038322389125824: 1, 0.9953116178512573: 1, 0.5000193119049072: 1, 0.8744557499885559: 1, 0.6104775071144104: 1, 1.473071813583374: 1, 1.069445013999939: 1, -0.30856987833976746: 1, 1.2513844966888428: 1, 0.1911333203315735: 1, 0.39437854290008545: 1, 1.4644227027893066: 1, 0.2846507132053375: 1, 1.3862555027008057: 1, 1.5425565242767334: 1, -0.41700541973114014: 1, 1.5978120565414429: 1, 0.881543755531311: 1, 1.2155990600585938: 1, -1.1900941133499146: 1, 0.3661552369594574: 1, 1.5401815176010132: 1, 0.8500742316246033: 1, 1.4411144256591797: 1, 1.622040867805481: 1, 1.1854524612426758: 1, 0.39806419610977173: 1, 1.5731940269470215: 1, 0.98076331615448: 1, 0.4243623912334442: 1, 0.3602719008922577: 1, 0.04298178479075432: 1, -0.11455459892749786: 1, 0.30982303619384766: 1, 0.340713769197464: 1, 0.2596873939037323: 1, 0.822748601436615: 1, 1.4639532566070557: 1, 1.1571227312088013: 1, 1.540098786354065: 1, 1.0418422222137451: 1, 0.1144905686378479: 1, 1.2591054439544678: 1, -0.20360040664672852: 1, 0.2816910445690155: 1, -0.15476621687412262: 1, -0.10053509473800659: 1, 0.339995801448822: 1, 0.11328306794166565: 1, 0.2568971812725067: 1, 0.5611000061035156: 1, 0.968934953212738: 1, 1.007346510887146: 1, 1.4793431758880615: 1, 0.8512904644012451: 1, 1.5063830614089966: 1, -0.4644731879234314: 1, -0.11862857639789581: 1, -0.11317183822393417: 1, 0.3683989942073822: 1, 0.1563034951686859: 1, -0.1884830892086029: 1, 0.35628634691238403: 1, -0.2168547362089157: 1, 0.30258285999298096: 1, 0.6513680219650269: 1, -0.040548212826251984: 1, -0.4737872779369354: 1, -0.2417261004447937: 1, 1.1731380224227905: 1, -0.20986565947532654: 1, 0.6668532490730286: 1, 1.0762102603912354: 1, -1.024586796760559: 1, 0.8943637013435364: 1, 1.4529069662094116: 1, 1.916335940361023: 1, 0.5241292119026184: 1, -0.6390724778175354: 1, 0.7008569240570068: 1, 1.0041277408599854: 1, -1.1742432117462158: 1, -0.6223658919334412: 1, 1.4056357145309448: 1, -0.09898030012845993: 1, 0.29443448781967163: 1, 0.06307864934206009: 1, 0.3499395549297333: 1, -0.1565595418214798: 1, 0.9952307939529419: 1, -0.28925973176956177: 1, 1.5599995851516724: 1, 1.5686708688735962: 1, 1.7404301166534424: 1, -0.08143828064203262: 1, 0.2925977110862732: 1, -0.2210041582584381: 1, 0.31707215309143066: 1, 0.9266675710678101: 1, -0.3739321231842041: 1, 0.8433452844619751: 1, -1.2014092206954956: 1, -1.2016316652297974: 1, 0.09289468824863434: 1, -0.24570585787296295: 1, 0.7288377285003662: 1, 1.3004077672958374: 1, -1.0489486455917358: 1, -0.4453357756137848: 1, -0.12074612081050873: 1, 1.150854468345642: 1, 1.2192449569702148: 1, -0.8759819269180298: 1, 0.9730016589164734: 1, 0.889022946357727: 1, -0.674019455909729: 1, -1.1656997203826904: 1, -1.1972460746765137: 1, 0.055386364459991455: 1, -0.28629931807518005: 1, 0.009196557104587555: 1, 1.256977915763855: 1, 0.23997803032398224: 1, 1.1922990083694458: 1, -0.8705235719680786: 1, 0.8834699392318726: 1, 0.23272329568862915: 1, 0.5795131325721741: 1, 1.0863690376281738: 1, -0.002975589595735073: 1, -0.42487016320228577: 1, -1.1928194761276245: 1, 1.179612159729004: 1, -0.37873539328575134: 1, 0.9235488772392273: 1, -0.5479841232299805: 1, 0.4542813301086426: 1, -0.22304323315620422: 1, -0.12660875916481018: 1, -0.24156887829303741: 1, -0.41674312949180603: 1, 0.654328465461731: 1, 1.1694233417510986: 1, 0.7264453172683716: 1, 0.23339834809303284: 1, 1.2736730575561523: 1, -0.16393840312957764: 1, 0.840314507484436: 1, -0.24846623837947845: 1, -1.2016277313232422: 1, -1.2004797458648682: 1, -0.0411478653550148: 1, -0.09275590628385544: 1, -0.06864805519580841: 1, 0.8631362915039062: 1, 0.8952587246894836: 1, -0.4684484004974365: 1, -0.5749701261520386: 1, 1.150696039199829: 1, 0.9305616617202759: 1, 1.0972230434417725: 1, 0.7325264811515808: 1, -0.25068432092666626: 1, 0.8119110465049744: 1, 0.4251300096511841: 1, -0.2664187550544739: 1, 1.2978951930999756: 1, 0.729964017868042: 1, -0.8322789072990417: 1, 0.4259852468967438: 1, -0.2597183287143707: 1, 0.6872115731239319: 1, 1.035197138786316: 1, -0.1517976075410843: 1, 1.196738839149475: 1, -0.9032037854194641: 1, -0.09905305504798889: 1, -0.37779542803764343: 1, -0.9053927063941956: 1, 0.008864369243383408: 1, 0.8562594652175903: 1, -0.20541705191135406: 1, 0.8532567620277405: 1, 0.6013088822364807: 1, -0.35842111706733704: 1, -1.126828908920288: 1, 1.185150146484375: 1, 0.9848727583885193: 1, -0.1916339248418808: 1, 0.938378632068634: 1, 0.9840258359909058: 1, 0.008470889180898666: 1, -0.2164342701435089: 1, -1.201277732849121: 1, 1.2822530269622803: 1, -0.27776581048965454: 1, -1.1879688501358032: 1, 1.2350752353668213: 1, -0.2299586683511734: 1, 0.008748067542910576: 1, 0.9413108229637146: 1, 0.6657525897026062: 1, -0.1855221837759018: 1, -0.20062661170959473: 1, -0.009843221865594387: 1, -1.1852235794067383: 1, -0.34817975759506226: 1, -1.1653438806533813: 1, 0.029728559777140617: 1, -0.35746997594833374: 1, 1.0611008405685425: 1, -0.025104349479079247: 1, -0.17582924664020538: 1, 0.32448264956474304: 1, 1.2197721004486084: 1, -0.6921688914299011: 1, -0.16533638536930084: 1, -0.9900954961776733: 1, 0.31131237745285034: 1, 1.2513697147369385: 1, -1.1878859996795654: 1, -0.8494775891304016: 1, -1.1774789094924927: 1, -0.12005668878555298: 1, 1.1650327444076538: 1, 1.1985303163528442: 1, 0.0796559751033783: 1, -0.20500345528125763: 1, 0.12959904968738556: 1, -1.194725751876831: 1, 1.2926610708236694: 1, 1.1819933652877808: 1, 1.2464758157730103: 1, 0.05186900869011879: 1, -0.5160067081451416: 1, -0.4923703670501709: 1, -1.0983095169067383: 1, 1.2668349742889404: 1, 0.21761490404605865: 1, -0.809281051158905: 1, -1.1104997396469116: 1, 0.34432777762413025: 1, 1.1265980005264282: 1, 0.02654222585260868: 1, 0.017293494194746017: 1, -0.32669803500175476: 1, -0.253052294254303: 1, -0.8852211833000183: 1, 0.5908839702606201: 1, 1.0087279081344604: 1, 1.2238743305206299: 1, -1.194933295249939: 1, 0.167218878865242: 1, 0.004906347021460533: 1, -0.19495585560798645: 1, -0.3641962707042694: 1, 0.042822714895009995: 1, 1.1279875040054321: 1, 0.7561059594154358: 1, -1.1981785297393799: 1, 0.1316474825143814: 1, 0.29174771904945374: 1, 1.0578463077545166: 1, -0.5372467637062073: 1, -0.24153171479701996: 1, -0.8713244199752808: 1, -0.14272816479206085: 1, 1.1865397691726685: 1, 0.02958042360842228: 1, -0.2098180055618286: 1, 1.2327803373336792: 1, -0.1759326308965683: 1, -0.19223442673683167: 1, 0.7244752645492554: 1, -0.29299479722976685: 1, -0.42109400033950806: 1, -0.6826592683792114: 1, 0.8838387131690979: 1, 0.15707539021968842: 1, 0.41254743933677673: 1, -1.2016282081604004: 1, -1.0583271980285645: 1, -0.7914682626724243: 1, 1.0062413215637207: 1, -0.02250954695045948: 1, -0.23942992091178894: 1, -0.11078709363937378: 1, -1.095828652381897: 1, -0.586733877658844: 1, -0.3953478932380676: 1, 0.7884150743484497: 1, -0.4121406674385071: 1, -1.0349785089492798: 1, 1.0134633779525757: 1, 0.44458866119384766: 1, 1.0013794898986816: 1, 0.02054119110107422: 1, -0.9308528304100037: 1, -0.9307324290275574: 1, -0.35524412989616394: 1, -0.39703771471977234: 1, -0.1827705055475235: 1, -1.2004859447479248: 1, 1.0347987413406372: 1, 0.6233600974082947: 1, 1.2749443054199219: 1, 0.469992071390152: 1, -0.3404213488101959: 1, -0.5937166213989258: 1, -1.2016299962997437: 1, 4.363720893859863: 1, -1.1943039894104004: 1, -1.2015916109085083: 1, -1.179785132408142: 1, -0.11716806888580322: 1, -0.14289136230945587: 1, 0.9705496430397034: 1, 0.36895880103111267: 1, 1.2229245901107788: 1, -1.2016286849975586: 1, -1.2016292810440063: 1, -1.2016345262527466: 1, -1.2015831470489502: 1, -1.2016255855560303: 1, -1.201633095741272: 1, -1.1318936347961426: 1, -1.1987295150756836: 1, -1.2013877630233765: 1, -1.2016326189041138: 1, -1.2015864849090576: 1, -1.201595425605774: 1, -1.201629877090454: 1, -1.1484540700912476: 1, -0.040736664086580276: 1, -0.2904500663280487: 1, 0.6481048464775085: 1, 1.4819786548614502: 1, -0.2126571536064148: 1, 1.018097996711731: 1, 0.2079305797815323: 1, -0.10643515735864639: 1, -1.1586899757385254: 1, -1.0591267347335815: 1, -1.2015695571899414: 1, -1.2016273736953735: 1, -0.16572129726409912: 1, -1.2015702724456787: 1, -1.201588749885559: 1, -1.201620101928711: 1, 0.1331777125597: 1, -1.2016350030899048: 1, 0.0891273021697998: 1, -1.201523780822754: 1, -1.2015215158462524: 1, -0.6771114468574524: 1, -1.1967262029647827: 1, -0.07099064439535141: 1, -0.6676098704338074: 1, -1.2016379833221436: 1, -1.194821834564209: 1, -0.045158740133047104: 1, -0.11703558266162872: 1, 0.1581522524356842: 1, 0.9015107750892639: 1, -0.9676279425621033: 1, -1.1257261037826538: 1, -1.2014601230621338: 1, -0.006573406048119068: 1, -0.5612114071846008: 1, 0.07849415391683578: 1, -1.2016295194625854: 1, 0.8498935103416443: 1, -1.201079249382019: 1, -0.9951181411743164: 1, -1.2003253698349: 1, 0.5647678971290588: 1, -1.1779894828796387: 1, -0.9857455492019653: 1, 0.15111742913722992: 1, -0.6758065819740295: 1, -1.0628424882888794: 1, -1.2016361951828003: 1, -1.2016332149505615: 1, -1.2015197277069092: 1, -1.201634407043457: 1, -1.201407790184021: 1, -1.2016335725784302: 1, -0.7989376187324524: 1, -0.8910970091819763: 1, -1.193373441696167: 1, 0.3015405535697937: 1, -1.1979888677597046: 1, -0.053435858339071274: 1, -1.0203382968902588: 1, 0.2464127391576767: 1, -1.201630711555481: 1, -0.4132004976272583: 1, -1.2011888027191162: 1, -0.4687884449958801: 1, -1.1849600076675415: 1, -0.4057319462299347: 1, -1.2015478610992432: 1, -0.08719541132450104: 1, 0.1322988122701645: 1, -0.10290281474590302: 1, 0.9786769151687622: 1, 1.0520529747009277: 1, -0.1427413374185562: 1, -0.0332360677421093: 1, -0.34523066878318787: 1, 1.1960792541503906: 1, 0.08744630217552185: 1, 0.01664043217897415: 1, -0.6421114802360535: 1, -0.3469093143939972: 1, -0.029267514124512672: 1, -0.21881107985973358: 1, 0.33690160512924194: 1, 0.04192548990249634: 1, -1.1634924411773682: 1, 0.27843114733695984: 1, 0.6842910647392273: 1, 0.5252094268798828: 1, -0.3879204988479614: 1, -0.14631414413452148: 1, -0.4360010027885437: 1, -0.0580214224755764: 1, 0.8548043966293335: 1, 1.2064505815505981: 1, -0.8740671277046204: 1, -0.5884501934051514: 1, -0.84548020362854: 1, 0.2588636577129364: 1, -1.2015588283538818: 1, -0.7430113554000854: 1, 1.3061707019805908: 1, -1.0537559986114502: 1, -1.1755220890045166: 1, -1.0965174436569214: 1, -0.08886344730854034: 1, -1.1905653476715088: 1, -0.6103007793426514: 1, 1.310013771057129: 1, 0.029840881004929543: 1, 0.5362502932548523: 1, 0.5997416377067566: 1, -0.5104274749755859: 1, -0.14243346452713013: 1, 0.5468651056289673: 1, 0.637002170085907: 1, -1.2005233764648438: 1, -0.05695538595318794: 1, -0.28555259108543396: 1, 0.8218473196029663: 1, 0.18457916378974915: 1, 1.200035810470581: 1, 0.5694330334663391: 1, 1.2526917457580566: 1, 1.0069524049758911: 1, 0.8965467214584351: 1, -1.0558182001113892: 1, 0.047311048954725266: 1, 1.1649004220962524: 1, -0.34107181429862976: 1, 1.295539140701294: 1, 1.1717408895492554: 1, 1.2726079225540161: 1, 0.30879271030426025: 1, -0.10037212073802948: 1, 0.9712859988212585: 1, 1.176633596420288: 1, -0.04390183836221695: 1, 0.005373222753405571: 1, 1.1995047330856323: 1, 1.0674824714660645: 1, 0.09893729537725449: 1, 0.8222996592521667: 1, 1.0808240175247192: 1, -0.21433545649051666: 1, 0.3951069414615631: 1, -0.11945565789937973: 1, 0.9529565572738647: 1, 0.29349300265312195: 1, 0.01943967677652836: 1, 0.04932570457458496: 1, -0.30726683139801025: 1, -0.9601404666900635: 1, -0.8751402497291565: 1, -1.1107620000839233: 1, 0.9265954494476318: 1, 1.2916063070297241: 1, -0.9734629392623901: 1, -1.193730115890503: 1, -0.19123347103595734: 1, -0.015705665573477745: 1, 0.8673886060714722: 1, -0.011501064524054527: 1, -0.3390061855316162: 1, 0.3886134922504425: 1, -1.175829291343689: 1, -0.463926762342453: 1, 0.7298784255981445: 1, 1.0769490003585815: 1, 1.28579580783844: 1, -1.2003904581069946: 1} test data: {-1.2016315460205078: 2, -1.2016383409500122: 2, -1.2016254663467407: 2, -1.2016366720199585: 2, -1.2015975713729858: 1, -1.2013576030731201: 1, 1.1950836181640625: 1, 1.621954083442688: 1, 1.5633379220962524: 1, 1.593016266822815: 1, -1.2016290426254272: 1, 1.5627902746200562: 1, 0.670230507850647: 1, -0.916256844997406: 1, -0.9931707978248596: 1, -0.8519163131713867: 1, -1.201623558998108: 1, -0.6465518474578857: 1, -0.5828713178634644: 1, -1.2016363143920898: 1, -1.2015149593353271: 1, -0.6686071157455444: 1, -1.2007629871368408: 1, -0.7713357210159302: 1, 0.21984633803367615: 1, -1.2015992403030396: 1, -1.201633095741272: 1, 1.5667779445648193: 1, 0.2470504194498062: 1, -0.297276109457016: 1, -0.2558962106704712: 1, 0.19487899541854858: 1, -0.06838630884885788: 1, 0.31109386682510376: 1, 0.5247108340263367: 1, 1.8254425525665283: 1, 0.8825770616531372: 1, 0.25592291355133057: 1, 0.2900018095970154: 1, 0.7778733968734741: 1, 0.002237366745248437: 1, 1.6063342094421387: 1, 0.02149348333477974: 1, 1.0667099952697754: 1, 1.5455868244171143: 1, -0.16630633175373077: 1, -0.7750424146652222: 1, 0.7218993902206421: 1, 0.0860099047422409: 1, -0.5330178141593933: 1, -0.008235386572778225: 1, -1.201627254486084: 1, -1.201564908027649: 1, -1.2016369104385376: 1, 1.0165932178497314: 1, 0.18236172199249268: 1, 1.2996296882629395: 1, 1.3062021732330322: 1, -0.37773171067237854: 1, -0.19289743900299072: 1, 1.8768579959869385: 1, 1.8398889303207397: 1, -1.009895920753479: 1, -1.2016119956970215: 1, -0.8897131681442261: 1, -0.408608615398407: 1, -0.2865978181362152: 1, -1.1897622346878052: 1, -0.5717604756355286: 1, 1.3883335590362549: 1, -0.47762712836265564: 1, -0.9396678805351257: 1, -0.7269378900527954: 1, 1.491416096687317: 1, -0.46731990575790405: 1, -0.5843047499656677: 1, 0.47734835743904114: 1, 1.4387927055358887: 1, 1.603068470954895: 1, -1.201596975326538: 1, -1.1976145505905151: 1, -0.09781666100025177: 1, -0.8503693342208862: 1, -1.2014657258987427: 1, -1.1972260475158691: 1, -1.1830931901931763: 1, -0.6864959597587585: 1, -0.33821895718574524: 1, -0.37911587953567505: 1, -1.1482487916946411: 1, -1.2015410661697388: 1, -1.1987890005111694: 1, -0.3579169511795044: 1, -1.1852269172668457: 1, -1.1929398775100708: 1, -0.34237241744995117: 1, -1.1237342357635498: 1, -1.2009947299957275: 1, -1.1961579322814941: 1, 0.8374537825584412: 1, -0.4268537759780884: 1, 0.16395387053489685: 1, 1.4029184579849243: 1, 1.5149681568145752: 1, 0.2610865831375122: 1, -0.35491982102394104: 1, -1.0024443864822388: 1, -1.1569617986679077: 1, 1.3982725143432617: 1, 1.2280430793762207: 1, 0.16367900371551514: 1, 0.5357815027236938: 1, -0.03840658441185951: 1, 0.4252239763736725: 1, 1.4774726629257202: 1, -1.1544640064239502: 1, -0.9035985469818115: 1, 0.342952162027359: 1, -0.9736310243606567: 1, -0.9859256744384766: 1, 1.425097107887268: 1, 1.287703037261963: 1, 1.4413039684295654: 1, -1.1940946578979492: 1, -0.1799832135438919: 1, 1.0683897733688354: 1, -0.514695405960083: 1, 0.9138351678848267: 1, 0.36046868562698364: 1, 0.05307941138744354: 1, -0.9981153011322021: 1, 0.3422311544418335: 1, -0.6311256289482117: 1, -1.1659823656082153: 1, -0.19042661786079407: 1, 1.6431117057800293: 1, -0.1989370882511139: 1, -1.0411947965621948: 1, -0.923246443271637: 1, -1.0751264095306396: 1, -0.35407769680023193: 1, 0.2903974652290344: 1, -0.930094301700592: 1, -0.9288858771324158: 1, 0.006390336435288191: 1, 0.7494425773620605: 1, -0.556195080280304: 1, -0.896551251411438: 1, 1.5516252517700195: 1, 0.7257186770439148: 1, -1.111975073814392: 1, -1.0634658336639404: 1, -1.1138004064559937: 1, 0.4990178942680359: 1, -0.4610443115234375: 1, 0.2775695323944092: 1, 0.28860634565353394: 1, -0.32526895403862: 1, -0.2711203992366791: 1, 1.2712597846984863: 1, 1.573736548423767: 1, 0.3604428768157959: 1, 1.4308977127075195: 1, 1.5277636051177979: 1, -0.7282882928848267: 1, -1.1950759887695312: 1, -0.993471086025238: 1, 0.17100460827350616: 1, 0.6386443972587585: 1, 0.16810350120067596: 1, 1.4644434452056885: 1, 0.6820655465126038: 1, -1.160044550895691: 1, -0.19042591750621796: 1, -1.201636791229248: 1, 0.15692748129367828: 1, 0.34191834926605225: 1, 1.4653488397598267: 1, 1.3042255640029907: 1, 0.15129715204238892: 1, -1.194837212562561: 1, 1.2730098962783813: 1, -0.37734130024909973: 1, 0.15925046801567078: 1, -1.2016338109970093: 1, -1.031872034072876: 1, 0.1467430293560028: 1, 1.7417840957641602: 1, 1.5755736827850342: 1, 0.4746771454811096: 1, 0.5222756862640381: 1, 0.9129876494407654: 1, 0.27488669753074646: 1, 0.7110782265663147: 1, 1.2984728813171387: 1, -0.8986467123031616: 1, -1.1914112567901611: 1, 1.5811641216278076: 1, -0.027011625468730927: 1, 1.3250715732574463: 1, 0.2308363914489746: 1, 1.2588475942611694: 1, -0.24819114804267883: 1, 1.427193522453308: 1, -0.39828142523765564: 1, 1.6178103685379028: 1, 0.16428887844085693: 1, -0.2670007050037384: 1, -0.29989245533943176: 1, 0.5326334834098816: 1, 1.2604477405548096: 1, 1.633592128753662: 1, 1.3466922044754028: 1, -1.042400598526001: 1, -1.201629877090454: 1, 0.14208731055259705: 1, -0.10222127288579941: 1, 1.7350994348526: 1, 1.5665374994277954: 1, 1.5698747634887695: 1, -0.9877029061317444: 1, 0.4860861599445343: 1, 0.23050570487976074: 1, -0.009660118259489536: 1, -1.174880027770996: 1, 1.7429335117340088: 1, 1.92928946018219: 1, 1.7397531270980835: 1, 1.2302463054656982: 1, -0.8916471600532532: 1, 1.8896540403366089: 1, 0.8007993698120117: 1, -1.1416743993759155: 1, -1.201619267463684: 1, 1.114488959312439: 1, 0.9832457304000854: 1, 1.8846700191497803: 1, 0.6905592679977417: 1, 1.1628241539001465: 1, -0.5665901303291321: 1, 1.6153100728988647: 1, -0.2079317569732666: 1, 1.6500415802001953: 1, 0.9454357028007507: 1, 0.521833598613739: 1, 1.0101338624954224: 1, 0.5497986674308777: 1, -0.41689032316207886: 1, 0.08808748424053192: 1, 1.6464146375656128: 1, 0.523788332939148: 1, -1.2016352415084839: 1, -0.2495342493057251: 1, 0.3117649555206299: 1, -1.1663979291915894: 1, 1.8360271453857422: 1, 1.5793328285217285: 1, 0.42848649621009827: 1, -0.4143541157245636: 1, 1.0329307317733765: 1, 0.05316608399152756: 1, 0.6376197934150696: 1, 0.261216938495636: 1, 1.271135687828064: 1, 0.562964141368866: 1, 0.6071187853813171: 1, 0.9784976243972778: 1, 1.610649824142456: 1, 0.8005363941192627: 1, -1.012891411781311: 1, -1.2016215324401855: 1, 0.963599681854248: 1, -0.2255825698375702: 1, 1.8285820484161377: 1, 1.3490053415298462: 1, -0.9789384603500366: 1, 1.3510494232177734: 1, -0.6586742401123047: 1, 1.3847272396087646: 1, 1.2369016408920288: 1, -1.2016295194625854: 1, 0.8895251154899597: 1, -1.2016236782073975: 1, 0.8969712257385254: 1, 1.519827961921692: 1, -0.538144052028656: 1, -1.2015867233276367: 1, 1.127722144126892: 1, -0.22918304800987244: 1, 1.7338974475860596: 1, -1.1562846899032593: 1, 0.8217967748641968: 1, 0.8861773610115051: 1, 1.887890100479126: 1, 0.09368380159139633: 1, -0.796614408493042: 1, -0.8602240085601807: 1, 1.746630072593689: 1, 5.270293235778809: 1, -0.1335328370332718: 1, 0.32922476530075073: 1, 1.7499927282333374: 1, 1.5742621421813965: 1, -0.420527845621109: 1, 1.6787821054458618: 1, 1.455495834350586: 1, -1.1579643487930298: 1, 0.8198267817497253: 1, 0.46223318576812744: 1, 0.7045637369155884: 1, 1.4337563514709473: 1, -0.08275699615478516: 1, -0.21501778066158295: 1, -0.28641819953918457: 1, -1.1991149187088013: 1, 0.6026607155799866: 1, -1.2014148235321045: 1, -1.193153738975525: 1, -0.3858093321323395: 1, 1.3953920602798462: 1, 1.878305196762085: 1, 1.5919677019119263: 1, -0.5378462672233582: 1, 1.6570682525634766: 1, 0.4647309482097626: 1, 0.16830426454544067: 1, 0.1589841991662979: 1, 1.8601784706115723: 1, -1.1991721391677856: 1, -0.7995052933692932: 1, -1.201271653175354: 1, 0.4937743842601776: 1, -0.539626955986023: 1, -0.7939702272415161: 1, -0.5884281992912292: 1, -0.7822737693786621: 1, -0.07709828019142151: 1, -0.3439752459526062: 1, 1.9232004880905151: 1, -0.5713714957237244: 1, -1.0160380601882935: 1, 0.9851498603820801: 1, -0.4548023045063019: 1, 1.6648616790771484: 1, 0.3571058511734009: 1, -0.3349834084510803: 1, 0.37810415029525757: 1, -1.0031712055206299: 1, -0.3702247440814972: 1, 1.4788439273834229: 1, 1.2964091300964355: 1, -1.2009780406951904: 1, -0.92970210313797: 1, -0.23732632398605347: 1, 0.7785534858703613: 1, 1.456301212310791: 1, 1.2142442464828491: 1, -0.29665860533714294: 1, -0.558110237121582: 1, 0.2143784463405609: 1, -1.1422733068466187: 1, -0.6015652418136597: 1, 0.7481110692024231: 1, -0.3426608741283417: 1, 1.7555813789367676: 1, -1.1569743156433105: 1, -0.28438881039619446: 1, 0.15644948184490204: 1, 1.4984081983566284: 1, 0.956155002117157: 1, -1.0813374519348145: 1, 1.525660514831543: 1, -0.6562255024909973: 1, 0.04079057276248932: 1, 1.905008316040039: 1, 0.012895430438220501: 1, -0.14373785257339478: 1, 0.611980676651001: 1, -0.27935805916786194: 1, -1.2016310691833496: 1, -0.9310538172721863: 1, -0.8965012431144714: 1, 1.6278821229934692: 1, 0.3740043342113495: 1, 0.056893277913331985: 1, -0.7948459982872009: 1, 1.6034691333770752: 1, -0.39972129464149475: 1, 0.8109627962112427: 1, -0.9147067666053772: 1, -0.36108481884002686: 1, -1.2016171216964722: 1, -1.201569676399231: 1, 4.282702445983887: 1, -1.1108695268630981: 1, -1.0460647344589233: 1, -1.1475187540054321: 1, 0.9462845921516418: 1, -1.1969398260116577: 1, -0.9751223921775818: 1, -1.199577808380127: 1, -0.7289202809333801: 1, -1.1694631576538086: 1, 3.341240167617798: 1, -1.1236215829849243: 1, -0.8602992296218872: 1, -1.201583981513977: 1, -1.2016146183013916: 1, 0.5123543739318848: 1, -1.1760764122009277: 1, -0.13741447031497955: 1, 3.259727716445923: 1, 0.17207567393779755: 1, -1.078048825263977: 1, 0.0014178809942677617: 1, -0.5374748706817627: 1, 0.040903303772211075: 1, 1.1114397048950195: 1, 0.03431294485926628: 1, -1.1873303651809692: 1, -0.35624369978904724: 1, -0.3301823139190674: 1, -1.2016326189041138: 1, -0.25587567687034607: 1, 1.0866621732711792: 1, -0.7650251388549805: 1, -0.01552529539912939: 1, -1.1513574123382568: 1, -0.16514527797698975: 1, -0.11618001013994217: 1, -1.1654343605041504: 1, -0.26975223422050476: 1, 0.05442709103226662: 1, -0.5088394284248352: 1, 1.2035483121871948: 1, -1.2008949518203735: 1, -0.22990889847278595: 1, 0.5761882066726685: 1, -0.40984249114990234: 1, -0.981871485710144: 1, -1.186597228050232: 1, 0.3790889084339142: 1, 0.6585915088653564: 1, 0.19746625423431396: 1, 1.1413462162017822: 1, 0.3299405574798584: 1, 0.9907589554786682: 1, -0.671938955783844: 1, -0.7864221334457397: 1, -0.7683218121528625: 1, -1.193078875541687: 1, -1.169080138206482: 1, -0.4976504147052765: 1, 0.7875488996505737: 1, -1.2011059522628784: 1, -0.8982016444206238: 1, -0.35893186926841736: 1, -1.0079307556152344: 1, -1.1992580890655518: 1, -0.4707425832748413: 1, -1.0164505243301392: 1, -1.1747305393218994: 1, -0.7597726583480835: 1, 1.2829649448394775: 1, 0.03461417928338051: 1, 0.7100340127944946: 1, 0.43160757422447205: 1, 1.274375557899475: 1, 1.0399528741836548: 1, -0.4809620976448059: 1, 0.2788977324962616: 1, -0.5744653940200806: 1, -1.201636552810669: 1, 0.2699461281299591: 1, -0.024909501895308495: 1, 0.8297379016876221: 1, -0.27802959084510803: 1, -0.12913857400417328: 1, -0.5908447504043579: 1, -0.5622422695159912: 1, 1.0467203855514526: 1, -0.7624772787094116: 1, 0.6605957746505737: 1, -0.019019143655896187: 1, 1.104429006576538: 1, 0.525627076625824: 1, -0.6482374668121338: 1, -0.13838951289653778: 1, -0.11709770560264587: 1, -0.19212104380130768: 1, -0.36745402216911316: 1, -0.3780987560749054: 1, 0.022439440712332726: 1, -0.15591250360012054: 1, 0.021945519372820854: 1, -0.32367783784866333: 1, -0.19999556243419647: 1, 0.26051849126815796: 1, 0.053155358880758286: 1, -0.056251220405101776: 1, -0.9567206501960754: 1, -0.2265346348285675: 1, -0.9494366645812988: 1, -0.18796367943286896: 1, -0.06410756707191467: 1, 0.6152291297912598: 1, 1.182131290435791: 1, -0.2588912844657898: 1, 1.3108209371566772: 1, -0.1045512706041336: 1, -0.01294313371181488: 1, 1.0150052309036255: 1, -1.2016351222991943: 1, -0.7266088128089905: 1, -0.29541367292404175: 1, -0.5272284746170044: 1, -0.7811260223388672: 1, -0.862193763256073: 1, -0.6588079929351807: 1, -1.1501390933990479: 1, -0.2236318439245224: 1, -1.200749397277832: 1, -0.5631559491157532: 1, -1.1923733949661255: 1, -1.1858601570129395: 1, -0.6137503981590271: 1, -0.788809597492218: 1, -1.1350377798080444: 1, -1.180148720741272: 1, -1.0963668823242188: 1, -1.201621413230896: 1, -1.2016000747680664: 1, -0.17838822305202484: 1, -1.0323843955993652: 1, -1.2013036012649536: 1, -1.1981604099273682: 1, -1.2016135454177856: 1, -1.0986745357513428: 1, -1.1784441471099854: 1, -0.7622874975204468: 1, -1.194840431213379: 1, -0.7443411946296692: 1, -1.2015254497528076: 1, -1.05448579788208: 1, -1.1445417404174805: 1, -0.07361169159412384: 1, 5.066896438598633: 1, -0.7420345544815063: 1, -0.21827441453933716: 1, -0.7001152634620667: 1, -0.9021695256233215: 1, -1.0501618385314941: 1, -0.9485399723052979: 1, -0.4794164001941681: 1, -0.8791208267211914: 1, 0.216363787651062: 1, -0.38882899284362793: 1, -1.167614459991455: 1, -1.1940333843231201: 1, 0.29684698581695557: 1, -0.712040364742279: 1, -0.5385211110115051: 1, -1.0223268270492554: 1, -1.0399388074874878: 1, -1.0281414985656738: 1, -0.5079992413520813: 1, -0.7359880805015564: 1, -1.192929744720459: 1, -1.2016303539276123: 1, 1.0147548913955688: 1, 1.2256038188934326: 1, -0.3316475749015808: 1, 0.21256738901138306: 1, -1.1689379215240479: 1, 0.5858985185623169: 1, -0.30494359135627747: 1, 0.7670885920524597: 1, 1.2671806812286377: 1, -1.0224525928497314: 1, -0.042717207223176956: 1, -1.2002415657043457: 1, -0.39157962799072266: 1, -1.2016355991363525: 1, 0.8209275603294373: 1, -0.41031748056411743: 1, 0.03998654708266258: 1, -1.1866058111190796: 1, 1.1708778142929077: 1, 1.177069067955017: 1, -1.1549781560897827: 1, 1.0160198211669922: 1, 0.4960012137889862: 1, -0.4005826711654663: 1, -0.8111313581466675: 1, -0.4003660976886749: 1, -1.1973918676376343: 1, -1.1970906257629395: 1, -1.1995155811309814: 1, -0.04312499612569809: 1, -0.7167530059814453: 1, 1.6113003492355347: 1, 0.29118791222572327: 1, 0.5599294900894165: 1, -0.6900844573974609: 1, -1.1882411241531372: 1, 1.5164122581481934: 1, -1.2010202407836914: 1, -0.23251420259475708: 1, -0.15451399981975555: 1, -0.1682627946138382: 1, -0.433001846075058: 1, 1.5002496242523193: 1, -1.1626108884811401: 1, 0.009732205420732498: 1, -0.45371508598327637: 1, -1.2016377449035645: 1, -0.23531334102153778: 1, -0.15267345309257507: 1, 1.2373515367507935: 1, 1.4972656965255737: 1, 1.1887938976287842: 1, -0.0526600144803524: 1, 0.06706182658672333: 1, 0.18826737999916077: 1, 1.4727312326431274: 1, 0.24058645963668823: 1, 0.3425867557525635: 1, 0.35902467370033264: 1, 1.2999117374420166: 1, 0.3157300651073456: 1, 1.0554699897766113: 1, 0.81052166223526: 1, 1.3231751918792725: 1, 1.556430697441101: 1, 0.7825908064842224: 1, 1.375723958015442: 1, -0.2681446373462677: 1, 0.3024345636367798: 1, 1.4752575159072876: 1, -0.2946179509162903: 1, 0.31122344732284546: 1, 1.0482161045074463: 1, 0.3319496512413025: 1, 1.4751214981079102: 1, -0.16230207681655884: 1, 0.2539007067680359: 1, 1.111115574836731: 1, 1.5909359455108643: 1, 0.017385760322213173: 1, 0.3231067657470703: 1, -0.06798223406076431: 1, 1.3918044567108154: 1, 1.4282907247543335: 1, 0.19260334968566895: 1, 0.36868804693222046: 1, 1.4527193307876587: 1, 1.5988372564315796: 1, 1.3192821741104126: 1, 1.533202886581421: 1, 0.19674475491046906: 1, -0.3147757053375244: 1, -0.11516252905130386: 1, 0.7623692750930786: 1, -1.1871042251586914: 1, -1.0057076215744019: 1, 0.8502970337867737: 1, -0.6454114317893982: 1, 0.5573470592498779: 1, 0.3554361164569855: 1, 1.5724915266036987: 1, -1.086830973625183: 1, 0.9053120613098145: 1, 0.7253832817077637: 1, 0.7161386013031006: 1, 0.27865079045295715: 1, 1.3759030103683472: 1, 0.7280606031417847: 1, 1.5217971801757812: 1, -0.091583751142025: 1, 0.2824403941631317: 1, 0.3697844445705414: 1, 0.6463801264762878: 1, -0.30866673588752747: 1, 0.7275790572166443: 1, 1.556864857673645: 1, 0.8999701142311096: 1, -1.1999285221099854: 1, 1.0112850666046143: 1, 0.638069212436676: 1, 0.6570013761520386: 1, 0.39953649044036865: 1, 0.0663938894867897: 1, 4.863577365875244: 1, 1.6108869314193726: 1, -0.16245944797992706: 1, -1.1837621927261353: 1, 0.24872112274169922: 1, 1.4051384925842285: 1, 0.5472216606140137: 1, 0.8098611235618591: 1, 1.5127235651016235: 1, 0.02118554152548313: 1, 0.8342987895011902: 1, 0.23464715480804443: 1, 0.9422100782394409: 1, 2.4050354957580566: 1, -0.20047886669635773: 1, 0.8222253322601318: 1, -1.0314160585403442: 1, 0.5769325494766235: 1, 1.4151798486709595: 1, -1.0716415643692017: 1, 1.368375539779663: 1, -0.11868320405483246: 1, 0.3126457929611206: 1, 1.5009726285934448: 1, 1.547389030456543: 1, -0.6971253156661987: 1, 0.7346283793449402: 1, -0.4757806956768036: 1, -0.11954380571842194: 1, 1.2413678169250488: 1, -0.2671576142311096: 1, 0.24701066315174103: 1, -1.1247143745422363: 1, -0.007039392367005348: 1, -0.621107280254364: 1, 1.0945255756378174: 1, -0.9168434739112854: 1, -0.8882886171340942: 1, 0.23268936574459076: 1, -0.39580973982810974: 1, 0.6777730584144592: 1, -0.9972583055496216: 1, -0.33581042289733887: 1, -0.16826894879341125: 1, 1.113309621810913: 1, 0.015011060051620007: 1, 0.09383262693881989: 1, 0.36468705534935: 1, 0.6232529878616333: 1, -1.1812138557434082: 1, 0.9394524693489075: 1, -1.1963841915130615: 1, 1.009254813194275: 1, -0.012521528638899326: 1, 0.8318881988525391: 1, 0.5979693531990051: 1, 1.039072036743164: 1, -0.0449078269302845: 1, -1.2016297578811646: 1, -0.04175446555018425: 1, 0.4987215995788574: 1, -0.816495954990387: 1, 0.4205377995967865: 1, 0.033690646290779114: 1, 1.0187321901321411: 1, 1.1683013439178467: 1, 1.1942393779754639: 1, 1.2479115724563599: 1, -0.22866979241371155: 1, -1.1798354387283325: 1, 0.6535213589668274: 1, 1.2529374361038208: 1, 1.2188843488693237: 1, 0.8476697206497192: 1, -0.18254651129245758: 1, 0.838370680809021: 1, -1.1211071014404297: 1, -0.14633353054523468: 1, 1.2879470586776733: 1, -1.140577793121338: 1, -1.2016345262527466: 1, -1.1363192796707153: 1, -1.2016382217407227: 1, -1.2015154361724854: 1, -1.1954847574234009: 1, -1.2013384103775024: 1, -1.1490250825881958: 1, -0.28742867708206177: 1, -0.518195390701294: 1, -0.5834662318229675: 1, -1.2015115022659302: 1, 0.86496901512146: 1, 0.9066123366355896: 1, -0.21048426628112793: 1, -1.2016206979751587: 1, -0.41403406858444214: 1, -1.180970549583435: 1, 0.20136801898479462: 1, -1.150158166885376: 1, -0.6426911950111389: 1, -0.8209242224693298: 1, -0.5093275308609009: 1, -0.0922759622335434: 1, -1.2015819549560547: 1, -1.1923046112060547: 1, -1.201493740081787: 1, -1.2016342878341675: 1, -1.1947468519210815: 1, 3.9672465324401855: 1, -0.37206733226776123: 1, 0.8610304594039917: 1, -0.33075013756752014: 1, -0.24580752849578857: 1, -1.0237786769866943: 1, -0.47594985365867615: 1, -0.16829612851142883: 1, -0.1688508540391922: 1, -0.3213060796260834: 1, -1.1992485523223877: 1, -1.2015316486358643: 1, -1.201473593711853: 1, 0.19668835401535034: 1, 2.26233172416687: 1, -0.848010778427124: 1, -1.1948002576828003: 1, -1.2015341520309448: 1, 0.1497194468975067: 1, -1.2014774084091187: 1, -0.8653848767280579: 1}
INFO     Network type: REG
INFO     Model: "sequential_1"
INFO     _________________________________________________________________
INFO     Layer (type)                 Output Shape              Param #
INFO     =================================================================
INFO     dense_5 (Dense)              (None, 1024)              2098176
INFO     _________________________________________________________________
INFO     dropout_4 (Dropout)          (None, 1024)              0
INFO     _________________________________________________________________
INFO     dense_6 (Dense)              (None, 512)               524800
INFO     _________________________________________________________________
INFO     dropout_5 (Dropout)          (None, 512)               0
INFO     _________________________________________________________________
INFO     dense_7 (Dense)              (None, 256)               131328
INFO     _________________________________________________________________
INFO     dropout_6 (Dropout)          (None, 256)               0
INFO     _________________________________________________________________
INFO     dense_8 (Dense)              (None, 128)               32896
INFO     _________________________________________________________________
INFO     dropout_7 (Dropout)          (None, 128)               0
INFO     _________________________________________________________________
INFO     dense_9 (Dense)              (None, 1)                 129
INFO     =================================================================
INFO     Total params: 2,787,329
INFO     Trainable params: 2,787,329
INFO     Non-trainable params: 0
INFO     _________________________________________________________________
WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.
WARNING  `period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.
2024-07-15 22:51:15.458304: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0
2024-07-15 22:51:15.458483: I tensorflow/core/grappler/clusters/single_machine.cc:357] Starting new session
2024-07-15 22:51:15.461020: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:1137] Optimization results for grappler item: graph_to_optimize
  function_optimizer: function_optimizer did nothing. time = 0.011ms.
  function_optimizer: function_optimizer did nothing. time = 0.001ms.
Epoch 1/5000
26/26 - 2s - loss: 4.7950 - val_loss: 4.7470
Epoch 2/5000
26/26 - 1s - loss: 4.7073 - val_loss: 4.6781
Epoch 3/5000
26/26 - 1s - loss: 4.6437 - val_loss: 4.6374
Epoch 4/5000
26/26 - 1s - loss: 4.6052 - val_loss: 4.6126
Epoch 5/5000
26/26 - 1s - loss: 4.5785 - val_loss: 4.5893
Epoch 6/5000
26/26 - 1s - loss: 4.5525 - val_loss: 4.5665
Epoch 7/5000
26/26 - 1s - loss: 4.5270 - val_loss: 4.5443
Epoch 8/5000
26/26 - 1s - loss: 4.5055 - val_loss: 4.5231
Epoch 9/5000
26/26 - 1s - loss: 4.4837 - val_loss: 4.5035
Epoch 10/5000
26/26 - 1s - loss: 4.4632 - val_loss: 4.4844
Epoch 00010: val_loss improved from inf to 4.48440, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 11/5000
26/26 - 1s - loss: 4.4431 - val_loss: 4.4662
Epoch 12/5000
26/26 - 1s - loss: 4.4226 - val_loss: 4.4493
Epoch 13/5000
26/26 - 1s - loss: 4.4046 - val_loss: 4.4337
Epoch 14/5000
26/26 - 1s - loss: 4.3874 - val_loss: 4.4196
Epoch 15/5000
26/26 - 1s - loss: 4.3704 - val_loss: 4.4058
Epoch 16/5000
26/26 - 1s - loss: 4.3528 - val_loss: 4.3920
Epoch 17/5000
26/26 - 1s - loss: 4.3369 - val_loss: 4.3792
Epoch 18/5000
26/26 - 1s - loss: 4.3209 - val_loss: 4.3663
Epoch 19/5000
26/26 - 1s - loss: 4.3037 - val_loss: 4.3538
Epoch 20/5000
26/26 - 1s - loss: 4.2898 - val_loss: 4.3418
Epoch 00020: val_loss improved from 4.48440 to 4.34175, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 21/5000
26/26 - 1s - loss: 4.2725 - val_loss: 4.3292
Epoch 22/5000
26/26 - 1s - loss: 4.2560 - val_loss: 4.3168
Epoch 23/5000
26/26 - 1s - loss: 4.2415 - val_loss: 4.3048
Epoch 24/5000
26/26 - 1s - loss: 4.2267 - val_loss: 4.2931
Epoch 25/5000
26/26 - 1s - loss: 4.2115 - val_loss: 4.2820
Epoch 26/5000
26/26 - 1s - loss: 4.1955 - val_loss: 4.2702
Epoch 27/5000
26/26 - 1s - loss: 4.1813 - val_loss: 4.2591
Epoch 28/5000
26/26 - 1s - loss: 4.1664 - val_loss: 4.2470
Epoch 29/5000
26/26 - 1s - loss: 4.1492 - val_loss: 4.2362
Epoch 30/5000
26/26 - 1s - loss: 4.1342 - val_loss: 4.2266
Epoch 00030: val_loss improved from 4.34175 to 4.22655, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 31/5000
26/26 - 1s - loss: 4.1208 - val_loss: 4.2172
Epoch 32/5000
26/26 - 1s - loss: 4.1067 - val_loss: 4.2045
Epoch 33/5000
26/26 - 1s - loss: 4.0918 - val_loss: 4.1951
Epoch 34/5000
26/26 - 1s - loss: 4.0774 - val_loss: 4.1855
Epoch 35/5000
26/26 - 1s - loss: 4.0630 - val_loss: 4.1752
Epoch 36/5000
26/26 - 1s - loss: 4.0466 - val_loss: 4.1640
Epoch 37/5000
26/26 - 1s - loss: 4.0347 - val_loss: 4.1565
Epoch 38/5000
26/26 - 1s - loss: 4.0209 - val_loss: 4.1454
Epoch 39/5000
26/26 - 1s - loss: 4.0078 - val_loss: 4.1353
Epoch 40/5000
26/26 - 1s - loss: 3.9941 - val_loss: 4.1270
Epoch 00040: val_loss improved from 4.22655 to 4.12702, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 41/5000
26/26 - 1s - loss: 3.9803 - val_loss: 4.1166
Epoch 42/5000
26/26 - 1s - loss: 3.9661 - val_loss: 4.1081
Epoch 43/5000
26/26 - 1s - loss: 3.9538 - val_loss: 4.1013
Epoch 44/5000
26/26 - 1s - loss: 3.9405 - val_loss: 4.0895
Epoch 45/5000
26/26 - 1s - loss: 3.9276 - val_loss: 4.0809
Epoch 46/5000
26/26 - 1s - loss: 3.9150 - val_loss: 4.0741
Epoch 47/5000
26/26 - 1s - loss: 3.9043 - val_loss: 4.0640
Epoch 48/5000
26/26 - 1s - loss: 3.8891 - val_loss: 4.0585
Epoch 49/5000
26/26 - 1s - loss: 3.8778 - val_loss: 4.0464
Epoch 50/5000
26/26 - 1s - loss: 3.8675 - val_loss: 4.0394
Epoch 00050: val_loss improved from 4.12702 to 4.03944, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 51/5000
26/26 - 1s - loss: 3.8535 - val_loss: 4.0291
Epoch 52/5000
26/26 - 1s - loss: 3.8446 - val_loss: 4.0226
Epoch 53/5000
26/26 - 1s - loss: 3.8320 - val_loss: 4.0148
Epoch 54/5000
26/26 - 1s - loss: 3.8205 - val_loss: 4.0085
Epoch 55/5000
26/26 - 1s - loss: 3.8101 - val_loss: 3.9995
Epoch 56/5000
26/26 - 1s - loss: 3.7971 - val_loss: 3.9902
Epoch 57/5000
26/26 - 1s - loss: 3.7868 - val_loss: 3.9826
Epoch 58/5000
26/26 - 1s - loss: 3.7763 - val_loss: 3.9752
Epoch 59/5000
26/26 - 1s - loss: 3.7668 - val_loss: 3.9679
Epoch 60/5000
26/26 - 1s - loss: 3.7543 - val_loss: 3.9614
Epoch 00060: val_loss improved from 4.03944 to 3.96139, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 61/5000
26/26 - 1s - loss: 3.7433 - val_loss: 3.9523
Epoch 62/5000
26/26 - 1s - loss: 3.7331 - val_loss: 3.9455
Epoch 63/5000
26/26 - 1s - loss: 3.7246 - val_loss: 3.9373
Epoch 64/5000
26/26 - 1s - loss: 3.7146 - val_loss: 3.9305
Epoch 65/5000
26/26 - 1s - loss: 3.7026 - val_loss: 3.9234
Epoch 66/5000
26/26 - 1s - loss: 3.6970 - val_loss: 3.9159
Epoch 67/5000
26/26 - 1s - loss: 3.6830 - val_loss: 3.9091
Epoch 68/5000
26/26 - 1s - loss: 3.6758 - val_loss: 3.9016
Epoch 69/5000
26/26 - 1s - loss: 3.6661 - val_loss: 3.8972
Epoch 70/5000
26/26 - 1s - loss: 3.6573 - val_loss: 3.8899
Epoch 00070: val_loss improved from 3.96139 to 3.88986, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 71/5000
26/26 - 1s - loss: 3.6474 - val_loss: 3.8837
Epoch 72/5000
26/26 - 1s - loss: 3.6364 - val_loss: 3.8759
Epoch 73/5000
26/26 - 1s - loss: 3.6312 - val_loss: 3.8694
Epoch 74/5000
26/26 - 1s - loss: 3.6197 - val_loss: 3.8632
Epoch 75/5000
26/26 - 1s - loss: 3.6116 - val_loss: 3.8557
Epoch 76/5000
26/26 - 1s - loss: 3.6036 - val_loss: 3.8508
Epoch 77/5000
26/26 - 2s - loss: 3.5948 - val_loss: 3.8431
Epoch 78/5000
26/26 - 1s - loss: 3.5880 - val_loss: 3.8364
Epoch 79/5000
26/26 - 1s - loss: 3.5778 - val_loss: 3.8309
Epoch 80/5000
26/26 - 1s - loss: 3.5698 - val_loss: 3.8253
Epoch 00080: val_loss improved from 3.88986 to 3.82529, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 81/5000
26/26 - 1s - loss: 3.5599 - val_loss: 3.8174
Epoch 82/5000
26/26 - 1s - loss: 3.5528 - val_loss: 3.8110
Epoch 83/5000
26/26 - 1s - loss: 3.5480 - val_loss: 3.8031
Epoch 84/5000
26/26 - 1s - loss: 3.5374 - val_loss: 3.7975
Epoch 85/5000
26/26 - 1s - loss: 3.5309 - val_loss: 3.7920
Epoch 86/5000
26/26 - 1s - loss: 3.5210 - val_loss: 3.7878
Epoch 87/5000
26/26 - 1s - loss: 3.5128 - val_loss: 3.7802
Epoch 88/5000
26/26 - 1s - loss: 3.5079 - val_loss: 3.7737
Epoch 89/5000
26/26 - 1s - loss: 3.4965 - val_loss: 3.7664
Epoch 90/5000
26/26 - 1s - loss: 3.4910 - val_loss: 3.7618
Epoch 00090: val_loss improved from 3.82529 to 3.76179, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 91/5000
26/26 - 1s - loss: 3.4827 - val_loss: 3.7573
Epoch 92/5000
26/26 - 1s - loss: 3.4749 - val_loss: 3.7506
Epoch 93/5000
26/26 - 1s - loss: 3.4688 - val_loss: 3.7426
Epoch 94/5000
26/26 - 1s - loss: 3.4604 - val_loss: 3.7386
Epoch 95/5000
26/26 - 1s - loss: 3.4553 - val_loss: 3.7313
Epoch 96/5000
26/26 - 1s - loss: 3.4483 - val_loss: 3.7250
Epoch 97/5000
26/26 - 1s - loss: 3.4395 - val_loss: 3.7202
Epoch 98/5000
26/26 - 1s - loss: 3.4307 - val_loss: 3.7136
Epoch 99/5000
26/26 - 1s - loss: 3.4279 - val_loss: 3.7080
Epoch 100/5000
26/26 - 1s - loss: 3.4191 - val_loss: 3.7026
Epoch 00100: val_loss improved from 3.76179 to 3.70260, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 101/5000
26/26 - 1s - loss: 3.4099 - val_loss: 3.6978
Epoch 102/5000
26/26 - 1s - loss: 3.4037 - val_loss: 3.6940
Epoch 103/5000
26/26 - 1s - loss: 3.3963 - val_loss: 3.6899
Epoch 104/5000
26/26 - 1s - loss: 3.3903 - val_loss: 3.6827
Epoch 105/5000
26/26 - 1s - loss: 3.3851 - val_loss: 3.6773
Epoch 106/5000
26/26 - 1s - loss: 3.3765 - val_loss: 3.6711
Epoch 107/5000
26/26 - 1s - loss: 3.3691 - val_loss: 3.6668
Epoch 108/5000
26/26 - 1s - loss: 3.3628 - val_loss: 3.6614
Epoch 109/5000
26/26 - 1s - loss: 3.3589 - val_loss: 3.6566
Epoch 110/5000
26/26 - 1s - loss: 3.3537 - val_loss: 3.6527
Epoch 00110: val_loss improved from 3.70260 to 3.65266, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 111/5000
26/26 - 1s - loss: 3.3446 - val_loss: 3.6449
Epoch 112/5000
26/26 - 1s - loss: 3.3408 - val_loss: 3.6417
Epoch 113/5000
26/26 - 1s - loss: 3.3321 - val_loss: 3.6370
Epoch 114/5000
26/26 - 1s - loss: 3.3258 - val_loss: 3.6310
Epoch 115/5000
26/26 - 1s - loss: 3.3203 - val_loss: 3.6284
Epoch 116/5000
26/26 - 1s - loss: 3.3116 - val_loss: 3.6220
Epoch 117/5000
26/26 - 1s - loss: 3.3092 - val_loss: 3.6150
Epoch 118/5000
26/26 - 1s - loss: 3.3000 - val_loss: 3.6112
Epoch 119/5000
26/26 - 1s - loss: 3.2968 - val_loss: 3.6058
Epoch 120/5000
26/26 - 1s - loss: 3.2905 - val_loss: 3.6008
Epoch 00120: val_loss improved from 3.65266 to 3.60083, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 121/5000
26/26 - 1s - loss: 3.2863 - val_loss: 3.5980
Epoch 122/5000
26/26 - 1s - loss: 3.2802 - val_loss: 3.5917
Epoch 123/5000
26/26 - 1s - loss: 3.2721 - val_loss: 3.5876
Epoch 124/5000
26/26 - 1s - loss: 3.2668 - val_loss: 3.5846
Epoch 125/5000
26/26 - 1s - loss: 3.2607 - val_loss: 3.5776
Epoch 126/5000
26/26 - 1s - loss: 3.2544 - val_loss: 3.5738
Epoch 127/5000
26/26 - 1s - loss: 3.2492 - val_loss: 3.5707
Epoch 128/5000
26/26 - 1s - loss: 3.2462 - val_loss: 3.5678
Epoch 129/5000
26/26 - 1s - loss: 3.2371 - val_loss: 3.5604
Epoch 130/5000
26/26 - 1s - loss: 3.2313 - val_loss: 3.5516
Epoch 00130: val_loss improved from 3.60083 to 3.55159, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 131/5000
26/26 - 1s - loss: 3.2275 - val_loss: 3.5495
Epoch 132/5000
26/26 - 1s - loss: 3.2198 - val_loss: 3.5467
Epoch 133/5000
26/26 - 1s - loss: 3.2198 - val_loss: 3.5414
Epoch 134/5000
26/26 - 1s - loss: 3.2089 - val_loss: 3.5437
Epoch 135/5000
26/26 - 1s - loss: 3.2050 - val_loss: 3.5346
Epoch 136/5000
26/26 - 1s - loss: 3.1981 - val_loss: 3.5276
Epoch 137/5000
26/26 - 1s - loss: 3.1960 - val_loss: 3.5231
Epoch 138/5000
26/26 - 1s - loss: 3.1884 - val_loss: 3.5170
Epoch 139/5000
26/26 - 1s - loss: 3.1843 - val_loss: 3.5148
Epoch 140/5000
26/26 - 1s - loss: 3.1785 - val_loss: 3.5107
Epoch 00140: val_loss improved from 3.55159 to 3.51065, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 141/5000
26/26 - 1s - loss: 3.1749 - val_loss: 3.5073
Epoch 142/5000
26/26 - 1s - loss: 3.1675 - val_loss: 3.5030
Epoch 143/5000
26/26 - 1s - loss: 3.1602 - val_loss: 3.4960
Epoch 144/5000
26/26 - 1s - loss: 3.1589 - val_loss: 3.4935
Epoch 145/5000
26/26 - 1s - loss: 3.1487 - val_loss: 3.4890
Epoch 146/5000
26/26 - 1s - loss: 3.1476 - val_loss: 3.4842
Epoch 147/5000
26/26 - 1s - loss: 3.1434 - val_loss: 3.4805
Epoch 148/5000
26/26 - 1s - loss: 3.1373 - val_loss: 3.4758
Epoch 149/5000
26/26 - 1s - loss: 3.1324 - val_loss: 3.4729
Epoch 150/5000
26/26 - 1s - loss: 3.1283 - val_loss: 3.4682
Epoch 00150: val_loss improved from 3.51065 to 3.46820, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 151/5000
26/26 - 1s - loss: 3.1231 - val_loss: 3.4635
Epoch 152/5000
26/26 - 1s - loss: 3.1187 - val_loss: 3.4568
Epoch 153/5000
26/26 - 1s - loss: 3.1148 - val_loss: 3.4540
Epoch 154/5000
26/26 - 1s - loss: 3.1068 - val_loss: 3.4497
Epoch 155/5000
26/26 - 2s - loss: 3.1035 - val_loss: 3.4445
Epoch 156/5000
26/26 - 1s - loss: 3.1007 - val_loss: 3.4403
Epoch 157/5000
26/26 - 1s - loss: 3.0937 - val_loss: 3.4370
Epoch 158/5000
26/26 - 2s - loss: 3.0901 - val_loss: 3.4319
Epoch 159/5000
26/26 - 2s - loss: 3.0878 - val_loss: 3.4295
Epoch 160/5000
26/26 - 1s - loss: 3.0802 - val_loss: 3.4244
Epoch 00160: val_loss improved from 3.46820 to 3.42444, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 161/5000
26/26 - 1s - loss: 3.0761 - val_loss: 3.4197
Epoch 162/5000
26/26 - 1s - loss: 3.0689 - val_loss: 3.4153
Epoch 163/5000
26/26 - 1s - loss: 3.0660 - val_loss: 3.4129
Epoch 164/5000
26/26 - 1s - loss: 3.0637 - val_loss: 3.4077
Epoch 165/5000
26/26 - 1s - loss: 3.0564 - val_loss: 3.4030
Epoch 166/5000
26/26 - 1s - loss: 3.0528 - val_loss: 3.3998
Epoch 167/5000
26/26 - 1s - loss: 3.0474 - val_loss: 3.3964
Epoch 168/5000
26/26 - 1s - loss: 3.0424 - val_loss: 3.3931
Epoch 169/5000
26/26 - 1s - loss: 3.0405 - val_loss: 3.3899
Epoch 170/5000
26/26 - 1s - loss: 3.0348 - val_loss: 3.3875
Epoch 00170: val_loss improved from 3.42444 to 3.38749, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 171/5000
26/26 - 1s - loss: 3.0308 - val_loss: 3.3820
Epoch 172/5000
26/26 - 1s - loss: 3.0257 - val_loss: 3.3778
Epoch 173/5000
26/26 - 1s - loss: 3.0231 - val_loss: 3.3717
Epoch 174/5000
26/26 - 1s - loss: 3.0167 - val_loss: 3.3703
Epoch 175/5000
26/26 - 1s - loss: 3.0159 - val_loss: 3.3638
Epoch 176/5000
26/26 - 1s - loss: 3.0095 - val_loss: 3.3619
Epoch 177/5000
26/26 - 1s - loss: 3.0048 - val_loss: 3.3579
Epoch 178/5000
26/26 - 1s - loss: 3.0001 - val_loss: 3.3537
Epoch 179/5000
26/26 - 1s - loss: 2.9965 - val_loss: 3.3502
Epoch 180/5000
26/26 - 1s - loss: 2.9930 - val_loss: 3.3496
Epoch 00180: val_loss improved from 3.38749 to 3.34962, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 181/5000
26/26 - 1s - loss: 2.9882 - val_loss: 3.3421
Epoch 182/5000
26/26 - 1s - loss: 2.9825 - val_loss: 3.3378
Epoch 183/5000
26/26 - 1s - loss: 2.9801 - val_loss: 3.3349
Epoch 184/5000
26/26 - 1s - loss: 2.9732 - val_loss: 3.3301
Epoch 185/5000
26/26 - 1s - loss: 2.9704 - val_loss: 3.3283
Epoch 186/5000
26/26 - 1s - loss: 2.9656 - val_loss: 3.3254
Epoch 187/5000
26/26 - 1s - loss: 2.9630 - val_loss: 3.3191
Epoch 188/5000
26/26 - 1s - loss: 2.9586 - val_loss: 3.3168
Epoch 189/5000
26/26 - 2s - loss: 2.9550 - val_loss: 3.3134
Epoch 190/5000
26/26 - 1s - loss: 2.9467 - val_loss: 3.3079
Epoch 00190: val_loss improved from 3.34962 to 3.30785, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 191/5000
26/26 - 1s - loss: 2.9446 - val_loss: 3.3050
Epoch 192/5000
26/26 - 1s - loss: 2.9431 - val_loss: 3.2998
Epoch 193/5000
26/26 - 1s - loss: 2.9405 - val_loss: 3.2969
Epoch 194/5000
26/26 - 1s - loss: 2.9352 - val_loss: 3.2921
Epoch 195/5000
26/26 - 1s - loss: 2.9284 - val_loss: 3.2860
Epoch 196/5000
26/26 - 1s - loss: 2.9225 - val_loss: 3.2857
Epoch 197/5000
26/26 - 1s - loss: 2.9216 - val_loss: 3.2810
Epoch 198/5000
26/26 - 1s - loss: 2.9149 - val_loss: 3.2794
Epoch 199/5000
26/26 - 1s - loss: 2.9107 - val_loss: 3.2753
Epoch 200/5000
26/26 - 1s - loss: 2.9108 - val_loss: 3.2719
Epoch 00200: val_loss improved from 3.30785 to 3.27186, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 201/5000
26/26 - 1s - loss: 2.9050 - val_loss: 3.2684
Epoch 202/5000
26/26 - 1s - loss: 2.9030 - val_loss: 3.2666
Epoch 203/5000
26/26 - 1s - loss: 2.8991 - val_loss: 3.2620
Epoch 204/5000
26/26 - 1s - loss: 2.8943 - val_loss: 3.2571
Epoch 205/5000
26/26 - 1s - loss: 2.8890 - val_loss: 3.2537
Epoch 206/5000
26/26 - 1s - loss: 2.8828 - val_loss: 3.2503
Epoch 207/5000
26/26 - 1s - loss: 2.8828 - val_loss: 3.2472
Epoch 208/5000
26/26 - 1s - loss: 2.8784 - val_loss: 3.2435
Epoch 209/5000
26/26 - 1s - loss: 2.8759 - val_loss: 3.2400
Epoch 210/5000
26/26 - 1s - loss: 2.8695 - val_loss: 3.2368
Epoch 00210: val_loss improved from 3.27186 to 3.23681, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 211/5000
26/26 - 1s - loss: 2.8666 - val_loss: 3.2331
Epoch 212/5000
26/26 - 1s - loss: 2.8618 - val_loss: 3.2286
Epoch 213/5000
26/26 - 1s - loss: 2.8593 - val_loss: 3.2255
Epoch 214/5000
26/26 - 1s - loss: 2.8558 - val_loss: 3.2204
Epoch 215/5000
26/26 - 1s - loss: 2.8499 - val_loss: 3.2197
Epoch 216/5000
26/26 - 1s - loss: 2.8470 - val_loss: 3.2177
Epoch 217/5000
26/26 - 1s - loss: 2.8422 - val_loss: 3.2114
Epoch 218/5000
26/26 - 1s - loss: 2.8411 - val_loss: 3.2083
Epoch 219/5000
26/26 - 1s - loss: 2.8393 - val_loss: 3.2045
Epoch 220/5000
26/26 - 1s - loss: 2.8351 - val_loss: 3.2025
Epoch 00220: val_loss improved from 3.23681 to 3.20252, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 221/5000
26/26 - 1s - loss: 2.8275 - val_loss: 3.1971
Epoch 222/5000
26/26 - 1s - loss: 2.8255 - val_loss: 3.1941
Epoch 223/5000
26/26 - 1s - loss: 2.8226 - val_loss: 3.1913
Epoch 224/5000
26/26 - 1s - loss: 2.8189 - val_loss: 3.1881
Epoch 225/5000
26/26 - 1s - loss: 2.8145 - val_loss: 3.1823
Epoch 226/5000
26/26 - 1s - loss: 2.8095 - val_loss: 3.1804
Epoch 227/5000
26/26 - 1s - loss: 2.8096 - val_loss: 3.1775
Epoch 228/5000
26/26 - 1s - loss: 2.8024 - val_loss: 3.1735
Epoch 229/5000
26/26 - 1s - loss: 2.7988 - val_loss: 3.1703
Epoch 230/5000
26/26 - 1s - loss: 2.7973 - val_loss: 3.1662
Epoch 00230: val_loss improved from 3.20252 to 3.16622, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 231/5000
26/26 - 1s - loss: 2.7931 - val_loss: 3.1657
Epoch 232/5000
26/26 - 1s - loss: 2.7900 - val_loss: 3.1609
Epoch 233/5000
26/26 - 1s - loss: 2.7870 - val_loss: 3.1585
Epoch 234/5000
26/26 - 1s - loss: 2.7854 - val_loss: 3.1554
Epoch 235/5000
26/26 - 1s - loss: 2.7781 - val_loss: 3.1530
Epoch 236/5000
26/26 - 1s - loss: 2.7756 - val_loss: 3.1485
Epoch 237/5000
26/26 - 1s - loss: 2.7700 - val_loss: 3.1442
Epoch 238/5000
26/26 - 1s - loss: 2.7692 - val_loss: 3.1421
Epoch 239/5000
26/26 - 1s - loss: 2.7632 - val_loss: 3.1380
Epoch 240/5000
26/26 - 1s - loss: 2.7627 - val_loss: 3.1363
Epoch 00240: val_loss improved from 3.16622 to 3.13625, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 241/5000
26/26 - 1s - loss: 2.7592 - val_loss: 3.1333
Epoch 242/5000
26/26 - 1s - loss: 2.7554 - val_loss: 3.1282
Epoch 243/5000
26/26 - 1s - loss: 2.7510 - val_loss: 3.1238
Epoch 244/5000
26/26 - 1s - loss: 2.7493 - val_loss: 3.1219
Epoch 245/5000
26/26 - 1s - loss: 2.7453 - val_loss: 3.1208
Epoch 246/5000
26/26 - 2s - loss: 2.7425 - val_loss: 3.1159
Epoch 247/5000
26/26 - 1s - loss: 2.7406 - val_loss: 3.1115
Epoch 248/5000
26/26 - 1s - loss: 2.7358 - val_loss: 3.1085
Epoch 249/5000
26/26 - 1s - loss: 2.7292 - val_loss: 3.1082
Epoch 250/5000
26/26 - 1s - loss: 2.7276 - val_loss: 3.1044
Epoch 00250: val_loss improved from 3.13625 to 3.10442, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 251/5000
26/26 - 1s - loss: 2.7241 - val_loss: 3.1032
Epoch 252/5000
26/26 - 1s - loss: 2.7211 - val_loss: 3.0992
Epoch 253/5000
26/26 - 1s - loss: 2.7180 - val_loss: 3.0946
Epoch 254/5000
26/26 - 1s - loss: 2.7138 - val_loss: 3.0919
Epoch 255/5000
26/26 - 1s - loss: 2.7136 - val_loss: 3.0896
Epoch 256/5000
26/26 - 1s - loss: 2.7074 - val_loss: 3.0824
Epoch 257/5000
26/26 - 1s - loss: 2.7044 - val_loss: 3.0788
Epoch 258/5000
26/26 - 1s - loss: 2.7034 - val_loss: 3.0772
Epoch 259/5000
26/26 - 1s - loss: 2.7000 - val_loss: 3.0772
Epoch 260/5000
26/26 - 1s - loss: 2.6952 - val_loss: 3.0729
Epoch 00260: val_loss improved from 3.10442 to 3.07294, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 261/5000
26/26 - 1s - loss: 2.6918 - val_loss: 3.0694
Epoch 262/5000
26/26 - 1s - loss: 2.6899 - val_loss: 3.0658
Epoch 263/5000
26/26 - 1s - loss: 2.6861 - val_loss: 3.0618
Epoch 264/5000
26/26 - 1s - loss: 2.6823 - val_loss: 3.0600
Epoch 265/5000
26/26 - 1s - loss: 2.6754 - val_loss: 3.0551
Epoch 266/5000
26/26 - 1s - loss: 2.6772 - val_loss: 3.0532
Epoch 267/5000
26/26 - 1s - loss: 2.6738 - val_loss: 3.0517
Epoch 268/5000
26/26 - 1s - loss: 2.6682 - val_loss: 3.0498
Epoch 269/5000
26/26 - 1s - loss: 2.6662 - val_loss: 3.0452
Epoch 270/5000
26/26 - 1s - loss: 2.6628 - val_loss: 3.0414
Epoch 00270: val_loss improved from 3.07294 to 3.04142, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 271/5000
26/26 - 1s - loss: 2.6597 - val_loss: 3.0387
Epoch 272/5000
26/26 - 1s - loss: 2.6582 - val_loss: 3.0363
Epoch 273/5000
26/26 - 1s - loss: 2.6530 - val_loss: 3.0323
Epoch 274/5000
26/26 - 1s - loss: 2.6509 - val_loss: 3.0315
Epoch 275/5000
26/26 - 1s - loss: 2.6472 - val_loss: 3.0282
Epoch 276/5000
26/26 - 1s - loss: 2.6439 - val_loss: 3.0256
Epoch 277/5000
26/26 - 1s - loss: 2.6423 - val_loss: 3.0242
Epoch 278/5000
26/26 - 1s - loss: 2.6391 - val_loss: 3.0199
Epoch 279/5000
26/26 - 1s - loss: 2.6340 - val_loss: 3.0157
Epoch 280/5000
26/26 - 1s - loss: 2.6309 - val_loss: 3.0157
Epoch 00280: val_loss improved from 3.04142 to 3.01574, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 281/5000
26/26 - 1s - loss: 2.6301 - val_loss: 3.0109
Epoch 282/5000
26/26 - 1s - loss: 2.6250 - val_loss: 3.0077
Epoch 283/5000
26/26 - 1s - loss: 2.6225 - val_loss: 3.0046
Epoch 284/5000
26/26 - 1s - loss: 2.6182 - val_loss: 3.0007
Epoch 285/5000
26/26 - 1s - loss: 2.6181 - val_loss: 2.9985
Epoch 286/5000
26/26 - 1s - loss: 2.6110 - val_loss: 2.9967
Epoch 287/5000
26/26 - 1s - loss: 2.6086 - val_loss: 2.9937
Epoch 288/5000
26/26 - 1s - loss: 2.6067 - val_loss: 2.9894
Epoch 289/5000
26/26 - 1s - loss: 2.6042 - val_loss: 2.9872
Epoch 290/5000
26/26 - 1s - loss: 2.5981 - val_loss: 2.9846
Epoch 00290: val_loss improved from 3.01574 to 2.98457, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 291/5000
26/26 - 1s - loss: 2.5976 - val_loss: 2.9811
Epoch 292/5000
26/26 - 1s - loss: 2.5944 - val_loss: 2.9788
Epoch 293/5000
26/26 - 1s - loss: 2.5920 - val_loss: 2.9746
Epoch 294/5000
26/26 - 1s - loss: 2.5898 - val_loss: 2.9738
Epoch 295/5000
26/26 - 1s - loss: 2.5856 - val_loss: 2.9731
Epoch 296/5000
26/26 - 1s - loss: 2.5819 - val_loss: 2.9687
Epoch 297/5000
26/26 - 1s - loss: 2.5799 - val_loss: 2.9664
Epoch 298/5000
26/26 - 1s - loss: 2.5790 - val_loss: 2.9641
Epoch 299/5000
26/26 - 1s - loss: 2.5740 - val_loss: 2.9595
Epoch 300/5000
26/26 - 1s - loss: 2.5715 - val_loss: 2.9596
Epoch 00300: val_loss improved from 2.98457 to 2.95956, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 301/5000
26/26 - 1s - loss: 2.5690 - val_loss: 2.9563
Epoch 302/5000
26/26 - 1s - loss: 2.5651 - val_loss: 2.9511
Epoch 303/5000
26/26 - 1s - loss: 2.5623 - val_loss: 2.9489
Epoch 304/5000
26/26 - 1s - loss: 2.5615 - val_loss: 2.9443
Epoch 305/5000
26/26 - 1s - loss: 2.5571 - val_loss: 2.9406
Epoch 306/5000
26/26 - 1s - loss: 2.5556 - val_loss: 2.9400
Epoch 307/5000
26/26 - 1s - loss: 2.5527 - val_loss: 2.9356
Epoch 308/5000
26/26 - 1s - loss: 2.5466 - val_loss: 2.9329
Epoch 309/5000
26/26 - 1s - loss: 2.5442 - val_loss: 2.9285
Epoch 310/5000
26/26 - 1s - loss: 2.5435 - val_loss: 2.9289
Epoch 00310: val_loss improved from 2.95956 to 2.92886, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 311/5000
26/26 - 1s - loss: 2.5376 - val_loss: 2.9265
Epoch 312/5000
26/26 - 1s - loss: 2.5353 - val_loss: 2.9238
Epoch 313/5000
26/26 - 1s - loss: 2.5307 - val_loss: 2.9194
Epoch 314/5000
26/26 - 1s - loss: 2.5284 - val_loss: 2.9174
Epoch 315/5000
26/26 - 1s - loss: 2.5293 - val_loss: 2.9160
Epoch 316/5000
26/26 - 1s - loss: 2.5240 - val_loss: 2.9104
Epoch 317/5000
26/26 - 1s - loss: 2.5206 - val_loss: 2.9094
Epoch 318/5000
26/26 - 1s - loss: 2.5194 - val_loss: 2.9049
Epoch 319/5000
26/26 - 1s - loss: 2.5144 - val_loss: 2.9025
Epoch 320/5000
26/26 - 1s - loss: 2.5137 - val_loss: 2.9005
Epoch 00320: val_loss improved from 2.92886 to 2.90046, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 321/5000
26/26 - 1s - loss: 2.5122 - val_loss: 2.8984
Epoch 322/5000
26/26 - 1s - loss: 2.5077 - val_loss: 2.8964
Epoch 323/5000
26/26 - 1s - loss: 2.5066 - val_loss: 2.8935
Epoch 324/5000
26/26 - 1s - loss: 2.5029 - val_loss: 2.8907
Epoch 325/5000
26/26 - 1s - loss: 2.5007 - val_loss: 2.8879
Epoch 326/5000
26/26 - 1s - loss: 2.4964 - val_loss: 2.8849
Epoch 327/5000
26/26 - 1s - loss: 2.4954 - val_loss: 2.8821
Epoch 328/5000
26/26 - 1s - loss: 2.4903 - val_loss: 2.8790
Epoch 329/5000
26/26 - 1s - loss: 2.4851 - val_loss: 2.8765
Epoch 330/5000
26/26 - 1s - loss: 2.4860 - val_loss: 2.8742
Epoch 00330: val_loss improved from 2.90046 to 2.87420, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 331/5000
26/26 - 1s - loss: 2.4801 - val_loss: 2.8727
Epoch 332/5000
26/26 - 1s - loss: 2.4816 - val_loss: 2.8685
Epoch 333/5000
26/26 - 1s - loss: 2.4761 - val_loss: 2.8661
Epoch 334/5000
26/26 - 1s - loss: 2.4752 - val_loss: 2.8629
Epoch 335/5000
26/26 - 1s - loss: 2.4712 - val_loss: 2.8616
Epoch 336/5000
26/26 - 1s - loss: 2.4681 - val_loss: 2.8585
Epoch 337/5000
26/26 - 1s - loss: 2.4661 - val_loss: 2.8582
Epoch 338/5000
26/26 - 1s - loss: 2.4622 - val_loss: 2.8529
Epoch 339/5000
26/26 - 1s - loss: 2.4613 - val_loss: 2.8524
Epoch 340/5000
26/26 - 1s - loss: 2.4568 - val_loss: 2.8493
Epoch 00340: val_loss improved from 2.87420 to 2.84933, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 341/5000
26/26 - 1s - loss: 2.4566 - val_loss: 2.8442
Epoch 342/5000
26/26 - 1s - loss: 2.4524 - val_loss: 2.8424
Epoch 343/5000
26/26 - 1s - loss: 2.4503 - val_loss: 2.8408
Epoch 344/5000
26/26 - 1s - loss: 2.4475 - val_loss: 2.8373
Epoch 345/5000
26/26 - 1s - loss: 2.4451 - val_loss: 2.8334
Epoch 346/5000
26/26 - 1s - loss: 2.4417 - val_loss: 2.8304
Epoch 347/5000
26/26 - 1s - loss: 2.4373 - val_loss: 2.8295
Epoch 348/5000
26/26 - 1s - loss: 2.4347 - val_loss: 2.8283
Epoch 349/5000
26/26 - 1s - loss: 2.4331 - val_loss: 2.8251
Epoch 350/5000
26/26 - 1s - loss: 2.4310 - val_loss: 2.8224
Epoch 00350: val_loss improved from 2.84933 to 2.82245, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 351/5000
26/26 - 1s - loss: 2.4278 - val_loss: 2.8211
Epoch 352/5000
26/26 - 1s - loss: 2.4270 - val_loss: 2.8175
Epoch 353/5000
26/26 - 1s - loss: 2.4241 - val_loss: 2.8155
Epoch 354/5000
26/26 - 1s - loss: 2.4185 - val_loss: 2.8122
Epoch 355/5000
26/26 - 1s - loss: 2.4173 - val_loss: 2.8111
Epoch 356/5000
26/26 - 1s - loss: 2.4165 - val_loss: 2.8076
Epoch 357/5000
26/26 - 1s - loss: 2.4102 - val_loss: 2.8053
Epoch 358/5000
26/26 - 1s - loss: 2.4104 - val_loss: 2.8018
Epoch 359/5000
26/26 - 1s - loss: 2.4064 - val_loss: 2.8005
Epoch 360/5000
26/26 - 1s - loss: 2.4051 - val_loss: 2.7992
Epoch 00360: val_loss improved from 2.82245 to 2.79920, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 361/5000
26/26 - 1s - loss: 2.4037 - val_loss: 2.7938
Epoch 362/5000
26/26 - 1s - loss: 2.3984 - val_loss: 2.7908
Epoch 363/5000
26/26 - 1s - loss: 2.3952 - val_loss: 2.7876
Epoch 364/5000
26/26 - 1s - loss: 2.3940 - val_loss: 2.7872
Epoch 365/5000
26/26 - 1s - loss: 2.3910 - val_loss: 2.7848
Epoch 366/5000
26/26 - 1s - loss: 2.3889 - val_loss: 2.7821
Epoch 367/5000
26/26 - 1s - loss: 2.3852 - val_loss: 2.7785
Epoch 368/5000
26/26 - 1s - loss: 2.3826 - val_loss: 2.7766
Epoch 369/5000
26/26 - 1s - loss: 2.3794 - val_loss: 2.7741
Epoch 370/5000
26/26 - 1s - loss: 2.3792 - val_loss: 2.7709
Epoch 00370: val_loss improved from 2.79920 to 2.77087, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 371/5000
26/26 - 1s - loss: 2.3779 - val_loss: 2.7676
Epoch 372/5000
26/26 - 1s - loss: 2.3731 - val_loss: 2.7666
Epoch 373/5000
26/26 - 1s - loss: 2.3698 - val_loss: 2.7647
Epoch 374/5000
26/26 - 1s - loss: 2.3686 - val_loss: 2.7605
Epoch 375/5000
26/26 - 1s - loss: 2.3659 - val_loss: 2.7559
Epoch 376/5000
26/26 - 1s - loss: 2.3630 - val_loss: 2.7551
Epoch 377/5000
26/26 - 1s - loss: 2.3617 - val_loss: 2.7538
Epoch 378/5000
26/26 - 1s - loss: 2.3554 - val_loss: 2.7521
Epoch 379/5000
26/26 - 1s - loss: 2.3554 - val_loss: 2.7485
Epoch 380/5000
26/26 - 1s - loss: 2.3509 - val_loss: 2.7461
Epoch 00380: val_loss improved from 2.77087 to 2.74610, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 381/5000
26/26 - 1s - loss: 2.3492 - val_loss: 2.7447
Epoch 382/5000
26/26 - 1s - loss: 2.3485 - val_loss: 2.7421
Epoch 383/5000
26/26 - 1s - loss: 2.3468 - val_loss: 2.7407
Epoch 384/5000
26/26 - 1s - loss: 2.3432 - val_loss: 2.7360
Epoch 385/5000
26/26 - 1s - loss: 2.3418 - val_loss: 2.7355
Epoch 386/5000
26/26 - 1s - loss: 2.3371 - val_loss: 2.7332
Epoch 387/5000
26/26 - 1s - loss: 2.3369 - val_loss: 2.7313
Epoch 388/5000
26/26 - 1s - loss: 2.3328 - val_loss: 2.7290
Epoch 389/5000
26/26 - 1s - loss: 2.3297 - val_loss: 2.7268
Epoch 390/5000
26/26 - 1s - loss: 2.3283 - val_loss: 2.7243
Epoch 00390: val_loss improved from 2.74610 to 2.72426, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 391/5000
26/26 - 1s - loss: 2.3221 - val_loss: 2.7231
Epoch 392/5000
26/26 - 1s - loss: 2.3226 - val_loss: 2.7185
Epoch 393/5000
26/26 - 1s - loss: 2.3217 - val_loss: 2.7168
Epoch 394/5000
26/26 - 1s - loss: 2.3173 - val_loss: 2.7152
Epoch 395/5000
26/26 - 1s - loss: 2.3132 - val_loss: 2.7122
Epoch 396/5000
26/26 - 1s - loss: 2.3146 - val_loss: 2.7093
Epoch 397/5000
26/26 - 1s - loss: 2.3106 - val_loss: 2.7051
Epoch 398/5000
26/26 - 1s - loss: 2.3098 - val_loss: 2.7036
Epoch 399/5000
26/26 - 1s - loss: 2.3030 - val_loss: 2.7002
Epoch 400/5000
26/26 - 1s - loss: 2.3049 - val_loss: 2.6973
Epoch 00400: val_loss improved from 2.72426 to 2.69730, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 401/5000
26/26 - 1s - loss: 2.3018 - val_loss: 2.6962
Epoch 402/5000
26/26 - 1s - loss: 2.2985 - val_loss: 2.6945
Epoch 403/5000
26/26 - 1s - loss: 2.2977 - val_loss: 2.6948
Epoch 404/5000
26/26 - 1s - loss: 2.2952 - val_loss: 2.6902
Epoch 405/5000
26/26 - 1s - loss: 2.2903 - val_loss: 2.6886
Epoch 406/5000
26/26 - 1s - loss: 2.2882 - val_loss: 2.6865
Epoch 407/5000
26/26 - 1s - loss: 2.2885 - val_loss: 2.6828
Epoch 408/5000
26/26 - 1s - loss: 2.2845 - val_loss: 2.6801
Epoch 409/5000
26/26 - 1s - loss: 2.2812 - val_loss: 2.6793
Epoch 410/5000
26/26 - 1s - loss: 2.2802 - val_loss: 2.6784
Epoch 00410: val_loss improved from 2.69730 to 2.67842, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 411/5000
26/26 - 1s - loss: 2.2790 - val_loss: 2.6741
Epoch 412/5000
26/26 - 1s - loss: 2.2744 - val_loss: 2.6704
Epoch 413/5000
26/26 - 1s - loss: 2.2706 - val_loss: 2.6677
Epoch 414/5000
26/26 - 1s - loss: 2.2688 - val_loss: 2.6674
Epoch 415/5000
26/26 - 1s - loss: 2.2684 - val_loss: 2.6633
Epoch 416/5000
26/26 - 1s - loss: 2.2644 - val_loss: 2.6643
Epoch 417/5000
26/26 - 1s - loss: 2.2635 - val_loss: 2.6598
Epoch 418/5000
26/26 - 1s - loss: 2.2593 - val_loss: 2.6576
Epoch 419/5000
26/26 - 1s - loss: 2.2555 - val_loss: 2.6541
Epoch 420/5000
26/26 - 1s - loss: 2.2534 - val_loss: 2.6509
Epoch 00420: val_loss improved from 2.67842 to 2.65086, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 421/5000
26/26 - 1s - loss: 2.2554 - val_loss: 2.6496
Epoch 422/5000
26/26 - 1s - loss: 2.2518 - val_loss: 2.6479
Epoch 423/5000
26/26 - 1s - loss: 2.2479 - val_loss: 2.6472
Epoch 424/5000
26/26 - 1s - loss: 2.2453 - val_loss: 2.6440
Epoch 425/5000
26/26 - 1s - loss: 2.2426 - val_loss: 2.6424
Epoch 426/5000
26/26 - 1s - loss: 2.2401 - val_loss: 2.6386
Epoch 427/5000
26/26 - 1s - loss: 2.2405 - val_loss: 2.6369
Epoch 428/5000
26/26 - 1s - loss: 2.2353 - val_loss: 2.6344
Epoch 429/5000
26/26 - 1s - loss: 2.2355 - val_loss: 2.6332
Epoch 430/5000
26/26 - 1s - loss: 2.2307 - val_loss: 2.6295
Epoch 00430: val_loss improved from 2.65086 to 2.62951, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 431/5000
26/26 - 1s - loss: 2.2307 - val_loss: 2.6294
Epoch 432/5000
26/26 - 1s - loss: 2.2280 - val_loss: 2.6255
Epoch 433/5000
26/26 - 1s - loss: 2.2242 - val_loss: 2.6228
Epoch 434/5000
26/26 - 1s - loss: 2.2234 - val_loss: 2.6206
Epoch 435/5000
26/26 - 1s - loss: 2.2214 - val_loss: 2.6177
Epoch 436/5000
26/26 - 1s - loss: 2.2174 - val_loss: 2.6169
Epoch 437/5000
26/26 - 1s - loss: 2.2150 - val_loss: 2.6161
Epoch 438/5000
26/26 - 1s - loss: 2.2137 - val_loss: 2.6124
Epoch 439/5000
26/26 - 1s - loss: 2.2122 - val_loss: 2.6102
Epoch 440/5000
26/26 - 1s - loss: 2.2094 - val_loss: 2.6056
Epoch 00440: val_loss improved from 2.62951 to 2.60557, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 441/5000
26/26 - 1s - loss: 2.2078 - val_loss: 2.6043
Epoch 442/5000
26/26 - 1s - loss: 2.2020 - val_loss: 2.6016
Epoch 443/5000
26/26 - 1s - loss: 2.1994 - val_loss: 2.5991
Epoch 444/5000
26/26 - 1s - loss: 2.2002 - val_loss: 2.5985
Epoch 445/5000
26/26 - 1s - loss: 2.1979 - val_loss: 2.5967
Epoch 446/5000
26/26 - 1s - loss: 2.1961 - val_loss: 2.5931
Epoch 447/5000
26/26 - 1s - loss: 2.1942 - val_loss: 2.5922
Epoch 448/5000
26/26 - 1s - loss: 2.1914 - val_loss: 2.5898
Epoch 449/5000
26/26 - 1s - loss: 2.1889 - val_loss: 2.5872
Epoch 450/5000
26/26 - 1s - loss: 2.1870 - val_loss: 2.5844
Epoch 00450: val_loss improved from 2.60557 to 2.58442, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 451/5000
26/26 - 1s - loss: 2.1842 - val_loss: 2.5817
Epoch 452/5000
26/26 - 1s - loss: 2.1824 - val_loss: 2.5795
Epoch 453/5000
26/26 - 1s - loss: 2.1790 - val_loss: 2.5806
Epoch 454/5000
26/26 - 1s - loss: 2.1754 - val_loss: 2.5756
Epoch 455/5000
26/26 - 1s - loss: 2.1770 - val_loss: 2.5738
Epoch 456/5000
26/26 - 1s - loss: 2.1737 - val_loss: 2.5712
Epoch 457/5000
26/26 - 1s - loss: 2.1689 - val_loss: 2.5678
Epoch 458/5000
26/26 - 1s - loss: 2.1687 - val_loss: 2.5654
Epoch 459/5000
26/26 - 1s - loss: 2.1657 - val_loss: 2.5649
Epoch 460/5000
26/26 - 1s - loss: 2.1629 - val_loss: 2.5642
Epoch 00460: val_loss improved from 2.58442 to 2.56415, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 461/5000
26/26 - 1s - loss: 2.1622 - val_loss: 2.5606
Epoch 462/5000
26/26 - 1s - loss: 2.1616 - val_loss: 2.5566
Epoch 463/5000
26/26 - 1s - loss: 2.1572 - val_loss: 2.5552
Epoch 464/5000
26/26 - 1s - loss: 2.1580 - val_loss: 2.5536
Epoch 465/5000
26/26 - 1s - loss: 2.1539 - val_loss: 2.5508
Epoch 466/5000
26/26 - 1s - loss: 2.1497 - val_loss: 2.5490
Epoch 467/5000
26/26 - 1s - loss: 2.1472 - val_loss: 2.5480
Epoch 468/5000
26/26 - 1s - loss: 2.1459 - val_loss: 2.5463
Epoch 469/5000
26/26 - 1s - loss: 2.1448 - val_loss: 2.5446
Epoch 470/5000
26/26 - 1s - loss: 2.1424 - val_loss: 2.5412
Epoch 00470: val_loss improved from 2.56415 to 2.54121, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 471/5000
26/26 - 1s - loss: 2.1393 - val_loss: 2.5404
Epoch 472/5000
26/26 - 1s - loss: 2.1368 - val_loss: 2.5382
Epoch 473/5000
26/26 - 1s - loss: 2.1359 - val_loss: 2.5360
Epoch 474/5000
26/26 - 1s - loss: 2.1338 - val_loss: 2.5343
Epoch 475/5000
26/26 - 1s - loss: 2.1316 - val_loss: 2.5340
Epoch 476/5000
26/26 - 1s - loss: 2.1302 - val_loss: 2.5321
Epoch 477/5000
26/26 - 1s - loss: 2.1285 - val_loss: 2.5274
Epoch 478/5000
26/26 - 1s - loss: 2.1240 - val_loss: 2.5258
Epoch 479/5000
26/26 - 1s - loss: 2.1240 - val_loss: 2.5235
Epoch 480/5000
26/26 - 1s - loss: 2.1201 - val_loss: 2.5207
Epoch 00480: val_loss improved from 2.54121 to 2.52074, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 481/5000
26/26 - 1s - loss: 2.1196 - val_loss: 2.5204
Epoch 482/5000
26/26 - 1s - loss: 2.1174 - val_loss: 2.5162
Epoch 483/5000
26/26 - 1s - loss: 2.1156 - val_loss: 2.5138
Epoch 484/5000
26/26 - 1s - loss: 2.1116 - val_loss: 2.5128
Epoch 485/5000
26/26 - 1s - loss: 2.1094 - val_loss: 2.5108
Epoch 486/5000
26/26 - 1s - loss: 2.1092 - val_loss: 2.5100
Epoch 487/5000
26/26 - 1s - loss: 2.1039 - val_loss: 2.5051
Epoch 488/5000
26/26 - 1s - loss: 2.1041 - val_loss: 2.5029
Epoch 489/5000
26/26 - 1s - loss: 2.1008 - val_loss: 2.5019
Epoch 490/5000
26/26 - 1s - loss: 2.0988 - val_loss: 2.4992
Epoch 00490: val_loss improved from 2.52074 to 2.49919, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 491/5000
26/26 - 1s - loss: 2.0987 - val_loss: 2.4978
Epoch 492/5000
26/26 - 1s - loss: 2.0956 - val_loss: 2.4958
Epoch 493/5000
26/26 - 1s - loss: 2.0924 - val_loss: 2.4934
Epoch 494/5000
26/26 - 1s - loss: 2.0902 - val_loss: 2.4917
Epoch 495/5000
26/26 - 1s - loss: 2.0875 - val_loss: 2.4899
Epoch 496/5000
26/26 - 1s - loss: 2.0887 - val_loss: 2.4888
Epoch 497/5000
26/26 - 1s - loss: 2.0846 - val_loss: 2.4867
Epoch 498/5000
26/26 - 2s - loss: 2.0815 - val_loss: 2.4834
Epoch 499/5000
26/26 - 1s - loss: 2.0826 - val_loss: 2.4809
Epoch 500/5000
26/26 - 1s - loss: 2.0763 - val_loss: 2.4794
Epoch 00500: val_loss improved from 2.49919 to 2.47939, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 501/5000
26/26 - 1s - loss: 2.0765 - val_loss: 2.4796
Epoch 502/5000
26/26 - 1s - loss: 2.0724 - val_loss: 2.4772
Epoch 503/5000
26/26 - 1s - loss: 2.0737 - val_loss: 2.4747
Epoch 504/5000
26/26 - 1s - loss: 2.0705 - val_loss: 2.4717
Epoch 505/5000
26/26 - 1s - loss: 2.0683 - val_loss: 2.4704
Epoch 506/5000
26/26 - 1s - loss: 2.0686 - val_loss: 2.4663
Epoch 507/5000
26/26 - 1s - loss: 2.0648 - val_loss: 2.4659
Epoch 508/5000
26/26 - 1s - loss: 2.0630 - val_loss: 2.4642
Epoch 509/5000
26/26 - 1s - loss: 2.0593 - val_loss: 2.4620
Epoch 510/5000
26/26 - 1s - loss: 2.0582 - val_loss: 2.4590
Epoch 00510: val_loss improved from 2.47939 to 2.45902, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 511/5000
26/26 - 1s - loss: 2.0560 - val_loss: 2.4578
Epoch 512/5000
26/26 - 1s - loss: 2.0555 - val_loss: 2.4556
Epoch 513/5000
26/26 - 1s - loss: 2.0508 - val_loss: 2.4537
Epoch 514/5000
26/26 - 1s - loss: 2.0485 - val_loss: 2.4539
Epoch 515/5000
26/26 - 1s - loss: 2.0470 - val_loss: 2.4491
Epoch 516/5000
26/26 - 1s - loss: 2.0482 - val_loss: 2.4468
Epoch 517/5000
26/26 - 1s - loss: 2.0439 - val_loss: 2.4444
Epoch 518/5000
26/26 - 1s - loss: 2.0415 - val_loss: 2.4431
Epoch 519/5000
26/26 - 1s - loss: 2.0403 - val_loss: 2.4400
Epoch 520/5000
26/26 - 1s - loss: 2.0379 - val_loss: 2.4379
Epoch 00520: val_loss improved from 2.45902 to 2.43789, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 521/5000
26/26 - 1s - loss: 2.0359 - val_loss: 2.4379
Epoch 522/5000
26/26 - 1s - loss: 2.0324 - val_loss: 2.4357
Epoch 523/5000
26/26 - 1s - loss: 2.0320 - val_loss: 2.4337
Epoch 524/5000
26/26 - 1s - loss: 2.0294 - val_loss: 2.4316
Epoch 525/5000
26/26 - 1s - loss: 2.0275 - val_loss: 2.4316
Epoch 526/5000
26/26 - 1s - loss: 2.0267 - val_loss: 2.4279
Epoch 527/5000
26/26 - 1s - loss: 2.0236 - val_loss: 2.4263
Epoch 528/5000
26/26 - 1s - loss: 2.0237 - val_loss: 2.4252
Epoch 529/5000
26/26 - 1s - loss: 2.0215 - val_loss: 2.4226
Epoch 530/5000
26/26 - 1s - loss: 2.0175 - val_loss: 2.4202
Epoch 00530: val_loss improved from 2.43789 to 2.42025, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 531/5000
26/26 - 1s - loss: 2.0175 - val_loss: 2.4200
Epoch 532/5000
26/26 - 1s - loss: 2.0159 - val_loss: 2.4171
Epoch 533/5000
26/26 - 1s - loss: 2.0114 - val_loss: 2.4148
Epoch 534/5000
26/26 - 1s - loss: 2.0091 - val_loss: 2.4138
Epoch 535/5000
26/26 - 1s - loss: 2.0086 - val_loss: 2.4127
Epoch 536/5000
26/26 - 1s - loss: 2.0058 - val_loss: 2.4086
Epoch 537/5000
26/26 - 1s - loss: 2.0055 - val_loss: 2.4067
Epoch 538/5000
26/26 - 1s - loss: 2.0037 - val_loss: 2.4056
Epoch 539/5000
26/26 - 1s - loss: 2.0014 - val_loss: 2.4021
Epoch 540/5000
26/26 - 1s - loss: 1.9975 - val_loss: 2.4008
Epoch 00540: val_loss improved from 2.42025 to 2.40080, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 541/5000
26/26 - 1s - loss: 1.9963 - val_loss: 2.4005
Epoch 542/5000
26/26 - 1s - loss: 1.9971 - val_loss: 2.3984
Epoch 543/5000
26/26 - 1s - loss: 1.9939 - val_loss: 2.3937
Epoch 544/5000
26/26 - 1s - loss: 1.9920 - val_loss: 2.3945
Epoch 545/5000
26/26 - 1s - loss: 1.9903 - val_loss: 2.3926
Epoch 546/5000
26/26 - 1s - loss: 1.9862 - val_loss: 2.3908
Epoch 547/5000
26/26 - 1s - loss: 1.9855 - val_loss: 2.3886
Epoch 548/5000
26/26 - 1s - loss: 1.9853 - val_loss: 2.3872
Epoch 549/5000
26/26 - 1s - loss: 1.9809 - val_loss: 2.3848
Epoch 550/5000
26/26 - 1s - loss: 1.9799 - val_loss: 2.3843
Epoch 00550: val_loss improved from 2.40080 to 2.38434, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 551/5000
26/26 - 1s - loss: 1.9786 - val_loss: 2.3804
Epoch 552/5000
26/26 - 1s - loss: 1.9758 - val_loss: 2.3785
Epoch 553/5000
26/26 - 1s - loss: 1.9738 - val_loss: 2.3766
Epoch 554/5000
26/26 - 1s - loss: 1.9713 - val_loss: 2.3744
Epoch 555/5000
26/26 - 1s - loss: 1.9691 - val_loss: 2.3719
Epoch 556/5000
26/26 - 1s - loss: 1.9669 - val_loss: 2.3691
Epoch 557/5000
26/26 - 1s - loss: 1.9657 - val_loss: 2.3687
Epoch 558/5000
26/26 - 1s - loss: 1.9643 - val_loss: 2.3659
Epoch 559/5000
26/26 - 1s - loss: 1.9627 - val_loss: 2.3633
Epoch 560/5000
26/26 - 1s - loss: 1.9618 - val_loss: 2.3615
Epoch 00560: val_loss improved from 2.38434 to 2.36150, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 561/5000
26/26 - 1s - loss: 1.9605 - val_loss: 2.3604
Epoch 562/5000
26/26 - 1s - loss: 1.9564 - val_loss: 2.3594
Epoch 563/5000
26/26 - 1s - loss: 1.9571 - val_loss: 2.3576
Epoch 564/5000
26/26 - 1s - loss: 1.9532 - val_loss: 2.3565
Epoch 565/5000
26/26 - 1s - loss: 1.9517 - val_loss: 2.3550
Epoch 566/5000
26/26 - 1s - loss: 1.9496 - val_loss: 2.3525
Epoch 567/5000
26/26 - 1s - loss: 1.9474 - val_loss: 2.3497
Epoch 568/5000
26/26 - 1s - loss: 1.9463 - val_loss: 2.3479
Epoch 569/5000
26/26 - 1s - loss: 1.9429 - val_loss: 2.3463
Epoch 570/5000
26/26 - 1s - loss: 1.9411 - val_loss: 2.3450
Epoch 00570: val_loss improved from 2.36150 to 2.34498, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 571/5000
26/26 - 1s - loss: 1.9386 - val_loss: 2.3450
Epoch 572/5000
26/26 - 1s - loss: 1.9375 - val_loss: 2.3428
Epoch 573/5000
26/26 - 1s - loss: 1.9355 - val_loss: 2.3416
Epoch 574/5000
26/26 - 1s - loss: 1.9341 - val_loss: 2.3382
Epoch 575/5000
26/26 - 1s - loss: 1.9312 - val_loss: 2.3359
Epoch 576/5000
26/26 - 1s - loss: 1.9318 - val_loss: 2.3344
Epoch 577/5000
26/26 - 1s - loss: 1.9295 - val_loss: 2.3331
Epoch 578/5000
26/26 - 1s - loss: 1.9285 - val_loss: 2.3312
Epoch 579/5000
26/26 - 1s - loss: 1.9251 - val_loss: 2.3284
Epoch 580/5000
26/26 - 1s - loss: 1.9240 - val_loss: 2.3272
Epoch 00580: val_loss improved from 2.34498 to 2.32723, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 581/5000
26/26 - 1s - loss: 1.9235 - val_loss: 2.3240
Epoch 582/5000
26/26 - 1s - loss: 1.9208 - val_loss: 2.3230
Epoch 583/5000
26/26 - 1s - loss: 1.9205 - val_loss: 2.3209
Epoch 584/5000
26/26 - 1s - loss: 1.9157 - val_loss: 2.3195
Epoch 585/5000
26/26 - 1s - loss: 1.9151 - val_loss: 2.3188
Epoch 586/5000
26/26 - 1s - loss: 1.9136 - val_loss: 2.3162
Epoch 587/5000
26/26 - 1s - loss: 1.9120 - val_loss: 2.3150
Epoch 588/5000
26/26 - 1s - loss: 1.9077 - val_loss: 2.3146
Epoch 589/5000
26/26 - 1s - loss: 1.9083 - val_loss: 2.3098
Epoch 590/5000
26/26 - 1s - loss: 1.9061 - val_loss: 2.3061
Epoch 00590: val_loss improved from 2.32723 to 2.30606, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 591/5000
26/26 - 1s - loss: 1.9030 - val_loss: 2.3053
Epoch 592/5000
26/26 - 1s - loss: 1.9009 - val_loss: 2.3046
Epoch 593/5000
26/26 - 1s - loss: 1.9008 - val_loss: 2.3015
Epoch 594/5000
26/26 - 1s - loss: 1.8948 - val_loss: 2.3005
Epoch 595/5000
26/26 - 1s - loss: 1.8961 - val_loss: 2.2996
Epoch 596/5000
26/26 - 1s - loss: 1.8922 - val_loss: 2.2979
Epoch 597/5000
26/26 - 1s - loss: 1.8943 - val_loss: 2.2947
Epoch 598/5000
26/26 - 1s - loss: 1.8936 - val_loss: 2.2950
Epoch 599/5000
26/26 - 1s - loss: 1.8882 - val_loss: 2.2925
Epoch 600/5000
26/26 - 1s - loss: 1.8889 - val_loss: 2.2916
Epoch 00600: val_loss improved from 2.30606 to 2.29163, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 601/5000
26/26 - 1s - loss: 1.8849 - val_loss: 2.2897
Epoch 602/5000
26/26 - 1s - loss: 1.8822 - val_loss: 2.2870
Epoch 603/5000
26/26 - 1s - loss: 1.8826 - val_loss: 2.2845
Epoch 604/5000
26/26 - 1s - loss: 1.8831 - val_loss: 2.2850
Epoch 605/5000
26/26 - 1s - loss: 1.8786 - val_loss: 2.2847
Epoch 606/5000
26/26 - 1s - loss: 1.8772 - val_loss: 2.2822
Epoch 607/5000
26/26 - 1s - loss: 1.8750 - val_loss: 2.2812
Epoch 608/5000
26/26 - 1s - loss: 1.8741 - val_loss: 2.2785
Epoch 609/5000
26/26 - 1s - loss: 1.8723 - val_loss: 2.2783
Epoch 610/5000
26/26 - 1s - loss: 1.8687 - val_loss: 2.2753
Epoch 00610: val_loss improved from 2.29163 to 2.27533, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 611/5000
26/26 - 1s - loss: 1.8691 - val_loss: 2.2746
Epoch 612/5000
26/26 - 1s - loss: 1.8662 - val_loss: 2.2738
Epoch 613/5000
26/26 - 1s - loss: 1.8651 - val_loss: 2.2721
Epoch 614/5000
26/26 - 1s - loss: 1.8639 - val_loss: 2.2715
Epoch 615/5000
26/26 - 1s - loss: 1.8623 - val_loss: 2.2674
Epoch 616/5000
26/26 - 1s - loss: 1.8592 - val_loss: 2.2649
Epoch 617/5000
26/26 - 1s - loss: 1.8591 - val_loss: 2.2621
Epoch 618/5000
26/26 - 1s - loss: 1.8552 - val_loss: 2.2622
Epoch 619/5000
26/26 - 1s - loss: 1.8546 - val_loss: 2.2617
Epoch 620/5000
26/26 - 1s - loss: 1.8529 - val_loss: 2.2580
Epoch 00620: val_loss improved from 2.27533 to 2.25802, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 621/5000
26/26 - 1s - loss: 1.8534 - val_loss: 2.2570
Epoch 622/5000
26/26 - 1s - loss: 1.8499 - val_loss: 2.2551
Epoch 623/5000
26/26 - 1s - loss: 1.8471 - val_loss: 2.2522
Epoch 624/5000
26/26 - 1s - loss: 1.8466 - val_loss: 2.2531
Epoch 625/5000
26/26 - 1s - loss: 1.8431 - val_loss: 2.2503
Epoch 626/5000
26/26 - 1s - loss: 1.8432 - val_loss: 2.2473
Epoch 627/5000
26/26 - 1s - loss: 1.8417 - val_loss: 2.2457
Epoch 628/5000
26/26 - 1s - loss: 1.8409 - val_loss: 2.2440
Epoch 629/5000
26/26 - 1s - loss: 1.8389 - val_loss: 2.2436
Epoch 630/5000
26/26 - 1s - loss: 1.8369 - val_loss: 2.2418
Epoch 00630: val_loss improved from 2.25802 to 2.24176, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 631/5000
26/26 - 1s - loss: 1.8339 - val_loss: 2.2392
Epoch 632/5000
26/26 - 1s - loss: 1.8341 - val_loss: 2.2366
Epoch 633/5000
26/26 - 1s - loss: 1.8319 - val_loss: 2.2358
Epoch 634/5000
26/26 - 1s - loss: 1.8305 - val_loss: 2.2348
Epoch 635/5000
26/26 - 1s - loss: 1.8278 - val_loss: 2.2334
Epoch 636/5000
26/26 - 1s - loss: 1.8266 - val_loss: 2.2302
Epoch 637/5000
26/26 - 1s - loss: 1.8255 - val_loss: 2.2280
Epoch 638/5000
26/26 - 1s - loss: 1.8224 - val_loss: 2.2262
Epoch 639/5000
26/26 - 1s - loss: 1.8212 - val_loss: 2.2251
Epoch 640/5000
26/26 - 1s - loss: 1.8207 - val_loss: 2.2227
Epoch 00640: val_loss improved from 2.24176 to 2.22274, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 641/5000
26/26 - 1s - loss: 1.8165 - val_loss: 2.2207
Epoch 642/5000
26/26 - 1s - loss: 1.8164 - val_loss: 2.2220
Epoch 643/5000
26/26 - 1s - loss: 1.8139 - val_loss: 2.2212
Epoch 644/5000
26/26 - 1s - loss: 1.8109 - val_loss: 2.2190
Epoch 645/5000
26/26 - 1s - loss: 1.8123 - val_loss: 2.2163
Epoch 646/5000
26/26 - 1s - loss: 1.8104 - val_loss: 2.2157
Epoch 647/5000
26/26 - 1s - loss: 1.8089 - val_loss: 2.2132
Epoch 648/5000
26/26 - 1s - loss: 1.8054 - val_loss: 2.2122
Epoch 649/5000
26/26 - 1s - loss: 1.8052 - val_loss: 2.2107
Epoch 650/5000
26/26 - 1s - loss: 1.8023 - val_loss: 2.2077
Epoch 00650: val_loss improved from 2.22274 to 2.20771, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 651/5000
26/26 - 1s - loss: 1.8018 - val_loss: 2.2078
Epoch 652/5000
26/26 - 1s - loss: 1.7984 - val_loss: 2.2051
Epoch 653/5000
26/26 - 1s - loss: 1.7986 - val_loss: 2.2048
Epoch 654/5000
26/26 - 1s - loss: 1.7986 - val_loss: 2.2034
Epoch 655/5000
26/26 - 1s - loss: 1.7919 - val_loss: 2.2012
Epoch 656/5000
26/26 - 1s - loss: 1.7943 - val_loss: 2.1995
Epoch 657/5000
26/26 - 1s - loss: 1.7929 - val_loss: 2.1963
Epoch 658/5000
26/26 - 1s - loss: 1.7895 - val_loss: 2.1961
Epoch 659/5000
26/26 - 1s - loss: 1.7871 - val_loss: 2.1943
Epoch 660/5000
26/26 - 1s - loss: 1.7870 - val_loss: 2.1932
Epoch 00660: val_loss improved from 2.20771 to 2.19323, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 661/5000
26/26 - 1s - loss: 1.7831 - val_loss: 2.1895
Epoch 662/5000
26/26 - 1s - loss: 1.7836 - val_loss: 2.1879
Epoch 663/5000
26/26 - 1s - loss: 1.7816 - val_loss: 2.1857
Epoch 664/5000
26/26 - 1s - loss: 1.7777 - val_loss: 2.1852
Epoch 665/5000
26/26 - 1s - loss: 1.7786 - val_loss: 2.1847
Epoch 666/5000
26/26 - 2s - loss: 1.7754 - val_loss: 2.1823
Epoch 667/5000
26/26 - 1s - loss: 1.7758 - val_loss: 2.1790
Epoch 668/5000
26/26 - 1s - loss: 1.7736 - val_loss: 2.1783
Epoch 669/5000
26/26 - 1s - loss: 1.7725 - val_loss: 2.1765
Epoch 670/5000
26/26 - 1s - loss: 1.7704 - val_loss: 2.1752
Epoch 00670: val_loss improved from 2.19323 to 2.17519, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 671/5000
26/26 - 1s - loss: 1.7701 - val_loss: 2.1730
Epoch 672/5000
26/26 - 1s - loss: 1.7663 - val_loss: 2.1736
Epoch 673/5000
26/26 - 1s - loss: 1.7670 - val_loss: 2.1708
Epoch 674/5000
26/26 - 1s - loss: 1.7645 - val_loss: 2.1696
Epoch 675/5000
26/26 - 1s - loss: 1.7632 - val_loss: 2.1675
Epoch 676/5000
26/26 - 1s - loss: 1.7622 - val_loss: 2.1656
Epoch 677/5000
26/26 - 1s - loss: 1.7593 - val_loss: 2.1644
Epoch 678/5000
26/26 - 1s - loss: 1.7582 - val_loss: 2.1626
Epoch 679/5000
26/26 - 1s - loss: 1.7561 - val_loss: 2.1610
Epoch 680/5000
26/26 - 1s - loss: 1.7549 - val_loss: 2.1585
Epoch 00680: val_loss improved from 2.17519 to 2.15845, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 681/5000
26/26 - 1s - loss: 1.7526 - val_loss: 2.1566
Epoch 682/5000
26/26 - 1s - loss: 1.7502 - val_loss: 2.1567
Epoch 683/5000
26/26 - 1s - loss: 1.7512 - val_loss: 2.1551
Epoch 684/5000
26/26 - 1s - loss: 1.7479 - val_loss: 2.1537
Epoch 685/5000
26/26 - 1s - loss: 1.7477 - val_loss: 2.1541
Epoch 686/5000
26/26 - 1s - loss: 1.7455 - val_loss: 2.1511
Epoch 687/5000
26/26 - 1s - loss: 1.7427 - val_loss: 2.1494
Epoch 688/5000
26/26 - 1s - loss: 1.7425 - val_loss: 2.1469
Epoch 689/5000
26/26 - 1s - loss: 1.7420 - val_loss: 2.1464
Epoch 690/5000
26/26 - 1s - loss: 1.7400 - val_loss: 2.1437
Epoch 00690: val_loss improved from 2.15845 to 2.14373, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 691/5000
26/26 - 1s - loss: 1.7404 - val_loss: 2.1434
Epoch 692/5000
26/26 - 1s - loss: 1.7356 - val_loss: 2.1401
Epoch 693/5000
26/26 - 1s - loss: 1.7341 - val_loss: 2.1383
Epoch 694/5000
26/26 - 1s - loss: 1.7335 - val_loss: 2.1361
Epoch 695/5000
26/26 - 1s - loss: 1.7300 - val_loss: 2.1359
Epoch 696/5000
26/26 - 1s - loss: 1.7310 - val_loss: 2.1361
Epoch 697/5000
26/26 - 1s - loss: 1.7279 - val_loss: 2.1341
Epoch 698/5000
26/26 - 1s - loss: 1.7273 - val_loss: 2.1325
Epoch 699/5000
26/26 - 1s - loss: 1.7260 - val_loss: 2.1318
Epoch 700/5000
26/26 - 1s - loss: 1.7234 - val_loss: 2.1300
Epoch 00700: val_loss improved from 2.14373 to 2.13004, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 701/5000
26/26 - 1s - loss: 1.7257 - val_loss: 2.1279
Epoch 702/5000
26/26 - 1s - loss: 1.7223 - val_loss: 2.1257
Epoch 703/5000
26/26 - 1s - loss: 1.7209 - val_loss: 2.1263
Epoch 704/5000
26/26 - 1s - loss: 1.7166 - val_loss: 2.1231
Epoch 705/5000
26/26 - 1s - loss: 1.7159 - val_loss: 2.1214
Epoch 706/5000
26/26 - 1s - loss: 1.7157 - val_loss: 2.1201
Epoch 707/5000
26/26 - 1s - loss: 1.7134 - val_loss: 2.1192
Epoch 708/5000
26/26 - 1s - loss: 1.7104 - val_loss: 2.1186
Epoch 709/5000
26/26 - 1s - loss: 1.7113 - val_loss: 2.1153
Epoch 710/5000
26/26 - 1s - loss: 1.7083 - val_loss: 2.1144
Epoch 00710: val_loss improved from 2.13004 to 2.11439, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 711/5000
26/26 - 1s - loss: 1.7074 - val_loss: 2.1139
Epoch 712/5000
26/26 - 1s - loss: 1.7074 - val_loss: 2.1120
Epoch 713/5000
26/26 - 1s - loss: 1.7028 - val_loss: 2.1110
Epoch 714/5000
26/26 - 1s - loss: 1.7007 - val_loss: 2.1092
Epoch 715/5000
26/26 - 1s - loss: 1.7009 - val_loss: 2.1066
Epoch 716/5000
26/26 - 1s - loss: 1.6990 - val_loss: 2.1054
Epoch 717/5000
26/26 - 1s - loss: 1.7003 - val_loss: 2.1048
Epoch 718/5000
26/26 - 1s - loss: 1.6961 - val_loss: 2.1046
Epoch 719/5000
26/26 - 1s - loss: 1.6975 - val_loss: 2.1033
Epoch 720/5000
26/26 - 1s - loss: 1.6943 - val_loss: 2.1002
Epoch 00720: val_loss improved from 2.11439 to 2.10016, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 721/5000
26/26 - 1s - loss: 1.6932 - val_loss: 2.0984
Epoch 722/5000
26/26 - 1s - loss: 1.6898 - val_loss: 2.0984
Epoch 723/5000
26/26 - 1s - loss: 1.6908 - val_loss: 2.0971
Epoch 724/5000
26/26 - 1s - loss: 1.6880 - val_loss: 2.0952
Epoch 725/5000
26/26 - 1s - loss: 1.6858 - val_loss: 2.0944
Epoch 726/5000
26/26 - 1s - loss: 1.6838 - val_loss: 2.0932
Epoch 727/5000
26/26 - 1s - loss: 1.6837 - val_loss: 2.0913
Epoch 728/5000
26/26 - 1s - loss: 1.6821 - val_loss: 2.0881
Epoch 729/5000
26/26 - 1s - loss: 1.6832 - val_loss: 2.0897
Epoch 730/5000
26/26 - 1s - loss: 1.6786 - val_loss: 2.0868
Epoch 00730: val_loss improved from 2.10016 to 2.08679, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 731/5000
26/26 - 1s - loss: 1.6770 - val_loss: 2.0856
Epoch 732/5000
26/26 - 1s - loss: 1.6762 - val_loss: 2.0842
Epoch 733/5000
26/26 - 1s - loss: 1.6739 - val_loss: 2.0823
Epoch 734/5000
26/26 - 1s - loss: 1.6728 - val_loss: 2.0813
Epoch 735/5000
26/26 - 1s - loss: 1.6738 - val_loss: 2.0811
Epoch 736/5000
26/26 - 1s - loss: 1.6713 - val_loss: 2.0792
Epoch 737/5000
26/26 - 1s - loss: 1.6704 - val_loss: 2.0761
Epoch 738/5000
26/26 - 1s - loss: 1.6681 - val_loss: 2.0761
Epoch 739/5000
26/26 - 1s - loss: 1.6654 - val_loss: 2.0745
Epoch 740/5000
26/26 - 1s - loss: 1.6653 - val_loss: 2.0727
Epoch 00740: val_loss improved from 2.08679 to 2.07267, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 741/5000
26/26 - 1s - loss: 1.6643 - val_loss: 2.0702
Epoch 742/5000
26/26 - 1s - loss: 1.6622 - val_loss: 2.0699
Epoch 743/5000
26/26 - 1s - loss: 1.6618 - val_loss: 2.0684
Epoch 744/5000
26/26 - 1s - loss: 1.6581 - val_loss: 2.0654
Epoch 745/5000
26/26 - 1s - loss: 1.6566 - val_loss: 2.0657
Epoch 746/5000
26/26 - 1s - loss: 1.6563 - val_loss: 2.0634
Epoch 747/5000
26/26 - 1s - loss: 1.6537 - val_loss: 2.0619
Epoch 748/5000
26/26 - 1s - loss: 1.6539 - val_loss: 2.0620
Epoch 749/5000
26/26 - 1s - loss: 1.6513 - val_loss: 2.0626
Epoch 750/5000
26/26 - 1s - loss: 1.6520 - val_loss: 2.0591
Epoch 00750: val_loss improved from 2.07267 to 2.05907, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 751/5000
26/26 - 1s - loss: 1.6497 - val_loss: 2.0577
Epoch 752/5000
26/26 - 1s - loss: 1.6471 - val_loss: 2.0557
Epoch 753/5000
26/26 - 1s - loss: 1.6459 - val_loss: 2.0544
Epoch 754/5000
26/26 - 1s - loss: 1.6453 - val_loss: 2.0543
Epoch 755/5000
26/26 - 1s - loss: 1.6453 - val_loss: 2.0529
Epoch 756/5000
26/26 - 1s - loss: 1.6413 - val_loss: 2.0505
Epoch 757/5000
26/26 - 1s - loss: 1.6429 - val_loss: 2.0493
Epoch 758/5000
26/26 - 1s - loss: 1.6382 - val_loss: 2.0486
Epoch 759/5000
26/26 - 1s - loss: 1.6369 - val_loss: 2.0469
Epoch 760/5000
26/26 - 1s - loss: 1.6382 - val_loss: 2.0430
Epoch 00760: val_loss improved from 2.05907 to 2.04296, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 761/5000
26/26 - 1s - loss: 1.6347 - val_loss: 2.0423
Epoch 762/5000
26/26 - 1s - loss: 1.6350 - val_loss: 2.0409
Epoch 763/5000
26/26 - 1s - loss: 1.6330 - val_loss: 2.0390
Epoch 764/5000
26/26 - 1s - loss: 1.6331 - val_loss: 2.0400
Epoch 765/5000
26/26 - 1s - loss: 1.6302 - val_loss: 2.0391
Epoch 766/5000
26/26 - 1s - loss: 1.6274 - val_loss: 2.0369
Epoch 767/5000
26/26 - 1s - loss: 1.6275 - val_loss: 2.0340
Epoch 768/5000
26/26 - 1s - loss: 1.6267 - val_loss: 2.0326
Epoch 769/5000
26/26 - 1s - loss: 1.6249 - val_loss: 2.0304
Epoch 770/5000
26/26 - 1s - loss: 1.6260 - val_loss: 2.0310
Epoch 00770: val_loss improved from 2.04296 to 2.03099, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 771/5000
26/26 - 1s - loss: 1.6226 - val_loss: 2.0317
Epoch 772/5000
26/26 - 1s - loss: 1.6195 - val_loss: 2.0284
Epoch 773/5000
26/26 - 1s - loss: 1.6195 - val_loss: 2.0268
Epoch 774/5000
26/26 - 1s - loss: 1.6175 - val_loss: 2.0289
Epoch 775/5000
26/26 - 1s - loss: 1.6183 - val_loss: 2.0255
Epoch 776/5000
26/26 - 1s - loss: 1.6145 - val_loss: 2.0228
Epoch 777/5000
26/26 - 1s - loss: 1.6143 - val_loss: 2.0223
Epoch 778/5000
26/26 - 1s - loss: 1.6113 - val_loss: 2.0191
Epoch 779/5000
26/26 - 1s - loss: 1.6107 - val_loss: 2.0180
Epoch 780/5000
26/26 - 1s - loss: 1.6095 - val_loss: 2.0166
Epoch 00780: val_loss improved from 2.03099 to 2.01658, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 781/5000
26/26 - 1s - loss: 1.6070 - val_loss: 2.0151
Epoch 782/5000
26/26 - 1s - loss: 1.6059 - val_loss: 2.0163
Epoch 783/5000
26/26 - 2s - loss: 1.6061 - val_loss: 2.0146
Epoch 784/5000
26/26 - 1s - loss: 1.6042 - val_loss: 2.0121
Epoch 785/5000
26/26 - 1s - loss: 1.6015 - val_loss: 2.0103
Epoch 786/5000
26/26 - 1s - loss: 1.6040 - val_loss: 2.0094
Epoch 787/5000
26/26 - 1s - loss: 1.6006 - val_loss: 2.0079
Epoch 788/5000
26/26 - 1s - loss: 1.5980 - val_loss: 2.0070
Epoch 789/5000
26/26 - 1s - loss: 1.5972 - val_loss: 2.0058
Epoch 790/5000
26/26 - 1s - loss: 1.5953 - val_loss: 2.0053
Epoch 00790: val_loss improved from 2.01658 to 2.00533, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 791/5000
26/26 - 1s - loss: 1.5958 - val_loss: 2.0033
Epoch 792/5000
26/26 - 1s - loss: 1.5932 - val_loss: 2.0021
Epoch 793/5000
26/26 - 1s - loss: 1.5924 - val_loss: 1.9997
Epoch 794/5000
26/26 - 1s - loss: 1.5903 - val_loss: 1.9988
Epoch 795/5000
26/26 - 1s - loss: 1.5921 - val_loss: 1.9961
Epoch 796/5000
26/26 - 1s - loss: 1.5884 - val_loss: 1.9958
Epoch 797/5000
26/26 - 1s - loss: 1.5865 - val_loss: 1.9936
Epoch 798/5000
26/26 - 1s - loss: 1.5847 - val_loss: 1.9943
Epoch 799/5000
26/26 - 1s - loss: 1.5842 - val_loss: 1.9926
Epoch 800/5000
26/26 - 1s - loss: 1.5829 - val_loss: 1.9912
Epoch 00800: val_loss improved from 2.00533 to 1.99123, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 801/5000
26/26 - 1s - loss: 1.5786 - val_loss: 1.9893
Epoch 802/5000
26/26 - 1s - loss: 1.5805 - val_loss: 1.9880
Epoch 803/5000
26/26 - 1s - loss: 1.5792 - val_loss: 1.9854
Epoch 804/5000
26/26 - 1s - loss: 1.5780 - val_loss: 1.9854
Epoch 805/5000
26/26 - 1s - loss: 1.5765 - val_loss: 1.9838
Epoch 806/5000
26/26 - 1s - loss: 1.5758 - val_loss: 1.9840
Epoch 807/5000
26/26 - 1s - loss: 1.5738 - val_loss: 1.9819
Epoch 808/5000
26/26 - 1s - loss: 1.5729 - val_loss: 1.9806
Epoch 809/5000
26/26 - 1s - loss: 1.5708 - val_loss: 1.9809
Epoch 810/5000
26/26 - 1s - loss: 1.5703 - val_loss: 1.9785
Epoch 00810: val_loss improved from 1.99123 to 1.97847, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 811/5000
26/26 - 1s - loss: 1.5663 - val_loss: 1.9789
Epoch 812/5000
26/26 - 1s - loss: 1.5688 - val_loss: 1.9749
Epoch 813/5000
26/26 - 1s - loss: 1.5658 - val_loss: 1.9725
Epoch 814/5000
26/26 - 1s - loss: 1.5632 - val_loss: 1.9722
Epoch 815/5000
26/26 - 1s - loss: 1.5641 - val_loss: 1.9702
Epoch 816/5000
26/26 - 1s - loss: 1.5610 - val_loss: 1.9684
Epoch 817/5000
26/26 - 1s - loss: 1.5604 - val_loss: 1.9675
Epoch 818/5000
26/26 - 1s - loss: 1.5604 - val_loss: 1.9668
Epoch 819/5000
26/26 - 1s - loss: 1.5575 - val_loss: 1.9656
Epoch 820/5000
26/26 - 1s - loss: 1.5570 - val_loss: 1.9653
Epoch 00820: val_loss improved from 1.97847 to 1.96528, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 821/5000
26/26 - 1s - loss: 1.5559 - val_loss: 1.9651
Epoch 822/5000
26/26 - 1s - loss: 1.5553 - val_loss: 1.9633
Epoch 823/5000
26/26 - 1s - loss: 1.5516 - val_loss: 1.9617
Epoch 824/5000
26/26 - 1s - loss: 1.5534 - val_loss: 1.9599
Epoch 825/5000
26/26 - 1s - loss: 1.5503 - val_loss: 1.9583
Epoch 826/5000
26/26 - 1s - loss: 1.5480 - val_loss: 1.9564
Epoch 827/5000
26/26 - 1s - loss: 1.5474 - val_loss: 1.9571
Epoch 828/5000
26/26 - 1s - loss: 1.5457 - val_loss: 1.9561
Epoch 829/5000
26/26 - 1s - loss: 1.5465 - val_loss: 1.9543
Epoch 830/5000
26/26 - 1s - loss: 1.5464 - val_loss: 1.9521
Epoch 00830: val_loss improved from 1.96528 to 1.95205, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 831/5000
26/26 - 1s - loss: 1.5419 - val_loss: 1.9529
Epoch 832/5000
26/26 - 1s - loss: 1.5418 - val_loss: 1.9496
Epoch 833/5000
26/26 - 1s - loss: 1.5395 - val_loss: 1.9491
Epoch 834/5000
26/26 - 1s - loss: 1.5378 - val_loss: 1.9480
Epoch 835/5000
26/26 - 1s - loss: 1.5374 - val_loss: 1.9459
Epoch 836/5000
26/26 - 2s - loss: 1.5363 - val_loss: 1.9447
Epoch 837/5000
26/26 - 1s - loss: 1.5362 - val_loss: 1.9430
Epoch 838/5000
26/26 - 1s - loss: 1.5321 - val_loss: 1.9433
Epoch 839/5000
26/26 - 1s - loss: 1.5335 - val_loss: 1.9418
Epoch 840/5000
26/26 - 1s - loss: 1.5306 - val_loss: 1.9383
Epoch 00840: val_loss improved from 1.95205 to 1.93830, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 841/5000
26/26 - 1s - loss: 1.5323 - val_loss: 1.9371
Epoch 842/5000
26/26 - 1s - loss: 1.5302 - val_loss: 1.9361
Epoch 843/5000
26/26 - 1s - loss: 1.5280 - val_loss: 1.9346
Epoch 844/5000
26/26 - 1s - loss: 1.5272 - val_loss: 1.9344
Epoch 845/5000
26/26 - 1s - loss: 1.5258 - val_loss: 1.9334
Epoch 846/5000
26/26 - 1s - loss: 1.5242 - val_loss: 1.9342
Epoch 847/5000
26/26 - 1s - loss: 1.5226 - val_loss: 1.9317
Epoch 848/5000
26/26 - 1s - loss: 1.5211 - val_loss: 1.9301
Epoch 849/5000
26/26 - 1s - loss: 1.5212 - val_loss: 1.9273
Epoch 850/5000
26/26 - 1s - loss: 1.5187 - val_loss: 1.9278
Epoch 00850: val_loss improved from 1.93830 to 1.92784, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 851/5000
26/26 - 1s - loss: 1.5169 - val_loss: 1.9263
Epoch 852/5000
26/26 - 1s - loss: 1.5164 - val_loss: 1.9250
Epoch 853/5000
26/26 - 1s - loss: 1.5146 - val_loss: 1.9235
Epoch 854/5000
26/26 - 1s - loss: 1.5149 - val_loss: 1.9220
Epoch 855/5000
26/26 - 1s - loss: 1.5153 - val_loss: 1.9197
Epoch 856/5000
26/26 - 1s - loss: 1.5114 - val_loss: 1.9210
Epoch 857/5000
26/26 - 1s - loss: 1.5103 - val_loss: 1.9194
Epoch 858/5000
26/26 - 1s - loss: 1.5100 - val_loss: 1.9186
Epoch 859/5000
26/26 - 1s - loss: 1.5073 - val_loss: 1.9168
Epoch 860/5000
26/26 - 1s - loss: 1.5063 - val_loss: 1.9160
Epoch 00860: val_loss improved from 1.92784 to 1.91602, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 861/5000
26/26 - 1s - loss: 1.5059 - val_loss: 1.9146
Epoch 862/5000
26/26 - 1s - loss: 1.5049 - val_loss: 1.9152
Epoch 863/5000
26/26 - 1s - loss: 1.5041 - val_loss: 1.9128
Epoch 864/5000
26/26 - 1s - loss: 1.5003 - val_loss: 1.9094
Epoch 865/5000
26/26 - 1s - loss: 1.5011 - val_loss: 1.9100
Epoch 866/5000
26/26 - 1s - loss: 1.4989 - val_loss: 1.9097
Epoch 867/5000
26/26 - 1s - loss: 1.4998 - val_loss: 1.9078
Epoch 868/5000
26/26 - 1s - loss: 1.4976 - val_loss: 1.9061
Epoch 869/5000
26/26 - 1s - loss: 1.4950 - val_loss: 1.9049
Epoch 870/5000
26/26 - 1s - loss: 1.4948 - val_loss: 1.9037
Epoch 00870: val_loss improved from 1.91602 to 1.90373, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 871/5000
26/26 - 1s - loss: 1.4938 - val_loss: 1.9043
Epoch 872/5000
26/26 - 1s - loss: 1.4920 - val_loss: 1.9017
Epoch 873/5000
26/26 - 1s - loss: 1.4913 - val_loss: 1.9004
Epoch 874/5000
26/26 - 1s - loss: 1.4880 - val_loss: 1.9001
Epoch 875/5000
26/26 - 1s - loss: 1.4897 - val_loss: 1.8988
Epoch 876/5000
26/26 - 1s - loss: 1.4887 - val_loss: 1.8980
Epoch 877/5000
26/26 - 1s - loss: 1.4856 - val_loss: 1.8956
Epoch 878/5000
26/26 - 1s - loss: 1.4848 - val_loss: 1.8961
Epoch 879/5000
26/26 - 1s - loss: 1.4825 - val_loss: 1.8945
Epoch 880/5000
26/26 - 1s - loss: 1.4844 - val_loss: 1.8922
Epoch 00880: val_loss improved from 1.90373 to 1.89219, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 881/5000
26/26 - 1s - loss: 1.4818 - val_loss: 1.8925
Epoch 882/5000
26/26 - 1s - loss: 1.4796 - val_loss: 1.8909
Epoch 883/5000
26/26 - 1s - loss: 1.4786 - val_loss: 1.8894
Epoch 884/5000
26/26 - 1s - loss: 1.4781 - val_loss: 1.8867
Epoch 885/5000
26/26 - 1s - loss: 1.4770 - val_loss: 1.8856
Epoch 886/5000
26/26 - 1s - loss: 1.4760 - val_loss: 1.8849
Epoch 887/5000
26/26 - 1s - loss: 1.4730 - val_loss: 1.8835
Epoch 888/5000
26/26 - 1s - loss: 1.4725 - val_loss: 1.8825
Epoch 889/5000
26/26 - 1s - loss: 1.4731 - val_loss: 1.8816
Epoch 890/5000
26/26 - 1s - loss: 1.4719 - val_loss: 1.8801
Epoch 00890: val_loss improved from 1.89219 to 1.88014, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 891/5000
26/26 - 1s - loss: 1.4692 - val_loss: 1.8803
Epoch 892/5000
26/26 - 1s - loss: 1.4675 - val_loss: 1.8796
Epoch 893/5000
26/26 - 1s - loss: 1.4685 - val_loss: 1.8787
Epoch 894/5000
26/26 - 1s - loss: 1.4654 - val_loss: 1.8775
Epoch 895/5000
26/26 - 1s - loss: 1.4669 - val_loss: 1.8741
Epoch 896/5000
26/26 - 1s - loss: 1.4638 - val_loss: 1.8731
Epoch 897/5000
26/26 - 1s - loss: 1.4631 - val_loss: 1.8723
Epoch 898/5000
26/26 - 1s - loss: 1.4613 - val_loss: 1.8720
Epoch 899/5000
26/26 - 1s - loss: 1.4606 - val_loss: 1.8718
Epoch 900/5000
26/26 - 1s - loss: 1.4618 - val_loss: 1.8693
Epoch 00900: val_loss improved from 1.88014 to 1.86932, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 901/5000
26/26 - 1s - loss: 1.4605 - val_loss: 1.8675
Epoch 902/5000
26/26 - 1s - loss: 1.4573 - val_loss: 1.8670
Epoch 903/5000
26/26 - 1s - loss: 1.4551 - val_loss: 1.8657
Epoch 904/5000
26/26 - 1s - loss: 1.4535 - val_loss: 1.8639
Epoch 905/5000
26/26 - 1s - loss: 1.4542 - val_loss: 1.8644
Epoch 906/5000
26/26 - 1s - loss: 1.4533 - val_loss: 1.8644
Epoch 907/5000
26/26 - 1s - loss: 1.4533 - val_loss: 1.8600
Epoch 908/5000
26/26 - 1s - loss: 1.4508 - val_loss: 1.8597
Epoch 909/5000
26/26 - 1s - loss: 1.4509 - val_loss: 1.8588
Epoch 910/5000
26/26 - 1s - loss: 1.4485 - val_loss: 1.8557
Epoch 00910: val_loss improved from 1.86932 to 1.85569, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 911/5000
26/26 - 1s - loss: 1.4472 - val_loss: 1.8562
Epoch 912/5000
26/26 - 1s - loss: 1.4454 - val_loss: 1.8532
Epoch 913/5000
26/26 - 1s - loss: 1.4432 - val_loss: 1.8545
Epoch 914/5000
26/26 - 1s - loss: 1.4439 - val_loss: 1.8514
Epoch 915/5000
26/26 - 1s - loss: 1.4427 - val_loss: 1.8504
Epoch 916/5000
26/26 - 1s - loss: 1.4409 - val_loss: 1.8494
Epoch 917/5000
26/26 - 1s - loss: 1.4401 - val_loss: 1.8491
Epoch 918/5000
26/26 - 1s - loss: 1.4378 - val_loss: 1.8467
Epoch 919/5000
26/26 - 1s - loss: 1.4368 - val_loss: 1.8458
Epoch 920/5000
26/26 - 1s - loss: 1.4345 - val_loss: 1.8465
Epoch 00920: val_loss improved from 1.85569 to 1.84646, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 921/5000
26/26 - 1s - loss: 1.4358 - val_loss: 1.8461
Epoch 922/5000
26/26 - 1s - loss: 1.4353 - val_loss: 1.8454
Epoch 923/5000
26/26 - 1s - loss: 1.4336 - val_loss: 1.8427
Epoch 924/5000
26/26 - 1s - loss: 1.4320 - val_loss: 1.8418
Epoch 925/5000
26/26 - 1s - loss: 1.4317 - val_loss: 1.8413
Epoch 926/5000
26/26 - 1s - loss: 1.4288 - val_loss: 1.8398
Epoch 927/5000
26/26 - 1s - loss: 1.4296 - val_loss: 1.8387
Epoch 928/5000
26/26 - 1s - loss: 1.4279 - val_loss: 1.8382
Epoch 929/5000
26/26 - 1s - loss: 1.4262 - val_loss: 1.8363
Epoch 930/5000
26/26 - 1s - loss: 1.4253 - val_loss: 1.8353
Epoch 00930: val_loss improved from 1.84646 to 1.83527, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 931/5000
26/26 - 1s - loss: 1.4239 - val_loss: 1.8338
Epoch 932/5000
26/26 - 1s - loss: 1.4240 - val_loss: 1.8326
Epoch 933/5000
26/26 - 1s - loss: 1.4229 - val_loss: 1.8306
Epoch 934/5000
26/26 - 1s - loss: 1.4201 - val_loss: 1.8293
Epoch 935/5000
26/26 - 1s - loss: 1.4200 - val_loss: 1.8290
Epoch 936/5000
26/26 - 1s - loss: 1.4192 - val_loss: 1.8290
Epoch 937/5000
26/26 - 1s - loss: 1.4165 - val_loss: 1.8261
Epoch 938/5000
26/26 - 1s - loss: 1.4169 - val_loss: 1.8247
Epoch 939/5000
26/26 - 1s - loss: 1.4172 - val_loss: 1.8250
Epoch 940/5000
26/26 - 1s - loss: 1.4134 - val_loss: 1.8254
Epoch 00940: val_loss improved from 1.83527 to 1.82538, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 941/5000
26/26 - 1s - loss: 1.4140 - val_loss: 1.8231
Epoch 942/5000
26/26 - 1s - loss: 1.4144 - val_loss: 1.8226
Epoch 943/5000
26/26 - 1s - loss: 1.4112 - val_loss: 1.8211
Epoch 944/5000
26/26 - 1s - loss: 1.4101 - val_loss: 1.8202
Epoch 945/5000
26/26 - 1s - loss: 1.4092 - val_loss: 1.8194
Epoch 946/5000
26/26 - 1s - loss: 1.4087 - val_loss: 1.8194
Epoch 947/5000
26/26 - 1s - loss: 1.4070 - val_loss: 1.8162
Epoch 948/5000
26/26 - 1s - loss: 1.4053 - val_loss: 1.8165
Epoch 949/5000
26/26 - 1s - loss: 1.4064 - val_loss: 1.8167
Epoch 950/5000
26/26 - 1s - loss: 1.4040 - val_loss: 1.8158
Epoch 00950: val_loss improved from 1.82538 to 1.81575, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 951/5000
26/26 - 1s - loss: 1.4026 - val_loss: 1.8144
Epoch 952/5000
26/26 - 1s - loss: 1.4030 - val_loss: 1.8140
Epoch 953/5000
26/26 - 1s - loss: 1.4009 - val_loss: 1.8113
Epoch 954/5000
26/26 - 1s - loss: 1.4010 - val_loss: 1.8105
Epoch 955/5000
26/26 - 1s - loss: 1.3994 - val_loss: 1.8092
Epoch 956/5000
26/26 - 1s - loss: 1.3983 - val_loss: 1.8077
Epoch 957/5000
26/26 - 1s - loss: 1.3957 - val_loss: 1.8061
Epoch 958/5000
26/26 - 1s - loss: 1.3951 - val_loss: 1.8058
Epoch 959/5000
26/26 - 1s - loss: 1.3952 - val_loss: 1.8059
Epoch 960/5000
26/26 - 1s - loss: 1.3915 - val_loss: 1.8047
Epoch 00960: val_loss improved from 1.81575 to 1.80466, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 961/5000
26/26 - 1s - loss: 1.3928 - val_loss: 1.8029
Epoch 962/5000
26/26 - 1s - loss: 1.3925 - val_loss: 1.8018
Epoch 963/5000
26/26 - 1s - loss: 1.3900 - val_loss: 1.7997
Epoch 964/5000
26/26 - 1s - loss: 1.3893 - val_loss: 1.7997
Epoch 965/5000
26/26 - 1s - loss: 1.3886 - val_loss: 1.7975
Epoch 966/5000
26/26 - 1s - loss: 1.3849 - val_loss: 1.7967
Epoch 967/5000
26/26 - 1s - loss: 1.3856 - val_loss: 1.7960
Epoch 968/5000
26/26 - 1s - loss: 1.3840 - val_loss: 1.7958
Epoch 969/5000
26/26 - 1s - loss: 1.3848 - val_loss: 1.7934
Epoch 970/5000
26/26 - 1s - loss: 1.3830 - val_loss: 1.7916
Epoch 00970: val_loss improved from 1.80466 to 1.79164, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 971/5000
26/26 - 1s - loss: 1.3802 - val_loss: 1.7908
Epoch 972/5000
26/26 - 1s - loss: 1.3825 - val_loss: 1.7906
Epoch 973/5000
26/26 - 1s - loss: 1.3782 - val_loss: 1.7905
Epoch 974/5000
26/26 - 2s - loss: 1.3783 - val_loss: 1.7880
Epoch 975/5000
26/26 - 1s - loss: 1.3770 - val_loss: 1.7874
Epoch 976/5000
26/26 - 1s - loss: 1.3759 - val_loss: 1.7874
Epoch 977/5000
26/26 - 1s - loss: 1.3763 - val_loss: 1.7865
Epoch 978/5000
26/26 - 1s - loss: 1.3739 - val_loss: 1.7857
Epoch 979/5000
26/26 - 1s - loss: 1.3724 - val_loss: 1.7850
Epoch 980/5000
26/26 - 1s - loss: 1.3727 - val_loss: 1.7842
Epoch 00980: val_loss improved from 1.79164 to 1.78416, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 981/5000
26/26 - 1s - loss: 1.3706 - val_loss: 1.7805
Epoch 982/5000
26/26 - 1s - loss: 1.3698 - val_loss: 1.7815
Epoch 983/5000
26/26 - 1s - loss: 1.3700 - val_loss: 1.7803
Epoch 984/5000
26/26 - 1s - loss: 1.3680 - val_loss: 1.7793
Epoch 985/5000
26/26 - 1s - loss: 1.3665 - val_loss: 1.7773
Epoch 986/5000
26/26 - 1s - loss: 1.3670 - val_loss: 1.7774
Epoch 987/5000
26/26 - 1s - loss: 1.3652 - val_loss: 1.7756
Epoch 988/5000
26/26 - 1s - loss: 1.3654 - val_loss: 1.7737
Epoch 989/5000
26/26 - 1s - loss: 1.3639 - val_loss: 1.7728
Epoch 990/5000
26/26 - 1s - loss: 1.3607 - val_loss: 1.7731
Epoch 00990: val_loss improved from 1.78416 to 1.77314, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 991/5000
26/26 - 1s - loss: 1.3600 - val_loss: 1.7718
Epoch 992/5000
26/26 - 1s - loss: 1.3589 - val_loss: 1.7717
Epoch 993/5000
26/26 - 1s - loss: 1.3573 - val_loss: 1.7709
Epoch 994/5000
26/26 - 1s - loss: 1.3584 - val_loss: 1.7694
Epoch 995/5000
26/26 - 1s - loss: 1.3572 - val_loss: 1.7674
Epoch 996/5000
26/26 - 1s - loss: 1.3567 - val_loss: 1.7663
Epoch 997/5000
26/26 - 1s - loss: 1.3538 - val_loss: 1.7647
Epoch 998/5000
26/26 - 1s - loss: 1.3537 - val_loss: 1.7639
Epoch 999/5000
26/26 - 1s - loss: 1.3540 - val_loss: 1.7646
Epoch 1000/5000
26/26 - 1s - loss: 1.3520 - val_loss: 1.7630
Epoch 01000: val_loss improved from 1.77314 to 1.76298, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 1001/5000
26/26 - 1s - loss: 1.3499 - val_loss: 1.7616
Epoch 1002/5000
26/26 - 1s - loss: 1.3511 - val_loss: 1.7593
Epoch 1003/5000
26/26 - 1s - loss: 1.3491 - val_loss: 1.7593
Epoch 1004/5000
26/26 - 1s - loss: 1.3472 - val_loss: 1.7578
Epoch 1005/5000
26/26 - 1s - loss: 1.3456 - val_loss: 1.7572
Epoch 1006/5000
26/26 - 1s - loss: 1.3439 - val_loss: 1.7542
Epoch 1007/5000
26/26 - 1s - loss: 1.3458 - val_loss: 1.7555
Epoch 1008/5000
26/26 - 1s - loss: 1.3443 - val_loss: 1.7548
Epoch 1009/5000
26/26 - 1s - loss: 1.3429 - val_loss: 1.7529
Epoch 1010/5000
26/26 - 1s - loss: 1.3414 - val_loss: 1.7518
Epoch 01010: val_loss improved from 1.76298 to 1.75178, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 1011/5000
26/26 - 1s - loss: 1.3429 - val_loss: 1.7502
Epoch 1012/5000
26/26 - 1s - loss: 1.3404 - val_loss: 1.7499
Epoch 1013/5000
26/26 - 1s - loss: 1.3384 - val_loss: 1.7494
Epoch 1014/5000
26/26 - 1s - loss: 1.3381 - val_loss: 1.7495
Epoch 1015/5000
26/26 - 1s - loss: 1.3379 - val_loss: 1.7494
Epoch 1016/5000
26/26 - 1s - loss: 1.3371 - val_loss: 1.7460
Epoch 1017/5000
26/26 - 1s - loss: 1.3356 - val_loss: 1.7446
Epoch 1018/5000
26/26 - 1s - loss: 1.3343 - val_loss: 1.7445
Epoch 1019/5000
26/26 - 1s - loss: 1.3326 - val_loss: 1.7439
Epoch 1020/5000
26/26 - 1s - loss: 1.3334 - val_loss: 1.7424
Epoch 01020: val_loss improved from 1.75178 to 1.74241, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 1021/5000
26/26 - 1s - loss: 1.3313 - val_loss: 1.7400
Epoch 1022/5000
26/26 - 1s - loss: 1.3291 - val_loss: 1.7405
Epoch 1023/5000
26/26 - 1s - loss: 1.3307 - val_loss: 1.7410
Epoch 1024/5000
26/26 - 1s - loss: 1.3274 - val_loss: 1.7389
Epoch 1025/5000
26/26 - 1s - loss: 1.3263 - val_loss: 1.7383
Epoch 1026/5000
26/26 - 1s - loss: 1.3264 - val_loss: 1.7374
Epoch 1027/5000
26/26 - 1s - loss: 1.3245 - val_loss: 1.7364
Epoch 1028/5000
26/26 - 1s - loss: 1.3241 - val_loss: 1.7363
Epoch 1029/5000
26/26 - 1s - loss: 1.3251 - val_loss: 1.7340
Epoch 1030/5000
26/26 - 1s - loss: 1.3227 - val_loss: 1.7340
Epoch 01030: val_loss improved from 1.74241 to 1.73404, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 1031/5000
26/26 - 1s - loss: 1.3200 - val_loss: 1.7337
Epoch 1032/5000
26/26 - 1s - loss: 1.3207 - val_loss: 1.7300
Epoch 1033/5000
26/26 - 1s - loss: 1.3181 - val_loss: 1.7292
Epoch 1034/5000
26/26 - 1s - loss: 1.3184 - val_loss: 1.7288
Epoch 1035/5000
26/26 - 1s - loss: 1.3179 - val_loss: 1.7282
Epoch 1036/5000
26/26 - 1s - loss: 1.3159 - val_loss: 1.7281
Epoch 1037/5000
26/26 - 1s - loss: 1.3163 - val_loss: 1.7264
Epoch 1038/5000
26/26 - 1s - loss: 1.3145 - val_loss: 1.7258
Epoch 1039/5000
26/26 - 1s - loss: 1.3144 - val_loss: 1.7249
Epoch 1040/5000
26/26 - 1s - loss: 1.3122 - val_loss: 1.7230
Epoch 01040: val_loss improved from 1.73404 to 1.72304, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 1041/5000
26/26 - 1s - loss: 1.3140 - val_loss: 1.7219
Epoch 1042/5000
26/26 - 1s - loss: 1.3107 - val_loss: 1.7223
Epoch 1043/5000
26/26 - 1s - loss: 1.3106 - val_loss: 1.7221
Epoch 1044/5000
26/26 - 2s - loss: 1.3116 - val_loss: 1.7192
Epoch 1045/5000
26/26 - 1s - loss: 1.3093 - val_loss: 1.7177
Epoch 1046/5000
26/26 - 1s - loss: 1.3051 - val_loss: 1.7189
Epoch 1047/5000
26/26 - 1s - loss: 1.3059 - val_loss: 1.7177
Epoch 1048/5000
26/26 - 1s - loss: 1.3033 - val_loss: 1.7150
Epoch 1049/5000
26/26 - 1s - loss: 1.3053 - val_loss: 1.7148
Epoch 1050/5000
26/26 - 1s - loss: 1.3050 - val_loss: 1.7141
Epoch 01050: val_loss improved from 1.72304 to 1.71408, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 1051/5000
26/26 - 1s - loss: 1.3033 - val_loss: 1.7127
Epoch 1052/5000
26/26 - 1s - loss: 1.3020 - val_loss: 1.7125
Epoch 1053/5000
26/26 - 1s - loss: 1.3015 - val_loss: 1.7098
Epoch 1054/5000
26/26 - 1s - loss: 1.2978 - val_loss: 1.7103
Epoch 1055/5000
26/26 - 1s - loss: 1.2969 - val_loss: 1.7090
Epoch 1056/5000
26/26 - 1s - loss: 1.2973 - val_loss: 1.7084
Epoch 1057/5000
26/26 - 1s - loss: 1.2949 - val_loss: 1.7076
Epoch 1058/5000
26/26 - 1s - loss: 1.2956 - val_loss: 1.7064
Epoch 1059/5000
26/26 - 1s - loss: 1.2953 - val_loss: 1.7050
Epoch 1060/5000
26/26 - 1s - loss: 1.2950 - val_loss: 1.7057
Epoch 01060: val_loss improved from 1.71408 to 1.70568, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 1061/5000
26/26 - 1s - loss: 1.2931 - val_loss: 1.7038
Epoch 1062/5000
26/26 - 1s - loss: 1.2908 - val_loss: 1.7024
Epoch 1063/5000
26/26 - 1s - loss: 1.2913 - val_loss: 1.7006
Epoch 1064/5000
26/26 - 1s - loss: 1.2901 - val_loss: 1.7008
Epoch 1065/5000
26/26 - 1s - loss: 1.2905 - val_loss: 1.6996
Epoch 1066/5000
26/26 - 1s - loss: 1.2889 - val_loss: 1.6996
Epoch 1067/5000
26/26 - 1s - loss: 1.2870 - val_loss: 1.6983
Epoch 1068/5000
26/26 - 1s - loss: 1.2874 - val_loss: 1.6970
Epoch 1069/5000
26/26 - 1s - loss: 1.2856 - val_loss: 1.6959
Epoch 1070/5000
26/26 - 1s - loss: 1.2852 - val_loss: 1.6959
Epoch 01070: val_loss improved from 1.70568 to 1.69588, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 1071/5000
26/26 - 1s - loss: 1.2820 - val_loss: 1.6961
Epoch 1072/5000
26/26 - 1s - loss: 1.2822 - val_loss: 1.6939
Epoch 1073/5000
26/26 - 1s - loss: 1.2825 - val_loss: 1.6937
Epoch 1074/5000
26/26 - 1s - loss: 1.2798 - val_loss: 1.6918
Epoch 1075/5000
26/26 - 1s - loss: 1.2814 - val_loss: 1.6917
Epoch 1076/5000
26/26 - 1s - loss: 1.2784 - val_loss: 1.6907
Epoch 1077/5000
26/26 - 1s - loss: 1.2776 - val_loss: 1.6892
Epoch 1078/5000
26/26 - 1s - loss: 1.2779 - val_loss: 1.6885
Epoch 1079/5000
26/26 - 1s - loss: 1.2759 - val_loss: 1.6879
Epoch 1080/5000
26/26 - 1s - loss: 1.2749 - val_loss: 1.6871
Epoch 01080: val_loss improved from 1.69588 to 1.68713, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 1081/5000
26/26 - 1s - loss: 1.2749 - val_loss: 1.6873
Epoch 1082/5000
26/26 - 1s - loss: 1.2743 - val_loss: 1.6848
Epoch 1083/5000
26/26 - 1s - loss: 1.2717 - val_loss: 1.6840
Epoch 1084/5000
26/26 - 1s - loss: 1.2737 - val_loss: 1.6832
Epoch 1085/5000
26/26 - 1s - loss: 1.2706 - val_loss: 1.6825
Epoch 1086/5000
26/26 - 1s - loss: 1.2693 - val_loss: 1.6835
Epoch 1087/5000
26/26 - 2s - loss: 1.2695 - val_loss: 1.6812
Epoch 1088/5000
26/26 - 1s - loss: 1.2691 - val_loss: 1.6803
Epoch 1089/5000
26/26 - 1s - loss: 1.2664 - val_loss: 1.6802
Epoch 1090/5000
26/26 - 1s - loss: 1.2659 - val_loss: 1.6772
Epoch 01090: val_loss improved from 1.68713 to 1.67720, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 1091/5000
26/26 - 1s - loss: 1.2668 - val_loss: 1.6775
Epoch 1092/5000
26/26 - 1s - loss: 1.2657 - val_loss: 1.6774
Epoch 1093/5000
26/26 - 1s - loss: 1.2642 - val_loss: 1.6758
Epoch 1094/5000
26/26 - 1s - loss: 1.2621 - val_loss: 1.6750
Epoch 1095/5000
26/26 - 1s - loss: 1.2608 - val_loss: 1.6756
Epoch 1096/5000
26/26 - 1s - loss: 1.2603 - val_loss: 1.6723
Epoch 1097/5000
26/26 - 1s - loss: 1.2603 - val_loss: 1.6718
Epoch 1098/5000
26/26 - 1s - loss: 1.2593 - val_loss: 1.6714
Epoch 1099/5000
26/26 - 1s - loss: 1.2573 - val_loss: 1.6700
Epoch 1100/5000
26/26 - 1s - loss: 1.2553 - val_loss: 1.6689
Epoch 01100: val_loss improved from 1.67720 to 1.66895, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 1101/5000
26/26 - 1s - loss: 1.2560 - val_loss: 1.6683
Epoch 1102/5000
26/26 - 1s - loss: 1.2553 - val_loss: 1.6675
Epoch 1103/5000
26/26 - 1s - loss: 1.2550 - val_loss: 1.6665
Epoch 1104/5000
26/26 - 1s - loss: 1.2533 - val_loss: 1.6646
Epoch 1105/5000
26/26 - 1s - loss: 1.2525 - val_loss: 1.6646
Epoch 1106/5000
26/26 - 1s - loss: 1.2520 - val_loss: 1.6641
Epoch 1107/5000
26/26 - 1s - loss: 1.2525 - val_loss: 1.6625
Epoch 1108/5000
26/26 - 1s - loss: 1.2507 - val_loss: 1.6636
Epoch 1109/5000
26/26 - 1s - loss: 1.2498 - val_loss: 1.6615
Epoch 1110/5000
26/26 - 1s - loss: 1.2490 - val_loss: 1.6603
Epoch 01110: val_loss improved from 1.66895 to 1.66030, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 1111/5000
26/26 - 1s - loss: 1.2484 - val_loss: 1.6596
Epoch 1112/5000
26/26 - 1s - loss: 1.2467 - val_loss: 1.6589
Epoch 1113/5000
26/26 - 1s - loss: 1.2464 - val_loss: 1.6585
Epoch 1114/5000
26/26 - 1s - loss: 1.2460 - val_loss: 1.6571
Epoch 1115/5000
26/26 - 1s - loss: 1.2447 - val_loss: 1.6559
Epoch 1116/5000
26/26 - 1s - loss: 1.2428 - val_loss: 1.6560
Epoch 1117/5000
26/26 - 1s - loss: 1.2445 - val_loss: 1.6537
Epoch 1118/5000
26/26 - 1s - loss: 1.2416 - val_loss: 1.6538
Epoch 1119/5000
26/26 - 1s - loss: 1.2412 - val_loss: 1.6520
Epoch 1120/5000
26/26 - 1s - loss: 1.2411 - val_loss: 1.6517
Epoch 01120: val_loss improved from 1.66030 to 1.65170, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 1121/5000
26/26 - 1s - loss: 1.2398 - val_loss: 1.6506
Epoch 1122/5000
26/26 - 1s - loss: 1.2396 - val_loss: 1.6503
Epoch 1123/5000
26/26 - 1s - loss: 1.2357 - val_loss: 1.6499
Epoch 1124/5000
26/26 - 1s - loss: 1.2367 - val_loss: 1.6476
Epoch 1125/5000
26/26 - 1s - loss: 1.2372 - val_loss: 1.6495
Epoch 1126/5000
26/26 - 1s - loss: 1.2345 - val_loss: 1.6464
Epoch 1127/5000
26/26 - 1s - loss: 1.2331 - val_loss: 1.6468
Epoch 1128/5000
26/26 - 1s - loss: 1.2348 - val_loss: 1.6463
Epoch 1129/5000
26/26 - 1s - loss: 1.2308 - val_loss: 1.6435
Epoch 1130/5000
26/26 - 1s - loss: 1.2317 - val_loss: 1.6425
Epoch 01130: val_loss improved from 1.65170 to 1.64246, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 1131/5000
26/26 - 1s - loss: 1.2306 - val_loss: 1.6433
Epoch 1132/5000
26/26 - 1s - loss: 1.2308 - val_loss: 1.6411
Epoch 1133/5000
26/26 - 1s - loss: 1.2303 - val_loss: 1.6402
Epoch 1134/5000
26/26 - 1s - loss: 1.2289 - val_loss: 1.6391
Epoch 1135/5000
26/26 - 1s - loss: 1.2275 - val_loss: 1.6389
Epoch 1136/5000
26/26 - 1s - loss: 1.2269 - val_loss: 1.6376
Epoch 1137/5000
26/26 - 1s - loss: 1.2255 - val_loss: 1.6373
Epoch 1138/5000
26/26 - 1s - loss: 1.2242 - val_loss: 1.6342
Epoch 1139/5000
26/26 - 1s - loss: 1.2231 - val_loss: 1.6326
Epoch 1140/5000
26/26 - 1s - loss: 1.2242 - val_loss: 1.6327
Epoch 01140: val_loss improved from 1.64246 to 1.63270, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 1141/5000
26/26 - 1s - loss: 1.2221 - val_loss: 1.6336
Epoch 1142/5000
26/26 - 1s - loss: 1.2223 - val_loss: 1.6334
Epoch 1143/5000
26/26 - 1s - loss: 1.2196 - val_loss: 1.6316
Epoch 1144/5000
26/26 - 1s - loss: 1.2183 - val_loss: 1.6308
Epoch 1145/5000
26/26 - 1s - loss: 1.2200 - val_loss: 1.6315
Epoch 1146/5000
26/26 - 1s - loss: 1.2176 - val_loss: 1.6313
Epoch 1147/5000
26/26 - 1s - loss: 1.2172 - val_loss: 1.6291
Epoch 1148/5000
26/26 - 1s - loss: 1.2152 - val_loss: 1.6278
Epoch 1149/5000
26/26 - 1s - loss: 1.2170 - val_loss: 1.6265
Epoch 1150/5000
26/26 - 1s - loss: 1.2146 - val_loss: 1.6264
Epoch 01150: val_loss improved from 1.63270 to 1.62639, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 1151/5000
26/26 - 1s - loss: 1.2142 - val_loss: 1.6259
Epoch 1152/5000
26/26 - 1s - loss: 1.2127 - val_loss: 1.6254
Epoch 1153/5000
26/26 - 1s - loss: 1.2128 - val_loss: 1.6247
Epoch 1154/5000
26/26 - 1s - loss: 1.2106 - val_loss: 1.6229
Epoch 1155/5000
26/26 - 1s - loss: 1.2116 - val_loss: 1.6224
Epoch 1156/5000
26/26 - 1s - loss: 1.2105 - val_loss: 1.6228
Epoch 1157/5000
26/26 - 1s - loss: 1.2091 - val_loss: 1.6205
Epoch 1158/5000
26/26 - 1s - loss: 1.2083 - val_loss: 1.6208
Epoch 1159/5000
26/26 - 1s - loss: 1.2071 - val_loss: 1.6206
Epoch 1160/5000
26/26 - 1s - loss: 1.2052 - val_loss: 1.6183
Epoch 01160: val_loss improved from 1.62639 to 1.61826, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 1161/5000
26/26 - 1s - loss: 1.2048 - val_loss: 1.6172
Epoch 1162/5000
26/26 - 1s - loss: 1.2059 - val_loss: 1.6168
Epoch 1163/5000
26/26 - 1s - loss: 1.2036 - val_loss: 1.6179
Epoch 1164/5000
26/26 - 1s - loss: 1.2032 - val_loss: 1.6162
Epoch 1165/5000
26/26 - 1s - loss: 1.2011 - val_loss: 1.6143
Epoch 1166/5000
26/26 - 1s - loss: 1.2007 - val_loss: 1.6143
Epoch 1167/5000
26/26 - 1s - loss: 1.2000 - val_loss: 1.6119
Epoch 1168/5000
26/26 - 1s - loss: 1.1996 - val_loss: 1.6122
Epoch 1169/5000
26/26 - 1s - loss: 1.1989 - val_loss: 1.6099
Epoch 1170/5000
26/26 - 1s - loss: 1.1966 - val_loss: 1.6093
Epoch 01170: val_loss improved from 1.61826 to 1.60934, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 1171/5000
26/26 - 1s - loss: 1.1982 - val_loss: 1.6097
Epoch 1172/5000
26/26 - 1s - loss: 1.1948 - val_loss: 1.6100
Epoch 1173/5000
26/26 - 1s - loss: 1.1936 - val_loss: 1.6079
Epoch 1174/5000
26/26 - 1s - loss: 1.1951 - val_loss: 1.6075
Epoch 1175/5000
26/26 - 1s - loss: 1.1946 - val_loss: 1.6062
Epoch 1176/5000
26/26 - 1s - loss: 1.1948 - val_loss: 1.6069
Epoch 1177/5000
26/26 - 1s - loss: 1.1937 - val_loss: 1.6053
Epoch 1178/5000
26/26 - 1s - loss: 1.1900 - val_loss: 1.6045
Epoch 1179/5000
26/26 - 1s - loss: 1.1919 - val_loss: 1.6034
Epoch 1180/5000
26/26 - 1s - loss: 1.1903 - val_loss: 1.6034
Epoch 01180: val_loss improved from 1.60934 to 1.60342, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 1181/5000
26/26 - 1s - loss: 1.1908 - val_loss: 1.6048
Epoch 1182/5000
26/26 - 1s - loss: 1.1889 - val_loss: 1.6017
Epoch 1183/5000
26/26 - 1s - loss: 1.1854 - val_loss: 1.5995
Epoch 1184/5000
26/26 - 1s - loss: 1.1863 - val_loss: 1.5998
Epoch 1185/5000
26/26 - 1s - loss: 1.1868 - val_loss: 1.5986
Epoch 1186/5000
26/26 - 1s - loss: 1.1861 - val_loss: 1.5990
Epoch 1187/5000
26/26 - 1s - loss: 1.1848 - val_loss: 1.5969
Epoch 1188/5000
26/26 - 1s - loss: 1.1845 - val_loss: 1.5953
Epoch 1189/5000
26/26 - 1s - loss: 1.1821 - val_loss: 1.5968
Epoch 1190/5000
26/26 - 1s - loss: 1.1820 - val_loss: 1.5947
Epoch 01190: val_loss improved from 1.60342 to 1.59471, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 1191/5000
26/26 - 1s - loss: 1.1833 - val_loss: 1.5936
Epoch 1192/5000
26/26 - 1s - loss: 1.1792 - val_loss: 1.5928
Epoch 1193/5000
26/26 - 1s - loss: 1.1803 - val_loss: 1.5917
Epoch 1194/5000
26/26 - 1s - loss: 1.1786 - val_loss: 1.5912
Epoch 1195/5000
26/26 - 1s - loss: 1.1782 - val_loss: 1.5908
Epoch 1196/5000
26/26 - 1s - loss: 1.1761 - val_loss: 1.5901
Epoch 1197/5000
26/26 - 1s - loss: 1.1757 - val_loss: 1.5898
Epoch 1198/5000
26/26 - 1s - loss: 1.1762 - val_loss: 1.5885
Epoch 1199/5000
26/26 - 1s - loss: 1.1737 - val_loss: 1.5883
Epoch 1200/5000
26/26 - 1s - loss: 1.1743 - val_loss: 1.5863
Epoch 01200: val_loss improved from 1.59471 to 1.58629, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 1201/5000
26/26 - 1s - loss: 1.1745 - val_loss: 1.5862
Epoch 1202/5000
26/26 - 1s - loss: 1.1740 - val_loss: 1.5848
Epoch 1203/5000
26/26 - 1s - loss: 1.1713 - val_loss: 1.5842
Epoch 1204/5000
26/26 - 1s - loss: 1.1725 - val_loss: 1.5845
Epoch 1205/5000
26/26 - 1s - loss: 1.1704 - val_loss: 1.5839
Epoch 1206/5000
26/26 - 1s - loss: 1.1685 - val_loss: 1.5815
Epoch 1207/5000
26/26 - 1s - loss: 1.1692 - val_loss: 1.5814
Epoch 1208/5000
26/26 - 1s - loss: 1.1681 - val_loss: 1.5798
Epoch 1209/5000
26/26 - 1s - loss: 1.1660 - val_loss: 1.5797
Epoch 1210/5000
26/26 - 1s - loss: 1.1674 - val_loss: 1.5800
Epoch 01210: val_loss improved from 1.58629 to 1.58004, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 1211/5000
26/26 - 1s - loss: 1.1661 - val_loss: 1.5790
Epoch 1212/5000
26/26 - 1s - loss: 1.1657 - val_loss: 1.5785
Epoch 1213/5000
26/26 - 1s - loss: 1.1654 - val_loss: 1.5751
Epoch 1214/5000
26/26 - 1s - loss: 1.1616 - val_loss: 1.5768
Epoch 1215/5000
26/26 - 1s - loss: 1.1616 - val_loss: 1.5760
Epoch 1216/5000
26/26 - 1s - loss: 1.1624 - val_loss: 1.5739
Epoch 1217/5000
26/26 - 1s - loss: 1.1616 - val_loss: 1.5736
Epoch 1218/5000
26/26 - 1s - loss: 1.1618 - val_loss: 1.5732
Epoch 1219/5000
26/26 - 1s - loss: 1.1608 - val_loss: 1.5734
Epoch 1220/5000
26/26 - 1s - loss: 1.1574 - val_loss: 1.5711
Epoch 01220: val_loss improved from 1.58004 to 1.57114, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 1221/5000
26/26 - 1s - loss: 1.1580 - val_loss: 1.5717
Epoch 1222/5000
26/26 - 1s - loss: 1.1577 - val_loss: 1.5712
Epoch 1223/5000
26/26 - 1s - loss: 1.1564 - val_loss: 1.5696
Epoch 1224/5000
26/26 - 1s - loss: 1.1552 - val_loss: 1.5686
Epoch 1225/5000
26/26 - 1s - loss: 1.1558 - val_loss: 1.5676
Epoch 1226/5000
26/26 - 1s - loss: 1.1540 - val_loss: 1.5668
Epoch 1227/5000
26/26 - 1s - loss: 1.1536 - val_loss: 1.5670
Epoch 1228/5000
26/26 - 1s - loss: 1.1510 - val_loss: 1.5648
Epoch 1229/5000
26/26 - 1s - loss: 1.1511 - val_loss: 1.5648
Epoch 1230/5000
26/26 - 1s - loss: 1.1504 - val_loss: 1.5636
Epoch 01230: val_loss improved from 1.57114 to 1.56362, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 1231/5000
26/26 - 1s - loss: 1.1509 - val_loss: 1.5633
Epoch 1232/5000
26/26 - 1s - loss: 1.1491 - val_loss: 1.5625
Epoch 1233/5000
26/26 - 1s - loss: 1.1482 - val_loss: 1.5616
Epoch 1234/5000
26/26 - 1s - loss: 1.1475 - val_loss: 1.5602
Epoch 1235/5000
26/26 - 1s - loss: 1.1462 - val_loss: 1.5613
Epoch 1236/5000
26/26 - 1s - loss: 1.1450 - val_loss: 1.5600
Epoch 1237/5000
26/26 - 1s - loss: 1.1473 - val_loss: 1.5586
Epoch 1238/5000
26/26 - 1s - loss: 1.1456 - val_loss: 1.5578
Epoch 1239/5000
26/26 - 1s - loss: 1.1458 - val_loss: 1.5583
Epoch 1240/5000
26/26 - 1s - loss: 1.1441 - val_loss: 1.5568
Epoch 01240: val_loss improved from 1.56362 to 1.55679, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 1241/5000
26/26 - 1s - loss: 1.1405 - val_loss: 1.5573
Epoch 1242/5000
26/26 - 1s - loss: 1.1430 - val_loss: 1.5561
Epoch 1243/5000
26/26 - 1s - loss: 1.1401 - val_loss: 1.5549
Epoch 1244/5000
26/26 - 1s - loss: 1.1405 - val_loss: 1.5545
Epoch 1245/5000
26/26 - 1s - loss: 1.1392 - val_loss: 1.5531
Epoch 1246/5000
26/26 - 1s - loss: 1.1394 - val_loss: 1.5523
Epoch 1247/5000
26/26 - 1s - loss: 1.1394 - val_loss: 1.5525
Epoch 1248/5000
26/26 - 1s - loss: 1.1386 - val_loss: 1.5502
Epoch 1249/5000
26/26 - 1s - loss: 1.1370 - val_loss: 1.5507
Epoch 1250/5000
26/26 - 1s - loss: 1.1362 - val_loss: 1.5510
Epoch 01250: val_loss improved from 1.55679 to 1.55098, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 1251/5000
26/26 - 1s - loss: 1.1343 - val_loss: 1.5495
Epoch 1252/5000
26/26 - 1s - loss: 1.1338 - val_loss: 1.5479
Epoch 1253/5000
26/26 - 1s - loss: 1.1343 - val_loss: 1.5469
Epoch 1254/5000
26/26 - 1s - loss: 1.1329 - val_loss: 1.5457
Epoch 1255/5000
26/26 - 2s - loss: 1.1321 - val_loss: 1.5453
Epoch 1256/5000
26/26 - 1s - loss: 1.1316 - val_loss: 1.5447
Epoch 1257/5000
26/26 - 1s - loss: 1.1309 - val_loss: 1.5439
Epoch 1258/5000
26/26 - 1s - loss: 1.1303 - val_loss: 1.5431
Epoch 1259/5000
26/26 - 1s - loss: 1.1300 - val_loss: 1.5434
Epoch 1260/5000
26/26 - 1s - loss: 1.1289 - val_loss: 1.5413
Epoch 01260: val_loss improved from 1.55098 to 1.54132, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 1261/5000
26/26 - 1s - loss: 1.1266 - val_loss: 1.5405
Epoch 1262/5000
26/26 - 1s - loss: 1.1262 - val_loss: 1.5399
Epoch 1263/5000
26/26 - 1s - loss: 1.1256 - val_loss: 1.5402
Epoch 1264/5000
26/26 - 1s - loss: 1.1255 - val_loss: 1.5408
Epoch 1265/5000
26/26 - 1s - loss: 1.1250 - val_loss: 1.5392
Epoch 1266/5000
26/26 - 1s - loss: 1.1253 - val_loss: 1.5383
Epoch 1267/5000
26/26 - 1s - loss: 1.1227 - val_loss: 1.5376
Epoch 1268/5000
26/26 - 1s - loss: 1.1228 - val_loss: 1.5364
Epoch 1269/5000
26/26 - 1s - loss: 1.1230 - val_loss: 1.5361
Epoch 1270/5000
26/26 - 1s - loss: 1.1223 - val_loss: 1.5345
Epoch 01270: val_loss improved from 1.54132 to 1.53446, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 1271/5000
26/26 - 1s - loss: 1.1199 - val_loss: 1.5355
Epoch 1272/5000
26/26 - 1s - loss: 1.1205 - val_loss: 1.5331
Epoch 1273/5000
26/26 - 1s - loss: 1.1193 - val_loss: 1.5337
Epoch 1274/5000
26/26 - 1s - loss: 1.1182 - val_loss: 1.5338
Epoch 1275/5000
26/26 - 1s - loss: 1.1180 - val_loss: 1.5320
Epoch 1276/5000
26/26 - 1s - loss: 1.1166 - val_loss: 1.5304
Epoch 1277/5000
26/26 - 1s - loss: 1.1146 - val_loss: 1.5298
Epoch 1278/5000
26/26 - 1s - loss: 1.1167 - val_loss: 1.5289
Epoch 1279/5000
26/26 - 1s - loss: 1.1148 - val_loss: 1.5296
Epoch 1280/5000
26/26 - 1s - loss: 1.1137 - val_loss: 1.5285
Epoch 01280: val_loss improved from 1.53446 to 1.52852, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 1281/5000
26/26 - 1s - loss: 1.1140 - val_loss: 1.5270
Epoch 1282/5000
26/26 - 1s - loss: 1.1139 - val_loss: 1.5267
Epoch 1283/5000
26/26 - 1s - loss: 1.1132 - val_loss: 1.5255
Epoch 1284/5000
26/26 - 1s - loss: 1.1118 - val_loss: 1.5243
Epoch 1285/5000
26/26 - 1s - loss: 1.1115 - val_loss: 1.5246
Epoch 1286/5000
26/26 - 1s - loss: 1.1115 - val_loss: 1.5233
Epoch 1287/5000
26/26 - 1s - loss: 1.1101 - val_loss: 1.5225
Epoch 1288/5000
26/26 - 1s - loss: 1.1099 - val_loss: 1.5226
Epoch 1289/5000
26/26 - 1s - loss: 1.1087 - val_loss: 1.5221
Epoch 1290/5000
26/26 - 1s - loss: 1.1074 - val_loss: 1.5225
Epoch 01290: val_loss improved from 1.52852 to 1.52254, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 1291/5000
26/26 - 1s - loss: 1.1076 - val_loss: 1.5197
Epoch 1292/5000
26/26 - 1s - loss: 1.1044 - val_loss: 1.5198
Epoch 1293/5000
26/26 - 1s - loss: 1.1070 - val_loss: 1.5200
Epoch 1294/5000
26/26 - 1s - loss: 1.1045 - val_loss: 1.5187
Epoch 1295/5000
26/26 - 1s - loss: 1.1027 - val_loss: 1.5181
Epoch 1296/5000
26/26 - 1s - loss: 1.1016 - val_loss: 1.5161
Epoch 1297/5000
26/26 - 1s - loss: 1.1010 - val_loss: 1.5165
Epoch 1298/5000
26/26 - 1s - loss: 1.1001 - val_loss: 1.5140
Epoch 1299/5000
26/26 - 1s - loss: 1.0999 - val_loss: 1.5139
Epoch 1300/5000
26/26 - 1s - loss: 1.1013 - val_loss: 1.5137
Epoch 01300: val_loss improved from 1.52254 to 1.51370, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 1301/5000
26/26 - 1s - loss: 1.0988 - val_loss: 1.5140
Epoch 1302/5000
26/26 - 1s - loss: 1.0996 - val_loss: 1.5134
Epoch 1303/5000
26/26 - 1s - loss: 1.0959 - val_loss: 1.5132
Epoch 1304/5000
26/26 - 1s - loss: 1.0971 - val_loss: 1.5132
Epoch 1305/5000
26/26 - 1s - loss: 1.0957 - val_loss: 1.5108
Epoch 1306/5000
26/26 - 1s - loss: 1.0960 - val_loss: 1.5107
Epoch 1307/5000
26/26 - 1s - loss: 1.0952 - val_loss: 1.5096
Epoch 1308/5000
26/26 - 1s - loss: 1.0927 - val_loss: 1.5077
Epoch 1309/5000
26/26 - 1s - loss: 1.0934 - val_loss: 1.5087
Epoch 1310/5000
26/26 - 1s - loss: 1.0945 - val_loss: 1.5067
Epoch 01310: val_loss improved from 1.51370 to 1.50667, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 1311/5000
26/26 - 1s - loss: 1.0935 - val_loss: 1.5065
Epoch 1312/5000
26/26 - 1s - loss: 1.0933 - val_loss: 1.5066
Epoch 1313/5000
26/26 - 1s - loss: 1.0897 - val_loss: 1.5063
Epoch 1314/5000
26/26 - 1s - loss: 1.0904 - val_loss: 1.5047
Epoch 1315/5000
26/26 - 1s - loss: 1.0884 - val_loss: 1.5026
Epoch 1316/5000
26/26 - 1s - loss: 1.0891 - val_loss: 1.5028
Epoch 1317/5000
26/26 - 1s - loss: 1.0879 - val_loss: 1.5010
Epoch 1318/5000
26/26 - 1s - loss: 1.0868 - val_loss: 1.5024
Epoch 1319/5000
26/26 - 1s - loss: 1.0871 - val_loss: 1.5023
Epoch 1320/5000
26/26 - 1s - loss: 1.0867 - val_loss: 1.5008
Epoch 01320: val_loss improved from 1.50667 to 1.50076, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 1321/5000
26/26 - 1s - loss: 1.0855 - val_loss: 1.4993
Epoch 1322/5000
26/26 - 1s - loss: 1.0852 - val_loss: 1.4991
Epoch 1323/5000
26/26 - 1s - loss: 1.0857 - val_loss: 1.4985
Epoch 1324/5000
26/26 - 1s - loss: 1.0822 - val_loss: 1.4972
Epoch 1325/5000
26/26 - 1s - loss: 1.0831 - val_loss: 1.4977
Epoch 1326/5000
26/26 - 1s - loss: 1.0833 - val_loss: 1.4979
Epoch 1327/5000
26/26 - 1s - loss: 1.0829 - val_loss: 1.4963
Epoch 1328/5000
26/26 - 1s - loss: 1.0818 - val_loss: 1.4958
Epoch 1329/5000
26/26 - 1s - loss: 1.0806 - val_loss: 1.4949
Epoch 1330/5000
26/26 - 1s - loss: 1.0790 - val_loss: 1.4937
Epoch 01330: val_loss improved from 1.50076 to 1.49367, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 1331/5000
26/26 - 1s - loss: 1.0790 - val_loss: 1.4927
Epoch 1332/5000
26/26 - 1s - loss: 1.0775 - val_loss: 1.4909
Epoch 1333/5000
26/26 - 1s - loss: 1.0779 - val_loss: 1.4916
Epoch 1334/5000
26/26 - 1s - loss: 1.0771 - val_loss: 1.4903
Epoch 1335/5000
26/26 - 1s - loss: 1.0774 - val_loss: 1.4895
Epoch 1336/5000
26/26 - 1s - loss: 1.0755 - val_loss: 1.4890
Epoch 1337/5000
26/26 - 1s - loss: 1.0757 - val_loss: 1.4910
Epoch 1338/5000
26/26 - 1s - loss: 1.0744 - val_loss: 1.4892
Epoch 1339/5000
26/26 - 2s - loss: 1.0742 - val_loss: 1.4886
Epoch 1340/5000
26/26 - 1s - loss: 1.0726 - val_loss: 1.4847
Epoch 01340: val_loss improved from 1.49367 to 1.48475, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 1341/5000
26/26 - 1s - loss: 1.0721 - val_loss: 1.4833
Epoch 1342/5000
26/26 - 1s - loss: 1.0713 - val_loss: 1.4856
Epoch 1343/5000
26/26 - 1s - loss: 1.0697 - val_loss: 1.4846
Epoch 1344/5000
26/26 - 1s - loss: 1.0701 - val_loss: 1.4833
Epoch 1345/5000
26/26 - 1s - loss: 1.0709 - val_loss: 1.4839
Epoch 1346/5000
26/26 - 1s - loss: 1.0704 - val_loss: 1.4814
Epoch 1347/5000
26/26 - 1s - loss: 1.0695 - val_loss: 1.4811
Epoch 1348/5000
26/26 - 1s - loss: 1.0674 - val_loss: 1.4812
Epoch 1349/5000
26/26 - 1s - loss: 1.0677 - val_loss: 1.4811
Epoch 1350/5000
26/26 - 1s - loss: 1.0673 - val_loss: 1.4808
Epoch 01350: val_loss improved from 1.48475 to 1.48082, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 1351/5000
26/26 - 1s - loss: 1.0662 - val_loss: 1.4792
Epoch 1352/5000
26/26 - 1s - loss: 1.0663 - val_loss: 1.4786
Epoch 1353/5000
26/26 - 1s - loss: 1.0638 - val_loss: 1.4787
Epoch 1354/5000
26/26 - 1s - loss: 1.0647 - val_loss: 1.4776
Epoch 1355/5000
26/26 - 1s - loss: 1.0632 - val_loss: 1.4773
Epoch 1356/5000
26/26 - 1s - loss: 1.0622 - val_loss: 1.4768
Epoch 1357/5000
26/26 - 1s - loss: 1.0624 - val_loss: 1.4763
Epoch 1358/5000
26/26 - 1s - loss: 1.0630 - val_loss: 1.4753
Epoch 1359/5000
26/26 - 1s - loss: 1.0610 - val_loss: 1.4737
Epoch 1360/5000
26/26 - 1s - loss: 1.0583 - val_loss: 1.4741
Epoch 01360: val_loss improved from 1.48082 to 1.47411, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 1361/5000
26/26 - 1s - loss: 1.0597 - val_loss: 1.4738
Epoch 1362/5000
26/26 - 1s - loss: 1.0593 - val_loss: 1.4725
Epoch 1363/5000
26/26 - 1s - loss: 1.0589 - val_loss: 1.4720
Epoch 1364/5000
26/26 - 1s - loss: 1.0580 - val_loss: 1.4719
Epoch 1365/5000
26/26 - 1s - loss: 1.0574 - val_loss: 1.4715
Epoch 1366/5000
26/26 - 1s - loss: 1.0563 - val_loss: 1.4699
Epoch 1367/5000
26/26 - 1s - loss: 1.0558 - val_loss: 1.4691
Epoch 1368/5000
26/26 - 1s - loss: 1.0547 - val_loss: 1.4688
Epoch 1369/5000
26/26 - 1s - loss: 1.0537 - val_loss: 1.4671
Epoch 1370/5000
26/26 - 1s - loss: 1.0538 - val_loss: 1.4650
Epoch 01370: val_loss improved from 1.47411 to 1.46500, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 1371/5000
26/26 - 2s - loss: 1.0533 - val_loss: 1.4659
Epoch 1372/5000
26/26 - 2s - loss: 1.0540 - val_loss: 1.4668
Epoch 1373/5000
26/26 - 1s - loss: 1.0541 - val_loss: 1.4653
Epoch 1374/5000
26/26 - 1s - loss: 1.0522 - val_loss: 1.4667
Epoch 1375/5000
26/26 - 1s - loss: 1.0500 - val_loss: 1.4643
Epoch 1376/5000
26/26 - 1s - loss: 1.0502 - val_loss: 1.4654
Epoch 1377/5000
26/26 - 1s - loss: 1.0487 - val_loss: 1.4633
Epoch 1378/5000
26/26 - 1s - loss: 1.0486 - val_loss: 1.4618
Epoch 1379/5000
26/26 - 1s - loss: 1.0480 - val_loss: 1.4626
Epoch 1380/5000
26/26 - 2s - loss: 1.0473 - val_loss: 1.4618
Epoch 01380: val_loss improved from 1.46500 to 1.46182, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 1381/5000
26/26 - 1s - loss: 1.0459 - val_loss: 1.4598
Epoch 1382/5000
26/26 - 1s - loss: 1.0444 - val_loss: 1.4575
Epoch 1383/5000
26/26 - 1s - loss: 1.0444 - val_loss: 1.4579
Epoch 1384/5000
26/26 - 1s - loss: 1.0454 - val_loss: 1.4569
Epoch 1385/5000
26/26 - 1s - loss: 1.0433 - val_loss: 1.4552
Epoch 1386/5000
26/26 - 1s - loss: 1.0451 - val_loss: 1.4549
Epoch 1387/5000
26/26 - 1s - loss: 1.0421 - val_loss: 1.4569
Epoch 1388/5000
26/26 - 1s - loss: 1.0421 - val_loss: 1.4559
Epoch 1389/5000
26/26 - 1s - loss: 1.0426 - val_loss: 1.4537
Epoch 1390/5000
26/26 - 1s - loss: 1.0404 - val_loss: 1.4534
Epoch 01390: val_loss improved from 1.46182 to 1.45339, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 1391/5000
26/26 - 1s - loss: 1.0404 - val_loss: 1.4539
Epoch 1392/5000
26/26 - 1s - loss: 1.0405 - val_loss: 1.4530
Epoch 1393/5000
26/26 - 1s - loss: 1.0377 - val_loss: 1.4531
Epoch 1394/5000
26/26 - 1s - loss: 1.0370 - val_loss: 1.4533
Epoch 1395/5000
26/26 - 1s - loss: 1.0384 - val_loss: 1.4519
Epoch 1396/5000
26/26 - 1s - loss: 1.0381 - val_loss: 1.4501
Epoch 1397/5000
26/26 - 1s - loss: 1.0370 - val_loss: 1.4497
Epoch 1398/5000
26/26 - 1s - loss: 1.0366 - val_loss: 1.4496
Epoch 1399/5000
26/26 - 1s - loss: 1.0357 - val_loss: 1.4489
Epoch 1400/5000
26/26 - 1s - loss: 1.0348 - val_loss: 1.4472
Epoch 01400: val_loss improved from 1.45339 to 1.44721, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 1401/5000
26/26 - 1s - loss: 1.0359 - val_loss: 1.4456
Epoch 1402/5000
26/26 - 1s - loss: 1.0342 - val_loss: 1.4469
Epoch 1403/5000
26/26 - 1s - loss: 1.0320 - val_loss: 1.4486
Epoch 1404/5000
26/26 - 1s - loss: 1.0341 - val_loss: 1.4460
Epoch 1405/5000
26/26 - 2s - loss: 1.0324 - val_loss: 1.4465
Epoch 1406/5000
26/26 - 1s - loss: 1.0320 - val_loss: 1.4468
Epoch 1407/5000
26/26 - 1s - loss: 1.0320 - val_loss: 1.4450
Epoch 1408/5000
26/26 - 1s - loss: 1.0318 - val_loss: 1.4447
Epoch 1409/5000
26/26 - 1s - loss: 1.0289 - val_loss: 1.4433
Epoch 1410/5000
26/26 - 1s - loss: 1.0305 - val_loss: 1.4414
Epoch 01410: val_loss improved from 1.44721 to 1.44142, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 1411/5000
26/26 - 1s - loss: 1.0290 - val_loss: 1.4432
Epoch 1412/5000
26/26 - 1s - loss: 1.0271 - val_loss: 1.4430
Epoch 1413/5000
26/26 - 1s - loss: 1.0276 - val_loss: 1.4410
Epoch 1414/5000
26/26 - 1s - loss: 1.0268 - val_loss: 1.4426
Epoch 1415/5000
26/26 - 1s - loss: 1.0246 - val_loss: 1.4411
Epoch 1416/5000
26/26 - 1s - loss: 1.0244 - val_loss: 1.4400
Epoch 1417/5000
26/26 - 1s - loss: 1.0239 - val_loss: 1.4405
Epoch 1418/5000
26/26 - 1s - loss: 1.0242 - val_loss: 1.4382
Epoch 1419/5000
26/26 - 1s - loss: 1.0235 - val_loss: 1.4354
Epoch 1420/5000
26/26 - 1s - loss: 1.0219 - val_loss: 1.4350
Epoch 01420: val_loss improved from 1.44142 to 1.43499, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 1421/5000
26/26 - 1s - loss: 1.0224 - val_loss: 1.4367
Epoch 1422/5000
26/26 - 2s - loss: 1.0210 - val_loss: 1.4382
Epoch 1423/5000
26/26 - 1s - loss: 1.0195 - val_loss: 1.4369
Epoch 1424/5000
26/26 - 1s - loss: 1.0202 - val_loss: 1.4353
Epoch 1425/5000
26/26 - 1s - loss: 1.0200 - val_loss: 1.4345
Epoch 1426/5000
26/26 - 1s - loss: 1.0195 - val_loss: 1.4335
Epoch 1427/5000
26/26 - 1s - loss: 1.0178 - val_loss: 1.4328
Epoch 1428/5000
26/26 - 1s - loss: 1.0162 - val_loss: 1.4328
Epoch 1429/5000
26/26 - 1s - loss: 1.0180 - val_loss: 1.4296
Epoch 1430/5000
26/26 - 1s - loss: 1.0161 - val_loss: 1.4309
Epoch 01430: val_loss improved from 1.43499 to 1.43086, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 1431/5000
26/26 - 1s - loss: 1.0166 - val_loss: 1.4302
Epoch 1432/5000
26/26 - 1s - loss: 1.0151 - val_loss: 1.4289
Epoch 1433/5000
26/26 - 1s - loss: 1.0142 - val_loss: 1.4292
Epoch 1434/5000
26/26 - 1s - loss: 1.0144 - val_loss: 1.4291
Epoch 1435/5000
26/26 - 1s - loss: 1.0139 - val_loss: 1.4274
Epoch 1436/5000
26/26 - 1s - loss: 1.0126 - val_loss: 1.4276
Epoch 1437/5000
26/26 - 1s - loss: 1.0114 - val_loss: 1.4265
Epoch 1438/5000
26/26 - 1s - loss: 1.0110 - val_loss: 1.4273
Epoch 1439/5000
26/26 - 1s - loss: 1.0133 - val_loss: 1.4244
Epoch 1440/5000
26/26 - 1s - loss: 1.0129 - val_loss: 1.4240
Epoch 01440: val_loss improved from 1.43086 to 1.42403, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 1441/5000
26/26 - 1s - loss: 1.0114 - val_loss: 1.4255
Epoch 1442/5000
26/26 - 1s - loss: 1.0084 - val_loss: 1.4241
Epoch 1443/5000
26/26 - 1s - loss: 1.0099 - val_loss: 1.4236
Epoch 1444/5000
26/26 - 1s - loss: 1.0083 - val_loss: 1.4225
Epoch 1445/5000
26/26 - 1s - loss: 1.0075 - val_loss: 1.4216
Epoch 1446/5000
26/26 - 1s - loss: 1.0084 - val_loss: 1.4216
Epoch 1447/5000
26/26 - 1s - loss: 1.0069 - val_loss: 1.4204
Epoch 1448/5000
26/26 - 1s - loss: 1.0068 - val_loss: 1.4209
Epoch 1449/5000
26/26 - 1s - loss: 1.0041 - val_loss: 1.4184
Epoch 1450/5000
26/26 - 1s - loss: 1.0057 - val_loss: 1.4208
Epoch 01450: val_loss improved from 1.42403 to 1.42084, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 1451/5000
26/26 - 1s - loss: 1.0050 - val_loss: 1.4179
Epoch 1452/5000
26/26 - 1s - loss: 1.0041 - val_loss: 1.4183
Epoch 1453/5000
26/26 - 1s - loss: 1.0036 - val_loss: 1.4169
Epoch 1454/5000
26/26 - 1s - loss: 1.0023 - val_loss: 1.4160
Epoch 1455/5000
26/26 - 1s - loss: 1.0005 - val_loss: 1.4159
Epoch 1456/5000
26/26 - 1s - loss: 1.0011 - val_loss: 1.4160
Epoch 1457/5000
26/26 - 1s - loss: 1.0011 - val_loss: 1.4153
Epoch 1458/5000
26/26 - 1s - loss: 0.9998 - val_loss: 1.4150
Epoch 1459/5000
26/26 - 1s - loss: 0.9992 - val_loss: 1.4152
Epoch 1460/5000
26/26 - 1s - loss: 0.9990 - val_loss: 1.4144
Epoch 01460: val_loss improved from 1.42084 to 1.41439, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 1461/5000
26/26 - 1s - loss: 0.9976 - val_loss: 1.4128
Epoch 1462/5000
26/26 - 1s - loss: 0.9976 - val_loss: 1.4131
Epoch 1463/5000
26/26 - 1s - loss: 0.9969 - val_loss: 1.4136
Epoch 1464/5000
26/26 - 1s - loss: 0.9982 - val_loss: 1.4120
Epoch 1465/5000
26/26 - 1s - loss: 0.9959 - val_loss: 1.4102
Epoch 1466/5000
26/26 - 1s - loss: 0.9971 - val_loss: 1.4109
Epoch 1467/5000
26/26 - 1s - loss: 0.9956 - val_loss: 1.4098
Epoch 1468/5000
26/26 - 1s - loss: 0.9945 - val_loss: 1.4089
Epoch 1469/5000
26/26 - 1s - loss: 0.9944 - val_loss: 1.4080
Epoch 1470/5000
26/26 - 1s - loss: 0.9926 - val_loss: 1.4094
Epoch 01470: val_loss improved from 1.41439 to 1.40940, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 1471/5000
26/26 - 1s - loss: 0.9930 - val_loss: 1.4065
Epoch 1472/5000
26/26 - 1s - loss: 0.9925 - val_loss: 1.4071
Epoch 1473/5000
26/26 - 1s - loss: 0.9913 - val_loss: 1.4063
Epoch 1474/5000
26/26 - 1s - loss: 0.9908 - val_loss: 1.4056
Epoch 1475/5000
26/26 - 1s - loss: 0.9914 - val_loss: 1.4053
Epoch 1476/5000
26/26 - 1s - loss: 0.9889 - val_loss: 1.4031
Epoch 1477/5000
26/26 - 1s - loss: 0.9884 - val_loss: 1.4032
Epoch 1478/5000
26/26 - 1s - loss: 0.9895 - val_loss: 1.4029
Epoch 1479/5000
26/26 - 1s - loss: 0.9868 - val_loss: 1.4026
Epoch 1480/5000
26/26 - 1s - loss: 0.9883 - val_loss: 1.4034
Epoch 01480: val_loss improved from 1.40940 to 1.40344, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 1481/5000
26/26 - 1s - loss: 0.9845 - val_loss: 1.4008
Epoch 1482/5000
26/26 - 1s - loss: 0.9872 - val_loss: 1.4000
Epoch 1483/5000
26/26 - 1s - loss: 0.9860 - val_loss: 1.4009
Epoch 1484/5000
26/26 - 2s - loss: 0.9857 - val_loss: 1.4003
Epoch 1485/5000
26/26 - 1s - loss: 0.9861 - val_loss: 1.3990
Epoch 1486/5000
26/26 - 1s - loss: 0.9833 - val_loss: 1.3984
Epoch 1487/5000
26/26 - 1s - loss: 0.9845 - val_loss: 1.3984
Epoch 1488/5000
26/26 - 1s - loss: 0.9825 - val_loss: 1.3959
Epoch 1489/5000
26/26 - 1s - loss: 0.9809 - val_loss: 1.3975
Epoch 1490/5000
26/26 - 1s - loss: 0.9811 - val_loss: 1.3955
Epoch 01490: val_loss improved from 1.40344 to 1.39549, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 1491/5000
26/26 - 1s - loss: 0.9804 - val_loss: 1.3954
Epoch 1492/5000
26/26 - 1s - loss: 0.9799 - val_loss: 1.3953
Epoch 1493/5000
26/26 - 1s - loss: 0.9815 - val_loss: 1.3945
Epoch 1494/5000
26/26 - 1s - loss: 0.9793 - val_loss: 1.3933
Epoch 1495/5000
26/26 - 2s - loss: 0.9787 - val_loss: 1.3935
Epoch 1496/5000
26/26 - 1s - loss: 0.9792 - val_loss: 1.3939
Epoch 1497/5000
26/26 - 1s - loss: 0.9788 - val_loss: 1.3923
Epoch 1498/5000
26/26 - 1s - loss: 0.9765 - val_loss: 1.3911
Epoch 1499/5000
26/26 - 1s - loss: 0.9768 - val_loss: 1.3906
Epoch 1500/5000
26/26 - 1s - loss: 0.9763 - val_loss: 1.3915
Epoch 01500: val_loss improved from 1.39549 to 1.39154, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 1501/5000
26/26 - 1s - loss: 0.9766 - val_loss: 1.3895
Epoch 1502/5000
26/26 - 1s - loss: 0.9746 - val_loss: 1.3898
Epoch 1503/5000
26/26 - 1s - loss: 0.9747 - val_loss: 1.3891
Epoch 1504/5000
26/26 - 1s - loss: 0.9761 - val_loss: 1.3887
Epoch 1505/5000
26/26 - 1s - loss: 0.9747 - val_loss: 1.3889
Epoch 1506/5000
26/26 - 1s - loss: 0.9724 - val_loss: 1.3877
Epoch 1507/5000
26/26 - 1s - loss: 0.9734 - val_loss: 1.3877
Epoch 1508/5000
26/26 - 1s - loss: 0.9731 - val_loss: 1.3883
Epoch 1509/5000
26/26 - 1s - loss: 0.9706 - val_loss: 1.3863
Epoch 1510/5000
26/26 - 1s - loss: 0.9708 - val_loss: 1.3862
Epoch 01510: val_loss improved from 1.39154 to 1.38618, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 1511/5000
26/26 - 1s - loss: 0.9706 - val_loss: 1.3838
Epoch 1512/5000
26/26 - 1s - loss: 0.9692 - val_loss: 1.3845
Epoch 1513/5000
26/26 - 1s - loss: 0.9702 - val_loss: 1.3828
Epoch 1514/5000
26/26 - 1s - loss: 0.9680 - val_loss: 1.3830
Epoch 1515/5000
26/26 - 1s - loss: 0.9674 - val_loss: 1.3837
Epoch 1516/5000
26/26 - 1s - loss: 0.9663 - val_loss: 1.3835
Epoch 1517/5000
26/26 - 1s - loss: 0.9678 - val_loss: 1.3836
Epoch 1518/5000
26/26 - 1s - loss: 0.9681 - val_loss: 1.3827
Epoch 1519/5000
26/26 - 1s - loss: 0.9675 - val_loss: 1.3815
Epoch 1520/5000
26/26 - 1s - loss: 0.9668 - val_loss: 1.3810
Epoch 01520: val_loss improved from 1.38618 to 1.38098, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 1521/5000
26/26 - 1s - loss: 0.9667 - val_loss: 1.3800
Epoch 1522/5000
26/26 - 1s - loss: 0.9635 - val_loss: 1.3803
Epoch 1523/5000
26/26 - 1s - loss: 0.9636 - val_loss: 1.3783
Epoch 1524/5000
26/26 - 1s - loss: 0.9628 - val_loss: 1.3788
Epoch 1525/5000
26/26 - 1s - loss: 0.9637 - val_loss: 1.3758
Epoch 1526/5000
26/26 - 1s - loss: 0.9624 - val_loss: 1.3784
Epoch 1527/5000
26/26 - 1s - loss: 0.9624 - val_loss: 1.3760
Epoch 1528/5000
26/26 - 1s - loss: 0.9625 - val_loss: 1.3758
Epoch 1529/5000
26/26 - 1s - loss: 0.9616 - val_loss: 1.3756
Epoch 1530/5000
26/26 - 1s - loss: 0.9611 - val_loss: 1.3752
Epoch 01530: val_loss improved from 1.38098 to 1.37520, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 1531/5000
26/26 - 1s - loss: 0.9608 - val_loss: 1.3741
Epoch 1532/5000
26/26 - 1s - loss: 0.9583 - val_loss: 1.3732
Epoch 1533/5000
26/26 - 1s - loss: 0.9592 - val_loss: 1.3728
Epoch 1534/5000
26/26 - 1s - loss: 0.9588 - val_loss: 1.3714
Epoch 1535/5000
26/26 - 1s - loss: 0.9571 - val_loss: 1.3719
Epoch 1536/5000
26/26 - 1s - loss: 0.9570 - val_loss: 1.3716
Epoch 1537/5000
26/26 - 1s - loss: 0.9562 - val_loss: 1.3697
Epoch 1538/5000
26/26 - 1s - loss: 0.9557 - val_loss: 1.3700
Epoch 1539/5000
26/26 - 1s - loss: 0.9563 - val_loss: 1.3692
Epoch 1540/5000
26/26 - 1s - loss: 0.9554 - val_loss: 1.3721
Epoch 01540: val_loss improved from 1.37520 to 1.37210, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 1541/5000
26/26 - 1s - loss: 0.9546 - val_loss: 1.3707
Epoch 1542/5000
26/26 - 1s - loss: 0.9546 - val_loss: 1.3693
Epoch 1543/5000
26/26 - 1s - loss: 0.9553 - val_loss: 1.3688
Epoch 1544/5000
26/26 - 1s - loss: 0.9534 - val_loss: 1.3678
Epoch 1545/5000
26/26 - 1s - loss: 0.9524 - val_loss: 1.3679
Epoch 1546/5000
26/26 - 1s - loss: 0.9521 - val_loss: 1.3678
Epoch 1547/5000
26/26 - 1s - loss: 0.9512 - val_loss: 1.3674
Epoch 1548/5000
26/26 - 1s - loss: 0.9505 - val_loss: 1.3659
Epoch 1549/5000
26/26 - 2s - loss: 0.9497 - val_loss: 1.3665
Epoch 1550/5000
26/26 - 2s - loss: 0.9507 - val_loss: 1.3657
Epoch 01550: val_loss improved from 1.37210 to 1.36566, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 1551/5000
26/26 - 1s - loss: 0.9506 - val_loss: 1.3636
Epoch 1552/5000
26/26 - 1s - loss: 0.9463 - val_loss: 1.3640
Epoch 1553/5000
26/26 - 1s - loss: 0.9480 - val_loss: 1.3630
Epoch 1554/5000
26/26 - 1s - loss: 0.9469 - val_loss: 1.3611
Epoch 1555/5000
26/26 - 1s - loss: 0.9459 - val_loss: 1.3624
Epoch 1556/5000
26/26 - 1s - loss: 0.9458 - val_loss: 1.3613
Epoch 1557/5000
26/26 - 1s - loss: 0.9454 - val_loss: 1.3613
Epoch 1558/5000
26/26 - 1s - loss: 0.9435 - val_loss: 1.3612
Epoch 1559/5000
26/26 - 2s - loss: 0.9466 - val_loss: 1.3596
Epoch 1560/5000
26/26 - 1s - loss: 0.9435 - val_loss: 1.3585
Epoch 01560: val_loss improved from 1.36566 to 1.35854, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 1561/5000
26/26 - 1s - loss: 0.9427 - val_loss: 1.3593
Epoch 1562/5000
26/26 - 1s - loss: 0.9441 - val_loss: 1.3573
Epoch 1563/5000
26/26 - 1s - loss: 0.9437 - val_loss: 1.3569
Epoch 1564/5000
26/26 - 1s - loss: 0.9416 - val_loss: 1.3580
Epoch 1565/5000
26/26 - 1s - loss: 0.9422 - val_loss: 1.3565
Epoch 1566/5000
26/26 - 1s - loss: 0.9416 - val_loss: 1.3556
Epoch 1567/5000
26/26 - 1s - loss: 0.9414 - val_loss: 1.3553
Epoch 1568/5000
26/26 - 1s - loss: 0.9400 - val_loss: 1.3547
Epoch 1569/5000
26/26 - 1s - loss: 0.9391 - val_loss: 1.3561
Epoch 1570/5000
26/26 - 1s - loss: 0.9395 - val_loss: 1.3545
Epoch 01570: val_loss improved from 1.35854 to 1.35449, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 1571/5000
26/26 - 1s - loss: 0.9392 - val_loss: 1.3539
Epoch 1572/5000
26/26 - 1s - loss: 0.9400 - val_loss: 1.3538
Epoch 1573/5000
26/26 - 1s - loss: 0.9390 - val_loss: 1.3538
Epoch 1574/5000
26/26 - 1s - loss: 0.9377 - val_loss: 1.3532
Epoch 1575/5000
26/26 - 1s - loss: 0.9365 - val_loss: 1.3511
Epoch 1576/5000
26/26 - 1s - loss: 0.9355 - val_loss: 1.3514
Epoch 1577/5000
26/26 - 1s - loss: 0.9351 - val_loss: 1.3503
Epoch 1578/5000
26/26 - 1s - loss: 0.9343 - val_loss: 1.3511
Epoch 1579/5000
26/26 - 1s - loss: 0.9342 - val_loss: 1.3508
Epoch 1580/5000
26/26 - 1s - loss: 0.9334 - val_loss: 1.3496
Epoch 01580: val_loss improved from 1.35449 to 1.34955, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 1581/5000
26/26 - 1s - loss: 0.9332 - val_loss: 1.3484
Epoch 1582/5000
26/26 - 1s - loss: 0.9342 - val_loss: 1.3479
Epoch 1583/5000
26/26 - 1s - loss: 0.9318 - val_loss: 1.3478
Epoch 1584/5000
26/26 - 1s - loss: 0.9314 - val_loss: 1.3482
Epoch 1585/5000
26/26 - 1s - loss: 0.9337 - val_loss: 1.3469
Epoch 1586/5000
26/26 - 1s - loss: 0.9297 - val_loss: 1.3475
Epoch 1587/5000
26/26 - 1s - loss: 0.9313 - val_loss: 1.3461
Epoch 1588/5000
26/26 - 1s - loss: 0.9302 - val_loss: 1.3466
Epoch 1589/5000
26/26 - 1s - loss: 0.9292 - val_loss: 1.3457
Epoch 1590/5000
26/26 - 1s - loss: 0.9298 - val_loss: 1.3446
Epoch 01590: val_loss improved from 1.34955 to 1.34458, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 1591/5000
26/26 - 1s - loss: 0.9294 - val_loss: 1.3444
Epoch 1592/5000
26/26 - 1s - loss: 0.9298 - val_loss: 1.3436
Epoch 1593/5000
26/26 - 1s - loss: 0.9284 - val_loss: 1.3431
Epoch 1594/5000
26/26 - 1s - loss: 0.9262 - val_loss: 1.3436
Epoch 1595/5000
26/26 - 1s - loss: 0.9263 - val_loss: 1.3417
Epoch 1596/5000
26/26 - 1s - loss: 0.9250 - val_loss: 1.3436
Epoch 1597/5000
26/26 - 1s - loss: 0.9253 - val_loss: 1.3406
Epoch 1598/5000
26/26 - 1s - loss: 0.9259 - val_loss: 1.3417
Epoch 1599/5000
26/26 - 1s - loss: 0.9239 - val_loss: 1.3403
Epoch 1600/5000
26/26 - 1s - loss: 0.9240 - val_loss: 1.3401
Epoch 01600: val_loss improved from 1.34458 to 1.34009, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 1601/5000
26/26 - 1s - loss: 0.9240 - val_loss: 1.3384
Epoch 1602/5000
26/26 - 1s - loss: 0.9251 - val_loss: 1.3390
Epoch 1603/5000
26/26 - 1s - loss: 0.9233 - val_loss: 1.3397
Epoch 1604/5000
26/26 - 1s - loss: 0.9239 - val_loss: 1.3380
Epoch 1605/5000
26/26 - 1s - loss: 0.9222 - val_loss: 1.3372
Epoch 1606/5000
26/26 - 1s - loss: 0.9208 - val_loss: 1.3381
Epoch 1607/5000
26/26 - 1s - loss: 0.9194 - val_loss: 1.3379
Epoch 1608/5000
26/26 - 1s - loss: 0.9200 - val_loss: 1.3361
Epoch 1609/5000
26/26 - 1s - loss: 0.9207 - val_loss: 1.3346
Epoch 1610/5000
26/26 - 1s - loss: 0.9198 - val_loss: 1.3347
Epoch 01610: val_loss improved from 1.34009 to 1.33473, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 1611/5000
26/26 - 1s - loss: 0.9189 - val_loss: 1.3349
Epoch 1612/5000
26/26 - 1s - loss: 0.9168 - val_loss: 1.3352
Epoch 1613/5000
26/26 - 1s - loss: 0.9180 - val_loss: 1.3353
Epoch 1614/5000
26/26 - 1s - loss: 0.9176 - val_loss: 1.3331
Epoch 1615/5000
26/26 - 1s - loss: 0.9166 - val_loss: 1.3333
Epoch 1616/5000
26/26 - 1s - loss: 0.9150 - val_loss: 1.3316
Epoch 1617/5000
26/26 - 1s - loss: 0.9170 - val_loss: 1.3312
Epoch 1618/5000
26/26 - 1s - loss: 0.9148 - val_loss: 1.3306
Epoch 1619/5000
26/26 - 1s - loss: 0.9150 - val_loss: 1.3289
Epoch 1620/5000
26/26 - 1s - loss: 0.9142 - val_loss: 1.3315
Epoch 01620: val_loss improved from 1.33473 to 1.33148, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 1621/5000
26/26 - 1s - loss: 0.9135 - val_loss: 1.3301
Epoch 1622/5000
26/26 - 1s - loss: 0.9134 - val_loss: 1.3284
Epoch 1623/5000
26/26 - 1s - loss: 0.9135 - val_loss: 1.3285
Epoch 1624/5000
26/26 - 1s - loss: 0.9131 - val_loss: 1.3267
Epoch 1625/5000
26/26 - 1s - loss: 0.9111 - val_loss: 1.3276
Epoch 1626/5000
26/26 - 1s - loss: 0.9103 - val_loss: 1.3296
Epoch 1627/5000
26/26 - 1s - loss: 0.9108 - val_loss: 1.3279
Epoch 1628/5000
26/26 - 1s - loss: 0.9103 - val_loss: 1.3260
Epoch 1629/5000
26/26 - 1s - loss: 0.9109 - val_loss: 1.3260
Epoch 1630/5000
26/26 - 1s - loss: 0.9082 - val_loss: 1.3239
Epoch 01630: val_loss improved from 1.33148 to 1.32386, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 1631/5000
26/26 - 1s - loss: 0.9100 - val_loss: 1.3243
Epoch 1632/5000
26/26 - 1s - loss: 0.9088 - val_loss: 1.3228
Epoch 1633/5000
26/26 - 2s - loss: 0.9079 - val_loss: 1.3230
Epoch 1634/5000
26/26 - 1s - loss: 0.9072 - val_loss: 1.3224
Epoch 1635/5000
26/26 - 1s - loss: 0.9065 - val_loss: 1.3233
Epoch 1636/5000
26/26 - 1s - loss: 0.9073 - val_loss: 1.3224
Epoch 1637/5000
26/26 - 1s - loss: 0.9058 - val_loss: 1.3216
Epoch 1638/5000
26/26 - 1s - loss: 0.9055 - val_loss: 1.3216
Epoch 1639/5000
26/26 - 1s - loss: 0.9051 - val_loss: 1.3219
Epoch 1640/5000
26/26 - 1s - loss: 0.9048 - val_loss: 1.3210
Epoch 01640: val_loss improved from 1.32386 to 1.32099, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 1641/5000
26/26 - 1s - loss: 0.9037 - val_loss: 1.3219
Epoch 1642/5000
26/26 - 1s - loss: 0.9027 - val_loss: 1.3206
Epoch 1643/5000
26/26 - 1s - loss: 0.9031 - val_loss: 1.3203
Epoch 1644/5000
26/26 - 1s - loss: 0.9022 - val_loss: 1.3199
Epoch 1645/5000
26/26 - 1s - loss: 0.9034 - val_loss: 1.3170
Epoch 1646/5000
26/26 - 1s - loss: 0.9029 - val_loss: 1.3168
Epoch 1647/5000
26/26 - 1s - loss: 0.9014 - val_loss: 1.3171
Epoch 1648/5000
26/26 - 1s - loss: 0.9007 - val_loss: 1.3158
Epoch 1649/5000
26/26 - 1s - loss: 0.9004 - val_loss: 1.3160
Epoch 1650/5000
26/26 - 1s - loss: 0.8993 - val_loss: 1.3159
Epoch 01650: val_loss improved from 1.32099 to 1.31593, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 1651/5000
26/26 - 1s - loss: 0.9000 - val_loss: 1.3161
Epoch 1652/5000
26/26 - 1s - loss: 0.8987 - val_loss: 1.3159
Epoch 1653/5000
26/26 - 1s - loss: 0.8984 - val_loss: 1.3154
Epoch 1654/5000
26/26 - 1s - loss: 0.8984 - val_loss: 1.3145
Epoch 1655/5000
26/26 - 1s - loss: 0.8994 - val_loss: 1.3141
Epoch 1656/5000
26/26 - 1s - loss: 0.8968 - val_loss: 1.3136
Epoch 1657/5000
26/26 - 1s - loss: 0.8965 - val_loss: 1.3130
Epoch 1658/5000
26/26 - 1s - loss: 0.8969 - val_loss: 1.3130
Epoch 1659/5000
26/26 - 1s - loss: 0.8964 - val_loss: 1.3119
Epoch 1660/5000
26/26 - 1s - loss: 0.8935 - val_loss: 1.3134
Epoch 01660: val_loss improved from 1.31593 to 1.31338, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 1661/5000
26/26 - 1s - loss: 0.8951 - val_loss: 1.3130
Epoch 1662/5000
26/26 - 1s - loss: 0.8938 - val_loss: 1.3098
Epoch 1663/5000
26/26 - 1s - loss: 0.8960 - val_loss: 1.3112
Epoch 1664/5000
26/26 - 1s - loss: 0.8932 - val_loss: 1.3104
Epoch 1665/5000
26/26 - 1s - loss: 0.8943 - val_loss: 1.3106
Epoch 1666/5000
26/26 - 1s - loss: 0.8924 - val_loss: 1.3099
Epoch 1667/5000
26/26 - 1s - loss: 0.8931 - val_loss: 1.3088
Epoch 1668/5000
26/26 - 1s - loss: 0.8915 - val_loss: 1.3082
Epoch 1669/5000
26/26 - 1s - loss: 0.8930 - val_loss: 1.3087
Epoch 1670/5000
26/26 - 1s - loss: 0.8902 - val_loss: 1.3060
Epoch 01670: val_loss improved from 1.31338 to 1.30600, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 1671/5000
26/26 - 1s - loss: 0.8901 - val_loss: 1.3064
Epoch 1672/5000
26/26 - 1s - loss: 0.8890 - val_loss: 1.3068
Epoch 1673/5000
26/26 - 1s - loss: 0.8898 - val_loss: 1.3059
Epoch 1674/5000
26/26 - 2s - loss: 0.8888 - val_loss: 1.3043
Epoch 1675/5000
26/26 - 1s - loss: 0.8866 - val_loss: 1.3051
Epoch 1676/5000
26/26 - 1s - loss: 0.8880 - val_loss: 1.3034
Epoch 1677/5000
26/26 - 1s - loss: 0.8878 - val_loss: 1.3034
Epoch 1678/5000
26/26 - 1s - loss: 0.8880 - val_loss: 1.3048
Epoch 1679/5000
26/26 - 1s - loss: 0.8869 - val_loss: 1.3026
Epoch 1680/5000
26/26 - 1s - loss: 0.8853 - val_loss: 1.3020
Epoch 01680: val_loss improved from 1.30600 to 1.30197, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 1681/5000
26/26 - 1s - loss: 0.8861 - val_loss: 1.3009
Epoch 1682/5000
26/26 - 1s - loss: 0.8842 - val_loss: 1.3006
Epoch 1683/5000
26/26 - 1s - loss: 0.8832 - val_loss: 1.3000
Epoch 1684/5000
26/26 - 1s - loss: 0.8838 - val_loss: 1.3001
Epoch 1685/5000
26/26 - 1s - loss: 0.8844 - val_loss: 1.2991
Epoch 1686/5000
26/26 - 1s - loss: 0.8836 - val_loss: 1.2998
Epoch 1687/5000
26/26 - 1s - loss: 0.8818 - val_loss: 1.2989
Epoch 1688/5000
26/26 - 1s - loss: 0.8824 - val_loss: 1.2982
Epoch 1689/5000
26/26 - 1s - loss: 0.8819 - val_loss: 1.2998
Epoch 1690/5000
26/26 - 1s - loss: 0.8805 - val_loss: 1.2974
Epoch 01690: val_loss improved from 1.30197 to 1.29743, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 1691/5000
26/26 - 1s - loss: 0.8807 - val_loss: 1.2964
Epoch 1692/5000
26/26 - 1s - loss: 0.8805 - val_loss: 1.2973
Epoch 1693/5000
26/26 - 1s - loss: 0.8789 - val_loss: 1.2986
Epoch 1694/5000
26/26 - 1s - loss: 0.8787 - val_loss: 1.2953
Epoch 1695/5000
26/26 - 1s - loss: 0.8795 - val_loss: 1.2961
Epoch 1696/5000
26/26 - 1s - loss: 0.8779 - val_loss: 1.2954
Epoch 1697/5000
26/26 - 1s - loss: 0.8776 - val_loss: 1.2966
Epoch 1698/5000
26/26 - 1s - loss: 0.8778 - val_loss: 1.2941
Epoch 1699/5000
26/26 - 1s - loss: 0.8775 - val_loss: 1.2946
Epoch 1700/5000
26/26 - 1s - loss: 0.8766 - val_loss: 1.2950
Epoch 01700: val_loss improved from 1.29743 to 1.29504, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 1701/5000
26/26 - 1s - loss: 0.8785 - val_loss: 1.2929
Epoch 1702/5000
26/26 - 1s - loss: 0.8755 - val_loss: 1.2916
Epoch 1703/5000
26/26 - 1s - loss: 0.8758 - val_loss: 1.2921
Epoch 1704/5000
26/26 - 1s - loss: 0.8758 - val_loss: 1.2911
Epoch 1705/5000
26/26 - 1s - loss: 0.8763 - val_loss: 1.2901
Epoch 1706/5000
26/26 - 1s - loss: 0.8752 - val_loss: 1.2926
Epoch 1707/5000
26/26 - 1s - loss: 0.8740 - val_loss: 1.2898
Epoch 1708/5000
26/26 - 1s - loss: 0.8741 - val_loss: 1.2891
Epoch 1709/5000
26/26 - 1s - loss: 0.8747 - val_loss: 1.2910
Epoch 1710/5000
26/26 - 1s - loss: 0.8714 - val_loss: 1.2901
Epoch 01710: val_loss improved from 1.29504 to 1.29006, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 1711/5000
26/26 - 1s - loss: 0.8717 - val_loss: 1.2889
Epoch 1712/5000
26/26 - 1s - loss: 0.8726 - val_loss: 1.2892
Epoch 1713/5000
26/26 - 1s - loss: 0.8722 - val_loss: 1.2892
Epoch 1714/5000
26/26 - 1s - loss: 0.8710 - val_loss: 1.2888
Epoch 1715/5000
26/26 - 1s - loss: 0.8690 - val_loss: 1.2866
Epoch 1716/5000
26/26 - 1s - loss: 0.8693 - val_loss: 1.2870
Epoch 1717/5000
26/26 - 1s - loss: 0.8693 - val_loss: 1.2848
Epoch 1718/5000
26/26 - 1s - loss: 0.8673 - val_loss: 1.2849
Epoch 1719/5000
26/26 - 1s - loss: 0.8674 - val_loss: 1.2845
Epoch 1720/5000
26/26 - 1s - loss: 0.8686 - val_loss: 1.2849
Epoch 01720: val_loss improved from 1.29006 to 1.28493, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 1721/5000
26/26 - 1s - loss: 0.8693 - val_loss: 1.2834
Epoch 1722/5000
26/26 - 1s - loss: 0.8680 - val_loss: 1.2828
Epoch 1723/5000
26/26 - 1s - loss: 0.8661 - val_loss: 1.2829
Epoch 1724/5000
26/26 - 1s - loss: 0.8662 - val_loss: 1.2826
Epoch 1725/5000
26/26 - 1s - loss: 0.8664 - val_loss: 1.2810
Epoch 1726/5000
26/26 - 1s - loss: 0.8654 - val_loss: 1.2811
Epoch 1727/5000
26/26 - 2s - loss: 0.8656 - val_loss: 1.2801
Epoch 1728/5000
26/26 - 1s - loss: 0.8643 - val_loss: 1.2796
Epoch 1729/5000
26/26 - 1s - loss: 0.8636 - val_loss: 1.2799
Epoch 1730/5000
26/26 - 1s - loss: 0.8650 - val_loss: 1.2806
Epoch 01730: val_loss improved from 1.28493 to 1.28059, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 1731/5000
26/26 - 1s - loss: 0.8640 - val_loss: 1.2788
Epoch 1732/5000
26/26 - 1s - loss: 0.8616 - val_loss: 1.2781
Epoch 1733/5000
26/26 - 1s - loss: 0.8632 - val_loss: 1.2787
Epoch 1734/5000
26/26 - 1s - loss: 0.8633 - val_loss: 1.2773
Epoch 1735/5000
26/26 - 1s - loss: 0.8609 - val_loss: 1.2770
Epoch 1736/5000
26/26 - 1s - loss: 0.8611 - val_loss: 1.2769
Epoch 1737/5000
26/26 - 1s - loss: 0.8614 - val_loss: 1.2760
Epoch 1738/5000
26/26 - 1s - loss: 0.8603 - val_loss: 1.2772
Epoch 1739/5000
26/26 - 1s - loss: 0.8600 - val_loss: 1.2761
Epoch 1740/5000
26/26 - 1s - loss: 0.8587 - val_loss: 1.2758
Epoch 01740: val_loss improved from 1.28059 to 1.27577, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 1741/5000
26/26 - 1s - loss: 0.8581 - val_loss: 1.2749
Epoch 1742/5000
26/26 - 1s - loss: 0.8588 - val_loss: 1.2762
Epoch 1743/5000
26/26 - 1s - loss: 0.8588 - val_loss: 1.2759
Epoch 1744/5000
26/26 - 1s - loss: 0.8582 - val_loss: 1.2740
Epoch 1745/5000
26/26 - 1s - loss: 0.8561 - val_loss: 1.2742
Epoch 1746/5000
26/26 - 1s - loss: 0.8556 - val_loss: 1.2747
Epoch 1747/5000
26/26 - 1s - loss: 0.8561 - val_loss: 1.2736
Epoch 1748/5000
26/26 - 1s - loss: 0.8568 - val_loss: 1.2741
Epoch 1749/5000
26/26 - 1s - loss: 0.8562 - val_loss: 1.2716
Epoch 1750/5000
26/26 - 1s - loss: 0.8564 - val_loss: 1.2716
Epoch 01750: val_loss improved from 1.27577 to 1.27156, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 1751/5000
26/26 - 1s - loss: 0.8536 - val_loss: 1.2710
Epoch 1752/5000
26/26 - 1s - loss: 0.8545 - val_loss: 1.2706
Epoch 1753/5000
26/26 - 1s - loss: 0.8552 - val_loss: 1.2702
Epoch 1754/5000
26/26 - 1s - loss: 0.8540 - val_loss: 1.2701
Epoch 1755/5000
26/26 - 1s - loss: 0.8522 - val_loss: 1.2715
Epoch 1756/5000
26/26 - 1s - loss: 0.8531 - val_loss: 1.2703
Epoch 1757/5000
26/26 - 1s - loss: 0.8519 - val_loss: 1.2688
Epoch 1758/5000
26/26 - 1s - loss: 0.8501 - val_loss: 1.2675
Epoch 1759/5000
26/26 - 1s - loss: 0.8494 - val_loss: 1.2680
Epoch 1760/5000
26/26 - 1s - loss: 0.8501 - val_loss: 1.2682
Epoch 01760: val_loss improved from 1.27156 to 1.26818, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 1761/5000
26/26 - 1s - loss: 0.8504 - val_loss: 1.2683
Epoch 1762/5000
26/26 - 1s - loss: 0.8509 - val_loss: 1.2657
Epoch 1763/5000
26/26 - 1s - loss: 0.8489 - val_loss: 1.2657
Epoch 1764/5000
26/26 - 1s - loss: 0.8482 - val_loss: 1.2650
Epoch 1765/5000
26/26 - 1s - loss: 0.8470 - val_loss: 1.2658
Epoch 1766/5000
26/26 - 1s - loss: 0.8471 - val_loss: 1.2644
Epoch 1767/5000
26/26 - 1s - loss: 0.8477 - val_loss: 1.2643
Epoch 1768/5000
26/26 - 1s - loss: 0.8482 - val_loss: 1.2638
Epoch 1769/5000
26/26 - 1s - loss: 0.8460 - val_loss: 1.2643
Epoch 1770/5000
26/26 - 1s - loss: 0.8479 - val_loss: 1.2624
Epoch 01770: val_loss improved from 1.26818 to 1.26242, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 1771/5000
26/26 - 1s - loss: 0.8450 - val_loss: 1.2627
Epoch 1772/5000
26/26 - 1s - loss: 0.8444 - val_loss: 1.2607
Epoch 1773/5000
26/26 - 1s - loss: 0.8444 - val_loss: 1.2603
Epoch 1774/5000
26/26 - 1s - loss: 0.8451 - val_loss: 1.2604
Epoch 1775/5000
26/26 - 1s - loss: 0.8435 - val_loss: 1.2609
Epoch 1776/5000
26/26 - 1s - loss: 0.8448 - val_loss: 1.2594
Epoch 1777/5000
26/26 - 1s - loss: 0.8448 - val_loss: 1.2604
Epoch 1778/5000
26/26 - 1s - loss: 0.8436 - val_loss: 1.2598
Epoch 1779/5000
26/26 - 1s - loss: 0.8430 - val_loss: 1.2591
Epoch 1780/5000
26/26 - 1s - loss: 0.8418 - val_loss: 1.2584
Epoch 01780: val_loss improved from 1.26242 to 1.25840, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 1781/5000
26/26 - 1s - loss: 0.8428 - val_loss: 1.2577
Epoch 1782/5000
26/26 - 1s - loss: 0.8410 - val_loss: 1.2579
Epoch 1783/5000
26/26 - 1s - loss: 0.8410 - val_loss: 1.2591
Epoch 1784/5000
26/26 - 1s - loss: 0.8405 - val_loss: 1.2573
Epoch 1785/5000
26/26 - 1s - loss: 0.8419 - val_loss: 1.2558
Epoch 1786/5000
26/26 - 1s - loss: 0.8400 - val_loss: 1.2573
Epoch 1787/5000
26/26 - 1s - loss: 0.8400 - val_loss: 1.2564
Epoch 1788/5000
26/26 - 1s - loss: 0.8391 - val_loss: 1.2569
Epoch 1789/5000
26/26 - 1s - loss: 0.8380 - val_loss: 1.2553
Epoch 1790/5000
26/26 - 1s - loss: 0.8381 - val_loss: 1.2565
Epoch 01790: val_loss improved from 1.25840 to 1.25654, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 1791/5000
26/26 - 1s - loss: 0.8385 - val_loss: 1.2549
Epoch 1792/5000
26/26 - 1s - loss: 0.8376 - val_loss: 1.2530
Epoch 1793/5000
26/26 - 1s - loss: 0.8377 - val_loss: 1.2523
Epoch 1794/5000
26/26 - 1s - loss: 0.8386 - val_loss: 1.2524
Epoch 1795/5000
26/26 - 1s - loss: 0.8359 - val_loss: 1.2518
Epoch 1796/5000
26/26 - 1s - loss: 0.8354 - val_loss: 1.2527
Epoch 1797/5000
26/26 - 1s - loss: 0.8355 - val_loss: 1.2514
Epoch 1798/5000
26/26 - 1s - loss: 0.8353 - val_loss: 1.2497
Epoch 1799/5000
26/26 - 1s - loss: 0.8339 - val_loss: 1.2502
Epoch 1800/5000
26/26 - 1s - loss: 0.8350 - val_loss: 1.2500
Epoch 01800: val_loss improved from 1.25654 to 1.25004, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 1801/5000
26/26 - 2s - loss: 0.8345 - val_loss: 1.2499
Epoch 1802/5000
26/26 - 1s - loss: 0.8336 - val_loss: 1.2482
Epoch 1803/5000
26/26 - 1s - loss: 0.8324 - val_loss: 1.2473
Epoch 1804/5000
26/26 - 1s - loss: 0.8333 - val_loss: 1.2485
Epoch 1805/5000
26/26 - 1s - loss: 0.8314 - val_loss: 1.2484
Epoch 1806/5000
26/26 - 1s - loss: 0.8321 - val_loss: 1.2489
Epoch 1807/5000
26/26 - 1s - loss: 0.8322 - val_loss: 1.2477
Epoch 1808/5000
26/26 - 1s - loss: 0.8314 - val_loss: 1.2476
Epoch 1809/5000
26/26 - 1s - loss: 0.8312 - val_loss: 1.2465
Epoch 1810/5000
26/26 - 1s - loss: 0.8309 - val_loss: 1.2458
Epoch 01810: val_loss improved from 1.25004 to 1.24582, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 1811/5000
26/26 - 1s - loss: 0.8312 - val_loss: 1.2456
Epoch 1812/5000
26/26 - 1s - loss: 0.8282 - val_loss: 1.2463
Epoch 1813/5000
26/26 - 1s - loss: 0.8294 - val_loss: 1.2464
Epoch 1814/5000
26/26 - 1s - loss: 0.8293 - val_loss: 1.2459
Epoch 1815/5000
26/26 - 1s - loss: 0.8276 - val_loss: 1.2466
Epoch 1816/5000
26/26 - 1s - loss: 0.8275 - val_loss: 1.2465
Epoch 1817/5000
26/26 - 1s - loss: 0.8269 - val_loss: 1.2445
Epoch 1818/5000
26/26 - 1s - loss: 0.8269 - val_loss: 1.2458
Epoch 1819/5000
26/26 - 1s - loss: 0.8257 - val_loss: 1.2448
Epoch 1820/5000
26/26 - 1s - loss: 0.8274 - val_loss: 1.2441
Epoch 01820: val_loss improved from 1.24582 to 1.24409, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 1821/5000
26/26 - 1s - loss: 0.8249 - val_loss: 1.2436
Epoch 1822/5000
26/26 - 1s - loss: 0.8259 - val_loss: 1.2424
Epoch 1823/5000
26/26 - 1s - loss: 0.8256 - val_loss: 1.2411
Epoch 1824/5000
26/26 - 1s - loss: 0.8262 - val_loss: 1.2425
Epoch 1825/5000
26/26 - 1s - loss: 0.8252 - val_loss: 1.2407
Epoch 1826/5000
26/26 - 1s - loss: 0.8243 - val_loss: 1.2406
Epoch 1827/5000
26/26 - 1s - loss: 0.8236 - val_loss: 1.2394
Epoch 1828/5000
26/26 - 1s - loss: 0.8240 - val_loss: 1.2384
Epoch 1829/5000
26/26 - 1s - loss: 0.8233 - val_loss: 1.2384
Epoch 1830/5000
26/26 - 1s - loss: 0.8230 - val_loss: 1.2390
Epoch 01830: val_loss improved from 1.24409 to 1.23896, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 1831/5000
26/26 - 1s - loss: 0.8214 - val_loss: 1.2398
Epoch 1832/5000
26/26 - 1s - loss: 0.8226 - val_loss: 1.2388
Epoch 1833/5000
26/26 - 1s - loss: 0.8206 - val_loss: 1.2381
Epoch 1834/5000
26/26 - 1s - loss: 0.8214 - val_loss: 1.2366
Epoch 1835/5000
26/26 - 1s - loss: 0.8227 - val_loss: 1.2375
Epoch 1836/5000
26/26 - 1s - loss: 0.8204 - val_loss: 1.2375
Epoch 1837/5000
26/26 - 1s - loss: 0.8207 - val_loss: 1.2365
Epoch 1838/5000
26/26 - 1s - loss: 0.8204 - val_loss: 1.2366
Epoch 1839/5000
26/26 - 1s - loss: 0.8193 - val_loss: 1.2362
Epoch 1840/5000
26/26 - 1s - loss: 0.8167 - val_loss: 1.2372
Epoch 01840: val_loss improved from 1.23896 to 1.23724, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 1841/5000
26/26 - 1s - loss: 0.8177 - val_loss: 1.2360
Epoch 1842/5000
26/26 - 1s - loss: 0.8184 - val_loss: 1.2363
Epoch 1843/5000
26/26 - 2s - loss: 0.8169 - val_loss: 1.2350
Epoch 1844/5000
26/26 - 1s - loss: 0.8161 - val_loss: 1.2337
Epoch 1845/5000
26/26 - 1s - loss: 0.8168 - val_loss: 1.2330
Epoch 1846/5000
26/26 - 1s - loss: 0.8177 - val_loss: 1.2328
Epoch 1847/5000
26/26 - 1s - loss: 0.8161 - val_loss: 1.2319
Epoch 1848/5000
26/26 - 1s - loss: 0.8150 - val_loss: 1.2321
Epoch 1849/5000
26/26 - 1s - loss: 0.8145 - val_loss: 1.2330
Epoch 1850/5000
26/26 - 2s - loss: 0.8152 - val_loss: 1.2313
Epoch 01850: val_loss improved from 1.23724 to 1.23126, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 1851/5000
26/26 - 1s - loss: 0.8140 - val_loss: 1.2306
Epoch 1852/5000
26/26 - 1s - loss: 0.8134 - val_loss: 1.2315
Epoch 1853/5000
26/26 - 1s - loss: 0.8144 - val_loss: 1.2316
Epoch 1854/5000
26/26 - 1s - loss: 0.8119 - val_loss: 1.2306
Epoch 1855/5000
26/26 - 1s - loss: 0.8130 - val_loss: 1.2292
Epoch 1856/5000
26/26 - 1s - loss: 0.8133 - val_loss: 1.2294
Epoch 1857/5000
26/26 - 1s - loss: 0.8125 - val_loss: 1.2286
Epoch 1858/5000
26/26 - 1s - loss: 0.8109 - val_loss: 1.2295
Epoch 1859/5000
26/26 - 1s - loss: 0.8118 - val_loss: 1.2273
Epoch 1860/5000
26/26 - 1s - loss: 0.8104 - val_loss: 1.2279
Epoch 01860: val_loss improved from 1.23126 to 1.22791, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 1861/5000
26/26 - 1s - loss: 0.8093 - val_loss: 1.2287
Epoch 1862/5000
26/26 - 1s - loss: 0.8092 - val_loss: 1.2278
Epoch 1863/5000
26/26 - 1s - loss: 0.8090 - val_loss: 1.2261
Epoch 1864/5000
26/26 - 1s - loss: 0.8098 - val_loss: 1.2285
Epoch 1865/5000
26/26 - 1s - loss: 0.8099 - val_loss: 1.2262
Epoch 1866/5000
26/26 - 1s - loss: 0.8085 - val_loss: 1.2279
Epoch 1867/5000
26/26 - 1s - loss: 0.8083 - val_loss: 1.2255
Epoch 1868/5000
26/26 - 1s - loss: 0.8079 - val_loss: 1.2244
Epoch 1869/5000
26/26 - 1s - loss: 0.8085 - val_loss: 1.2256
Epoch 1870/5000
26/26 - 1s - loss: 0.8078 - val_loss: 1.2254
Epoch 01870: val_loss improved from 1.22791 to 1.22544, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 1871/5000
26/26 - 1s - loss: 0.8060 - val_loss: 1.2242
Epoch 1872/5000
26/26 - 1s - loss: 0.8060 - val_loss: 1.2250
Epoch 1873/5000
26/26 - 1s - loss: 0.8055 - val_loss: 1.2252
Epoch 1874/5000
26/26 - 1s - loss: 0.8041 - val_loss: 1.2226
Epoch 1875/5000
26/26 - 1s - loss: 0.8063 - val_loss: 1.2214
Epoch 1876/5000
26/26 - 1s - loss: 0.8058 - val_loss: 1.2238
Epoch 1877/5000
26/26 - 1s - loss: 0.8044 - val_loss: 1.2227
Epoch 1878/5000
26/26 - 1s - loss: 0.8045 - val_loss: 1.2212
Epoch 1879/5000
26/26 - 1s - loss: 0.8041 - val_loss: 1.2215
Epoch 1880/5000
26/26 - 1s - loss: 0.8049 - val_loss: 1.2209
Epoch 01880: val_loss improved from 1.22544 to 1.22093, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 1881/5000
26/26 - 1s - loss: 0.8022 - val_loss: 1.2205
Epoch 1882/5000
26/26 - 1s - loss: 0.8023 - val_loss: 1.2184
Epoch 1883/5000
26/26 - 1s - loss: 0.8022 - val_loss: 1.2194
Epoch 1884/5000
26/26 - 1s - loss: 0.8042 - val_loss: 1.2204
Epoch 1885/5000
26/26 - 1s - loss: 0.8013 - val_loss: 1.2172
Epoch 1886/5000
26/26 - 1s - loss: 0.8002 - val_loss: 1.2189
Epoch 1887/5000
26/26 - 1s - loss: 0.8013 - val_loss: 1.2183
Epoch 1888/5000
26/26 - 1s - loss: 0.8003 - val_loss: 1.2186
Epoch 1889/5000
26/26 - 1s - loss: 0.8010 - val_loss: 1.2178
Epoch 1890/5000
26/26 - 1s - loss: 0.7993 - val_loss: 1.2183
Epoch 01890: val_loss improved from 1.22093 to 1.21833, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 1891/5000
26/26 - 1s - loss: 0.7996 - val_loss: 1.2168
Epoch 1892/5000
26/26 - 1s - loss: 0.7994 - val_loss: 1.2174
Epoch 1893/5000
26/26 - 1s - loss: 0.7985 - val_loss: 1.2147
Epoch 1894/5000
26/26 - 1s - loss: 0.7995 - val_loss: 1.2142
Epoch 1895/5000
26/26 - 1s - loss: 0.7966 - val_loss: 1.2160
Epoch 1896/5000
26/26 - 1s - loss: 0.7973 - val_loss: 1.2136
Epoch 1897/5000
26/26 - 1s - loss: 0.7981 - val_loss: 1.2148
Epoch 1898/5000
26/26 - 1s - loss: 0.7972 - val_loss: 1.2144
Epoch 1899/5000
26/26 - 1s - loss: 0.7962 - val_loss: 1.2130
Epoch 1900/5000
26/26 - 1s - loss: 0.7951 - val_loss: 1.2149
Epoch 01900: val_loss improved from 1.21833 to 1.21492, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 1901/5000
26/26 - 1s - loss: 0.7944 - val_loss: 1.2135
Epoch 1902/5000
26/26 - 1s - loss: 0.7947 - val_loss: 1.2129
Epoch 1903/5000
26/26 - 1s - loss: 0.7939 - val_loss: 1.2123
Epoch 1904/5000
26/26 - 1s - loss: 0.7949 - val_loss: 1.2124
Epoch 1905/5000
26/26 - 1s - loss: 0.7937 - val_loss: 1.2129
Epoch 1906/5000
26/26 - 1s - loss: 0.7937 - val_loss: 1.2101
Epoch 1907/5000
26/26 - 1s - loss: 0.7927 - val_loss: 1.2096
Epoch 1908/5000
26/26 - 1s - loss: 0.7931 - val_loss: 1.2111
Epoch 1909/5000
26/26 - 1s - loss: 0.7944 - val_loss: 1.2096
Epoch 1910/5000
26/26 - 1s - loss: 0.7929 - val_loss: 1.2095
Epoch 01910: val_loss improved from 1.21492 to 1.20953, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 1911/5000
26/26 - 1s - loss: 0.7913 - val_loss: 1.2104
Epoch 1912/5000
26/26 - 1s - loss: 0.7911 - val_loss: 1.2105
Epoch 1913/5000
26/26 - 1s - loss: 0.7912 - val_loss: 1.2090
Epoch 1914/5000
26/26 - 1s - loss: 0.7895 - val_loss: 1.2083
Epoch 1915/5000
26/26 - 1s - loss: 0.7894 - val_loss: 1.2091
Epoch 1916/5000
26/26 - 1s - loss: 0.7905 - val_loss: 1.2089
Epoch 1917/5000
26/26 - 1s - loss: 0.7897 - val_loss: 1.2076
Epoch 1918/5000
26/26 - 1s - loss: 0.7910 - val_loss: 1.2070
Epoch 1919/5000
26/26 - 1s - loss: 0.7902 - val_loss: 1.2075
Epoch 1920/5000
26/26 - 1s - loss: 0.7887 - val_loss: 1.2087
Epoch 01920: val_loss improved from 1.20953 to 1.20868, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 1921/5000
26/26 - 1s - loss: 0.7882 - val_loss: 1.2078
Epoch 1922/5000
26/26 - 1s - loss: 0.7878 - val_loss: 1.2064
Epoch 1923/5000
26/26 - 1s - loss: 0.7880 - val_loss: 1.2069
Epoch 1924/5000
26/26 - 1s - loss: 0.7872 - val_loss: 1.2033
Epoch 1925/5000
26/26 - 1s - loss: 0.7854 - val_loss: 1.2034
Epoch 1926/5000
26/26 - 1s - loss: 0.7879 - val_loss: 1.2035
Epoch 1927/5000
26/26 - 1s - loss: 0.7849 - val_loss: 1.2034
Epoch 1928/5000
26/26 - 1s - loss: 0.7863 - val_loss: 1.2038
Epoch 1929/5000
26/26 - 1s - loss: 0.7854 - val_loss: 1.2020
Epoch 1930/5000
26/26 - 1s - loss: 0.7850 - val_loss: 1.2028
Epoch 01930: val_loss improved from 1.20868 to 1.20279, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 1931/5000
26/26 - 1s - loss: 0.7846 - val_loss: 1.2019
Epoch 1932/5000
26/26 - 1s - loss: 0.7843 - val_loss: 1.2045
Epoch 1933/5000
26/26 - 1s - loss: 0.7847 - val_loss: 1.2021
Epoch 1934/5000
26/26 - 1s - loss: 0.7831 - val_loss: 1.2000
Epoch 1935/5000
26/26 - 1s - loss: 0.7828 - val_loss: 1.2009
Epoch 1936/5000
26/26 - 1s - loss: 0.7814 - val_loss: 1.2002
Epoch 1937/5000
26/26 - 1s - loss: 0.7825 - val_loss: 1.2000
Epoch 1938/5000
26/26 - 1s - loss: 0.7811 - val_loss: 1.1996
Epoch 1939/5000
26/26 - 1s - loss: 0.7813 - val_loss: 1.1999
Epoch 1940/5000
26/26 - 1s - loss: 0.7828 - val_loss: 1.2008
Epoch 01940: val_loss improved from 1.20279 to 1.20078, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 1941/5000
26/26 - 1s - loss: 0.7800 - val_loss: 1.1994
Epoch 1942/5000
26/26 - 1s - loss: 0.7810 - val_loss: 1.1996
Epoch 1943/5000
26/26 - 1s - loss: 0.7797 - val_loss: 1.1975
Epoch 1944/5000
26/26 - 1s - loss: 0.7797 - val_loss: 1.1982
Epoch 1945/5000
26/26 - 1s - loss: 0.7806 - val_loss: 1.1966
Epoch 1946/5000
26/26 - 1s - loss: 0.7787 - val_loss: 1.1953
Epoch 1947/5000
26/26 - 1s - loss: 0.7794 - val_loss: 1.1948
Epoch 1948/5000
26/26 - 1s - loss: 0.7777 - val_loss: 1.1948
Epoch 1949/5000
26/26 - 1s - loss: 0.7786 - val_loss: 1.1952
Epoch 1950/5000
26/26 - 1s - loss: 0.7793 - val_loss: 1.1945
Epoch 01950: val_loss improved from 1.20078 to 1.19448, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 1951/5000
26/26 - 1s - loss: 0.7781 - val_loss: 1.1970
Epoch 1952/5000
26/26 - 1s - loss: 0.7762 - val_loss: 1.1946
Epoch 1953/5000
26/26 - 1s - loss: 0.7766 - val_loss: 1.1943
Epoch 1954/5000
26/26 - 1s - loss: 0.7773 - val_loss: 1.1948
Epoch 1955/5000
26/26 - 1s - loss: 0.7766 - val_loss: 1.1944
Epoch 1956/5000
26/26 - 1s - loss: 0.7762 - val_loss: 1.1937
Epoch 1957/5000
26/26 - 1s - loss: 0.7752 - val_loss: 1.1926
Epoch 1958/5000
26/26 - 1s - loss: 0.7748 - val_loss: 1.1956
Epoch 1959/5000
26/26 - 1s - loss: 0.7756 - val_loss: 1.1931
Epoch 1960/5000
26/26 - 1s - loss: 0.7733 - val_loss: 1.1918
Epoch 01960: val_loss improved from 1.19448 to 1.19183, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 1961/5000
26/26 - 1s - loss: 0.7751 - val_loss: 1.1923
Epoch 1962/5000
26/26 - 1s - loss: 0.7736 - val_loss: 1.1901
Epoch 1963/5000
26/26 - 1s - loss: 0.7734 - val_loss: 1.1907
Epoch 1964/5000
26/26 - 1s - loss: 0.7743 - val_loss: 1.1914
Epoch 1965/5000
26/26 - 1s - loss: 0.7731 - val_loss: 1.1916
Epoch 1966/5000
26/26 - 1s - loss: 0.7737 - val_loss: 1.1921
Epoch 1967/5000
26/26 - 1s - loss: 0.7731 - val_loss: 1.1899
Epoch 1968/5000
26/26 - 1s - loss: 0.7718 - val_loss: 1.1902
Epoch 1969/5000
26/26 - 1s - loss: 0.7721 - val_loss: 1.1896
Epoch 1970/5000
26/26 - 1s - loss: 0.7708 - val_loss: 1.1901
Epoch 01970: val_loss improved from 1.19183 to 1.19013, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 1971/5000
26/26 - 1s - loss: 0.7724 - val_loss: 1.1897
Epoch 1972/5000
26/26 - 1s - loss: 0.7713 - val_loss: 1.1874
Epoch 1973/5000
26/26 - 1s - loss: 0.7705 - val_loss: 1.1888
Epoch 1974/5000
26/26 - 1s - loss: 0.7703 - val_loss: 1.1869
Epoch 1975/5000
26/26 - 1s - loss: 0.7691 - val_loss: 1.1880
Epoch 1976/5000
26/26 - 1s - loss: 0.7696 - val_loss: 1.1861
Epoch 1977/5000
26/26 - 1s - loss: 0.7687 - val_loss: 1.1862
Epoch 1978/5000
26/26 - 1s - loss: 0.7681 - val_loss: 1.1855
Epoch 1979/5000
26/26 - 1s - loss: 0.7693 - val_loss: 1.1853
Epoch 1980/5000
26/26 - 1s - loss: 0.7668 - val_loss: 1.1843
Epoch 01980: val_loss improved from 1.19013 to 1.18428, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 1981/5000
26/26 - 1s - loss: 0.7682 - val_loss: 1.1845
Epoch 1982/5000
26/26 - 1s - loss: 0.7662 - val_loss: 1.1841
Epoch 1983/5000
26/26 - 1s - loss: 0.7677 - val_loss: 1.1847
Epoch 1984/5000
26/26 - 1s - loss: 0.7662 - val_loss: 1.1850
Epoch 1985/5000
26/26 - 1s - loss: 0.7684 - val_loss: 1.1844
Epoch 1986/5000
26/26 - 1s - loss: 0.7651 - val_loss: 1.1831
Epoch 1987/5000
26/26 - 1s - loss: 0.7650 - val_loss: 1.1834
Epoch 1988/5000
26/26 - 1s - loss: 0.7633 - val_loss: 1.1819
Epoch 1989/5000
26/26 - 1s - loss: 0.7654 - val_loss: 1.1824
Epoch 1990/5000
26/26 - 1s - loss: 0.7637 - val_loss: 1.1827
Epoch 01990: val_loss improved from 1.18428 to 1.18268, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 1991/5000
26/26 - 1s - loss: 0.7643 - val_loss: 1.1810
Epoch 1992/5000
26/26 - 1s - loss: 0.7635 - val_loss: 1.1816
Epoch 1993/5000
26/26 - 1s - loss: 0.7639 - val_loss: 1.1809
Epoch 1994/5000
26/26 - 1s - loss: 0.7627 - val_loss: 1.1820
Epoch 1995/5000
26/26 - 1s - loss: 0.7636 - val_loss: 1.1796
Epoch 1996/5000
26/26 - 1s - loss: 0.7620 - val_loss: 1.1798
Epoch 1997/5000
26/26 - 1s - loss: 0.7608 - val_loss: 1.1798
Epoch 1998/5000
26/26 - 1s - loss: 0.7609 - val_loss: 1.1794
Epoch 1999/5000
26/26 - 1s - loss: 0.7615 - val_loss: 1.1770
Epoch 2000/5000
26/26 - 1s - loss: 0.7617 - val_loss: 1.1787
Epoch 02000: val_loss improved from 1.18268 to 1.17865, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 2001/5000
26/26 - 1s - loss: 0.7605 - val_loss: 1.1784
Epoch 2002/5000
26/26 - 1s - loss: 0.7608 - val_loss: 1.1778
Epoch 2003/5000
26/26 - 1s - loss: 0.7602 - val_loss: 1.1772
Epoch 2004/5000
26/26 - 1s - loss: 0.7597 - val_loss: 1.1774
Epoch 2005/5000
26/26 - 1s - loss: 0.7593 - val_loss: 1.1776
Epoch 2006/5000
26/26 - 1s - loss: 0.7602 - val_loss: 1.1750
Epoch 2007/5000
26/26 - 1s - loss: 0.7579 - val_loss: 1.1756
Epoch 2008/5000
26/26 - 2s - loss: 0.7587 - val_loss: 1.1784
Epoch 2009/5000
26/26 - 1s - loss: 0.7599 - val_loss: 1.1763
Epoch 2010/5000
26/26 - 2s - loss: 0.7562 - val_loss: 1.1749
Epoch 02010: val_loss improved from 1.17865 to 1.17495, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 2011/5000
26/26 - 1s - loss: 0.7566 - val_loss: 1.1743
Epoch 2012/5000
26/26 - 1s - loss: 0.7573 - val_loss: 1.1745
Epoch 2013/5000
26/26 - 1s - loss: 0.7568 - val_loss: 1.1743
Epoch 2014/5000
26/26 - 1s - loss: 0.7562 - val_loss: 1.1731
Epoch 2015/5000
26/26 - 1s - loss: 0.7563 - val_loss: 1.1739
Epoch 2016/5000
26/26 - 1s - loss: 0.7567 - val_loss: 1.1714
Epoch 2017/5000
26/26 - 1s - loss: 0.7557 - val_loss: 1.1725
Epoch 2018/5000
26/26 - 1s - loss: 0.7560 - val_loss: 1.1735
Epoch 2019/5000
26/26 - 1s - loss: 0.7554 - val_loss: 1.1739
Epoch 2020/5000
26/26 - 1s - loss: 0.7545 - val_loss: 1.1718
Epoch 02020: val_loss improved from 1.17495 to 1.17185, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 2021/5000
26/26 - 1s - loss: 0.7538 - val_loss: 1.1725
Epoch 2022/5000
26/26 - 1s - loss: 0.7547 - val_loss: 1.1731
Epoch 2023/5000
26/26 - 1s - loss: 0.7535 - val_loss: 1.1704
Epoch 2024/5000
26/26 - 1s - loss: 0.7528 - val_loss: 1.1701
Epoch 2025/5000
26/26 - 1s - loss: 0.7544 - val_loss: 1.1714
Epoch 2026/5000
26/26 - 1s - loss: 0.7535 - val_loss: 1.1704
Epoch 2027/5000
26/26 - 2s - loss: 0.7504 - val_loss: 1.1706
Epoch 2028/5000
26/26 - 1s - loss: 0.7526 - val_loss: 1.1701
Epoch 2029/5000
26/26 - 1s - loss: 0.7520 - val_loss: 1.1698
Epoch 2030/5000
26/26 - 1s - loss: 0.7506 - val_loss: 1.1690
Epoch 02030: val_loss improved from 1.17185 to 1.16902, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 2031/5000
26/26 - 1s - loss: 0.7506 - val_loss: 1.1699
Epoch 2032/5000
26/26 - 1s - loss: 0.7514 - val_loss: 1.1679
Epoch 2033/5000
26/26 - 1s - loss: 0.7494 - val_loss: 1.1680
Epoch 2034/5000
26/26 - 1s - loss: 0.7506 - val_loss: 1.1668
Epoch 2035/5000
26/26 - 1s - loss: 0.7491 - val_loss: 1.1668
Epoch 2036/5000
26/26 - 1s - loss: 0.7489 - val_loss: 1.1663
Epoch 2037/5000
26/26 - 1s - loss: 0.7491 - val_loss: 1.1660
Epoch 2038/5000
26/26 - 1s - loss: 0.7490 - val_loss: 1.1677
Epoch 2039/5000
26/26 - 1s - loss: 0.7480 - val_loss: 1.1664
Epoch 2040/5000
26/26 - 2s - loss: 0.7471 - val_loss: 1.1658
Epoch 02040: val_loss improved from 1.16902 to 1.16579, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 2041/5000
26/26 - 1s - loss: 0.7472 - val_loss: 1.1656
Epoch 2042/5000
26/26 - 1s - loss: 0.7472 - val_loss: 1.1648
Epoch 2043/5000
26/26 - 1s - loss: 0.7473 - val_loss: 1.1658
Epoch 2044/5000
26/26 - 1s - loss: 0.7458 - val_loss: 1.1647
Epoch 2045/5000
26/26 - 1s - loss: 0.7464 - val_loss: 1.1636
Epoch 2046/5000
26/26 - 1s - loss: 0.7458 - val_loss: 1.1626
Epoch 2047/5000
26/26 - 1s - loss: 0.7462 - val_loss: 1.1620
Epoch 2048/5000
26/26 - 1s - loss: 0.7465 - val_loss: 1.1633
Epoch 2049/5000
26/26 - 1s - loss: 0.7451 - val_loss: 1.1639
Epoch 2050/5000
26/26 - 1s - loss: 0.7448 - val_loss: 1.1640
Epoch 02050: val_loss improved from 1.16579 to 1.16402, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 2051/5000
26/26 - 1s - loss: 0.7461 - val_loss: 1.1637
Epoch 2052/5000
26/26 - 1s - loss: 0.7453 - val_loss: 1.1644
Epoch 2053/5000
26/26 - 1s - loss: 0.7445 - val_loss: 1.1635
Epoch 2054/5000
26/26 - 1s - loss: 0.7436 - val_loss: 1.1613
Epoch 2055/5000
26/26 - 1s - loss: 0.7432 - val_loss: 1.1635
Epoch 2056/5000
26/26 - 1s - loss: 0.7429 - val_loss: 1.1612
Epoch 2057/5000
26/26 - 1s - loss: 0.7430 - val_loss: 1.1617
Epoch 2058/5000
26/26 - 1s - loss: 0.7426 - val_loss: 1.1605
Epoch 2059/5000
26/26 - 1s - loss: 0.7428 - val_loss: 1.1619
Epoch 2060/5000
26/26 - 1s - loss: 0.7412 - val_loss: 1.1614
Epoch 02060: val_loss improved from 1.16402 to 1.16139, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 2061/5000
26/26 - 1s - loss: 0.7419 - val_loss: 1.1621
Epoch 2062/5000
26/26 - 1s - loss: 0.7403 - val_loss: 1.1607
Epoch 2063/5000
26/26 - 1s - loss: 0.7408 - val_loss: 1.1595
Epoch 2064/5000
26/26 - 1s - loss: 0.7408 - val_loss: 1.1581
Epoch 2065/5000
26/26 - 1s - loss: 0.7405 - val_loss: 1.1592
Epoch 2066/5000
26/26 - 1s - loss: 0.7401 - val_loss: 1.1578
Epoch 2067/5000
26/26 - 1s - loss: 0.7400 - val_loss: 1.1568
Epoch 2068/5000
26/26 - 1s - loss: 0.7382 - val_loss: 1.1567
Epoch 2069/5000
26/26 - 1s - loss: 0.7390 - val_loss: 1.1564
Epoch 2070/5000
26/26 - 1s - loss: 0.7380 - val_loss: 1.1567
Epoch 02070: val_loss improved from 1.16139 to 1.15667, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 2071/5000
26/26 - 1s - loss: 0.7369 - val_loss: 1.1556
Epoch 2072/5000
26/26 - 1s - loss: 0.7371 - val_loss: 1.1565
Epoch 2073/5000
26/26 - 1s - loss: 0.7369 - val_loss: 1.1564
Epoch 2074/5000
26/26 - 1s - loss: 0.7371 - val_loss: 1.1552
Epoch 2075/5000
26/26 - 1s - loss: 0.7362 - val_loss: 1.1555
Epoch 2076/5000
26/26 - 1s - loss: 0.7369 - val_loss: 1.1564
Epoch 2077/5000
26/26 - 1s - loss: 0.7363 - val_loss: 1.1558
Epoch 2078/5000
26/26 - 1s - loss: 0.7369 - val_loss: 1.1547
Epoch 2079/5000
26/26 - 1s - loss: 0.7350 - val_loss: 1.1528
Epoch 2080/5000
26/26 - 1s - loss: 0.7364 - val_loss: 1.1521
Epoch 02080: val_loss improved from 1.15667 to 1.15213, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 2081/5000
26/26 - 1s - loss: 0.7355 - val_loss: 1.1528
Epoch 2082/5000
26/26 - 1s - loss: 0.7359 - val_loss: 1.1524
Epoch 2083/5000
26/26 - 1s - loss: 0.7339 - val_loss: 1.1534
Epoch 2084/5000
26/26 - 1s - loss: 0.7344 - val_loss: 1.1518
Epoch 2085/5000
26/26 - 1s - loss: 0.7334 - val_loss: 1.1518
Epoch 2086/5000
26/26 - 1s - loss: 0.7350 - val_loss: 1.1507
Epoch 2087/5000
26/26 - 1s - loss: 0.7336 - val_loss: 1.1509
Epoch 2088/5000
26/26 - 1s - loss: 0.7316 - val_loss: 1.1504
Epoch 2089/5000
26/26 - 1s - loss: 0.7336 - val_loss: 1.1513
Epoch 2090/5000
26/26 - 1s - loss: 0.7333 - val_loss: 1.1509
Epoch 02090: val_loss improved from 1.15213 to 1.15093, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 2091/5000
26/26 - 1s - loss: 0.7332 - val_loss: 1.1507
Epoch 2092/5000
26/26 - 2s - loss: 0.7304 - val_loss: 1.1507
Epoch 2093/5000
26/26 - 1s - loss: 0.7306 - val_loss: 1.1512
Epoch 2094/5000
26/26 - 1s - loss: 0.7318 - val_loss: 1.1493
Epoch 2095/5000
26/26 - 1s - loss: 0.7305 - val_loss: 1.1498
Epoch 2096/5000
26/26 - 1s - loss: 0.7296 - val_loss: 1.1487
Epoch 2097/5000
26/26 - 1s - loss: 0.7300 - val_loss: 1.1498
Epoch 2098/5000
26/26 - 1s - loss: 0.7304 - val_loss: 1.1476
Epoch 2099/5000
26/26 - 1s - loss: 0.7299 - val_loss: 1.1491
Epoch 2100/5000
26/26 - 1s - loss: 0.7303 - val_loss: 1.1481
Epoch 02100: val_loss improved from 1.15093 to 1.14815, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 2101/5000
26/26 - 1s - loss: 0.7290 - val_loss: 1.1477
Epoch 2102/5000
26/26 - 1s - loss: 0.7290 - val_loss: 1.1473
Epoch 2103/5000
26/26 - 1s - loss: 0.7287 - val_loss: 1.1467
Epoch 2104/5000
26/26 - 1s - loss: 0.7276 - val_loss: 1.1468
Epoch 2105/5000
26/26 - 1s - loss: 0.7276 - val_loss: 1.1477
Epoch 2106/5000
26/26 - 1s - loss: 0.7279 - val_loss: 1.1473
Epoch 2107/5000
26/26 - 1s - loss: 0.7270 - val_loss: 1.1463
Epoch 2108/5000
26/26 - 1s - loss: 0.7269 - val_loss: 1.1471
Epoch 2109/5000
26/26 - 1s - loss: 0.7262 - val_loss: 1.1468
Epoch 2110/5000
26/26 - 1s - loss: 0.7267 - val_loss: 1.1460
Epoch 02110: val_loss improved from 1.14815 to 1.14601, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 2111/5000
26/26 - 1s - loss: 0.7262 - val_loss: 1.1448
Epoch 2112/5000
26/26 - 1s - loss: 0.7256 - val_loss: 1.1461
Epoch 2113/5000
26/26 - 1s - loss: 0.7262 - val_loss: 1.1464
Epoch 2114/5000
26/26 - 1s - loss: 0.7253 - val_loss: 1.1443
Epoch 2115/5000
26/26 - 1s - loss: 0.7247 - val_loss: 1.1432
Epoch 2116/5000
26/26 - 1s - loss: 0.7244 - val_loss: 1.1435
Epoch 2117/5000
26/26 - 1s - loss: 0.7251 - val_loss: 1.1431
Epoch 2118/5000
26/26 - 1s - loss: 0.7235 - val_loss: 1.1424
Epoch 2119/5000
26/26 - 1s - loss: 0.7233 - val_loss: 1.1426
Epoch 2120/5000
26/26 - 1s - loss: 0.7242 - val_loss: 1.1436
Epoch 02120: val_loss improved from 1.14601 to 1.14365, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 2121/5000
26/26 - 1s - loss: 0.7228 - val_loss: 1.1410
Epoch 2122/5000
26/26 - 1s - loss: 0.7230 - val_loss: 1.1413
Epoch 2123/5000
26/26 - 1s - loss: 0.7224 - val_loss: 1.1416
Epoch 2124/5000
26/26 - 1s - loss: 0.7228 - val_loss: 1.1408
Epoch 2125/5000
26/26 - 1s - loss: 0.7212 - val_loss: 1.1398
Epoch 2126/5000
26/26 - 1s - loss: 0.7205 - val_loss: 1.1400
Epoch 2127/5000
26/26 - 1s - loss: 0.7225 - val_loss: 1.1407
Epoch 2128/5000
26/26 - 1s - loss: 0.7219 - val_loss: 1.1399
Epoch 2129/5000
26/26 - 1s - loss: 0.7187 - val_loss: 1.1384
Epoch 2130/5000
26/26 - 1s - loss: 0.7206 - val_loss: 1.1382
Epoch 02130: val_loss improved from 1.14365 to 1.13821, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 2131/5000
26/26 - 1s - loss: 0.7199 - val_loss: 1.1393
Epoch 2132/5000
26/26 - 1s - loss: 0.7189 - val_loss: 1.1380
Epoch 2133/5000
26/26 - 1s - loss: 0.7205 - val_loss: 1.1396
Epoch 2134/5000
26/26 - 1s - loss: 0.7197 - val_loss: 1.1372
Epoch 2135/5000
26/26 - 1s - loss: 0.7199 - val_loss: 1.1363
Epoch 2136/5000
26/26 - 1s - loss: 0.7181 - val_loss: 1.1376
Epoch 2137/5000
26/26 - 1s - loss: 0.7189 - val_loss: 1.1366
Epoch 2138/5000
26/26 - 1s - loss: 0.7183 - val_loss: 1.1359
Epoch 2139/5000
26/26 - 1s - loss: 0.7169 - val_loss: 1.1361
Epoch 2140/5000
26/26 - 1s - loss: 0.7170 - val_loss: 1.1351
Epoch 02140: val_loss improved from 1.13821 to 1.13509, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 2141/5000
26/26 - 1s - loss: 0.7178 - val_loss: 1.1368
Epoch 2142/5000
26/26 - 1s - loss: 0.7168 - val_loss: 1.1357
Epoch 2143/5000
26/26 - 1s - loss: 0.7165 - val_loss: 1.1363
Epoch 2144/5000
26/26 - 1s - loss: 0.7165 - val_loss: 1.1374
Epoch 2145/5000
26/26 - 1s - loss: 0.7178 - val_loss: 1.1351
Epoch 2146/5000
26/26 - 1s - loss: 0.7162 - val_loss: 1.1350
Epoch 2147/5000
26/26 - 1s - loss: 0.7168 - val_loss: 1.1367
Epoch 2148/5000
26/26 - 1s - loss: 0.7156 - val_loss: 1.1346
Epoch 2149/5000
26/26 - 1s - loss: 0.7145 - val_loss: 1.1346
Epoch 2150/5000
26/26 - 1s - loss: 0.7137 - val_loss: 1.1336
Epoch 02150: val_loss improved from 1.13509 to 1.13362, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 2151/5000
26/26 - 1s - loss: 0.7144 - val_loss: 1.1333
Epoch 2152/5000
26/26 - 1s - loss: 0.7130 - val_loss: 1.1328
Epoch 2153/5000
26/26 - 1s - loss: 0.7127 - val_loss: 1.1323
Epoch 2154/5000
26/26 - 1s - loss: 0.7138 - val_loss: 1.1319
Epoch 2155/5000
26/26 - 1s - loss: 0.7133 - val_loss: 1.1319
Epoch 2156/5000
26/26 - 2s - loss: 0.7133 - val_loss: 1.1305
Epoch 2157/5000
26/26 - 1s - loss: 0.7128 - val_loss: 1.1318
Epoch 2158/5000
26/26 - 1s - loss: 0.7113 - val_loss: 1.1303
Epoch 2159/5000
26/26 - 1s - loss: 0.7109 - val_loss: 1.1295
Epoch 2160/5000
26/26 - 1s - loss: 0.7111 - val_loss: 1.1297
Epoch 02160: val_loss improved from 1.13362 to 1.12971, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 2161/5000
26/26 - 1s - loss: 0.7112 - val_loss: 1.1297
Epoch 2162/5000
26/26 - 1s - loss: 0.7103 - val_loss: 1.1296
Epoch 2163/5000
26/26 - 1s - loss: 0.7098 - val_loss: 1.1295
Epoch 2164/5000
26/26 - 1s - loss: 0.7115 - val_loss: 1.1298
Epoch 2165/5000
26/26 - 1s - loss: 0.7114 - val_loss: 1.1297
Epoch 2166/5000
26/26 - 1s - loss: 0.7101 - val_loss: 1.1304
Epoch 2167/5000
26/26 - 1s - loss: 0.7102 - val_loss: 1.1295
Epoch 2168/5000
26/26 - 1s - loss: 0.7076 - val_loss: 1.1290
Epoch 2169/5000
26/26 - 1s - loss: 0.7091 - val_loss: 1.1273
Epoch 2170/5000
26/26 - 1s - loss: 0.7098 - val_loss: 1.1275
Epoch 02170: val_loss improved from 1.12971 to 1.12753, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 2171/5000
26/26 - 1s - loss: 0.7081 - val_loss: 1.1269
Epoch 2172/5000
26/26 - 1s - loss: 0.7088 - val_loss: 1.1279
Epoch 2173/5000
26/26 - 1s - loss: 0.7096 - val_loss: 1.1272
Epoch 2174/5000
26/26 - 1s - loss: 0.7063 - val_loss: 1.1256
Epoch 2175/5000
26/26 - 2s - loss: 0.7066 - val_loss: 1.1266
Epoch 2176/5000
26/26 - 1s - loss: 0.7063 - val_loss: 1.1247
Epoch 2177/5000
26/26 - 1s - loss: 0.7077 - val_loss: 1.1253
Epoch 2178/5000
26/26 - 1s - loss: 0.7073 - val_loss: 1.1241
Epoch 2179/5000
26/26 - 1s - loss: 0.7070 - val_loss: 1.1237
Epoch 2180/5000
26/26 - 1s - loss: 0.7067 - val_loss: 1.1261
Epoch 02180: val_loss improved from 1.12753 to 1.12606, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 2181/5000
26/26 - 1s - loss: 0.7060 - val_loss: 1.1246
Epoch 2182/5000
26/26 - 1s - loss: 0.7062 - val_loss: 1.1253
Epoch 2183/5000
26/26 - 1s - loss: 0.7059 - val_loss: 1.1249
Epoch 2184/5000
26/26 - 1s - loss: 0.7049 - val_loss: 1.1246
Epoch 2185/5000
26/26 - 1s - loss: 0.7044 - val_loss: 1.1237
Epoch 2186/5000
26/26 - 1s - loss: 0.7047 - val_loss: 1.1250
Epoch 2187/5000
26/26 - 1s - loss: 0.7044 - val_loss: 1.1234
Epoch 2188/5000
26/26 - 1s - loss: 0.7032 - val_loss: 1.1230
Epoch 2189/5000
26/26 - 1s - loss: 0.7037 - val_loss: 1.1232
Epoch 2190/5000
26/26 - 1s - loss: 0.7044 - val_loss: 1.1224
Epoch 02190: val_loss improved from 1.12606 to 1.12238, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 2191/5000
26/26 - 1s - loss: 0.7017 - val_loss: 1.1217
Epoch 2192/5000
26/26 - 1s - loss: 0.7031 - val_loss: 1.1210
Epoch 2193/5000
26/26 - 1s - loss: 0.7025 - val_loss: 1.1205
Epoch 2194/5000
26/26 - 1s - loss: 0.7016 - val_loss: 1.1217
Epoch 2195/5000
26/26 - 1s - loss: 0.7011 - val_loss: 1.1208
Epoch 2196/5000
26/26 - 1s - loss: 0.7002 - val_loss: 1.1209
Epoch 2197/5000
26/26 - 1s - loss: 0.7024 - val_loss: 1.1208
Epoch 2198/5000
26/26 - 1s - loss: 0.7018 - val_loss: 1.1207
Epoch 2199/5000
26/26 - 2s - loss: 0.7002 - val_loss: 1.1198
Epoch 2200/5000
26/26 - 1s - loss: 0.7017 - val_loss: 1.1205
Epoch 02200: val_loss improved from 1.12238 to 1.12053, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 2201/5000
26/26 - 1s - loss: 0.6994 - val_loss: 1.1206
Epoch 2202/5000
26/26 - 1s - loss: 0.7004 - val_loss: 1.1195
Epoch 2203/5000
26/26 - 1s - loss: 0.7009 - val_loss: 1.1201
Epoch 2204/5000
26/26 - 1s - loss: 0.6994 - val_loss: 1.1186
Epoch 2205/5000
26/26 - 1s - loss: 0.6999 - val_loss: 1.1204
Epoch 2206/5000
26/26 - 1s - loss: 0.6982 - val_loss: 1.1198
Epoch 2207/5000
26/26 - 1s - loss: 0.6996 - val_loss: 1.1182
Epoch 2208/5000
26/26 - 1s - loss: 0.6990 - val_loss: 1.1194
Epoch 2209/5000
26/26 - 1s - loss: 0.6979 - val_loss: 1.1177
Epoch 2210/5000
26/26 - 1s - loss: 0.6979 - val_loss: 1.1167
Epoch 02210: val_loss improved from 1.12053 to 1.11668, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 2211/5000
26/26 - 1s - loss: 0.6990 - val_loss: 1.1163
Epoch 2212/5000
26/26 - 1s - loss: 0.6970 - val_loss: 1.1167
Epoch 2213/5000
26/26 - 1s - loss: 0.6968 - val_loss: 1.1174
Epoch 2214/5000
26/26 - 1s - loss: 0.6973 - val_loss: 1.1158
Epoch 2215/5000
26/26 - 1s - loss: 0.6985 - val_loss: 1.1154
Epoch 2216/5000
26/26 - 2s - loss: 0.6958 - val_loss: 1.1160
Epoch 2217/5000
26/26 - 1s - loss: 0.6956 - val_loss: 1.1144
Epoch 2218/5000
26/26 - 1s - loss: 0.6966 - val_loss: 1.1156
Epoch 2219/5000
26/26 - 1s - loss: 0.6958 - val_loss: 1.1150
Epoch 2220/5000
26/26 - 1s - loss: 0.6958 - val_loss: 1.1156
Epoch 02220: val_loss improved from 1.11668 to 1.11559, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 2221/5000
26/26 - 1s - loss: 0.6952 - val_loss: 1.1134
Epoch 2222/5000
26/26 - 1s - loss: 0.6948 - val_loss: 1.1125
Epoch 2223/5000
26/26 - 1s - loss: 0.6939 - val_loss: 1.1134
Epoch 2224/5000
26/26 - 1s - loss: 0.6949 - val_loss: 1.1119
Epoch 2225/5000
26/26 - 1s - loss: 0.6937 - val_loss: 1.1119
Epoch 2226/5000
26/26 - 1s - loss: 0.6930 - val_loss: 1.1129
Epoch 2227/5000
26/26 - 1s - loss: 0.6924 - val_loss: 1.1133
Epoch 2228/5000
26/26 - 1s - loss: 0.6928 - val_loss: 1.1130
Epoch 2229/5000
26/26 - 1s - loss: 0.6927 - val_loss: 1.1112
Epoch 2230/5000
26/26 - 1s - loss: 0.6931 - val_loss: 1.1123
Epoch 02230: val_loss improved from 1.11559 to 1.11232, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 2231/5000
26/26 - 1s - loss: 0.6946 - val_loss: 1.1122
Epoch 2232/5000
26/26 - 1s - loss: 0.6933 - val_loss: 1.1128
Epoch 2233/5000
26/26 - 1s - loss: 0.6903 - val_loss: 1.1119
Epoch 2234/5000
26/26 - 1s - loss: 0.6913 - val_loss: 1.1109
Epoch 2235/5000
26/26 - 1s - loss: 0.6910 - val_loss: 1.1118
Epoch 2236/5000
26/26 - 1s - loss: 0.6918 - val_loss: 1.1102
Epoch 2237/5000
26/26 - 1s - loss: 0.6904 - val_loss: 1.1094
Epoch 2238/5000
26/26 - 1s - loss: 0.6924 - val_loss: 1.1103
Epoch 2239/5000
26/26 - 1s - loss: 0.6889 - val_loss: 1.1106
Epoch 2240/5000
26/26 - 1s - loss: 0.6909 - val_loss: 1.1091
Epoch 02240: val_loss improved from 1.11232 to 1.10909, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 2241/5000
26/26 - 1s - loss: 0.6882 - val_loss: 1.1098
Epoch 2242/5000
26/26 - 1s - loss: 0.6894 - val_loss: 1.1087
Epoch 2243/5000
26/26 - 1s - loss: 0.6872 - val_loss: 1.1087
Epoch 2244/5000
26/26 - 1s - loss: 0.6899 - val_loss: 1.1090
Epoch 2245/5000
26/26 - 1s - loss: 0.6879 - val_loss: 1.1077
Epoch 2246/5000
26/26 - 1s - loss: 0.6896 - val_loss: 1.1069
Epoch 2247/5000
26/26 - 1s - loss: 0.6891 - val_loss: 1.1092
Epoch 2248/5000
26/26 - 1s - loss: 0.6863 - val_loss: 1.1073
Epoch 2249/5000
26/26 - 1s - loss: 0.6880 - val_loss: 1.1081
Epoch 2250/5000
26/26 - 1s - loss: 0.6869 - val_loss: 1.1077
Epoch 02250: val_loss improved from 1.10909 to 1.10774, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 2251/5000
26/26 - 1s - loss: 0.6861 - val_loss: 1.1070
Epoch 2252/5000
26/26 - 1s - loss: 0.6875 - val_loss: 1.1061
Epoch 2253/5000
26/26 - 1s - loss: 0.6866 - val_loss: 1.1066
Epoch 2254/5000
26/26 - 1s - loss: 0.6865 - val_loss: 1.1062
Epoch 2255/5000
26/26 - 1s - loss: 0.6870 - val_loss: 1.1062
Epoch 2256/5000
26/26 - 1s - loss: 0.6864 - val_loss: 1.1048
Epoch 2257/5000
26/26 - 1s - loss: 0.6851 - val_loss: 1.1064
Epoch 2258/5000
26/26 - 1s - loss: 0.6854 - val_loss: 1.1046
Epoch 2259/5000
26/26 - 1s - loss: 0.6849 - val_loss: 1.1037
Epoch 2260/5000
26/26 - 1s - loss: 0.6857 - val_loss: 1.1047
Epoch 02260: val_loss improved from 1.10774 to 1.10466, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 2261/5000
26/26 - 1s - loss: 0.6858 - val_loss: 1.1032
Epoch 2262/5000
26/26 - 1s - loss: 0.6846 - val_loss: 1.1036
Epoch 2263/5000
26/26 - 1s - loss: 0.6848 - val_loss: 1.1034
Epoch 2264/5000
26/26 - 1s - loss: 0.6845 - val_loss: 1.1033
Epoch 2265/5000
26/26 - 1s - loss: 0.6842 - val_loss: 1.1026
Epoch 2266/5000
26/26 - 1s - loss: 0.6821 - val_loss: 1.1017
Epoch 2267/5000
26/26 - 1s - loss: 0.6821 - val_loss: 1.1012
Epoch 2268/5000
26/26 - 1s - loss: 0.6835 - val_loss: 1.1020
Epoch 2269/5000
26/26 - 1s - loss: 0.6819 - val_loss: 1.1008
Epoch 2270/5000
26/26 - 1s - loss: 0.6827 - val_loss: 1.1010
Epoch 02270: val_loss improved from 1.10466 to 1.10096, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 2271/5000
26/26 - 1s - loss: 0.6816 - val_loss: 1.1015
Epoch 2272/5000
26/26 - 2s - loss: 0.6819 - val_loss: 1.1018
Epoch 2273/5000
26/26 - 1s - loss: 0.6792 - val_loss: 1.1012
Epoch 2274/5000
26/26 - 1s - loss: 0.6804 - val_loss: 1.1011
Epoch 2275/5000
26/26 - 1s - loss: 0.6804 - val_loss: 1.1020
Epoch 2276/5000
26/26 - 1s - loss: 0.6807 - val_loss: 1.0994
Epoch 2277/5000
26/26 - 1s - loss: 0.6810 - val_loss: 1.0995
Epoch 2278/5000
26/26 - 1s - loss: 0.6804 - val_loss: 1.0994
Epoch 2279/5000
26/26 - 1s - loss: 0.6802 - val_loss: 1.0995
Epoch 2280/5000
26/26 - 1s - loss: 0.6802 - val_loss: 1.0990
Epoch 02280: val_loss improved from 1.10096 to 1.09901, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 2281/5000
26/26 - 1s - loss: 0.6791 - val_loss: 1.0993
Epoch 2282/5000
26/26 - 2s - loss: 0.6785 - val_loss: 1.0982
Epoch 2283/5000
26/26 - 1s - loss: 0.6798 - val_loss: 1.0974
Epoch 2284/5000
26/26 - 1s - loss: 0.6768 - val_loss: 1.0979
Epoch 2285/5000
26/26 - 1s - loss: 0.6774 - val_loss: 1.0966
Epoch 2286/5000
26/26 - 1s - loss: 0.6782 - val_loss: 1.0969
Epoch 2287/5000
26/26 - 1s - loss: 0.6773 - val_loss: 1.0971
Epoch 2288/5000
26/26 - 1s - loss: 0.6793 - val_loss: 1.0984
Epoch 2289/5000
26/26 - 1s - loss: 0.6773 - val_loss: 1.0958
Epoch 2290/5000
26/26 - 1s - loss: 0.6783 - val_loss: 1.0968
Epoch 02290: val_loss improved from 1.09901 to 1.09683, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 2291/5000
26/26 - 1s - loss: 0.6773 - val_loss: 1.0959
Epoch 2292/5000
26/26 - 1s - loss: 0.6763 - val_loss: 1.0955
Epoch 2293/5000
26/26 - 1s - loss: 0.6764 - val_loss: 1.0959
Epoch 2294/5000
26/26 - 2s - loss: 0.6770 - val_loss: 1.0956
Epoch 2295/5000
26/26 - 1s - loss: 0.6750 - val_loss: 1.0967
Epoch 2296/5000
26/26 - 1s - loss: 0.6758 - val_loss: 1.0950
Epoch 2297/5000
26/26 - 1s - loss: 0.6751 - val_loss: 1.0959
Epoch 2298/5000
26/26 - 1s - loss: 0.6751 - val_loss: 1.0951
Epoch 2299/5000
26/26 - 1s - loss: 0.6755 - val_loss: 1.0947
Epoch 2300/5000
26/26 - 2s - loss: 0.6743 - val_loss: 1.0957
Epoch 02300: val_loss improved from 1.09683 to 1.09570, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 2301/5000
26/26 - 1s - loss: 0.6762 - val_loss: 1.0939
Epoch 2302/5000
26/26 - 1s - loss: 0.6739 - val_loss: 1.0948
Epoch 2303/5000
26/26 - 1s - loss: 0.6757 - val_loss: 1.0928
Epoch 2304/5000
26/26 - 1s - loss: 0.6736 - val_loss: 1.0929
Epoch 2305/5000
26/26 - 1s - loss: 0.6729 - val_loss: 1.0942
Epoch 2306/5000
26/26 - 1s - loss: 0.6724 - val_loss: 1.0920
Epoch 2307/5000
26/26 - 1s - loss: 0.6728 - val_loss: 1.0923
Epoch 2308/5000
26/26 - 1s - loss: 0.6736 - val_loss: 1.0921
Epoch 2309/5000
26/26 - 1s - loss: 0.6723 - val_loss: 1.0915
Epoch 2310/5000
26/26 - 1s - loss: 0.6718 - val_loss: 1.0918
Epoch 02310: val_loss improved from 1.09570 to 1.09181, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 2311/5000
26/26 - 1s - loss: 0.6730 - val_loss: 1.0934
Epoch 2312/5000
26/26 - 1s - loss: 0.6727 - val_loss: 1.0914
Epoch 2313/5000
26/26 - 1s - loss: 0.6722 - val_loss: 1.0905
Epoch 2314/5000
26/26 - 1s - loss: 0.6707 - val_loss: 1.0913
Epoch 2315/5000
26/26 - 1s - loss: 0.6709 - val_loss: 1.0907
Epoch 2316/5000
26/26 - 1s - loss: 0.6705 - val_loss: 1.0903
Epoch 2317/5000
26/26 - 1s - loss: 0.6706 - val_loss: 1.0894
Epoch 2318/5000
26/26 - 1s - loss: 0.6708 - val_loss: 1.0894
Epoch 2319/5000
26/26 - 1s - loss: 0.6702 - val_loss: 1.0889
Epoch 2320/5000
26/26 - 1s - loss: 0.6707 - val_loss: 1.0893
Epoch 02320: val_loss improved from 1.09181 to 1.08934, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 2321/5000
26/26 - 1s - loss: 0.6701 - val_loss: 1.0885
Epoch 2322/5000
26/26 - 1s - loss: 0.6684 - val_loss: 1.0889
Epoch 2323/5000
26/26 - 1s - loss: 0.6685 - val_loss: 1.0875
Epoch 2324/5000
26/26 - 1s - loss: 0.6691 - val_loss: 1.0881
Epoch 2325/5000
26/26 - 1s - loss: 0.6700 - val_loss: 1.0877
Epoch 2326/5000
26/26 - 1s - loss: 0.6675 - val_loss: 1.0877
Epoch 2327/5000
26/26 - 1s - loss: 0.6687 - val_loss: 1.0893
Epoch 2328/5000
26/26 - 1s - loss: 0.6701 - val_loss: 1.0861
Epoch 2329/5000
26/26 - 1s - loss: 0.6662 - val_loss: 1.0854
Epoch 2330/5000
26/26 - 1s - loss: 0.6670 - val_loss: 1.0859
Epoch 02330: val_loss improved from 1.08934 to 1.08589, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 2331/5000
26/26 - 1s - loss: 0.6681 - val_loss: 1.0856
Epoch 2332/5000
26/26 - 1s - loss: 0.6659 - val_loss: 1.0854
Epoch 2333/5000
26/26 - 1s - loss: 0.6659 - val_loss: 1.0836
Epoch 2334/5000
26/26 - 1s - loss: 0.6668 - val_loss: 1.0836
Epoch 2335/5000
26/26 - 1s - loss: 0.6669 - val_loss: 1.0852
Epoch 2336/5000
26/26 - 1s - loss: 0.6663 - val_loss: 1.0853
Epoch 2337/5000
26/26 - 1s - loss: 0.6649 - val_loss: 1.0846
Epoch 2338/5000
26/26 - 1s - loss: 0.6656 - val_loss: 1.0851
Epoch 2339/5000
26/26 - 1s - loss: 0.6658 - val_loss: 1.0852
Epoch 2340/5000
26/26 - 1s - loss: 0.6642 - val_loss: 1.0839
Epoch 02340: val_loss improved from 1.08589 to 1.08393, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 2341/5000
26/26 - 1s - loss: 0.6651 - val_loss: 1.0844
Epoch 2342/5000
26/26 - 1s - loss: 0.6648 - val_loss: 1.0845
Epoch 2343/5000
26/26 - 1s - loss: 0.6644 - val_loss: 1.0844
Epoch 2344/5000
26/26 - 1s - loss: 0.6634 - val_loss: 1.0830
Epoch 2345/5000
26/26 - 1s - loss: 0.6641 - val_loss: 1.0836
Epoch 2346/5000
26/26 - 1s - loss: 0.6634 - val_loss: 1.0828
Epoch 2347/5000
26/26 - 1s - loss: 0.6632 - val_loss: 1.0826
Epoch 2348/5000
26/26 - 1s - loss: 0.6616 - val_loss: 1.0823
Epoch 2349/5000
26/26 - 1s - loss: 0.6618 - val_loss: 1.0828
Epoch 2350/5000
26/26 - 1s - loss: 0.6616 - val_loss: 1.0824
Epoch 02350: val_loss improved from 1.08393 to 1.08236, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 2351/5000
26/26 - 1s - loss: 0.6609 - val_loss: 1.0815
Epoch 2352/5000
26/26 - 1s - loss: 0.6615 - val_loss: 1.0817
Epoch 2353/5000
26/26 - 2s - loss: 0.6627 - val_loss: 1.0810
Epoch 2354/5000
26/26 - 1s - loss: 0.6623 - val_loss: 1.0815
Epoch 2355/5000
26/26 - 1s - loss: 0.6614 - val_loss: 1.0808
Epoch 2356/5000
26/26 - 1s - loss: 0.6624 - val_loss: 1.0809
Epoch 2357/5000
26/26 - 1s - loss: 0.6605 - val_loss: 1.0807
Epoch 2358/5000
26/26 - 1s - loss: 0.6603 - val_loss: 1.0788
Epoch 2359/5000
26/26 - 1s - loss: 0.6600 - val_loss: 1.0798
Epoch 2360/5000
26/26 - 1s - loss: 0.6615 - val_loss: 1.0789
Epoch 02360: val_loss improved from 1.08236 to 1.07886, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 2361/5000
26/26 - 1s - loss: 0.6601 - val_loss: 1.0810
Epoch 2362/5000
26/26 - 1s - loss: 0.6608 - val_loss: 1.0797
Epoch 2363/5000
26/26 - 1s - loss: 0.6593 - val_loss: 1.0799
Epoch 2364/5000
26/26 - 1s - loss: 0.6597 - val_loss: 1.0792
Epoch 2365/5000
26/26 - 1s - loss: 0.6592 - val_loss: 1.0798
Epoch 2366/5000
26/26 - 1s - loss: 0.6590 - val_loss: 1.0786
Epoch 2367/5000
26/26 - 1s - loss: 0.6579 - val_loss: 1.0790
Epoch 2368/5000
26/26 - 1s - loss: 0.6582 - val_loss: 1.0786
Epoch 2369/5000
26/26 - 1s - loss: 0.6587 - val_loss: 1.0786
Epoch 2370/5000
26/26 - 1s - loss: 0.6576 - val_loss: 1.0778
Epoch 02370: val_loss improved from 1.07886 to 1.07779, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 2371/5000
26/26 - 1s - loss: 0.6577 - val_loss: 1.0773
Epoch 2372/5000
26/26 - 1s - loss: 0.6573 - val_loss: 1.0779
Epoch 2373/5000
26/26 - 1s - loss: 0.6582 - val_loss: 1.0771
Epoch 2374/5000
26/26 - 1s - loss: 0.6565 - val_loss: 1.0764
Epoch 2375/5000
26/26 - 1s - loss: 0.6571 - val_loss: 1.0760
Epoch 2376/5000
26/26 - 1s - loss: 0.6569 - val_loss: 1.0753
Epoch 2377/5000
26/26 - 1s - loss: 0.6562 - val_loss: 1.0741
Epoch 2378/5000
26/26 - 1s - loss: 0.6559 - val_loss: 1.0752
Epoch 2379/5000
26/26 - 1s - loss: 0.6555 - val_loss: 1.0731
Epoch 2380/5000
26/26 - 1s - loss: 0.6570 - val_loss: 1.0753
Epoch 02380: val_loss improved from 1.07779 to 1.07528, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 2381/5000
26/26 - 1s - loss: 0.6552 - val_loss: 1.0745
Epoch 2382/5000
26/26 - 1s - loss: 0.6552 - val_loss: 1.0725
Epoch 2383/5000
26/26 - 2s - loss: 0.6540 - val_loss: 1.0742
Epoch 2384/5000
26/26 - 1s - loss: 0.6553 - val_loss: 1.0740
Epoch 2385/5000
26/26 - 1s - loss: 0.6536 - val_loss: 1.0735
Epoch 2386/5000
26/26 - 1s - loss: 0.6545 - val_loss: 1.0734
Epoch 2387/5000
26/26 - 1s - loss: 0.6544 - val_loss: 1.0743
Epoch 2388/5000
26/26 - 1s - loss: 0.6533 - val_loss: 1.0721
Epoch 2389/5000
26/26 - 1s - loss: 0.6531 - val_loss: 1.0721
Epoch 2390/5000
26/26 - 1s - loss: 0.6525 - val_loss: 1.0729
Epoch 02390: val_loss improved from 1.07528 to 1.07292, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 2391/5000
26/26 - 1s - loss: 0.6542 - val_loss: 1.0730
Epoch 2392/5000
26/26 - 1s - loss: 0.6528 - val_loss: 1.0735
Epoch 2393/5000
26/26 - 1s - loss: 0.6524 - val_loss: 1.0723
Epoch 2394/5000
26/26 - 1s - loss: 0.6514 - val_loss: 1.0730
Epoch 2395/5000
26/26 - 1s - loss: 0.6517 - val_loss: 1.0731
Epoch 2396/5000
26/26 - 1s - loss: 0.6515 - val_loss: 1.0706
Epoch 2397/5000
26/26 - 1s - loss: 0.6522 - val_loss: 1.0711
Epoch 2398/5000
26/26 - 1s - loss: 0.6510 - val_loss: 1.0702
Epoch 2399/5000
26/26 - 1s - loss: 0.6519 - val_loss: 1.0700
Epoch 2400/5000
26/26 - 1s - loss: 0.6523 - val_loss: 1.0694
Epoch 02400: val_loss improved from 1.07292 to 1.06941, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 2401/5000
26/26 - 1s - loss: 0.6508 - val_loss: 1.0708
Epoch 2402/5000
26/26 - 1s - loss: 0.6498 - val_loss: 1.0695
Epoch 2403/5000
26/26 - 1s - loss: 0.6507 - val_loss: 1.0699
Epoch 2404/5000
26/26 - 1s - loss: 0.6500 - val_loss: 1.0687
Epoch 2405/5000
26/26 - 1s - loss: 0.6483 - val_loss: 1.0696
Epoch 2406/5000
26/26 - 1s - loss: 0.6484 - val_loss: 1.0690
Epoch 2407/5000
26/26 - 1s - loss: 0.6483 - val_loss: 1.0691
Epoch 2408/5000
26/26 - 1s - loss: 0.6495 - val_loss: 1.0684
Epoch 2409/5000
26/26 - 1s - loss: 0.6484 - val_loss: 1.0683
Epoch 2410/5000
26/26 - 1s - loss: 0.6494 - val_loss: 1.0678
Epoch 02410: val_loss improved from 1.06941 to 1.06782, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 2411/5000
26/26 - 1s - loss: 0.6484 - val_loss: 1.0660
Epoch 2412/5000
26/26 - 1s - loss: 0.6486 - val_loss: 1.0670
Epoch 2413/5000
26/26 - 1s - loss: 0.6475 - val_loss: 1.0673
Epoch 2414/5000
26/26 - 1s - loss: 0.6470 - val_loss: 1.0670
Epoch 2415/5000
26/26 - 1s - loss: 0.6469 - val_loss: 1.0664
Epoch 2416/5000
26/26 - 1s - loss: 0.6462 - val_loss: 1.0660
Epoch 2417/5000
26/26 - 1s - loss: 0.6458 - val_loss: 1.0658
Epoch 2418/5000
26/26 - 1s - loss: 0.6456 - val_loss: 1.0650
Epoch 2419/5000
26/26 - 1s - loss: 0.6463 - val_loss: 1.0661
Epoch 2420/5000
26/26 - 1s - loss: 0.6462 - val_loss: 1.0652
Epoch 02420: val_loss improved from 1.06782 to 1.06516, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 2421/5000
26/26 - 1s - loss: 0.6446 - val_loss: 1.0643
Epoch 2422/5000
26/26 - 1s - loss: 0.6451 - val_loss: 1.0643
Epoch 2423/5000
26/26 - 1s - loss: 0.6464 - val_loss: 1.0652
Epoch 2424/5000
26/26 - 1s - loss: 0.6450 - val_loss: 1.0654
Epoch 2425/5000
26/26 - 2s - loss: 0.6456 - val_loss: 1.0650
Epoch 2426/5000
26/26 - 1s - loss: 0.6448 - val_loss: 1.0661
Epoch 2427/5000
26/26 - 1s - loss: 0.6450 - val_loss: 1.0645
Epoch 2428/5000
26/26 - 2s - loss: 0.6440 - val_loss: 1.0645
Epoch 2429/5000
26/26 - 1s - loss: 0.6445 - val_loss: 1.0634
Epoch 2430/5000
26/26 - 1s - loss: 0.6434 - val_loss: 1.0637
Epoch 02430: val_loss improved from 1.06516 to 1.06366, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 2431/5000
26/26 - 1s - loss: 0.6445 - val_loss: 1.0639
Epoch 2432/5000
26/26 - 1s - loss: 0.6437 - val_loss: 1.0645
Epoch 2433/5000
26/26 - 1s - loss: 0.6435 - val_loss: 1.0639
Epoch 2434/5000
26/26 - 1s - loss: 0.6436 - val_loss: 1.0642
Epoch 2435/5000
26/26 - 1s - loss: 0.6431 - val_loss: 1.0628
Epoch 2436/5000
26/26 - 1s - loss: 0.6425 - val_loss: 1.0631
Epoch 2437/5000
26/26 - 1s - loss: 0.6430 - val_loss: 1.0620
Epoch 2438/5000
26/26 - 1s - loss: 0.6416 - val_loss: 1.0621
Epoch 2439/5000
26/26 - 1s - loss: 0.6421 - val_loss: 1.0616
Epoch 2440/5000
26/26 - 1s - loss: 0.6424 - val_loss: 1.0610
Epoch 02440: val_loss improved from 1.06366 to 1.06101, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 2441/5000
26/26 - 1s - loss: 0.6413 - val_loss: 1.0603
Epoch 2442/5000
26/26 - 1s - loss: 0.6413 - val_loss: 1.0604
Epoch 2443/5000
26/26 - 1s - loss: 0.6405 - val_loss: 1.0616
Epoch 2444/5000
26/26 - 1s - loss: 0.6397 - val_loss: 1.0618
Epoch 2445/5000
26/26 - 1s - loss: 0.6411 - val_loss: 1.0623
Epoch 2446/5000
26/26 - 1s - loss: 0.6406 - val_loss: 1.0616
Epoch 2447/5000
26/26 - 1s - loss: 0.6408 - val_loss: 1.0608
Epoch 2448/5000
26/26 - 1s - loss: 0.6404 - val_loss: 1.0595
Epoch 2449/5000
26/26 - 1s - loss: 0.6389 - val_loss: 1.0584
Epoch 2450/5000
26/26 - 1s - loss: 0.6394 - val_loss: 1.0622
Epoch 02450: val_loss did not improve from 1.06101
Epoch 2451/5000
26/26 - 1s - loss: 0.6407 - val_loss: 1.0596
Epoch 2452/5000
26/26 - 1s - loss: 0.6399 - val_loss: 1.0592
Epoch 2453/5000
26/26 - 1s - loss: 0.6380 - val_loss: 1.0584
Epoch 2454/5000
26/26 - 1s - loss: 0.6386 - val_loss: 1.0582
Epoch 2455/5000
26/26 - 1s - loss: 0.6379 - val_loss: 1.0588
Epoch 2456/5000
26/26 - 1s - loss: 0.6381 - val_loss: 1.0594
Epoch 2457/5000
26/26 - 1s - loss: 0.6399 - val_loss: 1.0587
Epoch 2458/5000
26/26 - 1s - loss: 0.6382 - val_loss: 1.0578
Epoch 2459/5000
26/26 - 1s - loss: 0.6380 - val_loss: 1.0588
Epoch 2460/5000
26/26 - 1s - loss: 0.6380 - val_loss: 1.0565
Epoch 02460: val_loss improved from 1.06101 to 1.05651, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 2461/5000
26/26 - 1s - loss: 0.6374 - val_loss: 1.0579
Epoch 2462/5000
26/26 - 1s - loss: 0.6373 - val_loss: 1.0569
Epoch 2463/5000
26/26 - 1s - loss: 0.6368 - val_loss: 1.0562
Epoch 2464/5000
26/26 - 1s - loss: 0.6358 - val_loss: 1.0583
Epoch 2465/5000
26/26 - 1s - loss: 0.6360 - val_loss: 1.0568
Epoch 2466/5000
26/26 - 2s - loss: 0.6365 - val_loss: 1.0561
Epoch 2467/5000
26/26 - 1s - loss: 0.6352 - val_loss: 1.0569
Epoch 2468/5000
26/26 - 1s - loss: 0.6351 - val_loss: 1.0555
Epoch 2469/5000
26/26 - 1s - loss: 0.6359 - val_loss: 1.0557
Epoch 2470/5000
26/26 - 1s - loss: 0.6339 - val_loss: 1.0566
Epoch 02470: val_loss did not improve from 1.05651
Epoch 2471/5000
26/26 - 1s - loss: 0.6357 - val_loss: 1.0560
Epoch 2472/5000
26/26 - 1s - loss: 0.6350 - val_loss: 1.0564
Epoch 2473/5000
26/26 - 1s - loss: 0.6352 - val_loss: 1.0550
Epoch 2474/5000
26/26 - 1s - loss: 0.6346 - val_loss: 1.0558
Epoch 2475/5000
26/26 - 1s - loss: 0.6362 - val_loss: 1.0547
Epoch 2476/5000
26/26 - 1s - loss: 0.6330 - val_loss: 1.0544
Epoch 2477/5000
26/26 - 1s - loss: 0.6339 - val_loss: 1.0544
Epoch 2478/5000
26/26 - 1s - loss: 0.6349 - val_loss: 1.0534
Epoch 2479/5000
26/26 - 1s - loss: 0.6325 - val_loss: 1.0531
Epoch 2480/5000
26/26 - 1s - loss: 0.6326 - val_loss: 1.0534
Epoch 02480: val_loss improved from 1.05651 to 1.05338, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 2481/5000
26/26 - 1s - loss: 0.6332 - val_loss: 1.0529
Epoch 2482/5000
26/26 - 1s - loss: 0.6317 - val_loss: 1.0517
Epoch 2483/5000
26/26 - 1s - loss: 0.6323 - val_loss: 1.0531
Epoch 2484/5000
26/26 - 1s - loss: 0.6318 - val_loss: 1.0514
Epoch 2485/5000
26/26 - 1s - loss: 0.6320 - val_loss: 1.0514
Epoch 2486/5000
26/26 - 1s - loss: 0.6314 - val_loss: 1.0518
Epoch 2487/5000
26/26 - 1s - loss: 0.6317 - val_loss: 1.0524
Epoch 2488/5000
26/26 - 1s - loss: 0.6299 - val_loss: 1.0514
Epoch 2489/5000
26/26 - 1s - loss: 0.6322 - val_loss: 1.0517
Epoch 2490/5000
26/26 - 1s - loss: 0.6308 - val_loss: 1.0518
Epoch 02490: val_loss improved from 1.05338 to 1.05179, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 2491/5000
26/26 - 1s - loss: 0.6302 - val_loss: 1.0516
Epoch 2492/5000
26/26 - 1s - loss: 0.6304 - val_loss: 1.0510
Epoch 2493/5000
26/26 - 1s - loss: 0.6305 - val_loss: 1.0501
Epoch 2494/5000
26/26 - 1s - loss: 0.6312 - val_loss: 1.0512
Epoch 2495/5000
26/26 - 1s - loss: 0.6305 - val_loss: 1.0502
Epoch 2496/5000
26/26 - 1s - loss: 0.6287 - val_loss: 1.0513
Epoch 2497/5000
26/26 - 1s - loss: 0.6298 - val_loss: 1.0485
Epoch 2498/5000
26/26 - 1s - loss: 0.6298 - val_loss: 1.0493
Epoch 2499/5000
26/26 - 1s - loss: 0.6302 - val_loss: 1.0490
Epoch 2500/5000
26/26 - 1s - loss: 0.6281 - val_loss: 1.0490
Epoch 02500: val_loss improved from 1.05179 to 1.04900, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 2501/5000
26/26 - 1s - loss: 0.6286 - val_loss: 1.0494
Epoch 2502/5000
26/26 - 1s - loss: 0.6288 - val_loss: 1.0485
Epoch 2503/5000
26/26 - 1s - loss: 0.6283 - val_loss: 1.0486
Epoch 2504/5000
26/26 - 1s - loss: 0.6289 - val_loss: 1.0500
Epoch 2505/5000
26/26 - 1s - loss: 0.6283 - val_loss: 1.0489
Epoch 2506/5000
26/26 - 1s - loss: 0.6266 - val_loss: 1.0494
Epoch 2507/5000
26/26 - 2s - loss: 0.6274 - val_loss: 1.0464
Epoch 2508/5000
26/26 - 1s - loss: 0.6277 - val_loss: 1.0476
Epoch 2509/5000
26/26 - 1s - loss: 0.6277 - val_loss: 1.0477
Epoch 2510/5000
26/26 - 1s - loss: 0.6265 - val_loss: 1.0480
Epoch 02510: val_loss improved from 1.04900 to 1.04801, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 2511/5000
26/26 - 1s - loss: 0.6274 - val_loss: 1.0472
Epoch 2512/5000
26/26 - 1s - loss: 0.6280 - val_loss: 1.0463
Epoch 2513/5000
26/26 - 1s - loss: 0.6267 - val_loss: 1.0464
Epoch 2514/5000
26/26 - 1s - loss: 0.6258 - val_loss: 1.0473
Epoch 2515/5000
26/26 - 1s - loss: 0.6248 - val_loss: 1.0467
Epoch 2516/5000
26/26 - 1s - loss: 0.6244 - val_loss: 1.0459
Epoch 2517/5000
26/26 - 1s - loss: 0.6250 - val_loss: 1.0443
Epoch 2518/5000
26/26 - 1s - loss: 0.6259 - val_loss: 1.0452
Epoch 2519/5000
26/26 - 1s - loss: 0.6254 - val_loss: 1.0469
Epoch 2520/5000
26/26 - 1s - loss: 0.6240 - val_loss: 1.0438
Epoch 02520: val_loss improved from 1.04801 to 1.04379, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 2521/5000
26/26 - 1s - loss: 0.6241 - val_loss: 1.0450
Epoch 2522/5000
26/26 - 1s - loss: 0.6248 - val_loss: 1.0447
Epoch 2523/5000
26/26 - 1s - loss: 0.6239 - val_loss: 1.0455
Epoch 2524/5000
26/26 - 1s - loss: 0.6232 - val_loss: 1.0456
Epoch 2525/5000
26/26 - 1s - loss: 0.6244 - val_loss: 1.0441
Epoch 2526/5000
26/26 - 1s - loss: 0.6238 - val_loss: 1.0448
Epoch 2527/5000
26/26 - 1s - loss: 0.6235 - val_loss: 1.0442
Epoch 2528/5000
26/26 - 1s - loss: 0.6237 - val_loss: 1.0446
Epoch 2529/5000
26/26 - 1s - loss: 0.6226 - val_loss: 1.0428
Epoch 2530/5000
26/26 - 1s - loss: 0.6225 - val_loss: 1.0428
Epoch 02530: val_loss improved from 1.04379 to 1.04277, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 2531/5000
26/26 - 1s - loss: 0.6225 - val_loss: 1.0406
Epoch 2532/5000
26/26 - 1s - loss: 0.6227 - val_loss: 1.0437
Epoch 2533/5000
26/26 - 1s - loss: 0.6230 - val_loss: 1.0431
Epoch 2534/5000
26/26 - 1s - loss: 0.6230 - val_loss: 1.0436
Epoch 2535/5000
26/26 - 1s - loss: 0.6212 - val_loss: 1.0422
Epoch 2536/5000
26/26 - 1s - loss: 0.6211 - val_loss: 1.0420
Epoch 2537/5000
26/26 - 1s - loss: 0.6214 - val_loss: 1.0410
Epoch 2538/5000
26/26 - 1s - loss: 0.6216 - val_loss: 1.0432
Epoch 2539/5000
26/26 - 1s - loss: 0.6214 - val_loss: 1.0408
Epoch 2540/5000
26/26 - 1s - loss: 0.6213 - val_loss: 1.0406
Epoch 02540: val_loss improved from 1.04277 to 1.04059, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 2541/5000
26/26 - 1s - loss: 0.6202 - val_loss: 1.0399
Epoch 2542/5000
26/26 - 1s - loss: 0.6196 - val_loss: 1.0391
Epoch 2543/5000
26/26 - 1s - loss: 0.6202 - val_loss: 1.0416
Epoch 2544/5000
26/26 - 1s - loss: 0.6204 - val_loss: 1.0399
Epoch 2545/5000
26/26 - 1s - loss: 0.6194 - val_loss: 1.0396
Epoch 2546/5000
26/26 - 1s - loss: 0.6187 - val_loss: 1.0410
Epoch 2547/5000
26/26 - 1s - loss: 0.6200 - val_loss: 1.0415
Epoch 2548/5000
26/26 - 1s - loss: 0.6199 - val_loss: 1.0382
Epoch 2549/5000
26/26 - 1s - loss: 0.6201 - val_loss: 1.0404
Epoch 2550/5000
26/26 - 1s - loss: 0.6175 - val_loss: 1.0396
Epoch 02550: val_loss improved from 1.04059 to 1.03965, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 2551/5000
26/26 - 1s - loss: 0.6170 - val_loss: 1.0393
Epoch 2552/5000
26/26 - 1s - loss: 0.6183 - val_loss: 1.0387
Epoch 2553/5000
26/26 - 1s - loss: 0.6175 - val_loss: 1.0405
Epoch 2554/5000
26/26 - 1s - loss: 0.6186 - val_loss: 1.0373
Epoch 2555/5000
26/26 - 1s - loss: 0.6183 - val_loss: 1.0376
Epoch 2556/5000
26/26 - 1s - loss: 0.6180 - val_loss: 1.0384
Epoch 2557/5000
26/26 - 1s - loss: 0.6166 - val_loss: 1.0377
Epoch 2558/5000
26/26 - 1s - loss: 0.6177 - val_loss: 1.0341
Epoch 2559/5000
26/26 - 1s - loss: 0.6167 - val_loss: 1.0373
Epoch 2560/5000
26/26 - 1s - loss: 0.6159 - val_loss: 1.0364
Epoch 02560: val_loss improved from 1.03965 to 1.03641, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 2561/5000
26/26 - 1s - loss: 0.6174 - val_loss: 1.0364
Epoch 2562/5000
26/26 - 1s - loss: 0.6158 - val_loss: 1.0355
Epoch 2563/5000
26/26 - 1s - loss: 0.6159 - val_loss: 1.0365
Epoch 2564/5000
26/26 - 1s - loss: 0.6160 - val_loss: 1.0358
Epoch 2565/5000
26/26 - 1s - loss: 0.6160 - val_loss: 1.0359
Epoch 2566/5000
26/26 - 1s - loss: 0.6161 - val_loss: 1.0348
Epoch 2567/5000
26/26 - 1s - loss: 0.6156 - val_loss: 1.0342
Epoch 2568/5000
26/26 - 1s - loss: 0.6145 - val_loss: 1.0354
Epoch 2569/5000
26/26 - 1s - loss: 0.6151 - val_loss: 1.0334
Epoch 2570/5000
26/26 - 1s - loss: 0.6133 - val_loss: 1.0356
Epoch 02570: val_loss improved from 1.03641 to 1.03556, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 2571/5000
26/26 - 1s - loss: 0.6159 - val_loss: 1.0343
Epoch 2572/5000
26/26 - 1s - loss: 0.6149 - val_loss: 1.0330
Epoch 2573/5000
26/26 - 1s - loss: 0.6139 - val_loss: 1.0331
Epoch 2574/5000
26/26 - 1s - loss: 0.6137 - val_loss: 1.0312
Epoch 2575/5000
26/26 - 1s - loss: 0.6154 - val_loss: 1.0354
Epoch 2576/5000
26/26 - 1s - loss: 0.6140 - val_loss: 1.0325
Epoch 2577/5000
26/26 - 1s - loss: 0.6135 - val_loss: 1.0327
Epoch 2578/5000
26/26 - 1s - loss: 0.6129 - val_loss: 1.0314
Epoch 2579/5000
26/26 - 1s - loss: 0.6146 - val_loss: 1.0329
Epoch 2580/5000
26/26 - 1s - loss: 0.6127 - val_loss: 1.0329
Epoch 02580: val_loss improved from 1.03556 to 1.03294, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 2581/5000
26/26 - 1s - loss: 0.6122 - val_loss: 1.0320
Epoch 2582/5000
26/26 - 1s - loss: 0.6126 - val_loss: 1.0325
Epoch 2583/5000
26/26 - 1s - loss: 0.6128 - val_loss: 1.0323
Epoch 2584/5000
26/26 - 1s - loss: 0.6113 - val_loss: 1.0326
Epoch 2585/5000
26/26 - 1s - loss: 0.6125 - val_loss: 1.0314
Epoch 2586/5000
26/26 - 1s - loss: 0.6118 - val_loss: 1.0334
Epoch 2587/5000
26/26 - 1s - loss: 0.6119 - val_loss: 1.0320
Epoch 2588/5000
26/26 - 1s - loss: 0.6121 - val_loss: 1.0307
Epoch 2589/5000
26/26 - 1s - loss: 0.6112 - val_loss: 1.0326
Epoch 2590/5000
26/26 - 1s - loss: 0.6111 - val_loss: 1.0312
Epoch 02590: val_loss improved from 1.03294 to 1.03116, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 2591/5000
26/26 - 2s - loss: 0.6109 - val_loss: 1.0318
Epoch 2592/5000
26/26 - 1s - loss: 0.6103 - val_loss: 1.0320
Epoch 2593/5000
26/26 - 2s - loss: 0.6105 - val_loss: 1.0324
Epoch 2594/5000
26/26 - 1s - loss: 0.6100 - val_loss: 1.0312
Epoch 2595/5000
26/26 - 1s - loss: 0.6097 - val_loss: 1.0305
Epoch 2596/5000
26/26 - 1s - loss: 0.6098 - val_loss: 1.0312
Epoch 2597/5000
26/26 - 1s - loss: 0.6094 - val_loss: 1.0307
Epoch 2598/5000
26/26 - 1s - loss: 0.6103 - val_loss: 1.0294
Epoch 2599/5000
26/26 - 1s - loss: 0.6092 - val_loss: 1.0294
Epoch 2600/5000
26/26 - 1s - loss: 0.6090 - val_loss: 1.0294
Epoch 02600: val_loss improved from 1.03116 to 1.02942, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 2601/5000
26/26 - 1s - loss: 0.6105 - val_loss: 1.0306
Epoch 2602/5000
26/26 - 1s - loss: 0.6093 - val_loss: 1.0323
Epoch 2603/5000
26/26 - 1s - loss: 0.6083 - val_loss: 1.0293
Epoch 2604/5000
26/26 - 1s - loss: 0.6086 - val_loss: 1.0284
Epoch 2605/5000
26/26 - 1s - loss: 0.6088 - val_loss: 1.0309
Epoch 2606/5000
26/26 - 1s - loss: 0.6084 - val_loss: 1.0290
Epoch 2607/5000
26/26 - 1s - loss: 0.6076 - val_loss: 1.0281
Epoch 2608/5000
26/26 - 1s - loss: 0.6081 - val_loss: 1.0280
Epoch 2609/5000
26/26 - 1s - loss: 0.6073 - val_loss: 1.0304
Epoch 2610/5000
26/26 - 1s - loss: 0.6063 - val_loss: 1.0284
Epoch 02610: val_loss improved from 1.02942 to 1.02844, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 2611/5000
26/26 - 1s - loss: 0.6077 - val_loss: 1.0283
Epoch 2612/5000
26/26 - 1s - loss: 0.6084 - val_loss: 1.0283
Epoch 2613/5000
26/26 - 1s - loss: 0.6070 - val_loss: 1.0265
Epoch 2614/5000
26/26 - 1s - loss: 0.6056 - val_loss: 1.0294
Epoch 2615/5000
26/26 - 1s - loss: 0.6054 - val_loss: 1.0265
Epoch 2616/5000
26/26 - 1s - loss: 0.6058 - val_loss: 1.0264
Epoch 2617/5000
26/26 - 1s - loss: 0.6060 - val_loss: 1.0255
Epoch 2618/5000
26/26 - 1s - loss: 0.6060 - val_loss: 1.0265
Epoch 2619/5000
26/26 - 1s - loss: 0.6054 - val_loss: 1.0264
Epoch 2620/5000
26/26 - 1s - loss: 0.6059 - val_loss: 1.0276
Epoch 02620: val_loss improved from 1.02844 to 1.02763, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 2621/5000
26/26 - 1s - loss: 0.6041 - val_loss: 1.0248
Epoch 2622/5000
26/26 - 1s - loss: 0.6052 - val_loss: 1.0254
Epoch 2623/5000
26/26 - 1s - loss: 0.6039 - val_loss: 1.0249
Epoch 2624/5000
26/26 - 1s - loss: 0.6041 - val_loss: 1.0247
Epoch 2625/5000
26/26 - 1s - loss: 0.6054 - val_loss: 1.0256
Epoch 2626/5000
26/26 - 1s - loss: 0.6045 - val_loss: 1.0243
Epoch 2627/5000
26/26 - 1s - loss: 0.6044 - val_loss: 1.0254
Epoch 2628/5000
26/26 - 1s - loss: 0.6035 - val_loss: 1.0249
Epoch 2629/5000
26/26 - 1s - loss: 0.6030 - val_loss: 1.0243
Epoch 2630/5000
26/26 - 1s - loss: 0.6047 - val_loss: 1.0242
Epoch 02630: val_loss improved from 1.02763 to 1.02419, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 2631/5000
26/26 - 1s - loss: 0.6038 - val_loss: 1.0232
Epoch 2632/5000
26/26 - 1s - loss: 0.6025 - val_loss: 1.0235
Epoch 2633/5000
26/26 - 1s - loss: 0.6022 - val_loss: 1.0229
Epoch 2634/5000
26/26 - 1s - loss: 0.6030 - val_loss: 1.0250
Epoch 2635/5000
26/26 - 1s - loss: 0.6019 - val_loss: 1.0236
Epoch 2636/5000
26/26 - 1s - loss: 0.6025 - val_loss: 1.0231
Epoch 2637/5000
26/26 - 1s - loss: 0.6021 - val_loss: 1.0239
Epoch 2638/5000
26/26 - 1s - loss: 0.6014 - val_loss: 1.0216
Epoch 2639/5000
26/26 - 1s - loss: 0.6015 - val_loss: 1.0225
Epoch 2640/5000
26/26 - 1s - loss: 0.6022 - val_loss: 1.0229
Epoch 02640: val_loss improved from 1.02419 to 1.02288, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 2641/5000
26/26 - 1s - loss: 0.6019 - val_loss: 1.0213
Epoch 2642/5000
26/26 - 1s - loss: 0.6013 - val_loss: 1.0217
Epoch 2643/5000
26/26 - 1s - loss: 0.6001 - val_loss: 1.0224
Epoch 2644/5000
26/26 - 1s - loss: 0.6010 - val_loss: 1.0204
Epoch 2645/5000
26/26 - 1s - loss: 0.5999 - val_loss: 1.0209
Epoch 2646/5000
26/26 - 1s - loss: 0.6006 - val_loss: 1.0214
Epoch 2647/5000
26/26 - 1s - loss: 0.6001 - val_loss: 1.0215
Epoch 2648/5000
26/26 - 1s - loss: 0.6000 - val_loss: 1.0194
Epoch 2649/5000
26/26 - 2s - loss: 0.5998 - val_loss: 1.0196
Epoch 2650/5000
26/26 - 1s - loss: 0.5997 - val_loss: 1.0207
Epoch 02650: val_loss improved from 1.02288 to 1.02069, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 2651/5000
26/26 - 1s - loss: 0.6001 - val_loss: 1.0190
Epoch 2652/5000
26/26 - 1s - loss: 0.5990 - val_loss: 1.0201
Epoch 2653/5000
26/26 - 1s - loss: 0.5990 - val_loss: 1.0194
Epoch 2654/5000
26/26 - 1s - loss: 0.5995 - val_loss: 1.0189
Epoch 2655/5000
26/26 - 1s - loss: 0.5997 - val_loss: 1.0200
Epoch 2656/5000
26/26 - 1s - loss: 0.5990 - val_loss: 1.0187
Epoch 2657/5000
26/26 - 1s - loss: 0.5977 - val_loss: 1.0201
Epoch 2658/5000
26/26 - 1s - loss: 0.5963 - val_loss: 1.0181
Epoch 2659/5000
26/26 - 1s - loss: 0.5985 - val_loss: 1.0181
Epoch 2660/5000
26/26 - 1s - loss: 0.5982 - val_loss: 1.0179
Epoch 02660: val_loss improved from 1.02069 to 1.01791, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 2661/5000
26/26 - 1s - loss: 0.5973 - val_loss: 1.0177
Epoch 2662/5000
26/26 - 1s - loss: 0.5971 - val_loss: 1.0169
Epoch 2663/5000
26/26 - 1s - loss: 0.5981 - val_loss: 1.0170
Epoch 2664/5000
26/26 - 1s - loss: 0.5965 - val_loss: 1.0176
Epoch 2665/5000
26/26 - 1s - loss: 0.5956 - val_loss: 1.0170
Epoch 2666/5000
26/26 - 1s - loss: 0.5985 - val_loss: 1.0176
Epoch 2667/5000
26/26 - 1s - loss: 0.5962 - val_loss: 1.0174
Epoch 2668/5000
26/26 - 1s - loss: 0.5968 - val_loss: 1.0165
Epoch 2669/5000
26/26 - 1s - loss: 0.5949 - val_loss: 1.0170
Epoch 2670/5000
26/26 - 1s - loss: 0.5963 - val_loss: 1.0176
Epoch 02670: val_loss improved from 1.01791 to 1.01756, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 2671/5000
26/26 - 1s - loss: 0.5960 - val_loss: 1.0165
Epoch 2672/5000
26/26 - 1s - loss: 0.5966 - val_loss: 1.0169
Epoch 2673/5000
26/26 - 1s - loss: 0.5965 - val_loss: 1.0176
Epoch 2674/5000
26/26 - 1s - loss: 0.5956 - val_loss: 1.0176
Epoch 2675/5000
26/26 - 1s - loss: 0.5959 - val_loss: 1.0171
Epoch 2676/5000
26/26 - 1s - loss: 0.5957 - val_loss: 1.0161
Epoch 2677/5000
26/26 - 1s - loss: 0.5942 - val_loss: 1.0151
Epoch 2678/5000
26/26 - 1s - loss: 0.5937 - val_loss: 1.0147
Epoch 2679/5000
26/26 - 1s - loss: 0.5949 - val_loss: 1.0147
Epoch 2680/5000
26/26 - 1s - loss: 0.5935 - val_loss: 1.0155
Epoch 02680: val_loss improved from 1.01756 to 1.01548, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 2681/5000
26/26 - 1s - loss: 0.5942 - val_loss: 1.0156
Epoch 2682/5000
26/26 - 1s - loss: 0.5941 - val_loss: 1.0144
Epoch 2683/5000
26/26 - 1s - loss: 0.5943 - val_loss: 1.0145
Epoch 2684/5000
26/26 - 1s - loss: 0.5927 - val_loss: 1.0151
Epoch 2685/5000
26/26 - 1s - loss: 0.5945 - val_loss: 1.0148
Epoch 2686/5000
26/26 - 1s - loss: 0.5931 - val_loss: 1.0128
Epoch 2687/5000
26/26 - 1s - loss: 0.5935 - val_loss: 1.0136
Epoch 2688/5000
26/26 - 1s - loss: 0.5935 - val_loss: 1.0129
Epoch 2689/5000
26/26 - 1s - loss: 0.5932 - val_loss: 1.0137
Epoch 2690/5000
26/26 - 1s - loss: 0.5915 - val_loss: 1.0134
Epoch 02690: val_loss improved from 1.01548 to 1.01339, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 2691/5000
26/26 - 1s - loss: 0.5913 - val_loss: 1.0135
Epoch 2692/5000
26/26 - 1s - loss: 0.5925 - val_loss: 1.0140
Epoch 2693/5000
26/26 - 1s - loss: 0.5925 - val_loss: 1.0130
Epoch 2694/5000
26/26 - 1s - loss: 0.5928 - val_loss: 1.0121
Epoch 2695/5000
26/26 - 1s - loss: 0.5907 - val_loss: 1.0120
Epoch 2696/5000
26/26 - 1s - loss: 0.5902 - val_loss: 1.0131
Epoch 2697/5000
26/26 - 1s - loss: 0.5916 - val_loss: 1.0125
Epoch 2698/5000
26/26 - 1s - loss: 0.5920 - val_loss: 1.0119
Epoch 2699/5000
26/26 - 1s - loss: 0.5911 - val_loss: 1.0118
Epoch 2700/5000
26/26 - 1s - loss: 0.5910 - val_loss: 1.0106
Epoch 02700: val_loss improved from 1.01339 to 1.01057, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 2701/5000
26/26 - 1s - loss: 0.5902 - val_loss: 1.0104
Epoch 2702/5000
26/26 - 1s - loss: 0.5908 - val_loss: 1.0103
Epoch 2703/5000
26/26 - 1s - loss: 0.5891 - val_loss: 1.0129
Epoch 2704/5000
26/26 - 1s - loss: 0.5899 - val_loss: 1.0113
Epoch 2705/5000
26/26 - 1s - loss: 0.5897 - val_loss: 1.0100
Epoch 2706/5000
26/26 - 1s - loss: 0.5914 - val_loss: 1.0087
Epoch 2707/5000
26/26 - 1s - loss: 0.5893 - val_loss: 1.0120
Epoch 2708/5000
26/26 - 1s - loss: 0.5898 - val_loss: 1.0102
Epoch 2709/5000
26/26 - 1s - loss: 0.5884 - val_loss: 1.0103
Epoch 2710/5000
26/26 - 1s - loss: 0.5883 - val_loss: 1.0108
Epoch 02710: val_loss did not improve from 1.01057
Epoch 2711/5000
26/26 - 1s - loss: 0.5894 - val_loss: 1.0110
Epoch 2712/5000
26/26 - 1s - loss: 0.5893 - val_loss: 1.0102
Epoch 2713/5000
26/26 - 1s - loss: 0.5898 - val_loss: 1.0101
Epoch 2714/5000
26/26 - 1s - loss: 0.5874 - val_loss: 1.0086
Epoch 2715/5000
26/26 - 1s - loss: 0.5872 - val_loss: 1.0081
Epoch 2716/5000
26/26 - 1s - loss: 0.5875 - val_loss: 1.0093
Epoch 2717/5000
26/26 - 1s - loss: 0.5873 - val_loss: 1.0089
Epoch 2718/5000
26/26 - 1s - loss: 0.5872 - val_loss: 1.0078
Epoch 2719/5000
26/26 - 1s - loss: 0.5873 - val_loss: 1.0071
Epoch 2720/5000
26/26 - 1s - loss: 0.5882 - val_loss: 1.0082
Epoch 02720: val_loss improved from 1.01057 to 1.00824, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 2721/5000
26/26 - 1s - loss: 0.5869 - val_loss: 1.0062
Epoch 2722/5000
26/26 - 1s - loss: 0.5865 - val_loss: 1.0066
Epoch 2723/5000
26/26 - 1s - loss: 0.5873 - val_loss: 1.0071
Epoch 2724/5000
26/26 - 1s - loss: 0.5873 - val_loss: 1.0079
Epoch 2725/5000
26/26 - 1s - loss: 0.5862 - val_loss: 1.0074
Epoch 2726/5000
26/26 - 1s - loss: 0.5857 - val_loss: 1.0075
Epoch 2727/5000
26/26 - 1s - loss: 0.5859 - val_loss: 1.0069
Epoch 2728/5000
26/26 - 1s - loss: 0.5847 - val_loss: 1.0076
Epoch 2729/5000
26/26 - 1s - loss: 0.5862 - val_loss: 1.0070
Epoch 2730/5000
26/26 - 1s - loss: 0.5863 - val_loss: 1.0069
Epoch 02730: val_loss improved from 1.00824 to 1.00687, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 2731/5000
26/26 - 1s - loss: 0.5838 - val_loss: 1.0052
Epoch 2732/5000
26/26 - 1s - loss: 0.5853 - val_loss: 1.0054
Epoch 2733/5000
26/26 - 1s - loss: 0.5859 - val_loss: 1.0059
Epoch 2734/5000
26/26 - 1s - loss: 0.5842 - val_loss: 1.0052
Epoch 2735/5000
26/26 - 1s - loss: 0.5848 - val_loss: 1.0053
Epoch 2736/5000
26/26 - 1s - loss: 0.5850 - val_loss: 1.0053
Epoch 2737/5000
26/26 - 1s - loss: 0.5831 - val_loss: 1.0049
Epoch 2738/5000
26/26 - 1s - loss: 0.5845 - val_loss: 1.0038
Epoch 2739/5000
26/26 - 1s - loss: 0.5851 - val_loss: 1.0069
Epoch 2740/5000
26/26 - 1s - loss: 0.5833 - val_loss: 1.0057
Epoch 02740: val_loss improved from 1.00687 to 1.00565, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 2741/5000
26/26 - 1s - loss: 0.5828 - val_loss: 1.0027
Epoch 2742/5000
26/26 - 1s - loss: 0.5824 - val_loss: 1.0028
Epoch 2743/5000
26/26 - 1s - loss: 0.5826 - val_loss: 1.0031
Epoch 2744/5000
26/26 - 1s - loss: 0.5839 - val_loss: 1.0036
Epoch 2745/5000
26/26 - 1s - loss: 0.5835 - val_loss: 1.0029
Epoch 2746/5000
26/26 - 1s - loss: 0.5829 - val_loss: 1.0026
Epoch 2747/5000
26/26 - 1s - loss: 0.5823 - val_loss: 1.0028
Epoch 2748/5000
26/26 - 1s - loss: 0.5816 - val_loss: 1.0039
Epoch 2749/5000
26/26 - 1s - loss: 0.5831 - val_loss: 1.0021
Epoch 2750/5000
26/26 - 1s - loss: 0.5812 - val_loss: 1.0041
Epoch 02750: val_loss improved from 1.00565 to 1.00409, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 2751/5000
26/26 - 1s - loss: 0.5809 - val_loss: 1.0030
Epoch 2752/5000
26/26 - 1s - loss: 0.5817 - val_loss: 1.0029
Epoch 2753/5000
26/26 - 1s - loss: 0.5840 - val_loss: 1.0029
Epoch 2754/5000
26/26 - 1s - loss: 0.5809 - val_loss: 1.0017
Epoch 2755/5000
26/26 - 1s - loss: 0.5808 - val_loss: 1.0028
Epoch 2756/5000
26/26 - 1s - loss: 0.5816 - val_loss: 1.0018
Epoch 2757/5000
26/26 - 1s - loss: 0.5817 - val_loss: 1.0013
Epoch 2758/5000
26/26 - 1s - loss: 0.5819 - val_loss: 1.0006
Epoch 2759/5000
26/26 - 1s - loss: 0.5818 - val_loss: 1.0013
Epoch 2760/5000
26/26 - 1s - loss: 0.5804 - val_loss: 1.0011
Epoch 02760: val_loss improved from 1.00409 to 1.00106, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 2761/5000
26/26 - 1s - loss: 0.5793 - val_loss: 1.0012
Epoch 2762/5000
26/26 - 1s - loss: 0.5806 - val_loss: 1.0012
Epoch 2763/5000
26/26 - 1s - loss: 0.5814 - val_loss: 1.0006
Epoch 2764/5000
26/26 - 1s - loss: 0.5800 - val_loss: 1.0005
Epoch 2765/5000
26/26 - 1s - loss: 0.5807 - val_loss: 0.9998
Epoch 2766/5000
26/26 - 1s - loss: 0.5801 - val_loss: 0.9999
Epoch 2767/5000
26/26 - 1s - loss: 0.5800 - val_loss: 0.9995
Epoch 2768/5000
26/26 - 1s - loss: 0.5793 - val_loss: 0.9998
Epoch 2769/5000
26/26 - 1s - loss: 0.5803 - val_loss: 0.9994
Epoch 2770/5000
26/26 - 1s - loss: 0.5787 - val_loss: 0.9996
Epoch 02770: val_loss improved from 1.00106 to 0.99955, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 2771/5000
26/26 - 1s - loss: 0.5796 - val_loss: 0.9998
Epoch 2772/5000
26/26 - 1s - loss: 0.5789 - val_loss: 0.9988
Epoch 2773/5000
26/26 - 1s - loss: 0.5774 - val_loss: 0.9986
Epoch 2774/5000
26/26 - 1s - loss: 0.5782 - val_loss: 0.9980
Epoch 2775/5000
26/26 - 1s - loss: 0.5775 - val_loss: 0.9984
Epoch 2776/5000
26/26 - 1s - loss: 0.5780 - val_loss: 0.9979
Epoch 2777/5000
26/26 - 1s - loss: 0.5775 - val_loss: 0.9979
Epoch 2778/5000
26/26 - 1s - loss: 0.5779 - val_loss: 0.9997
Epoch 2779/5000
26/26 - 1s - loss: 0.5779 - val_loss: 0.9979
Epoch 2780/5000
26/26 - 1s - loss: 0.5773 - val_loss: 0.9981
Epoch 02780: val_loss improved from 0.99955 to 0.99814, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 2781/5000
26/26 - 1s - loss: 0.5764 - val_loss: 0.9965
Epoch 2782/5000
26/26 - 1s - loss: 0.5770 - val_loss: 0.9976
Epoch 2783/5000
26/26 - 1s - loss: 0.5769 - val_loss: 0.9965
Epoch 2784/5000
26/26 - 1s - loss: 0.5775 - val_loss: 0.9965
Epoch 2785/5000
26/26 - 1s - loss: 0.5757 - val_loss: 0.9967
Epoch 2786/5000
26/26 - 1s - loss: 0.5763 - val_loss: 0.9970
Epoch 2787/5000
26/26 - 1s - loss: 0.5745 - val_loss: 0.9969
Epoch 2788/5000
26/26 - 1s - loss: 0.5751 - val_loss: 0.9977
Epoch 2789/5000
26/26 - 1s - loss: 0.5755 - val_loss: 0.9964
Epoch 2790/5000
26/26 - 1s - loss: 0.5751 - val_loss: 0.9950
Epoch 02790: val_loss improved from 0.99814 to 0.99505, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 2791/5000
26/26 - 1s - loss: 0.5752 - val_loss: 0.9953
Epoch 2792/5000
26/26 - 1s - loss: 0.5751 - val_loss: 0.9978
Epoch 2793/5000
26/26 - 1s - loss: 0.5741 - val_loss: 0.9972
Epoch 2794/5000
26/26 - 1s - loss: 0.5745 - val_loss: 0.9967
Epoch 2795/5000
26/26 - 1s - loss: 0.5754 - val_loss: 0.9968
Epoch 2796/5000
26/26 - 1s - loss: 0.5742 - val_loss: 0.9963
Epoch 2797/5000
26/26 - 1s - loss: 0.5741 - val_loss: 0.9971
Epoch 2798/5000
26/26 - 1s - loss: 0.5749 - val_loss: 0.9976
Epoch 2799/5000
26/26 - 1s - loss: 0.5742 - val_loss: 0.9966
Epoch 2800/5000
26/26 - 1s - loss: 0.5744 - val_loss: 0.9958
Epoch 02800: val_loss did not improve from 0.99505
Epoch 2801/5000
26/26 - 1s - loss: 0.5747 - val_loss: 0.9962
Epoch 2802/5000
26/26 - 1s - loss: 0.5733 - val_loss: 0.9942
Epoch 2803/5000
26/26 - 1s - loss: 0.5731 - val_loss: 0.9946
Epoch 2804/5000
26/26 - 1s - loss: 0.5743 - val_loss: 0.9946
Epoch 2805/5000
26/26 - 1s - loss: 0.5727 - val_loss: 0.9936
Epoch 2806/5000
26/26 - 1s - loss: 0.5726 - val_loss: 0.9933
Epoch 2807/5000
26/26 - 1s - loss: 0.5727 - val_loss: 0.9938
Epoch 2808/5000
26/26 - 1s - loss: 0.5721 - val_loss: 0.9932
Epoch 2809/5000
26/26 - 1s - loss: 0.5724 - val_loss: 0.9937
Epoch 2810/5000
26/26 - 1s - loss: 0.5727 - val_loss: 0.9931
Epoch 02810: val_loss improved from 0.99505 to 0.99306, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 2811/5000
26/26 - 1s - loss: 0.5718 - val_loss: 0.9922
Epoch 2812/5000
26/26 - 1s - loss: 0.5723 - val_loss: 0.9939
Epoch 2813/5000
26/26 - 1s - loss: 0.5721 - val_loss: 0.9925
Epoch 2814/5000
26/26 - 1s - loss: 0.5719 - val_loss: 0.9919
Epoch 2815/5000
26/26 - 1s - loss: 0.5713 - val_loss: 0.9927
Epoch 2816/5000
26/26 - 1s - loss: 0.5717 - val_loss: 0.9929
Epoch 2817/5000
26/26 - 1s - loss: 0.5714 - val_loss: 0.9919
Epoch 2818/5000
26/26 - 1s - loss: 0.5710 - val_loss: 0.9914
Epoch 2819/5000
26/26 - 1s - loss: 0.5710 - val_loss: 0.9923
Epoch 2820/5000
26/26 - 1s - loss: 0.5708 - val_loss: 0.9907
Epoch 02820: val_loss improved from 0.99306 to 0.99070, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 2821/5000
26/26 - 1s - loss: 0.5703 - val_loss: 0.9916
Epoch 2822/5000
26/26 - 1s - loss: 0.5706 - val_loss: 0.9909
Epoch 2823/5000
26/26 - 1s - loss: 0.5700 - val_loss: 0.9918
Epoch 2824/5000
26/26 - 1s - loss: 0.5713 - val_loss: 0.9918
Epoch 2825/5000
26/26 - 1s - loss: 0.5701 - val_loss: 0.9899
Epoch 2826/5000
26/26 - 1s - loss: 0.5693 - val_loss: 0.9902
Epoch 2827/5000
26/26 - 1s - loss: 0.5697 - val_loss: 0.9912
Epoch 2828/5000
26/26 - 1s - loss: 0.5685 - val_loss: 0.9896
Epoch 2829/5000
26/26 - 1s - loss: 0.5692 - val_loss: 0.9912
Epoch 2830/5000
26/26 - 1s - loss: 0.5695 - val_loss: 0.9912
Epoch 02830: val_loss did not improve from 0.99070
Epoch 2831/5000
26/26 - 1s - loss: 0.5691 - val_loss: 0.9887
Epoch 2832/5000
26/26 - 1s - loss: 0.5701 - val_loss: 0.9893
Epoch 2833/5000
26/26 - 1s - loss: 0.5691 - val_loss: 0.9906
Epoch 2834/5000
26/26 - 2s - loss: 0.5690 - val_loss: 0.9885
Epoch 2835/5000
26/26 - 1s - loss: 0.5691 - val_loss: 0.9889
Epoch 2836/5000
26/26 - 1s - loss: 0.5677 - val_loss: 0.9885
Epoch 2837/5000
26/26 - 1s - loss: 0.5676 - val_loss: 0.9879
Epoch 2838/5000
26/26 - 1s - loss: 0.5680 - val_loss: 0.9883
Epoch 2839/5000
26/26 - 1s - loss: 0.5681 - val_loss: 0.9890
Epoch 2840/5000
26/26 - 1s - loss: 0.5682 - val_loss: 0.9870
Epoch 02840: val_loss improved from 0.99070 to 0.98698, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 2841/5000
26/26 - 2s - loss: 0.5680 - val_loss: 0.9879
Epoch 2842/5000
26/26 - 1s - loss: 0.5667 - val_loss: 0.9892
Epoch 2843/5000
26/26 - 1s - loss: 0.5665 - val_loss: 0.9869
Epoch 2844/5000
26/26 - 1s - loss: 0.5668 - val_loss: 0.9882
Epoch 2845/5000
26/26 - 1s - loss: 0.5664 - val_loss: 0.9872
Epoch 2846/5000
26/26 - 1s - loss: 0.5666 - val_loss: 0.9862
Epoch 2847/5000
26/26 - 1s - loss: 0.5668 - val_loss: 0.9859
Epoch 2848/5000
26/26 - 1s - loss: 0.5663 - val_loss: 0.9879
Epoch 2849/5000
26/26 - 1s - loss: 0.5669 - val_loss: 0.9859
Epoch 2850/5000
26/26 - 1s - loss: 0.5673 - val_loss: 0.9866
Epoch 02850: val_loss improved from 0.98698 to 0.98661, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 2851/5000
26/26 - 1s - loss: 0.5664 - val_loss: 0.9858
Epoch 2852/5000
26/26 - 1s - loss: 0.5667 - val_loss: 0.9871
Epoch 2853/5000
26/26 - 1s - loss: 0.5654 - val_loss: 0.9862
Epoch 2854/5000
26/26 - 2s - loss: 0.5656 - val_loss: 0.9846
Epoch 2855/5000
26/26 - 1s - loss: 0.5645 - val_loss: 0.9849
Epoch 2856/5000
26/26 - 1s - loss: 0.5663 - val_loss: 0.9859
Epoch 2857/5000
26/26 - 1s - loss: 0.5640 - val_loss: 0.9858
Epoch 2858/5000
26/26 - 1s - loss: 0.5647 - val_loss: 0.9859
Epoch 2859/5000
26/26 - 1s - loss: 0.5646 - val_loss: 0.9854
Epoch 2860/5000
26/26 - 1s - loss: 0.5651 - val_loss: 0.9844
Epoch 02860: val_loss improved from 0.98661 to 0.98437, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 2861/5000
26/26 - 1s - loss: 0.5637 - val_loss: 0.9854
Epoch 2862/5000
26/26 - 1s - loss: 0.5647 - val_loss: 0.9852
Epoch 2863/5000
26/26 - 1s - loss: 0.5646 - val_loss: 0.9854
Epoch 2864/5000
26/26 - 1s - loss: 0.5660 - val_loss: 0.9835
Epoch 2865/5000
26/26 - 1s - loss: 0.5640 - val_loss: 0.9846
Epoch 2866/5000
26/26 - 1s - loss: 0.5638 - val_loss: 0.9826
Epoch 2867/5000
26/26 - 1s - loss: 0.5630 - val_loss: 0.9840
Epoch 2868/5000
26/26 - 1s - loss: 0.5630 - val_loss: 0.9834
Epoch 2869/5000
26/26 - 2s - loss: 0.5627 - val_loss: 0.9842
Epoch 2870/5000
26/26 - 1s - loss: 0.5621 - val_loss: 0.9833
Epoch 02870: val_loss improved from 0.98437 to 0.98329, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 2871/5000
26/26 - 1s - loss: 0.5640 - val_loss: 0.9845
Epoch 2872/5000
26/26 - 1s - loss: 0.5611 - val_loss: 0.9841
Epoch 2873/5000
26/26 - 1s - loss: 0.5632 - val_loss: 0.9839
Epoch 2874/5000
26/26 - 1s - loss: 0.5636 - val_loss: 0.9837
Epoch 2875/5000
26/26 - 1s - loss: 0.5620 - val_loss: 0.9828
Epoch 2876/5000
26/26 - 1s - loss: 0.5624 - val_loss: 0.9828
Epoch 2877/5000
26/26 - 1s - loss: 0.5618 - val_loss: 0.9824
Epoch 2878/5000
26/26 - 1s - loss: 0.5628 - val_loss: 0.9837
Epoch 2879/5000
26/26 - 1s - loss: 0.5616 - val_loss: 0.9831
Epoch 2880/5000
26/26 - 1s - loss: 0.5609 - val_loss: 0.9815
Epoch 02880: val_loss improved from 0.98329 to 0.98152, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 2881/5000
26/26 - 1s - loss: 0.5613 - val_loss: 0.9814
Epoch 2882/5000
26/26 - 1s - loss: 0.5611 - val_loss: 0.9807
Epoch 2883/5000
26/26 - 1s - loss: 0.5610 - val_loss: 0.9810
Epoch 2884/5000
26/26 - 1s - loss: 0.5612 - val_loss: 0.9815
Epoch 2885/5000
26/26 - 1s - loss: 0.5608 - val_loss: 0.9805
Epoch 2886/5000
26/26 - 1s - loss: 0.5608 - val_loss: 0.9818
Epoch 2887/5000
26/26 - 1s - loss: 0.5610 - val_loss: 0.9811
Epoch 2888/5000
26/26 - 1s - loss: 0.5605 - val_loss: 0.9816
Epoch 2889/5000
26/26 - 1s - loss: 0.5602 - val_loss: 0.9805
Epoch 2890/5000
26/26 - 1s - loss: 0.5589 - val_loss: 0.9798
Epoch 02890: val_loss improved from 0.98152 to 0.97984, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 2891/5000
26/26 - 1s - loss: 0.5611 - val_loss: 0.9820
Epoch 2892/5000
26/26 - 1s - loss: 0.5596 - val_loss: 0.9800
Epoch 2893/5000
26/26 - 1s - loss: 0.5604 - val_loss: 0.9798
Epoch 2894/5000
26/26 - 1s - loss: 0.5597 - val_loss: 0.9805
Epoch 2895/5000
26/26 - 1s - loss: 0.5593 - val_loss: 0.9797
Epoch 2896/5000
26/26 - 1s - loss: 0.5611 - val_loss: 0.9781
Epoch 2897/5000
26/26 - 1s - loss: 0.5598 - val_loss: 0.9799
Epoch 2898/5000
26/26 - 1s - loss: 0.5580 - val_loss: 0.9792
Epoch 2899/5000
26/26 - 1s - loss: 0.5587 - val_loss: 0.9776
Epoch 2900/5000
26/26 - 1s - loss: 0.5594 - val_loss: 0.9788
Epoch 02900: val_loss improved from 0.97984 to 0.97880, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 2901/5000
26/26 - 1s - loss: 0.5588 - val_loss: 0.9782
Epoch 2902/5000
26/26 - 1s - loss: 0.5589 - val_loss: 0.9791
Epoch 2903/5000
26/26 - 1s - loss: 0.5585 - val_loss: 0.9786
Epoch 2904/5000
26/26 - 1s - loss: 0.5589 - val_loss: 0.9772
Epoch 2905/5000
26/26 - 1s - loss: 0.5574 - val_loss: 0.9778
Epoch 2906/5000
26/26 - 1s - loss: 0.5572 - val_loss: 0.9793
Epoch 2907/5000
26/26 - 1s - loss: 0.5571 - val_loss: 0.9784
Epoch 2908/5000
26/26 - 1s - loss: 0.5584 - val_loss: 0.9779
Epoch 2909/5000
26/26 - 1s - loss: 0.5577 - val_loss: 0.9774
Epoch 2910/5000
26/26 - 1s - loss: 0.5575 - val_loss: 0.9789
Epoch 02910: val_loss did not improve from 0.97880
Epoch 2911/5000
26/26 - 1s - loss: 0.5561 - val_loss: 0.9780
Epoch 2912/5000
26/26 - 1s - loss: 0.5556 - val_loss: 0.9778
Epoch 2913/5000
26/26 - 1s - loss: 0.5563 - val_loss: 0.9759
Epoch 2914/5000
26/26 - 1s - loss: 0.5570 - val_loss: 0.9763
Epoch 2915/5000
26/26 - 1s - loss: 0.5560 - val_loss: 0.9762
Epoch 2916/5000
26/26 - 1s - loss: 0.5560 - val_loss: 0.9763
Epoch 2917/5000
26/26 - 1s - loss: 0.5563 - val_loss: 0.9761
Epoch 2918/5000
26/26 - 1s - loss: 0.5554 - val_loss: 0.9763
Epoch 2919/5000
26/26 - 1s - loss: 0.5554 - val_loss: 0.9756
Epoch 2920/5000
26/26 - 1s - loss: 0.5558 - val_loss: 0.9747
Epoch 02920: val_loss improved from 0.97880 to 0.97466, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 2921/5000
26/26 - 1s - loss: 0.5572 - val_loss: 0.9750
Epoch 2922/5000
26/26 - 1s - loss: 0.5560 - val_loss: 0.9750
Epoch 2923/5000
26/26 - 1s - loss: 0.5534 - val_loss: 0.9747
Epoch 2924/5000
26/26 - 1s - loss: 0.5550 - val_loss: 0.9758
Epoch 2925/5000
26/26 - 1s - loss: 0.5557 - val_loss: 0.9762
Epoch 2926/5000
26/26 - 1s - loss: 0.5541 - val_loss: 0.9750
Epoch 2927/5000
26/26 - 1s - loss: 0.5554 - val_loss: 0.9755
Epoch 2928/5000
26/26 - 1s - loss: 0.5547 - val_loss: 0.9756
Epoch 2929/5000
26/26 - 1s - loss: 0.5543 - val_loss: 0.9763
Epoch 2930/5000
26/26 - 1s - loss: 0.5550 - val_loss: 0.9742
Epoch 02930: val_loss improved from 0.97466 to 0.97419, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 2931/5000
26/26 - 1s - loss: 0.5527 - val_loss: 0.9746
Epoch 2932/5000
26/26 - 1s - loss: 0.5530 - val_loss: 0.9742
Epoch 2933/5000
26/26 - 1s - loss: 0.5527 - val_loss: 0.9735
Epoch 2934/5000
26/26 - 1s - loss: 0.5536 - val_loss: 0.9737
Epoch 2935/5000
26/26 - 1s - loss: 0.5532 - val_loss: 0.9728
Epoch 2936/5000
26/26 - 1s - loss: 0.5545 - val_loss: 0.9730
Epoch 2937/5000
26/26 - 1s - loss: 0.5527 - val_loss: 0.9729
Epoch 2938/5000
26/26 - 1s - loss: 0.5526 - val_loss: 0.9725
Epoch 2939/5000
26/26 - 1s - loss: 0.5527 - val_loss: 0.9724
Epoch 2940/5000
26/26 - 1s - loss: 0.5540 - val_loss: 0.9728
Epoch 02940: val_loss improved from 0.97419 to 0.97285, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 2941/5000
26/26 - 1s - loss: 0.5531 - val_loss: 0.9733
Epoch 2942/5000
26/26 - 1s - loss: 0.5527 - val_loss: 0.9723
Epoch 2943/5000
26/26 - 1s - loss: 0.5518 - val_loss: 0.9741
Epoch 2944/5000
26/26 - 1s - loss: 0.5523 - val_loss: 0.9721
Epoch 2945/5000
26/26 - 1s - loss: 0.5515 - val_loss: 0.9730
Epoch 2946/5000
26/26 - 1s - loss: 0.5518 - val_loss: 0.9720
Epoch 2947/5000
26/26 - 1s - loss: 0.5509 - val_loss: 0.9728
Epoch 2948/5000
26/26 - 1s - loss: 0.5521 - val_loss: 0.9713
Epoch 2949/5000
26/26 - 1s - loss: 0.5537 - val_loss: 0.9700
Epoch 2950/5000
26/26 - 1s - loss: 0.5513 - val_loss: 0.9715
Epoch 02950: val_loss improved from 0.97285 to 0.97154, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 2951/5000
26/26 - 1s - loss: 0.5507 - val_loss: 0.9714
Epoch 2952/5000
26/26 - 1s - loss: 0.5506 - val_loss: 0.9711
Epoch 2953/5000
26/26 - 1s - loss: 0.5515 - val_loss: 0.9718
Epoch 2954/5000
26/26 - 1s - loss: 0.5505 - val_loss: 0.9702
Epoch 2955/5000
26/26 - 1s - loss: 0.5494 - val_loss: 0.9703
Epoch 2956/5000
26/26 - 1s - loss: 0.5505 - val_loss: 0.9719
Epoch 2957/5000
26/26 - 1s - loss: 0.5508 - val_loss: 0.9708
Epoch 2958/5000
26/26 - 1s - loss: 0.5507 - val_loss: 0.9701
Epoch 2959/5000
26/26 - 1s - loss: 0.5491 - val_loss: 0.9708
Epoch 2960/5000
26/26 - 1s - loss: 0.5495 - val_loss: 0.9703
Epoch 02960: val_loss improved from 0.97154 to 0.97029, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 2961/5000
26/26 - 1s - loss: 0.5493 - val_loss: 0.9697
Epoch 2962/5000
26/26 - 1s - loss: 0.5495 - val_loss: 0.9696
Epoch 2963/5000
26/26 - 1s - loss: 0.5491 - val_loss: 0.9708
Epoch 2964/5000
26/26 - 1s - loss: 0.5496 - val_loss: 0.9701
Epoch 2965/5000
26/26 - 1s - loss: 0.5507 - val_loss: 0.9698
Epoch 2966/5000
26/26 - 1s - loss: 0.5496 - val_loss: 0.9677
Epoch 2967/5000
26/26 - 1s - loss: 0.5483 - val_loss: 0.9701
Epoch 2968/5000
26/26 - 1s - loss: 0.5486 - val_loss: 0.9681
Epoch 2969/5000
26/26 - 1s - loss: 0.5478 - val_loss: 0.9684
Epoch 2970/5000
26/26 - 1s - loss: 0.5481 - val_loss: 0.9683
Epoch 02970: val_loss improved from 0.97029 to 0.96834, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 2971/5000
26/26 - 1s - loss: 0.5473 - val_loss: 0.9693
Epoch 2972/5000
26/26 - 1s - loss: 0.5479 - val_loss: 0.9696
Epoch 2973/5000
26/26 - 1s - loss: 0.5477 - val_loss: 0.9691
Epoch 2974/5000
26/26 - 1s - loss: 0.5477 - val_loss: 0.9680
Epoch 2975/5000
26/26 - 1s - loss: 0.5479 - val_loss: 0.9672
Epoch 2976/5000
26/26 - 1s - loss: 0.5473 - val_loss: 0.9676
Epoch 2977/5000
26/26 - 1s - loss: 0.5475 - val_loss: 0.9693
Epoch 2978/5000
26/26 - 1s - loss: 0.5472 - val_loss: 0.9697
Epoch 2979/5000
26/26 - 1s - loss: 0.5471 - val_loss: 0.9684
Epoch 2980/5000
26/26 - 1s - loss: 0.5470 - val_loss: 0.9672
Epoch 02980: val_loss improved from 0.96834 to 0.96719, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 2981/5000
26/26 - 1s - loss: 0.5472 - val_loss: 0.9674
Epoch 2982/5000
26/26 - 1s - loss: 0.5457 - val_loss: 0.9669
Epoch 2983/5000
26/26 - 1s - loss: 0.5465 - val_loss: 0.9665
Epoch 2984/5000
26/26 - 1s - loss: 0.5462 - val_loss: 0.9676
Epoch 2985/5000
26/26 - 1s - loss: 0.5460 - val_loss: 0.9683
Epoch 2986/5000
26/26 - 1s - loss: 0.5467 - val_loss: 0.9666
Epoch 2987/5000
26/26 - 1s - loss: 0.5464 - val_loss: 0.9665
Epoch 2988/5000
26/26 - 1s - loss: 0.5451 - val_loss: 0.9667
Epoch 2989/5000
26/26 - 1s - loss: 0.5456 - val_loss: 0.9658
Epoch 2990/5000
26/26 - 1s - loss: 0.5450 - val_loss: 0.9667
Epoch 02990: val_loss improved from 0.96719 to 0.96673, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 2991/5000
26/26 - 1s - loss: 0.5463 - val_loss: 0.9665
Epoch 2992/5000
26/26 - 1s - loss: 0.5457 - val_loss: 0.9657
Epoch 2993/5000
26/26 - 1s - loss: 0.5457 - val_loss: 0.9659
Epoch 2994/5000
26/26 - 1s - loss: 0.5455 - val_loss: 0.9651
Epoch 2995/5000
26/26 - 1s - loss: 0.5436 - val_loss: 0.9655
Epoch 2996/5000
26/26 - 1s - loss: 0.5446 - val_loss: 0.9656
Epoch 2997/5000
26/26 - 1s - loss: 0.5448 - val_loss: 0.9673
Epoch 2998/5000
26/26 - 1s - loss: 0.5436 - val_loss: 0.9659
Epoch 2999/5000
26/26 - 1s - loss: 0.5443 - val_loss: 0.9647
Epoch 3000/5000
26/26 - 1s - loss: 0.5431 - val_loss: 0.9651
Epoch 03000: val_loss improved from 0.96673 to 0.96507, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 3001/5000
26/26 - 1s - loss: 0.5441 - val_loss: 0.9639
Epoch 3002/5000
26/26 - 1s - loss: 0.5453 - val_loss: 0.9658
Epoch 3003/5000
26/26 - 1s - loss: 0.5453 - val_loss: 0.9648
Epoch 3004/5000
26/26 - 1s - loss: 0.5426 - val_loss: 0.9631
Epoch 3005/5000
26/26 - 1s - loss: 0.5437 - val_loss: 0.9648
Epoch 3006/5000
26/26 - 1s - loss: 0.5429 - val_loss: 0.9646
Epoch 3007/5000
26/26 - 1s - loss: 0.5438 - val_loss: 0.9647
Epoch 3008/5000
26/26 - 2s - loss: 0.5423 - val_loss: 0.9643
Epoch 3009/5000
26/26 - 2s - loss: 0.5438 - val_loss: 0.9642
Epoch 3010/5000
26/26 - 1s - loss: 0.5430 - val_loss: 0.9645
Epoch 03010: val_loss improved from 0.96507 to 0.96452, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 3011/5000
26/26 - 1s - loss: 0.5429 - val_loss: 0.9631
Epoch 3012/5000
26/26 - 1s - loss: 0.5429 - val_loss: 0.9621
Epoch 3013/5000
26/26 - 1s - loss: 0.5420 - val_loss: 0.9631
Epoch 3014/5000
26/26 - 1s - loss: 0.5419 - val_loss: 0.9641
Epoch 3015/5000
26/26 - 1s - loss: 0.5414 - val_loss: 0.9634
Epoch 3016/5000
26/26 - 1s - loss: 0.5421 - val_loss: 0.9622
Epoch 3017/5000
26/26 - 1s - loss: 0.5416 - val_loss: 0.9633
Epoch 3018/5000
26/26 - 1s - loss: 0.5426 - val_loss: 0.9640
Epoch 3019/5000
26/26 - 1s - loss: 0.5428 - val_loss: 0.9629
Epoch 3020/5000
26/26 - 1s - loss: 0.5403 - val_loss: 0.9641
Epoch 03020: val_loss improved from 0.96452 to 0.96414, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 3021/5000
26/26 - 1s - loss: 0.5406 - val_loss: 0.9628
Epoch 3022/5000
26/26 - 1s - loss: 0.5417 - val_loss: 0.9619
Epoch 3023/5000
26/26 - 1s - loss: 0.5417 - val_loss: 0.9621
Epoch 3024/5000
26/26 - 1s - loss: 0.5410 - val_loss: 0.9616
Epoch 3025/5000
26/26 - 1s - loss: 0.5398 - val_loss: 0.9618
Epoch 3026/5000
26/26 - 1s - loss: 0.5411 - val_loss: 0.9608
Epoch 3027/5000
26/26 - 1s - loss: 0.5405 - val_loss: 0.9616
Epoch 3028/5000
26/26 - 1s - loss: 0.5405 - val_loss: 0.9621
Epoch 3029/5000
26/26 - 1s - loss: 0.5408 - val_loss: 0.9616
Epoch 3030/5000
26/26 - 1s - loss: 0.5399 - val_loss: 0.9631
Epoch 03030: val_loss improved from 0.96414 to 0.96310, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 3031/5000
26/26 - 1s - loss: 0.5409 - val_loss: 0.9603
Epoch 3032/5000
26/26 - 1s - loss: 0.5397 - val_loss: 0.9607
Epoch 3033/5000
26/26 - 1s - loss: 0.5402 - val_loss: 0.9603
Epoch 3034/5000
26/26 - 1s - loss: 0.5392 - val_loss: 0.9605
Epoch 3035/5000
26/26 - 1s - loss: 0.5404 - val_loss: 0.9592
Epoch 3036/5000
26/26 - 1s - loss: 0.5402 - val_loss: 0.9593
Epoch 3037/5000
26/26 - 1s - loss: 0.5386 - val_loss: 0.9585
Epoch 3038/5000
26/26 - 1s - loss: 0.5392 - val_loss: 0.9596
Epoch 3039/5000
26/26 - 1s - loss: 0.5387 - val_loss: 0.9588
Epoch 3040/5000
26/26 - 1s - loss: 0.5387 - val_loss: 0.9601
Epoch 03040: val_loss improved from 0.96310 to 0.96012, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 3041/5000
26/26 - 1s - loss: 0.5370 - val_loss: 0.9598
Epoch 3042/5000
26/26 - 1s - loss: 0.5392 - val_loss: 0.9575
Epoch 3043/5000
26/26 - 1s - loss: 0.5382 - val_loss: 0.9576
Epoch 3044/5000
26/26 - 1s - loss: 0.5390 - val_loss: 0.9581
Epoch 3045/5000
26/26 - 1s - loss: 0.5385 - val_loss: 0.9597
Epoch 3046/5000
26/26 - 1s - loss: 0.5376 - val_loss: 0.9597
Epoch 3047/5000
26/26 - 1s - loss: 0.5376 - val_loss: 0.9588
Epoch 3048/5000
26/26 - 1s - loss: 0.5380 - val_loss: 0.9585
Epoch 3049/5000
26/26 - 1s - loss: 0.5375 - val_loss: 0.9576
Epoch 3050/5000
26/26 - 1s - loss: 0.5370 - val_loss: 0.9575
Epoch 03050: val_loss improved from 0.96012 to 0.95754, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 3051/5000
26/26 - 1s - loss: 0.5384 - val_loss: 0.9582
Epoch 3052/5000
26/26 - 1s - loss: 0.5379 - val_loss: 0.9567
Epoch 3053/5000
26/26 - 1s - loss: 0.5365 - val_loss: 0.9562
Epoch 3054/5000
26/26 - 1s - loss: 0.5380 - val_loss: 0.9566
Epoch 3055/5000
26/26 - 1s - loss: 0.5380 - val_loss: 0.9580
Epoch 3056/5000
26/26 - 1s - loss: 0.5359 - val_loss: 0.9562
Epoch 3057/5000
26/26 - 1s - loss: 0.5362 - val_loss: 0.9571
Epoch 3058/5000
26/26 - 1s - loss: 0.5354 - val_loss: 0.9587
Epoch 3059/5000
26/26 - 1s - loss: 0.5360 - val_loss: 0.9571
Epoch 3060/5000
26/26 - 1s - loss: 0.5361 - val_loss: 0.9567
Epoch 03060: val_loss improved from 0.95754 to 0.95667, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 3061/5000
26/26 - 1s - loss: 0.5357 - val_loss: 0.9570
Epoch 3062/5000
26/26 - 1s - loss: 0.5355 - val_loss: 0.9568
Epoch 3063/5000
26/26 - 1s - loss: 0.5360 - val_loss: 0.9566
Epoch 3064/5000
26/26 - 1s - loss: 0.5348 - val_loss: 0.9586
Epoch 3065/5000
26/26 - 1s - loss: 0.5363 - val_loss: 0.9553
Epoch 3066/5000
26/26 - 1s - loss: 0.5361 - val_loss: 0.9565
Epoch 3067/5000
26/26 - 1s - loss: 0.5348 - val_loss: 0.9562
Epoch 3068/5000
26/26 - 1s - loss: 0.5352 - val_loss: 0.9557
Epoch 3069/5000
26/26 - 1s - loss: 0.5359 - val_loss: 0.9563
Epoch 3070/5000
26/26 - 1s - loss: 0.5341 - val_loss: 0.9557
Epoch 03070: val_loss improved from 0.95667 to 0.95570, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 3071/5000
26/26 - 1s - loss: 0.5355 - val_loss: 0.9554
Epoch 3072/5000
26/26 - 1s - loss: 0.5348 - val_loss: 0.9549
Epoch 3073/5000
26/26 - 1s - loss: 0.5347 - val_loss: 0.9550
Epoch 3074/5000
26/26 - 1s - loss: 0.5339 - val_loss: 0.9563
Epoch 3075/5000
26/26 - 1s - loss: 0.5341 - val_loss: 0.9553
Epoch 3076/5000
26/26 - 1s - loss: 0.5333 - val_loss: 0.9542
Epoch 3077/5000
26/26 - 1s - loss: 0.5328 - val_loss: 0.9540
Epoch 3078/5000
26/26 - 1s - loss: 0.5334 - val_loss: 0.9544
Epoch 3079/5000
26/26 - 1s - loss: 0.5339 - val_loss: 0.9537
Epoch 3080/5000
26/26 - 1s - loss: 0.5332 - val_loss: 0.9561
Epoch 03080: val_loss did not improve from 0.95570
Epoch 3081/5000
26/26 - 1s - loss: 0.5332 - val_loss: 0.9548
Epoch 3082/5000
26/26 - 1s - loss: 0.5334 - val_loss: 0.9537
Epoch 3083/5000
26/26 - 1s - loss: 0.5318 - val_loss: 0.9540
Epoch 3084/5000
26/26 - 1s - loss: 0.5335 - val_loss: 0.9544
Epoch 3085/5000
26/26 - 1s - loss: 0.5331 - val_loss: 0.9542
Epoch 3086/5000
26/26 - 1s - loss: 0.5325 - val_loss: 0.9535
Epoch 3087/5000
26/26 - 1s - loss: 0.5325 - val_loss: 0.9525
Epoch 3088/5000
26/26 - 1s - loss: 0.5329 - val_loss: 0.9532
Epoch 3089/5000
26/26 - 1s - loss: 0.5325 - val_loss: 0.9524
Epoch 3090/5000
26/26 - 1s - loss: 0.5321 - val_loss: 0.9517
Epoch 03090: val_loss improved from 0.95570 to 0.95169, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 3091/5000
26/26 - 1s - loss: 0.5311 - val_loss: 0.9518
Epoch 3092/5000
26/26 - 2s - loss: 0.5315 - val_loss: 0.9518
Epoch 3093/5000
26/26 - 1s - loss: 0.5317 - val_loss: 0.9512
Epoch 3094/5000
26/26 - 1s - loss: 0.5315 - val_loss: 0.9530
Epoch 3095/5000
26/26 - 1s - loss: 0.5315 - val_loss: 0.9516
Epoch 3096/5000
26/26 - 1s - loss: 0.5318 - val_loss: 0.9529
Epoch 3097/5000
26/26 - 1s - loss: 0.5319 - val_loss: 0.9538
Epoch 3098/5000
26/26 - 1s - loss: 0.5316 - val_loss: 0.9537
Epoch 3099/5000
26/26 - 1s - loss: 0.5312 - val_loss: 0.9527
Epoch 3100/5000
26/26 - 1s - loss: 0.5319 - val_loss: 0.9528
Epoch 03100: val_loss did not improve from 0.95169
Epoch 3101/5000
26/26 - 1s - loss: 0.5313 - val_loss: 0.9504
Epoch 3102/5000
26/26 - 1s - loss: 0.5309 - val_loss: 0.9518
Epoch 3103/5000
26/26 - 1s - loss: 0.5307 - val_loss: 0.9508
Epoch 3104/5000
26/26 - 1s - loss: 0.5312 - val_loss: 0.9520
Epoch 3105/5000
26/26 - 1s - loss: 0.5309 - val_loss: 0.9523
Epoch 3106/5000
26/26 - 1s - loss: 0.5310 - val_loss: 0.9521
Epoch 3107/5000
26/26 - 1s - loss: 0.5299 - val_loss: 0.9522
Epoch 3108/5000
26/26 - 1s - loss: 0.5300 - val_loss: 0.9514
Epoch 3109/5000
26/26 - 1s - loss: 0.5296 - val_loss: 0.9521
Epoch 3110/5000
26/26 - 1s - loss: 0.5302 - val_loss: 0.9523
Epoch 03110: val_loss did not improve from 0.95169
Epoch 3111/5000
26/26 - 1s - loss: 0.5298 - val_loss: 0.9514
Epoch 3112/5000
26/26 - 1s - loss: 0.5294 - val_loss: 0.9501
Epoch 3113/5000
26/26 - 1s - loss: 0.5280 - val_loss: 0.9503
Epoch 3114/5000
26/26 - 1s - loss: 0.5301 - val_loss: 0.9507
Epoch 3115/5000
26/26 - 1s - loss: 0.5300 - val_loss: 0.9499
Epoch 3116/5000
26/26 - 1s - loss: 0.5299 - val_loss: 0.9506
Epoch 3117/5000
26/26 - 1s - loss: 0.5290 - val_loss: 0.9519
Epoch 3118/5000
26/26 - 2s - loss: 0.5284 - val_loss: 0.9495
Epoch 3119/5000
26/26 - 1s - loss: 0.5297 - val_loss: 0.9494
Epoch 3120/5000
26/26 - 1s - loss: 0.5281 - val_loss: 0.9506
Epoch 03120: val_loss improved from 0.95169 to 0.95065, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 3121/5000
26/26 - 1s - loss: 0.5281 - val_loss: 0.9493
Epoch 3122/5000
26/26 - 1s - loss: 0.5288 - val_loss: 0.9483
Epoch 3123/5000
26/26 - 1s - loss: 0.5280 - val_loss: 0.9492
Epoch 3124/5000
26/26 - 1s - loss: 0.5282 - val_loss: 0.9492
Epoch 3125/5000
26/26 - 1s - loss: 0.5282 - val_loss: 0.9484
Epoch 3126/5000
26/26 - 1s - loss: 0.5289 - val_loss: 0.9482
Epoch 3127/5000
26/26 - 1s - loss: 0.5275 - val_loss: 0.9498
Epoch 3128/5000
26/26 - 1s - loss: 0.5283 - val_loss: 0.9476
Epoch 3129/5000
26/26 - 1s - loss: 0.5273 - val_loss: 0.9479
Epoch 3130/5000
26/26 - 1s - loss: 0.5275 - val_loss: 0.9468
Epoch 03130: val_loss improved from 0.95065 to 0.94678, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 3131/5000
26/26 - 1s - loss: 0.5267 - val_loss: 0.9477
Epoch 3132/5000
26/26 - 1s - loss: 0.5279 - val_loss: 0.9478
Epoch 3133/5000
26/26 - 2s - loss: 0.5274 - val_loss: 0.9481
Epoch 3134/5000
26/26 - 1s - loss: 0.5256 - val_loss: 0.9474
Epoch 3135/5000
26/26 - 1s - loss: 0.5263 - val_loss: 0.9487
Epoch 3136/5000
26/26 - 1s - loss: 0.5270 - val_loss: 0.9477
Epoch 3137/5000
26/26 - 1s - loss: 0.5269 - val_loss: 0.9477
Epoch 3138/5000
26/26 - 1s - loss: 0.5260 - val_loss: 0.9485
Epoch 3139/5000
26/26 - 1s - loss: 0.5259 - val_loss: 0.9494
Epoch 3140/5000
26/26 - 1s - loss: 0.5264 - val_loss: 0.9480
Epoch 03140: val_loss did not improve from 0.94678
Epoch 3141/5000
26/26 - 1s - loss: 0.5262 - val_loss: 0.9476
Epoch 3142/5000
26/26 - 1s - loss: 0.5258 - val_loss: 0.9474
Epoch 3143/5000
26/26 - 1s - loss: 0.5253 - val_loss: 0.9454
Epoch 3144/5000
26/26 - 1s - loss: 0.5262 - val_loss: 0.9465
Epoch 3145/5000
26/26 - 1s - loss: 0.5260 - val_loss: 0.9460
Epoch 3146/5000
26/26 - 1s - loss: 0.5252 - val_loss: 0.9456
Epoch 3147/5000
26/26 - 1s - loss: 0.5257 - val_loss: 0.9451
Epoch 3148/5000
26/26 - 1s - loss: 0.5257 - val_loss: 0.9470
Epoch 3149/5000
26/26 - 1s - loss: 0.5237 - val_loss: 0.9469
Epoch 3150/5000
26/26 - 1s - loss: 0.5247 - val_loss: 0.9454
Epoch 03150: val_loss improved from 0.94678 to 0.94542, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 3151/5000
26/26 - 1s - loss: 0.5253 - val_loss: 0.9468
Epoch 3152/5000
26/26 - 1s - loss: 0.5257 - val_loss: 0.9448
Epoch 3153/5000
26/26 - 1s - loss: 0.5246 - val_loss: 0.9449
Epoch 3154/5000
26/26 - 1s - loss: 0.5256 - val_loss: 0.9463
Epoch 3155/5000
26/26 - 1s - loss: 0.5243 - val_loss: 0.9459
Epoch 3156/5000
26/26 - 1s - loss: 0.5237 - val_loss: 0.9450
Epoch 3157/5000
26/26 - 1s - loss: 0.5234 - val_loss: 0.9454
Epoch 3158/5000
26/26 - 1s - loss: 0.5231 - val_loss: 0.9450
Epoch 3159/5000
26/26 - 1s - loss: 0.5235 - val_loss: 0.9457
Epoch 3160/5000
26/26 - 1s - loss: 0.5233 - val_loss: 0.9455
Epoch 03160: val_loss did not improve from 0.94542
Epoch 3161/5000
26/26 - 1s - loss: 0.5227 - val_loss: 0.9463
Epoch 3162/5000
26/26 - 1s - loss: 0.5233 - val_loss: 0.9436
Epoch 3163/5000
26/26 - 1s - loss: 0.5236 - val_loss: 0.9450
Epoch 3164/5000
26/26 - 1s - loss: 0.5230 - val_loss: 0.9444
Epoch 3165/5000
26/26 - 1s - loss: 0.5234 - val_loss: 0.9427
Epoch 3166/5000
26/26 - 1s - loss: 0.5242 - val_loss: 0.9431
Epoch 3167/5000
26/26 - 1s - loss: 0.5231 - val_loss: 0.9429
Epoch 3168/5000
26/26 - 1s - loss: 0.5226 - val_loss: 0.9440
Epoch 3169/5000
26/26 - 1s - loss: 0.5221 - val_loss: 0.9426
Epoch 3170/5000
26/26 - 1s - loss: 0.5216 - val_loss: 0.9439
Epoch 03170: val_loss improved from 0.94542 to 0.94386, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 3171/5000
26/26 - 1s - loss: 0.5235 - val_loss: 0.9437
Epoch 3172/5000
26/26 - 1s - loss: 0.5229 - val_loss: 0.9424
Epoch 3173/5000
26/26 - 1s - loss: 0.5219 - val_loss: 0.9422
Epoch 3174/5000
26/26 - 1s - loss: 0.5216 - val_loss: 0.9436
Epoch 3175/5000
26/26 - 1s - loss: 0.5232 - val_loss: 0.9427
Epoch 3176/5000
26/26 - 1s - loss: 0.5219 - val_loss: 0.9426
Epoch 3177/5000
26/26 - 1s - loss: 0.5223 - val_loss: 0.9437
Epoch 3178/5000
26/26 - 1s - loss: 0.5213 - val_loss: 0.9435
Epoch 3179/5000
26/26 - 1s - loss: 0.5220 - val_loss: 0.9434
Epoch 3180/5000
26/26 - 2s - loss: 0.5220 - val_loss: 0.9443
Epoch 03180: val_loss did not improve from 0.94386
Epoch 3181/5000
26/26 - 1s - loss: 0.5210 - val_loss: 0.9427
Epoch 3182/5000
26/26 - 1s - loss: 0.5213 - val_loss: 0.9421
Epoch 3183/5000
26/26 - 1s - loss: 0.5206 - val_loss: 0.9428
Epoch 3184/5000
26/26 - 1s - loss: 0.5203 - val_loss: 0.9418
Epoch 3185/5000
26/26 - 1s - loss: 0.5203 - val_loss: 0.9430
Epoch 3186/5000
26/26 - 1s - loss: 0.5199 - val_loss: 0.9422
Epoch 3187/5000
26/26 - 1s - loss: 0.5207 - val_loss: 0.9411
Epoch 3188/5000
26/26 - 1s - loss: 0.5216 - val_loss: 0.9438
Epoch 3189/5000
26/26 - 1s - loss: 0.5213 - val_loss: 0.9429
Epoch 3190/5000
26/26 - 1s - loss: 0.5201 - val_loss: 0.9417
Epoch 03190: val_loss improved from 0.94386 to 0.94166, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 3191/5000
26/26 - 1s - loss: 0.5194 - val_loss: 0.9413
Epoch 3192/5000
26/26 - 1s - loss: 0.5208 - val_loss: 0.9405
Epoch 3193/5000
26/26 - 1s - loss: 0.5183 - val_loss: 0.9407
Epoch 3194/5000
26/26 - 1s - loss: 0.5208 - val_loss: 0.9416
Epoch 3195/5000
26/26 - 1s - loss: 0.5202 - val_loss: 0.9409
Epoch 3196/5000
26/26 - 1s - loss: 0.5193 - val_loss: 0.9402
Epoch 3197/5000
26/26 - 1s - loss: 0.5193 - val_loss: 0.9414
Epoch 3198/5000
26/26 - 1s - loss: 0.5187 - val_loss: 0.9407
Epoch 3199/5000
26/26 - 1s - loss: 0.5200 - val_loss: 0.9393
Epoch 3200/5000
26/26 - 1s - loss: 0.5185 - val_loss: 0.9411
Epoch 03200: val_loss improved from 0.94166 to 0.94114, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 3201/5000
26/26 - 1s - loss: 0.5179 - val_loss: 0.9390
Epoch 3202/5000
26/26 - 1s - loss: 0.5182 - val_loss: 0.9412
Epoch 3203/5000
26/26 - 1s - loss: 0.5188 - val_loss: 0.9399
Epoch 3204/5000
26/26 - 1s - loss: 0.5201 - val_loss: 0.9395
Epoch 3205/5000
26/26 - 1s - loss: 0.5191 - val_loss: 0.9398
Epoch 3206/5000
26/26 - 1s - loss: 0.5174 - val_loss: 0.9395
Epoch 3207/5000
26/26 - 1s - loss: 0.5179 - val_loss: 0.9399
Epoch 3208/5000
26/26 - 1s - loss: 0.5178 - val_loss: 0.9393
Epoch 3209/5000
26/26 - 1s - loss: 0.5184 - val_loss: 0.9400
Epoch 3210/5000
26/26 - 1s - loss: 0.5188 - val_loss: 0.9393
Epoch 03210: val_loss improved from 0.94114 to 0.93925, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 3211/5000
26/26 - 1s - loss: 0.5161 - val_loss: 0.9402
Epoch 3212/5000
26/26 - 1s - loss: 0.5166 - val_loss: 0.9391
Epoch 3213/5000
26/26 - 2s - loss: 0.5172 - val_loss: 0.9388
Epoch 3214/5000
26/26 - 1s - loss: 0.5185 - val_loss: 0.9379
Epoch 3215/5000
26/26 - 1s - loss: 0.5173 - val_loss: 0.9372
Epoch 3216/5000
26/26 - 1s - loss: 0.5175 - val_loss: 0.9393
Epoch 3217/5000
26/26 - 1s - loss: 0.5164 - val_loss: 0.9385
Epoch 3218/5000
26/26 - 1s - loss: 0.5150 - val_loss: 0.9382
Epoch 3219/5000
26/26 - 1s - loss: 0.5147 - val_loss: 0.9369
Epoch 3220/5000
26/26 - 1s - loss: 0.5178 - val_loss: 0.9390
Epoch 03220: val_loss improved from 0.93925 to 0.93903, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 3221/5000
26/26 - 1s - loss: 0.5163 - val_loss: 0.9390
Epoch 3222/5000
26/26 - 1s - loss: 0.5164 - val_loss: 0.9376
Epoch 3223/5000
26/26 - 1s - loss: 0.5159 - val_loss: 0.9380
Epoch 3224/5000
26/26 - 1s - loss: 0.5161 - val_loss: 0.9379
Epoch 3225/5000
26/26 - 1s - loss: 0.5158 - val_loss: 0.9373
Epoch 3226/5000
26/26 - 1s - loss: 0.5158 - val_loss: 0.9376
Epoch 3227/5000
26/26 - 1s - loss: 0.5148 - val_loss: 0.9369
Epoch 3228/5000
26/26 - 1s - loss: 0.5157 - val_loss: 0.9373
Epoch 3229/5000
26/26 - 1s - loss: 0.5153 - val_loss: 0.9370
Epoch 3230/5000
26/26 - 1s - loss: 0.5152 - val_loss: 0.9365
Epoch 03230: val_loss improved from 0.93903 to 0.93648, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 3231/5000
26/26 - 1s - loss: 0.5160 - val_loss: 0.9362
Epoch 3232/5000
26/26 - 1s - loss: 0.5145 - val_loss: 0.9370
Epoch 3233/5000
26/26 - 1s - loss: 0.5154 - val_loss: 0.9375
Epoch 3234/5000
26/26 - 1s - loss: 0.5149 - val_loss: 0.9355
Epoch 3235/5000
26/26 - 1s - loss: 0.5150 - val_loss: 0.9349
Epoch 3236/5000
26/26 - 1s - loss: 0.5145 - val_loss: 0.9360
Epoch 3237/5000
26/26 - 1s - loss: 0.5155 - val_loss: 0.9348
Epoch 3238/5000
26/26 - 1s - loss: 0.5150 - val_loss: 0.9365
Epoch 3239/5000
26/26 - 1s - loss: 0.5147 - val_loss: 0.9368
Epoch 3240/5000
26/26 - 1s - loss: 0.5155 - val_loss: 0.9358
Epoch 03240: val_loss improved from 0.93648 to 0.93583, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 3241/5000
26/26 - 1s - loss: 0.5142 - val_loss: 0.9364
Epoch 3242/5000
26/26 - 1s - loss: 0.5145 - val_loss: 0.9363
Epoch 3243/5000
26/26 - 1s - loss: 0.5141 - val_loss: 0.9359
Epoch 3244/5000
26/26 - 1s - loss: 0.5132 - val_loss: 0.9350
Epoch 3245/5000
26/26 - 1s - loss: 0.5132 - val_loss: 0.9372
Epoch 3246/5000
26/26 - 1s - loss: 0.5138 - val_loss: 0.9351
Epoch 3247/5000
26/26 - 1s - loss: 0.5140 - val_loss: 0.9343
Epoch 3248/5000
26/26 - 1s - loss: 0.5142 - val_loss: 0.9350
Epoch 3249/5000
26/26 - 1s - loss: 0.5130 - val_loss: 0.9343
Epoch 3250/5000
26/26 - 1s - loss: 0.5132 - val_loss: 0.9347
Epoch 03250: val_loss improved from 0.93583 to 0.93474, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 3251/5000
26/26 - 1s - loss: 0.5122 - val_loss: 0.9354
Epoch 3252/5000
26/26 - 1s - loss: 0.5146 - val_loss: 0.9345
Epoch 3253/5000
26/26 - 1s - loss: 0.5120 - val_loss: 0.9339
Epoch 3254/5000
26/26 - 1s - loss: 0.5127 - val_loss: 0.9355
Epoch 3255/5000
26/26 - 1s - loss: 0.5136 - val_loss: 0.9347
Epoch 3256/5000
26/26 - 1s - loss: 0.5135 - val_loss: 0.9331
Epoch 3257/5000
26/26 - 1s - loss: 0.5124 - val_loss: 0.9347
Epoch 3258/5000
26/26 - 1s - loss: 0.5124 - val_loss: 0.9345
Epoch 3259/5000
26/26 - 1s - loss: 0.5129 - val_loss: 0.9346
Epoch 3260/5000
26/26 - 1s - loss: 0.5130 - val_loss: 0.9349
Epoch 03260: val_loss did not improve from 0.93474
Epoch 3261/5000
26/26 - 1s - loss: 0.5112 - val_loss: 0.9350
Epoch 3262/5000
26/26 - 1s - loss: 0.5118 - val_loss: 0.9345
Epoch 3263/5000
26/26 - 1s - loss: 0.5122 - val_loss: 0.9328
Epoch 3264/5000
26/26 - 1s - loss: 0.5112 - val_loss: 0.9338
Epoch 3265/5000
26/26 - 1s - loss: 0.5127 - val_loss: 0.9337
Epoch 3266/5000
26/26 - 1s - loss: 0.5117 - val_loss: 0.9338
Epoch 3267/5000
26/26 - 1s - loss: 0.5125 - val_loss: 0.9318
Epoch 3268/5000
26/26 - 1s - loss: 0.5116 - val_loss: 0.9334
Epoch 3269/5000
26/26 - 1s - loss: 0.5118 - val_loss: 0.9332
Epoch 3270/5000
26/26 - 1s - loss: 0.5118 - val_loss: 0.9341
Epoch 03270: val_loss improved from 0.93474 to 0.93406, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 3271/5000
26/26 - 1s - loss: 0.5110 - val_loss: 0.9314
Epoch 3272/5000
26/26 - 1s - loss: 0.5102 - val_loss: 0.9329
Epoch 3273/5000
26/26 - 1s - loss: 0.5121 - val_loss: 0.9321
Epoch 3274/5000
26/26 - 1s - loss: 0.5106 - val_loss: 0.9314
Epoch 3275/5000
26/26 - 1s - loss: 0.5095 - val_loss: 0.9329
Epoch 3276/5000
26/26 - 1s - loss: 0.5105 - val_loss: 0.9327
Epoch 3277/5000
26/26 - 1s - loss: 0.5100 - val_loss: 0.9315
Epoch 3278/5000
26/26 - 1s - loss: 0.5103 - val_loss: 0.9324
Epoch 3279/5000
26/26 - 1s - loss: 0.5104 - val_loss: 0.9312
Epoch 3280/5000
26/26 - 1s - loss: 0.5110 - val_loss: 0.9331
Epoch 03280: val_loss improved from 0.93406 to 0.93313, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 3281/5000
26/26 - 1s - loss: 0.5106 - val_loss: 0.9329
Epoch 3282/5000
26/26 - 1s - loss: 0.5107 - val_loss: 0.9312
Epoch 3283/5000
26/26 - 1s - loss: 0.5099 - val_loss: 0.9315
Epoch 3284/5000
26/26 - 1s - loss: 0.5097 - val_loss: 0.9301
Epoch 3285/5000
26/26 - 1s - loss: 0.5101 - val_loss: 0.9312
Epoch 3286/5000
26/26 - 1s - loss: 0.5078 - val_loss: 0.9307
Epoch 3287/5000
26/26 - 1s - loss: 0.5091 - val_loss: 0.9306
Epoch 3288/5000
26/26 - 1s - loss: 0.5076 - val_loss: 0.9294
Epoch 3289/5000
26/26 - 1s - loss: 0.5089 - val_loss: 0.9316
Epoch 3290/5000
26/26 - 1s - loss: 0.5089 - val_loss: 0.9308
Epoch 03290: val_loss improved from 0.93313 to 0.93085, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 3291/5000
26/26 - 1s - loss: 0.5099 - val_loss: 0.9305
Epoch 3292/5000
26/26 - 1s - loss: 0.5082 - val_loss: 0.9308
Epoch 3293/5000
26/26 - 1s - loss: 0.5089 - val_loss: 0.9308
Epoch 3294/5000
26/26 - 1s - loss: 0.5093 - val_loss: 0.9315
Epoch 3295/5000
26/26 - 1s - loss: 0.5086 - val_loss: 0.9305
Epoch 3296/5000
26/26 - 1s - loss: 0.5088 - val_loss: 0.9288
Epoch 3297/5000
26/26 - 1s - loss: 0.5084 - val_loss: 0.9306
Epoch 3298/5000
26/26 - 1s - loss: 0.5093 - val_loss: 0.9299
Epoch 3299/5000
26/26 - 1s - loss: 0.5073 - val_loss: 0.9300
Epoch 3300/5000
26/26 - 2s - loss: 0.5077 - val_loss: 0.9293
Epoch 03300: val_loss improved from 0.93085 to 0.92928, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 3301/5000
26/26 - 1s - loss: 0.5083 - val_loss: 0.9291
Epoch 3302/5000
26/26 - 1s - loss: 0.5073 - val_loss: 0.9304
Epoch 3303/5000
26/26 - 1s - loss: 0.5074 - val_loss: 0.9292
Epoch 3304/5000
26/26 - 1s - loss: 0.5081 - val_loss: 0.9303
Epoch 3305/5000
26/26 - 2s - loss: 0.5088 - val_loss: 0.9286
Epoch 3306/5000
26/26 - 1s - loss: 0.5069 - val_loss: 0.9283
Epoch 3307/5000
26/26 - 1s - loss: 0.5066 - val_loss: 0.9273
Epoch 3308/5000
26/26 - 1s - loss: 0.5063 - val_loss: 0.9292
Epoch 3309/5000
26/26 - 1s - loss: 0.5073 - val_loss: 0.9287
Epoch 3310/5000
26/26 - 1s - loss: 0.5063 - val_loss: 0.9281
Epoch 03310: val_loss improved from 0.92928 to 0.92810, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 3311/5000
26/26 - 1s - loss: 0.5066 - val_loss: 0.9276
Epoch 3312/5000
26/26 - 1s - loss: 0.5063 - val_loss: 0.9273
Epoch 3313/5000
26/26 - 1s - loss: 0.5074 - val_loss: 0.9270
Epoch 3314/5000
26/26 - 1s - loss: 0.5069 - val_loss: 0.9278
Epoch 3315/5000
26/26 - 1s - loss: 0.5062 - val_loss: 0.9290
Epoch 3316/5000
26/26 - 1s - loss: 0.5070 - val_loss: 0.9282
Epoch 3317/5000
26/26 - 1s - loss: 0.5057 - val_loss: 0.9288
Epoch 3318/5000
26/26 - 1s - loss: 0.5061 - val_loss: 0.9273
Epoch 3319/5000
26/26 - 1s - loss: 0.5060 - val_loss: 0.9281
Epoch 3320/5000
26/26 - 1s - loss: 0.5066 - val_loss: 0.9290
Epoch 03320: val_loss did not improve from 0.92810
Epoch 3321/5000
26/26 - 1s - loss: 0.5052 - val_loss: 0.9275
Epoch 3322/5000
26/26 - 1s - loss: 0.5072 - val_loss: 0.9290
Epoch 3323/5000
26/26 - 1s - loss: 0.5053 - val_loss: 0.9269
Epoch 3324/5000
26/26 - 1s - loss: 0.5063 - val_loss: 0.9276
Epoch 3325/5000
26/26 - 1s - loss: 0.5058 - val_loss: 0.9277
Epoch 3326/5000
26/26 - 1s - loss: 0.5052 - val_loss: 0.9271
Epoch 3327/5000
26/26 - 1s - loss: 0.5040 - val_loss: 0.9266
Epoch 3328/5000
26/26 - 1s - loss: 0.5054 - val_loss: 0.9269
Epoch 3329/5000
26/26 - 1s - loss: 0.5053 - val_loss: 0.9260
Epoch 3330/5000
26/26 - 1s - loss: 0.5038 - val_loss: 0.9263
Epoch 03330: val_loss improved from 0.92810 to 0.92630, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 3331/5000
26/26 - 1s - loss: 0.5043 - val_loss: 0.9259
Epoch 3332/5000
26/26 - 1s - loss: 0.5051 - val_loss: 0.9266
Epoch 3333/5000
26/26 - 1s - loss: 0.5045 - val_loss: 0.9273
Epoch 3334/5000
26/26 - 1s - loss: 0.5037 - val_loss: 0.9261
Epoch 3335/5000
26/26 - 1s - loss: 0.5040 - val_loss: 0.9261
Epoch 3336/5000
26/26 - 1s - loss: 0.5044 - val_loss: 0.9258
Epoch 3337/5000
26/26 - 1s - loss: 0.5055 - val_loss: 0.9254
Epoch 3338/5000
26/26 - 1s - loss: 0.5035 - val_loss: 0.9247
Epoch 3339/5000
26/26 - 1s - loss: 0.5040 - val_loss: 0.9253
Epoch 3340/5000
26/26 - 1s - loss: 0.5036 - val_loss: 0.9258
Epoch 03340: val_loss improved from 0.92630 to 0.92576, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 3341/5000
26/26 - 1s - loss: 0.5028 - val_loss: 0.9246
Epoch 3342/5000
26/26 - 1s - loss: 0.5023 - val_loss: 0.9269
Epoch 3343/5000
26/26 - 1s - loss: 0.5041 - val_loss: 0.9260
Epoch 3344/5000
26/26 - 1s - loss: 0.5025 - val_loss: 0.9262
Epoch 3345/5000
26/26 - 1s - loss: 0.5031 - val_loss: 0.9255
Epoch 3346/5000
26/26 - 1s - loss: 0.5030 - val_loss: 0.9259
Epoch 3347/5000
26/26 - 1s - loss: 0.5028 - val_loss: 0.9250
Epoch 3348/5000
26/26 - 1s - loss: 0.5019 - val_loss: 0.9248
Epoch 3349/5000
26/26 - 1s - loss: 0.5033 - val_loss: 0.9252
Epoch 3350/5000
26/26 - 1s - loss: 0.5029 - val_loss: 0.9254
Epoch 03350: val_loss improved from 0.92576 to 0.92541, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 3351/5000
26/26 - 1s - loss: 0.5030 - val_loss: 0.9256
Epoch 3352/5000
26/26 - 1s - loss: 0.5027 - val_loss: 0.9235
Epoch 3353/5000
26/26 - 1s - loss: 0.5016 - val_loss: 0.9256
Epoch 3354/5000
26/26 - 1s - loss: 0.5018 - val_loss: 0.9248
Epoch 3355/5000
26/26 - 1s - loss: 0.5022 - val_loss: 0.9251
Epoch 3356/5000
26/26 - 1s - loss: 0.5022 - val_loss: 0.9235
Epoch 3357/5000
26/26 - 1s - loss: 0.5011 - val_loss: 0.9243
Epoch 3358/5000
26/26 - 1s - loss: 0.5022 - val_loss: 0.9241
Epoch 3359/5000
26/26 - 1s - loss: 0.5012 - val_loss: 0.9241
Epoch 3360/5000
26/26 - 1s - loss: 0.5016 - val_loss: 0.9238
Epoch 03360: val_loss improved from 0.92541 to 0.92377, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 3361/5000
26/26 - 1s - loss: 0.5021 - val_loss: 0.9225
Epoch 3362/5000
26/26 - 1s - loss: 0.5016 - val_loss: 0.9221
Epoch 3363/5000
26/26 - 1s - loss: 0.5008 - val_loss: 0.9228
Epoch 3364/5000
26/26 - 1s - loss: 0.5011 - val_loss: 0.9235
Epoch 3365/5000
26/26 - 1s - loss: 0.5007 - val_loss: 0.9228
Epoch 3366/5000
26/26 - 1s - loss: 0.5004 - val_loss: 0.9216
Epoch 3367/5000
26/26 - 1s - loss: 0.5017 - val_loss: 0.9212
Epoch 3368/5000
26/26 - 1s - loss: 0.5010 - val_loss: 0.9217
Epoch 3369/5000
26/26 - 1s - loss: 0.5007 - val_loss: 0.9229
Epoch 3370/5000
26/26 - 1s - loss: 0.5006 - val_loss: 0.9206
Epoch 03370: val_loss improved from 0.92377 to 0.92064, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 3371/5000
26/26 - 1s - loss: 0.5013 - val_loss: 0.9224
Epoch 3372/5000
26/26 - 1s - loss: 0.5004 - val_loss: 0.9217
Epoch 3373/5000
26/26 - 1s - loss: 0.5003 - val_loss: 0.9216
Epoch 3374/5000
26/26 - 1s - loss: 0.5005 - val_loss: 0.9222
Epoch 3375/5000
26/26 - 1s - loss: 0.4992 - val_loss: 0.9216
Epoch 3376/5000
26/26 - 1s - loss: 0.5005 - val_loss: 0.9217
Epoch 3377/5000
26/26 - 1s - loss: 0.5000 - val_loss: 0.9232
Epoch 3378/5000
26/26 - 1s - loss: 0.5004 - val_loss: 0.9219
Epoch 3379/5000
26/26 - 1s - loss: 0.4987 - val_loss: 0.9217
Epoch 3380/5000
26/26 - 1s - loss: 0.5008 - val_loss: 0.9215
Epoch 03380: val_loss did not improve from 0.92064
Epoch 3381/5000
26/26 - 1s - loss: 0.4996 - val_loss: 0.9222
Epoch 3382/5000
26/26 - 1s - loss: 0.4998 - val_loss: 0.9215
Epoch 3383/5000
26/26 - 1s - loss: 0.4987 - val_loss: 0.9213
Epoch 3384/5000
26/26 - 1s - loss: 0.4995 - val_loss: 0.9213
Epoch 3385/5000
26/26 - 1s - loss: 0.4987 - val_loss: 0.9201
Epoch 3386/5000
26/26 - 1s - loss: 0.4995 - val_loss: 0.9205
Epoch 3387/5000
26/26 - 1s - loss: 0.4994 - val_loss: 0.9209
Epoch 3388/5000
26/26 - 1s - loss: 0.4994 - val_loss: 0.9203
Epoch 3389/5000
26/26 - 1s - loss: 0.4993 - val_loss: 0.9218
Epoch 3390/5000
26/26 - 1s - loss: 0.4991 - val_loss: 0.9192
Epoch 03390: val_loss improved from 0.92064 to 0.91918, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 3391/5000
26/26 - 1s - loss: 0.4976 - val_loss: 0.9210
Epoch 3392/5000
26/26 - 1s - loss: 0.4996 - val_loss: 0.9194
Epoch 3393/5000
26/26 - 1s - loss: 0.4983 - val_loss: 0.9210
Epoch 3394/5000
26/26 - 1s - loss: 0.4974 - val_loss: 0.9207
Epoch 3395/5000
26/26 - 1s - loss: 0.4974 - val_loss: 0.9190
Epoch 3396/5000
26/26 - 1s - loss: 0.4979 - val_loss: 0.9207
Epoch 3397/5000
26/26 - 1s - loss: 0.4977 - val_loss: 0.9197
Epoch 3398/5000
26/26 - 1s - loss: 0.4989 - val_loss: 0.9201
Epoch 3399/5000
26/26 - 1s - loss: 0.4965 - val_loss: 0.9194
Epoch 3400/5000
26/26 - 1s - loss: 0.4969 - val_loss: 0.9196
Epoch 03400: val_loss did not improve from 0.91918
Epoch 3401/5000
26/26 - 1s - loss: 0.4975 - val_loss: 0.9184
Epoch 3402/5000
26/26 - 1s - loss: 0.4971 - val_loss: 0.9192
Epoch 3403/5000
26/26 - 1s - loss: 0.4972 - val_loss: 0.9182
Epoch 3404/5000
26/26 - 1s - loss: 0.4973 - val_loss: 0.9184
Epoch 3405/5000
26/26 - 1s - loss: 0.4981 - val_loss: 0.9175
Epoch 3406/5000
26/26 - 1s - loss: 0.4975 - val_loss: 0.9196
Epoch 3407/5000
26/26 - 1s - loss: 0.4973 - val_loss: 0.9177
Epoch 3408/5000
26/26 - 1s - loss: 0.4974 - val_loss: 0.9182
Epoch 3409/5000
26/26 - 1s - loss: 0.4968 - val_loss: 0.9176
Epoch 3410/5000
26/26 - 1s - loss: 0.4974 - val_loss: 0.9181
Epoch 03410: val_loss improved from 0.91918 to 0.91815, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 3411/5000
26/26 - 1s - loss: 0.4965 - val_loss: 0.9180
Epoch 3412/5000
26/26 - 1s - loss: 0.4957 - val_loss: 0.9186
Epoch 3413/5000
26/26 - 1s - loss: 0.4953 - val_loss: 0.9197
Epoch 3414/5000
26/26 - 1s - loss: 0.4962 - val_loss: 0.9174
Epoch 3415/5000
26/26 - 1s - loss: 0.4963 - val_loss: 0.9180
Epoch 3416/5000
26/26 - 1s - loss: 0.4952 - val_loss: 0.9180
Epoch 3417/5000
26/26 - 1s - loss: 0.4969 - val_loss: 0.9174
Epoch 3418/5000
26/26 - 1s - loss: 0.4970 - val_loss: 0.9193
Epoch 3419/5000
26/26 - 1s - loss: 0.4957 - val_loss: 0.9163
Epoch 3420/5000
26/26 - 1s - loss: 0.4958 - val_loss: 0.9180
Epoch 03420: val_loss improved from 0.91815 to 0.91796, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 3421/5000
26/26 - 1s - loss: 0.4948 - val_loss: 0.9172
Epoch 3422/5000
26/26 - 1s - loss: 0.4957 - val_loss: 0.9163
Epoch 3423/5000
26/26 - 1s - loss: 0.4956 - val_loss: 0.9161
Epoch 3424/5000
26/26 - 1s - loss: 0.4936 - val_loss: 0.9166
Epoch 3425/5000
26/26 - 1s - loss: 0.4944 - val_loss: 0.9166
Epoch 3426/5000
26/26 - 1s - loss: 0.4944 - val_loss: 0.9162
Epoch 3427/5000
26/26 - 1s - loss: 0.4945 - val_loss: 0.9159
Epoch 3428/5000
26/26 - 1s - loss: 0.4957 - val_loss: 0.9157
Epoch 3429/5000
26/26 - 1s - loss: 0.4947 - val_loss: 0.9165
Epoch 3430/5000
26/26 - 1s - loss: 0.4940 - val_loss: 0.9165
Epoch 03430: val_loss improved from 0.91796 to 0.91652, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 3431/5000
26/26 - 1s - loss: 0.4950 - val_loss: 0.9166
Epoch 3432/5000
26/26 - 1s - loss: 0.4949 - val_loss: 0.9166
Epoch 3433/5000
26/26 - 1s - loss: 0.4946 - val_loss: 0.9155
Epoch 3434/5000
26/26 - 1s - loss: 0.4952 - val_loss: 0.9161
Epoch 3435/5000
26/26 - 1s - loss: 0.4947 - val_loss: 0.9160
Epoch 3436/5000
26/26 - 1s - loss: 0.4940 - val_loss: 0.9141
Epoch 3437/5000
26/26 - 1s - loss: 0.4948 - val_loss: 0.9153
Epoch 3438/5000
26/26 - 1s - loss: 0.4940 - val_loss: 0.9139
Epoch 3439/5000
26/26 - 1s - loss: 0.4939 - val_loss: 0.9149
Epoch 3440/5000
26/26 - 1s - loss: 0.4933 - val_loss: 0.9153
Epoch 03440: val_loss improved from 0.91652 to 0.91531, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 3441/5000
26/26 - 1s - loss: 0.4940 - val_loss: 0.9141
Epoch 3442/5000
26/26 - 1s - loss: 0.4940 - val_loss: 0.9153
Epoch 3443/5000
26/26 - 1s - loss: 0.4933 - val_loss: 0.9150
Epoch 3444/5000
26/26 - 1s - loss: 0.4923 - val_loss: 0.9130
Epoch 3445/5000
26/26 - 1s - loss: 0.4937 - val_loss: 0.9141
Epoch 3446/5000
26/26 - 1s - loss: 0.4932 - val_loss: 0.9145
Epoch 3447/5000
26/26 - 1s - loss: 0.4919 - val_loss: 0.9146
Epoch 3448/5000
26/26 - 1s - loss: 0.4924 - val_loss: 0.9145
Epoch 3449/5000
26/26 - 1s - loss: 0.4927 - val_loss: 0.9148
Epoch 3450/5000
26/26 - 1s - loss: 0.4939 - val_loss: 0.9149
Epoch 03450: val_loss improved from 0.91531 to 0.91487, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 3451/5000
26/26 - 1s - loss: 0.4934 - val_loss: 0.9137
Epoch 3452/5000
26/26 - 1s - loss: 0.4921 - val_loss: 0.9151
Epoch 3453/5000
26/26 - 1s - loss: 0.4934 - val_loss: 0.9132
Epoch 3454/5000
26/26 - 2s - loss: 0.4914 - val_loss: 0.9135
Epoch 3455/5000
26/26 - 1s - loss: 0.4923 - val_loss: 0.9141
Epoch 3456/5000
26/26 - 1s - loss: 0.4910 - val_loss: 0.9120
Epoch 3457/5000
26/26 - 1s - loss: 0.4925 - val_loss: 0.9130
Epoch 3458/5000
26/26 - 1s - loss: 0.4920 - val_loss: 0.9136
Epoch 3459/5000
26/26 - 1s - loss: 0.4913 - val_loss: 0.9158
Epoch 3460/5000
26/26 - 1s - loss: 0.4923 - val_loss: 0.9138
Epoch 03460: val_loss improved from 0.91487 to 0.91382, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 3461/5000
26/26 - 1s - loss: 0.4919 - val_loss: 0.9132
Epoch 3462/5000
26/26 - 1s - loss: 0.4937 - val_loss: 0.9135
Epoch 3463/5000
26/26 - 1s - loss: 0.4921 - val_loss: 0.9116
Epoch 3464/5000
26/26 - 1s - loss: 0.4916 - val_loss: 0.9120
Epoch 3465/5000
26/26 - 1s - loss: 0.4906 - val_loss: 0.9130
Epoch 3466/5000
26/26 - 1s - loss: 0.4913 - val_loss: 0.9113
Epoch 3467/5000
26/26 - 1s - loss: 0.4912 - val_loss: 0.9120
Epoch 3468/5000
26/26 - 1s - loss: 0.4913 - val_loss: 0.9113
Epoch 3469/5000
26/26 - 1s - loss: 0.4915 - val_loss: 0.9120
Epoch 3470/5000
26/26 - 1s - loss: 0.4909 - val_loss: 0.9127
Epoch 03470: val_loss improved from 0.91382 to 0.91272, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 3471/5000
26/26 - 1s - loss: 0.4907 - val_loss: 0.9128
Epoch 3472/5000
26/26 - 1s - loss: 0.4909 - val_loss: 0.9122
Epoch 3473/5000
26/26 - 1s - loss: 0.4909 - val_loss: 0.9129
Epoch 3474/5000
26/26 - 1s - loss: 0.4905 - val_loss: 0.9114
Epoch 3475/5000
26/26 - 1s - loss: 0.4890 - val_loss: 0.9120
Epoch 3476/5000
26/26 - 1s - loss: 0.4899 - val_loss: 0.9113
Epoch 3477/5000
26/26 - 1s - loss: 0.4895 - val_loss: 0.9115
Epoch 3478/5000
26/26 - 1s - loss: 0.4892 - val_loss: 0.9119
Epoch 3479/5000
26/26 - 1s - loss: 0.4901 - val_loss: 0.9107
Epoch 3480/5000
26/26 - 1s - loss: 0.4894 - val_loss: 0.9116
Epoch 03480: val_loss improved from 0.91272 to 0.91164, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 3481/5000
26/26 - 1s - loss: 0.4889 - val_loss: 0.9107
Epoch 3482/5000
26/26 - 1s - loss: 0.4900 - val_loss: 0.9116
Epoch 3483/5000
26/26 - 1s - loss: 0.4898 - val_loss: 0.9125
Epoch 3484/5000
26/26 - 1s - loss: 0.4891 - val_loss: 0.9112
Epoch 3485/5000
26/26 - 1s - loss: 0.4895 - val_loss: 0.9101
Epoch 3486/5000
26/26 - 1s - loss: 0.4888 - val_loss: 0.9125
Epoch 3487/5000
26/26 - 1s - loss: 0.4896 - val_loss: 0.9110
Epoch 3488/5000
26/26 - 1s - loss: 0.4907 - val_loss: 0.9119
Epoch 3489/5000
26/26 - 1s - loss: 0.4894 - val_loss: 0.9123
Epoch 3490/5000
26/26 - 1s - loss: 0.4885 - val_loss: 0.9119
Epoch 03490: val_loss did not improve from 0.91164
Epoch 3491/5000
26/26 - 1s - loss: 0.4887 - val_loss: 0.9104
Epoch 3492/5000
26/26 - 1s - loss: 0.4889 - val_loss: 0.9112
Epoch 3493/5000
26/26 - 1s - loss: 0.4879 - val_loss: 0.9105
Epoch 3494/5000
26/26 - 1s - loss: 0.4883 - val_loss: 0.9083
Epoch 3495/5000
26/26 - 1s - loss: 0.4887 - val_loss: 0.9093
Epoch 3496/5000
26/26 - 1s - loss: 0.4884 - val_loss: 0.9099
Epoch 3497/5000
26/26 - 1s - loss: 0.4893 - val_loss: 0.9109
Epoch 3498/5000
26/26 - 1s - loss: 0.4884 - val_loss: 0.9106
Epoch 3499/5000
26/26 - 1s - loss: 0.4878 - val_loss: 0.9098
Epoch 3500/5000
26/26 - 2s - loss: 0.4883 - val_loss: 0.9108
Epoch 03500: val_loss improved from 0.91164 to 0.91076, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 3501/5000
26/26 - 1s - loss: 0.4884 - val_loss: 0.9094
Epoch 3502/5000
26/26 - 1s - loss: 0.4877 - val_loss: 0.9101
Epoch 3503/5000
26/26 - 1s - loss: 0.4885 - val_loss: 0.9096
Epoch 3504/5000
26/26 - 1s - loss: 0.4871 - val_loss: 0.9099
Epoch 3505/5000
26/26 - 1s - loss: 0.4881 - val_loss: 0.9102
Epoch 3506/5000
26/26 - 1s - loss: 0.4871 - val_loss: 0.9095
Epoch 3507/5000
26/26 - 2s - loss: 0.4875 - val_loss: 0.9089
Epoch 3508/5000
26/26 - 1s - loss: 0.4872 - val_loss: 0.9087
Epoch 3509/5000
26/26 - 1s - loss: 0.4884 - val_loss: 0.9074
Epoch 3510/5000
26/26 - 1s - loss: 0.4870 - val_loss: 0.9092
Epoch 03510: val_loss improved from 0.91076 to 0.90916, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 3511/5000
26/26 - 1s - loss: 0.4869 - val_loss: 0.9075
Epoch 3512/5000
26/26 - 1s - loss: 0.4863 - val_loss: 0.9078
Epoch 3513/5000
26/26 - 1s - loss: 0.4859 - val_loss: 0.9089
Epoch 3514/5000
26/26 - 1s - loss: 0.4869 - val_loss: 0.9081
Epoch 3515/5000
26/26 - 1s - loss: 0.4868 - val_loss: 0.9081
Epoch 3516/5000
26/26 - 1s - loss: 0.4862 - val_loss: 0.9086
Epoch 3517/5000
26/26 - 1s - loss: 0.4871 - val_loss: 0.9085
Epoch 3518/5000
26/26 - 1s - loss: 0.4870 - val_loss: 0.9076
Epoch 3519/5000
26/26 - 1s - loss: 0.4860 - val_loss: 0.9082
Epoch 3520/5000
26/26 - 1s - loss: 0.4864 - val_loss: 0.9084
Epoch 03520: val_loss improved from 0.90916 to 0.90837, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 3521/5000
26/26 - 1s - loss: 0.4866 - val_loss: 0.9074
Epoch 3522/5000
26/26 - 1s - loss: 0.4865 - val_loss: 0.9071
Epoch 3523/5000
26/26 - 1s - loss: 0.4866 - val_loss: 0.9081
Epoch 3524/5000
26/26 - 1s - loss: 0.4853 - val_loss: 0.9062
Epoch 3525/5000
26/26 - 1s - loss: 0.4860 - val_loss: 0.9077
Epoch 3526/5000
26/26 - 1s - loss: 0.4859 - val_loss: 0.9079
Epoch 3527/5000
26/26 - 1s - loss: 0.4856 - val_loss: 0.9057
Epoch 3528/5000
26/26 - 1s - loss: 0.4852 - val_loss: 0.9068
Epoch 3529/5000
26/26 - 1s - loss: 0.4848 - val_loss: 0.9070
Epoch 3530/5000
26/26 - 1s - loss: 0.4853 - val_loss: 0.9062
Epoch 03530: val_loss improved from 0.90837 to 0.90624, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 3531/5000
26/26 - 1s - loss: 0.4842 - val_loss: 0.9066
Epoch 3532/5000
26/26 - 1s - loss: 0.4852 - val_loss: 0.9058
Epoch 3533/5000
26/26 - 1s - loss: 0.4852 - val_loss: 0.9071
Epoch 3534/5000
26/26 - 1s - loss: 0.4839 - val_loss: 0.9065
Epoch 3535/5000
26/26 - 1s - loss: 0.4854 - val_loss: 0.9064
Epoch 3536/5000
26/26 - 1s - loss: 0.4839 - val_loss: 0.9073
Epoch 3537/5000
26/26 - 1s - loss: 0.4841 - val_loss: 0.9055
Epoch 3538/5000
26/26 - 1s - loss: 0.4849 - val_loss: 0.9071
Epoch 3539/5000
26/26 - 1s - loss: 0.4842 - val_loss: 0.9063
Epoch 3540/5000
26/26 - 1s - loss: 0.4845 - val_loss: 0.9063
Epoch 03540: val_loss did not improve from 0.90624
Epoch 3541/5000
26/26 - 1s - loss: 0.4842 - val_loss: 0.9076
Epoch 3542/5000
26/26 - 1s - loss: 0.4835 - val_loss: 0.9058
Epoch 3543/5000
26/26 - 1s - loss: 0.4838 - val_loss: 0.9068
Epoch 3544/5000
26/26 - 1s - loss: 0.4835 - val_loss: 0.9073
Epoch 3545/5000
26/26 - 1s - loss: 0.4842 - val_loss: 0.9053
Epoch 3546/5000
26/26 - 1s - loss: 0.4846 - val_loss: 0.9067
Epoch 3547/5000
26/26 - 1s - loss: 0.4846 - val_loss: 0.9066
Epoch 3548/5000
26/26 - 1s - loss: 0.4844 - val_loss: 0.9055
Epoch 3549/5000
26/26 - 1s - loss: 0.4823 - val_loss: 0.9062
Epoch 3550/5000
26/26 - 1s - loss: 0.4842 - val_loss: 0.9049
Epoch 03550: val_loss improved from 0.90624 to 0.90490, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 3551/5000
26/26 - 1s - loss: 0.4838 - val_loss: 0.9062
Epoch 3552/5000
26/26 - 1s - loss: 0.4832 - val_loss: 0.9060
Epoch 3553/5000
26/26 - 1s - loss: 0.4829 - val_loss: 0.9053
Epoch 3554/5000
26/26 - 1s - loss: 0.4827 - val_loss: 0.9047
Epoch 3555/5000
26/26 - 1s - loss: 0.4820 - val_loss: 0.9044
Epoch 3556/5000
26/26 - 1s - loss: 0.4833 - val_loss: 0.9043
Epoch 3557/5000
26/26 - 1s - loss: 0.4831 - val_loss: 0.9051
Epoch 3558/5000
26/26 - 1s - loss: 0.4826 - val_loss: 0.9047
Epoch 3559/5000
26/26 - 1s - loss: 0.4834 - val_loss: 0.9037
Epoch 3560/5000
26/26 - 1s - loss: 0.4839 - val_loss: 0.9039
Epoch 03560: val_loss improved from 0.90490 to 0.90393, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 3561/5000
26/26 - 1s - loss: 0.4825 - val_loss: 0.9048
Epoch 3562/5000
26/26 - 1s - loss: 0.4827 - val_loss: 0.9029
Epoch 3563/5000
26/26 - 1s - loss: 0.4831 - val_loss: 0.9038
Epoch 3564/5000
26/26 - 2s - loss: 0.4819 - val_loss: 0.9043
Epoch 3565/5000
26/26 - 1s - loss: 0.4827 - val_loss: 0.9036
Epoch 3566/5000
26/26 - 1s - loss: 0.4820 - val_loss: 0.9030
Epoch 3567/5000
26/26 - 1s - loss: 0.4819 - val_loss: 0.9038
Epoch 3568/5000
26/26 - 1s - loss: 0.4811 - val_loss: 0.9028
Epoch 3569/5000
26/26 - 1s - loss: 0.4829 - val_loss: 0.9049
Epoch 3570/5000
26/26 - 1s - loss: 0.4827 - val_loss: 0.9035
Epoch 03570: val_loss improved from 0.90393 to 0.90349, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 3571/5000
26/26 - 1s - loss: 0.4820 - val_loss: 0.9023
Epoch 3572/5000
26/26 - 1s - loss: 0.4821 - val_loss: 0.9033
Epoch 3573/5000
26/26 - 1s - loss: 0.4821 - val_loss: 0.9032
Epoch 3574/5000
26/26 - 1s - loss: 0.4808 - val_loss: 0.9039
Epoch 3575/5000
26/26 - 1s - loss: 0.4812 - val_loss: 0.9032
Epoch 3576/5000
26/26 - 1s - loss: 0.4802 - val_loss: 0.9029
Epoch 3577/5000
26/26 - 1s - loss: 0.4811 - val_loss: 0.9023
Epoch 3578/5000
26/26 - 1s - loss: 0.4817 - val_loss: 0.9020
Epoch 3579/5000
26/26 - 1s - loss: 0.4805 - val_loss: 0.9016
Epoch 3580/5000
26/26 - 1s - loss: 0.4804 - val_loss: 0.9034
Epoch 03580: val_loss improved from 0.90349 to 0.90343, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 3581/5000
26/26 - 1s - loss: 0.4800 - val_loss: 0.9019
Epoch 3582/5000
26/26 - 2s - loss: 0.4811 - val_loss: 0.9018
Epoch 3583/5000
26/26 - 1s - loss: 0.4807 - val_loss: 0.9025
Epoch 3584/5000
26/26 - 1s - loss: 0.4807 - val_loss: 0.9021
Epoch 3585/5000
26/26 - 1s - loss: 0.4811 - val_loss: 0.9004
Epoch 3586/5000
26/26 - 1s - loss: 0.4810 - val_loss: 0.9020
Epoch 3587/5000
26/26 - 2s - loss: 0.4808 - val_loss: 0.9026
Epoch 3588/5000
26/26 - 1s - loss: 0.4803 - val_loss: 0.9025
Epoch 3589/5000
26/26 - 1s - loss: 0.4796 - val_loss: 0.9009
Epoch 3590/5000
26/26 - 1s - loss: 0.4792 - val_loss: 0.9017
Epoch 03590: val_loss improved from 0.90343 to 0.90166, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 3591/5000
26/26 - 1s - loss: 0.4796 - val_loss: 0.8997
Epoch 3592/5000
26/26 - 1s - loss: 0.4802 - val_loss: 0.9016
Epoch 3593/5000
26/26 - 1s - loss: 0.4799 - val_loss: 0.9013
Epoch 3594/5000
26/26 - 1s - loss: 0.4809 - val_loss: 0.8994
Epoch 3595/5000
26/26 - 1s - loss: 0.4804 - val_loss: 0.9020
Epoch 3596/5000
26/26 - 1s - loss: 0.4791 - val_loss: 0.9012
Epoch 3597/5000
26/26 - 1s - loss: 0.4800 - val_loss: 0.9010
Epoch 3598/5000
26/26 - 1s - loss: 0.4785 - val_loss: 0.9019
Epoch 3599/5000
26/26 - 1s - loss: 0.4797 - val_loss: 0.9005
Epoch 3600/5000
26/26 - 1s - loss: 0.4799 - val_loss: 0.9009
Epoch 03600: val_loss improved from 0.90166 to 0.90091, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 3601/5000
26/26 - 1s - loss: 0.4784 - val_loss: 0.9019
Epoch 3602/5000
26/26 - 1s - loss: 0.4783 - val_loss: 0.9008
Epoch 3603/5000
26/26 - 1s - loss: 0.4801 - val_loss: 0.8996
Epoch 3604/5000
26/26 - 1s - loss: 0.4786 - val_loss: 0.9008
Epoch 3605/5000
26/26 - 1s - loss: 0.4796 - val_loss: 0.9004
Epoch 3606/5000
26/26 - 1s - loss: 0.4780 - val_loss: 0.9011
Epoch 3607/5000
26/26 - 1s - loss: 0.4785 - val_loss: 0.8992
Epoch 3608/5000
26/26 - 1s - loss: 0.4785 - val_loss: 0.9006
Epoch 3609/5000
26/26 - 1s - loss: 0.4790 - val_loss: 0.9012
Epoch 3610/5000
26/26 - 1s - loss: 0.4794 - val_loss: 0.8996
Epoch 03610: val_loss improved from 0.90091 to 0.89960, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 3611/5000
26/26 - 1s - loss: 0.4780 - val_loss: 0.8998
Epoch 3612/5000
26/26 - 1s - loss: 0.4786 - val_loss: 0.9001
Epoch 3613/5000
26/26 - 1s - loss: 0.4778 - val_loss: 0.9008
Epoch 3614/5000
26/26 - 1s - loss: 0.4777 - val_loss: 0.9000
Epoch 3615/5000
26/26 - 1s - loss: 0.4767 - val_loss: 0.9001
Epoch 3616/5000
26/26 - 1s - loss: 0.4791 - val_loss: 0.9019
Epoch 3617/5000
26/26 - 1s - loss: 0.4780 - val_loss: 0.9018
Epoch 3618/5000
26/26 - 1s - loss: 0.4771 - val_loss: 0.8998
Epoch 3619/5000
26/26 - 1s - loss: 0.4773 - val_loss: 0.8991
Epoch 3620/5000
26/26 - 1s - loss: 0.4774 - val_loss: 0.9003
Epoch 03620: val_loss did not improve from 0.89960
Epoch 3621/5000
26/26 - 1s - loss: 0.4778 - val_loss: 0.8984
Epoch 3622/5000
26/26 - 1s - loss: 0.4763 - val_loss: 0.8987
Epoch 3623/5000
26/26 - 1s - loss: 0.4772 - val_loss: 0.8983
Epoch 3624/5000
26/26 - 1s - loss: 0.4775 - val_loss: 0.9006
Epoch 3625/5000
26/26 - 2s - loss: 0.4763 - val_loss: 0.8987
Epoch 3626/5000
26/26 - 1s - loss: 0.4768 - val_loss: 0.9006
Epoch 3627/5000
26/26 - 1s - loss: 0.4770 - val_loss: 0.9012
Epoch 3628/5000
26/26 - 1s - loss: 0.4761 - val_loss: 0.8988
Epoch 3629/5000
26/26 - 1s - loss: 0.4774 - val_loss: 0.8994
Epoch 3630/5000
26/26 - 1s - loss: 0.4761 - val_loss: 0.8988
Epoch 03630: val_loss improved from 0.89960 to 0.89876, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 3631/5000
26/26 - 2s - loss: 0.4772 - val_loss: 0.8988
Epoch 3632/5000
26/26 - 1s - loss: 0.4765 - val_loss: 0.8986
Epoch 3633/5000
26/26 - 1s - loss: 0.4758 - val_loss: 0.8989
Epoch 3634/5000
26/26 - 1s - loss: 0.4757 - val_loss: 0.8981
Epoch 3635/5000
26/26 - 1s - loss: 0.4766 - val_loss: 0.8990
Epoch 3636/5000
26/26 - 1s - loss: 0.4762 - val_loss: 0.8972
Epoch 3637/5000
26/26 - 1s - loss: 0.4762 - val_loss: 0.8971
Epoch 3638/5000
26/26 - 1s - loss: 0.4756 - val_loss: 0.8978
Epoch 3639/5000
26/26 - 1s - loss: 0.4752 - val_loss: 0.8989
Epoch 3640/5000
26/26 - 1s - loss: 0.4759 - val_loss: 0.8975
Epoch 03640: val_loss improved from 0.89876 to 0.89746, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 3641/5000
26/26 - 1s - loss: 0.4752 - val_loss: 0.8966
Epoch 3642/5000
26/26 - 1s - loss: 0.4762 - val_loss: 0.8966
Epoch 3643/5000
26/26 - 1s - loss: 0.4759 - val_loss: 0.8957
Epoch 3644/5000
26/26 - 1s - loss: 0.4756 - val_loss: 0.8985
Epoch 3645/5000
26/26 - 1s - loss: 0.4753 - val_loss: 0.8967
Epoch 3646/5000
26/26 - 1s - loss: 0.4751 - val_loss: 0.8970
Epoch 3647/5000
26/26 - 1s - loss: 0.4761 - val_loss: 0.8980
Epoch 3648/5000
26/26 - 1s - loss: 0.4744 - val_loss: 0.8972
Epoch 3649/5000
26/26 - 1s - loss: 0.4740 - val_loss: 0.8971
Epoch 3650/5000
26/26 - 1s - loss: 0.4750 - val_loss: 0.8982
Epoch 03650: val_loss did not improve from 0.89746
Epoch 3651/5000
26/26 - 1s - loss: 0.4751 - val_loss: 0.8982
Epoch 3652/5000
26/26 - 2s - loss: 0.4754 - val_loss: 0.8969
Epoch 3653/5000
26/26 - 1s - loss: 0.4756 - val_loss: 0.8979
Epoch 3654/5000
26/26 - 1s - loss: 0.4747 - val_loss: 0.8965
Epoch 3655/5000
26/26 - 1s - loss: 0.4735 - val_loss: 0.8957
Epoch 3656/5000
26/26 - 1s - loss: 0.4754 - val_loss: 0.8970
Epoch 3657/5000
26/26 - 1s - loss: 0.4741 - val_loss: 0.8963
Epoch 3658/5000
26/26 - 1s - loss: 0.4746 - val_loss: 0.8947
Epoch 3659/5000
26/26 - 1s - loss: 0.4747 - val_loss: 0.8970
Epoch 3660/5000
26/26 - 1s - loss: 0.4746 - val_loss: 0.8967
Epoch 03660: val_loss improved from 0.89746 to 0.89673, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 3661/5000
26/26 - 1s - loss: 0.4753 - val_loss: 0.8968
Epoch 3662/5000
26/26 - 1s - loss: 0.4737 - val_loss: 0.8971
Epoch 3663/5000
26/26 - 1s - loss: 0.4737 - val_loss: 0.8974
Epoch 3664/5000
26/26 - 1s - loss: 0.4749 - val_loss: 0.8959
Epoch 3665/5000
26/26 - 2s - loss: 0.4730 - val_loss: 0.8958
Epoch 3666/5000
26/26 - 1s - loss: 0.4730 - val_loss: 0.8963
Epoch 3667/5000
26/26 - 1s - loss: 0.4742 - val_loss: 0.8943
Epoch 3668/5000
26/26 - 1s - loss: 0.4734 - val_loss: 0.8951
Epoch 3669/5000
26/26 - 1s - loss: 0.4745 - val_loss: 0.8963
Epoch 3670/5000
26/26 - 1s - loss: 0.4728 - val_loss: 0.8947
Epoch 03670: val_loss improved from 0.89673 to 0.89470, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 3671/5000
26/26 - 1s - loss: 0.4731 - val_loss: 0.8955
Epoch 3672/5000
26/26 - 2s - loss: 0.4725 - val_loss: 0.8951
Epoch 3673/5000
26/26 - 1s - loss: 0.4725 - val_loss: 0.8953
Epoch 3674/5000
26/26 - 1s - loss: 0.4730 - val_loss: 0.8940
Epoch 3675/5000
26/26 - 1s - loss: 0.4723 - val_loss: 0.8957
Epoch 3676/5000
26/26 - 1s - loss: 0.4728 - val_loss: 0.8960
Epoch 3677/5000
26/26 - 1s - loss: 0.4728 - val_loss: 0.8946
Epoch 3678/5000
26/26 - 1s - loss: 0.4731 - val_loss: 0.8956
Epoch 3679/5000
26/26 - 1s - loss: 0.4724 - val_loss: 0.8941
Epoch 3680/5000
26/26 - 1s - loss: 0.4734 - val_loss: 0.8950
Epoch 03680: val_loss did not improve from 0.89470
Epoch 3681/5000
26/26 - 1s - loss: 0.4728 - val_loss: 0.8943
Epoch 3682/5000
26/26 - 1s - loss: 0.4726 - val_loss: 0.8958
Epoch 3683/5000
26/26 - 1s - loss: 0.4712 - val_loss: 0.8958
Epoch 3684/5000
26/26 - 1s - loss: 0.4718 - val_loss: 0.8954
Epoch 3685/5000
26/26 - 1s - loss: 0.4725 - val_loss: 0.8948
Epoch 3686/5000
26/26 - 1s - loss: 0.4715 - val_loss: 0.8941
Epoch 3687/5000
26/26 - 1s - loss: 0.4713 - val_loss: 0.8959
Epoch 3688/5000
26/26 - 1s - loss: 0.4721 - val_loss: 0.8955
Epoch 3689/5000
26/26 - 1s - loss: 0.4719 - val_loss: 0.8958
Epoch 3690/5000
26/26 - 1s - loss: 0.4719 - val_loss: 0.8950
Epoch 03690: val_loss did not improve from 0.89470
Epoch 3691/5000
26/26 - 2s - loss: 0.4719 - val_loss: 0.8948
Epoch 3692/5000
26/26 - 1s - loss: 0.4719 - val_loss: 0.8935
Epoch 3693/5000
26/26 - 1s - loss: 0.4730 - val_loss: 0.8938
Epoch 3694/5000
26/26 - 1s - loss: 0.4715 - val_loss: 0.8946
Epoch 3695/5000
26/26 - 1s - loss: 0.4727 - val_loss: 0.8956
Epoch 3696/5000
26/26 - 1s - loss: 0.4718 - val_loss: 0.8942
Epoch 3697/5000
26/26 - 1s - loss: 0.4706 - val_loss: 0.8939
Epoch 3698/5000
26/26 - 1s - loss: 0.4710 - val_loss: 0.8934
Epoch 3699/5000
26/26 - 1s - loss: 0.4703 - val_loss: 0.8927
Epoch 3700/5000
26/26 - 1s - loss: 0.4710 - val_loss: 0.8928
Epoch 03700: val_loss improved from 0.89470 to 0.89283, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 3701/5000
26/26 - 1s - loss: 0.4705 - val_loss: 0.8928
Epoch 3702/5000
26/26 - 1s - loss: 0.4717 - val_loss: 0.8932
Epoch 3703/5000
26/26 - 1s - loss: 0.4714 - val_loss: 0.8927
Epoch 3704/5000
26/26 - 1s - loss: 0.4716 - val_loss: 0.8931
Epoch 3705/5000
26/26 - 1s - loss: 0.4697 - val_loss: 0.8936
Epoch 3706/5000
26/26 - 1s - loss: 0.4704 - val_loss: 0.8939
Epoch 3707/5000
26/26 - 1s - loss: 0.4699 - val_loss: 0.8932
Epoch 3708/5000
26/26 - 1s - loss: 0.4711 - val_loss: 0.8936
Epoch 3709/5000
26/26 - 1s - loss: 0.4707 - val_loss: 0.8930
Epoch 3710/5000
26/26 - 1s - loss: 0.4707 - val_loss: 0.8922
Epoch 03710: val_loss improved from 0.89283 to 0.89224, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 3711/5000
26/26 - 1s - loss: 0.4701 - val_loss: 0.8929
Epoch 3712/5000
26/26 - 1s - loss: 0.4703 - val_loss: 0.8915
Epoch 3713/5000
26/26 - 1s - loss: 0.4703 - val_loss: 0.8931
Epoch 3714/5000
26/26 - 2s - loss: 0.4703 - val_loss: 0.8921
Epoch 3715/5000
26/26 - 1s - loss: 0.4698 - val_loss: 0.8920
Epoch 3716/5000
26/26 - 1s - loss: 0.4708 - val_loss: 0.8923
Epoch 3717/5000
26/26 - 1s - loss: 0.4702 - val_loss: 0.8925
Epoch 3718/5000
26/26 - 1s - loss: 0.4694 - val_loss: 0.8905
Epoch 3719/5000
26/26 - 1s - loss: 0.4702 - val_loss: 0.8926
Epoch 3720/5000
26/26 - 1s - loss: 0.4693 - val_loss: 0.8918
Epoch 03720: val_loss improved from 0.89224 to 0.89181, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 3721/5000
26/26 - 1s - loss: 0.4684 - val_loss: 0.8914
Epoch 3722/5000
26/26 - 1s - loss: 0.4682 - val_loss: 0.8930
Epoch 3723/5000
26/26 - 1s - loss: 0.4701 - val_loss: 0.8911
Epoch 3724/5000
26/26 - 1s - loss: 0.4685 - val_loss: 0.8915
Epoch 3725/5000
26/26 - 1s - loss: 0.4691 - val_loss: 0.8914
Epoch 3726/5000
26/26 - 1s - loss: 0.4692 - val_loss: 0.8913
Epoch 3727/5000
26/26 - 1s - loss: 0.4693 - val_loss: 0.8921
Epoch 3728/5000
26/26 - 1s - loss: 0.4694 - val_loss: 0.8905
Epoch 3729/5000
26/26 - 2s - loss: 0.4682 - val_loss: 0.8907
Epoch 3730/5000
26/26 - 2s - loss: 0.4687 - val_loss: 0.8910
Epoch 03730: val_loss improved from 0.89181 to 0.89100, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 3731/5000
26/26 - 1s - loss: 0.4698 - val_loss: 0.8909
Epoch 3732/5000
26/26 - 1s - loss: 0.4696 - val_loss: 0.8901
Epoch 3733/5000
26/26 - 1s - loss: 0.4692 - val_loss: 0.8895
Epoch 3734/5000
26/26 - 1s - loss: 0.4684 - val_loss: 0.8892
Epoch 3735/5000
26/26 - 1s - loss: 0.4675 - val_loss: 0.8898
Epoch 3736/5000
26/26 - 1s - loss: 0.4687 - val_loss: 0.8909
Epoch 3737/5000
26/26 - 1s - loss: 0.4684 - val_loss: 0.8896
Epoch 3738/5000
26/26 - 1s - loss: 0.4683 - val_loss: 0.8909
Epoch 3739/5000
26/26 - 1s - loss: 0.4676 - val_loss: 0.8901
Epoch 3740/5000
26/26 - 1s - loss: 0.4676 - val_loss: 0.8887
Epoch 03740: val_loss improved from 0.89100 to 0.88870, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 3741/5000
26/26 - 1s - loss: 0.4685 - val_loss: 0.8879
Epoch 3742/5000
26/26 - 1s - loss: 0.4683 - val_loss: 0.8897
Epoch 3743/5000
26/26 - 1s - loss: 0.4659 - val_loss: 0.8884
Epoch 3744/5000
26/26 - 1s - loss: 0.4686 - val_loss: 0.8900
Epoch 3745/5000
26/26 - 2s - loss: 0.4669 - val_loss: 0.8887
Epoch 3746/5000
26/26 - 1s - loss: 0.4676 - val_loss: 0.8890
Epoch 3747/5000
26/26 - 1s - loss: 0.4673 - val_loss: 0.8882
Epoch 3748/5000
26/26 - 1s - loss: 0.4668 - val_loss: 0.8895
Epoch 3749/5000
26/26 - 1s - loss: 0.4668 - val_loss: 0.8891
Epoch 3750/5000
26/26 - 1s - loss: 0.4678 - val_loss: 0.8887
Epoch 03750: val_loss did not improve from 0.88870
Epoch 3751/5000
26/26 - 1s - loss: 0.4674 - val_loss: 0.8882
Epoch 3752/5000
26/26 - 1s - loss: 0.4665 - val_loss: 0.8896
Epoch 3753/5000
26/26 - 1s - loss: 0.4672 - val_loss: 0.8886
Epoch 3754/5000
26/26 - 1s - loss: 0.4677 - val_loss: 0.8892
Epoch 3755/5000
26/26 - 1s - loss: 0.4670 - val_loss: 0.8899
Epoch 3756/5000
26/26 - 1s - loss: 0.4674 - val_loss: 0.8883
Epoch 3757/5000
26/26 - 1s - loss: 0.4669 - val_loss: 0.8893
Epoch 3758/5000
26/26 - 1s - loss: 0.4667 - val_loss: 0.8901
Epoch 3759/5000
26/26 - 1s - loss: 0.4666 - val_loss: 0.8886
Epoch 3760/5000
26/26 - 1s - loss: 0.4662 - val_loss: 0.8873
Epoch 03760: val_loss improved from 0.88870 to 0.88730, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 3761/5000
26/26 - 1s - loss: 0.4664 - val_loss: 0.8883
Epoch 3762/5000
26/26 - 1s - loss: 0.4656 - val_loss: 0.8883
Epoch 3763/5000
26/26 - 1s - loss: 0.4660 - val_loss: 0.8889
Epoch 3764/5000
26/26 - 1s - loss: 0.4654 - val_loss: 0.8893
Epoch 3765/5000
26/26 - 1s - loss: 0.4661 - val_loss: 0.8886
Epoch 3766/5000
26/26 - 1s - loss: 0.4658 - val_loss: 0.8874
Epoch 3767/5000
26/26 - 1s - loss: 0.4657 - val_loss: 0.8880
Epoch 3768/5000
26/26 - 1s - loss: 0.4673 - val_loss: 0.8879
Epoch 3769/5000
26/26 - 1s - loss: 0.4653 - val_loss: 0.8857
Epoch 3770/5000
26/26 - 1s - loss: 0.4662 - val_loss: 0.8879
Epoch 03770: val_loss did not improve from 0.88730
Epoch 3771/5000
26/26 - 1s - loss: 0.4649 - val_loss: 0.8855
Epoch 3772/5000
26/26 - 1s - loss: 0.4656 - val_loss: 0.8861
Epoch 3773/5000
26/26 - 1s - loss: 0.4652 - val_loss: 0.8876
Epoch 3774/5000
26/26 - 1s - loss: 0.4650 - val_loss: 0.8871
Epoch 3775/5000
26/26 - 1s - loss: 0.4649 - val_loss: 0.8865
Epoch 3776/5000
26/26 - 1s - loss: 0.4652 - val_loss: 0.8872
Epoch 3777/5000
26/26 - 1s - loss: 0.4660 - val_loss: 0.8860
Epoch 3778/5000
26/26 - 1s - loss: 0.4647 - val_loss: 0.8865
Epoch 3779/5000
26/26 - 1s - loss: 0.4646 - val_loss: 0.8860
Epoch 3780/5000
26/26 - 1s - loss: 0.4651 - val_loss: 0.8871
Epoch 03780: val_loss improved from 0.88730 to 0.88711, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 3781/5000
26/26 - 1s - loss: 0.4647 - val_loss: 0.8875
Epoch 3782/5000
26/26 - 1s - loss: 0.4640 - val_loss: 0.8887
Epoch 3783/5000
26/26 - 1s - loss: 0.4645 - val_loss: 0.8878
Epoch 3784/5000
26/26 - 1s - loss: 0.4655 - val_loss: 0.8864
Epoch 3785/5000
26/26 - 1s - loss: 0.4644 - val_loss: 0.8868
Epoch 3786/5000
26/26 - 1s - loss: 0.4649 - val_loss: 0.8856
Epoch 3787/5000
26/26 - 1s - loss: 0.4646 - val_loss: 0.8865
Epoch 3788/5000
26/26 - 1s - loss: 0.4636 - val_loss: 0.8868
Epoch 3789/5000
26/26 - 1s - loss: 0.4641 - val_loss: 0.8868
Epoch 3790/5000
26/26 - 1s - loss: 0.4635 - val_loss: 0.8858
Epoch 03790: val_loss improved from 0.88711 to 0.88584, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-5.model.weights.hdf5
Epoch 3791/5000
INFO     Computation time for training the single-label model for AR: 91.44 min
INFO     Network type: REG
INFO     Model: "sequential"
INFO     _________________________________________________________________
INFO     Layer (type)                 Output Shape              Param #
INFO     =================================================================
INFO     dense (Dense)                (None, 1024)              2098176
INFO     _________________________________________________________________
INFO     dropout (Dropout)            (None, 1024)              0
INFO     _________________________________________________________________
INFO     dense_1 (Dense)              (None, 512)               524800
INFO     _________________________________________________________________
INFO     dropout_1 (Dropout)          (None, 512)               0
INFO     _________________________________________________________________
INFO     dense_2 (Dense)              (None, 256)               131328
INFO     _________________________________________________________________
INFO     dropout_2 (Dropout)          (None, 256)               0
INFO     _________________________________________________________________
INFO     dense_3 (Dense)              (None, 128)               32896
INFO     _________________________________________________________________
INFO     dropout_3 (Dropout)          (None, 128)               0
INFO     _________________________________________________________________
INFO     dense_4 (Dense)              (None, 1)                 129
INFO     =================================================================
INFO     Total params: 2,787,329
INFO     Trainable params: 2,787,329
INFO     Non-trainable params: 0
INFO     _________________________________________________________________
26/26 - 1s - loss: 0.4642 - val_loss: 0.8863
Restoring model weights from the end of the best epoch.
Epoch 03791: early stopping
INFO     Evaluating trained model 'AR single-labeled Fold-5' on test data
INFO     Network type: REG
INFO     Model: "sequential_1"
INFO     _________________________________________________________________
INFO     Layer (type)                 Output Shape              Param #
INFO     =================================================================
INFO     dense_5 (Dense)              (None, 1024)              2098176
INFO     _________________________________________________________________
INFO     dropout_4 (Dropout)          (None, 1024)              0
INFO     _________________________________________________________________
INFO     dense_6 (Dense)              (None, 512)               524800
INFO     _________________________________________________________________
INFO     dropout_5 (Dropout)          (None, 512)               0
INFO     _________________________________________________________________
INFO     dense_7 (Dense)              (None, 256)               131328
INFO     _________________________________________________________________
INFO     dropout_6 (Dropout)          (None, 256)               0
INFO     _________________________________________________________________
INFO     dense_8 (Dense)              (None, 128)               32896
INFO     _________________________________________________________________
INFO     dropout_7 (Dropout)          (None, 128)               0
INFO     _________________________________________________________________
INFO     dense_9 (Dense)              (None, 1)                 129
INFO     =================================================================
INFO     Total params: 2,787,329
INFO     Trainable params: 2,787,329
INFO     Non-trainable params: 0
INFO     _________________________________________________________________
2024-07-16 00:22:42.838043: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.
INFO:tensorflow:Assets written to: /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_saved_model/assets
INFO     Assets written to: /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_saved_model/assets