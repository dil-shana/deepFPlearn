ERROR    The specified wabTarget for Weights & Biases tracking does not exist: ARR
INFO     X training matrix of shape (4128, 2048) and type float32
INFO     Y training matrix of shape (4128,) and type float32
INFO     Training of fold number: 0
INFO     Training sample distribution: train data: {-1.2016366720199585: 9, -1.2016383409500122: 5, -1.201635479927063: 4, -1.2016324996948242: 4, -1.2016342878341675: 3, -1.2016355991363525: 3, -1.2016304731369019: 3, -1.2016377449035645: 3, -1.2016369104385376: 3, -1.20163094997406: 3, -1.2016363143920898: 3, -1.2016384601593018: 3, -1.2016353607177734: 3, -1.2016310691833496: 2, -1.2016041278839111: 2, -1.201621651649475: 2, -1.2016339302062988: 2, -1.2016253471374512: 2, -1.2016303539276123: 2, -1.2016191482543945: 2, -1.201633095741272: 2, -1.201637625694275: 2, -1.201635718345642: 2, -1.2009780406951904: 2, -1.2016290426254272: 2, -1.2016302347183228: 2, -1.2016347646713257: 2, -1.201636791229248: 2, -1.201629877090454: 2, -1.201596975326538: 2, -1.201627254486084: 2, -1.2016327381134033: 2, -1.2016288042068481: 2, -1.2015819549560547: 2, 0.7922017574310303: 1, -0.5338919758796692: 1, -0.9352316856384277: 1, -0.962668240070343: 1, 1.210314393043518: 1, -0.7282882928848267: 1, 0.889022946357727: 1, -0.16512484848499298: 1, -1.1993547677993774: 1, 0.3796524703502655: 1, -0.17455770075321198: 1, -0.8245752453804016: 1, -0.33821895718574524: 1, 1.4534815549850464: 1, -0.2101057469844818: 1, 0.7325264811515808: 1, -0.3992172181606293: 1, 1.4413039684295654: 1, -1.1664562225341797: 1, -0.25767362117767334: 1, -0.84548020362854: 1, 0.6293842196464539: 1, 0.6573249697685242: 1, -0.3587249517440796: 1, 1.133412480354309: 1, -0.5520062446594238: 1, 0.8268551230430603: 1, -0.24214474856853485: 1, -1.201595425605774: 1, 1.6570682525634766: 1, -1.1236215829849243: 1, -1.201583981513977: 1, 0.7257186770439148: 1, -1.1936330795288086: 1, 1.4145740270614624: 1, -1.1556631326675415: 1, -0.19042982161045074: 1, 0.8071705102920532: 1, -1.201361894607544: 1, -0.5843047499656677: 1, -0.9046985507011414: 1, -0.7776852250099182: 1, -0.16230207681655884: 1, -1.1205850839614868: 1, -0.20700129866600037: 1, -1.1950759887695312: 1, -0.7939702272415161: 1, -0.10395042598247528: 1, 0.22981515526771545: 1, 1.0551327466964722: 1, 0.6233600974082947: 1, 0.05442709103226662: 1, -0.28925973176956177: 1, -0.45371508598327637: 1, -1.157366394996643: 1, -0.90742427110672: 1, 0.05307941138744354: 1, -1.198758602142334: 1, -0.14243346452713013: 1, 1.5167564153671265: 1, -0.200442373752594: 1, -0.420527845621109: 1, -0.22873257100582123: 1, 0.39437854290008545: 1, 1.4644227027893066: 1, -1.2010711431503296: 1, 1.3883335590362549: 1, 0.7481110692024231: 1, 0.7304840087890625: 1, -1.2016345262527466: 1, -0.7010860443115234: 1, -0.091583751142025: 1, -0.10903797298669815: 1, -0.5369133949279785: 1, 1.6846264600753784: 1, 1.4694101810455322: 1, 0.9761800765991211: 1, -0.8395327925682068: 1, -0.6273359060287476: 1, 1.8398889303207397: 1, 0.2900018095970154: 1, 0.8136993050575256: 1, -1.1977088451385498: 1, 0.8498935103416443: 1, 0.20913533866405487: 1, 0.9276106357574463: 1, -0.06996402144432068: 1, -0.2059621661901474: 1, -1.1091188192367554: 1, 0.7908697128295898: 1, 1.4821670055389404: 1, -1.1300313472747803: 1, 0.5362502932548523: 1, 1.76934015750885: 1, -0.6900844573974609: 1, 1.6735926866531372: 1, 0.1897212415933609: 1, -0.669135332107544: 1, -1.1908975839614868: 1, -1.1211071014404297: 1, 0.24924464523792267: 1, -0.0332360677421093: 1, -0.49170398712158203: 1, 0.02054119110107422: 1, 0.6609118580818176: 1, -0.7380070686340332: 1, -1.1992988586425781: 1, -1.0081939697265625: 1, 0.13578376173973083: 1, 1.2659807205200195: 1, 2.0270323753356934: 1, -1.1981785297393799: 1, 0.858607292175293: 1, -0.6635865569114685: 1, 0.4933412969112396: 1, 1.3437533378601074: 1, -1.1338447332382202: 1, -0.015705665573477745: 1, -0.7790790796279907: 1, 1.3501269817352295: 1, 1.8799982070922852: 1, 1.7851945161819458: 1, 1.4308977127075195: 1, 0.3117649555206299: 1, 1.8707987070083618: 1, -0.9339156150817871: 1, 0.009009350091218948: 1, 1.3494865894317627: 1, -0.48933231830596924: 1, -0.17582924664020538: 1, -0.026365874335169792: 1, -0.9972583055496216: 1, 1.456301212310791: 1, 0.019096076488494873: 1, 0.5573470592498779: 1, -0.5222632884979248: 1, 1.4923255443572998: 1, -0.34237241744995117: 1, 1.896134853363037: 1, -0.20045150816440582: 1, 0.11937177926301956: 1, -1.0622771978378296: 1, 2.26233172416687: 1, 1.5811641216278076: 1, -0.2302703857421875: 1, -1.194373369216919: 1, 0.10666077584028244: 1, -1.2016338109970093: 1, -0.7541334629058838: 1, -0.9053927063941956: 1, 1.3597513437271118: 1, -1.094030499458313: 1, -0.896551251411438: 1, 0.9305616617202759: 1, -0.46576231718063354: 1, -0.5885310769081116: 1, -0.25539615750312805: 1, -1.1994960308074951: 1, 1.5100682973861694: 1, -1.2015953063964844: 1, -1.0963668823242188: 1, 1.058951735496521: 1, 1.7338974475860596: 1, -1.1212965250015259: 1, -0.16789886355400085: 1, 1.0962333679199219: 1, 0.18295887112617493: 1, 0.7494425773620605: 1, -1.1370965242385864: 1, -0.18478137254714966: 1, -0.63347989320755: 1, -0.2709258496761322: 1, -1.1632437705993652: 1, 1.28579580783844: 1, 0.6749281883239746: 1, 1.4527193307876587: 1, 1.2188843488693237: 1, 1.1865397691726685: 1, 0.008224710822105408: 1, -0.14395083487033844: 1, 0.48444247245788574: 1, -0.45086827874183655: 1, -1.2015149593353271: 1, -0.009843221865594387: 1, -0.2874172031879425: 1, 0.11243647336959839: 1, 0.9953116178512573: 1, -0.33581042289733887: 1, -0.8429122567176819: 1, -0.41631850600242615: 1, -1.1992580890655518: 1, 1.4907145500183105: 1, 0.6321052312850952: 1, -0.7838892936706543: 1, 0.04224063828587532: 1, 1.4500402212142944: 1, -0.3404213488101959: 1, -0.1883729249238968: 1, 0.2389289289712906: 1, -1.0422825813293457: 1, 0.46070119738578796: 1, 5.037554740905762: 1, -0.46823152899742126: 1, 0.5474697947502136: 1, -0.10369612276554108: 1, 1.4938876628875732: 1, 1.4836735725402832: 1, -1.0246316194534302: 1, 1.7511584758758545: 1, 0.2925977110862732: 1, 0.8030492067337036: 1, -0.809281051158905: 1, 1.3772739171981812: 1, -0.9223102331161499: 1, -1.1504056453704834: 1, -1.201597809791565: 1, 1.5920872688293457: 1, -0.023513898253440857: 1, 1.5755736827850342: 1, -0.558110237121582: 1, -0.7893494963645935: 1, -0.5342384576797485: 1, -0.3907565474510193: 1, 1.446770191192627: 1, 0.9278587102890015: 1, -1.2016328573226929: 1, -1.0406304597854614: 1, 0.2726168930530548: 1, 1.0041277408599854: 1, 0.606712281703949: 1, -1.1479105949401855: 1, 1.5428107976913452: 1, 0.5111210942268372: 1, -1.18907630443573: 1, -0.6715388298034668: 1, -1.0097246170043945: 1, 1.039072036743164: 1, 1.8416829109191895: 1, -0.5022063255310059: 1, -0.5899453163146973: 1, 0.013616573065519333: 1, -0.26567548513412476: 1, -0.608808159828186: 1, 1.473099708557129: 1, -1.0933367013931274: 1, 1.5044559240341187: 1, 0.31476086378097534: 1, -0.1421377956867218: 1, 1.2539737224578857: 1, 1.4819786548614502: 1, 0.9851498603820801: 1, -0.2531147599220276: 1, -1.193282127380371: 1, -1.2016111612319946: 1, 0.3255096673965454: 1, -1.1470420360565186: 1, -0.671938955783844: 1, 0.3074534237384796: 1, -1.188680648803711: 1, 1.5627902746200562: 1, -1.102870225906372: 1, 0.01674770377576351: 1, -1.1914066076278687: 1, -0.6316778063774109: 1, 0.16001862287521362: 1, -1.0489486455917358: 1, 0.24442099034786224: 1, -0.014453819021582603: 1, 0.16428887844085693: 1, -1.2015115022659302: 1, 1.597861409187317: 1, -1.1879688501358032: 1, 1.636014699935913: 1, -0.5174428224563599: 1, -1.1929867267608643: 1, 0.3824501037597656: 1, 0.2787120044231415: 1, 1.8017815351486206: 1, 0.31707215309143066: 1, -0.5717604756355286: 1, -1.1689379215240479: 1, -0.5144169330596924: 1, -0.8657602667808533: 1, -1.0349972248077393: 1, -0.2558027505874634: 1, -1.2015410661697388: 1, 1.2156920433044434: 1, -0.44012218713760376: 1, -1.0697368383407593: 1, 0.31867286562919617: 1, 0.7279888391494751: 1, -0.16336557269096375: 1, 1.257509708404541: 1, -0.981871485710144: 1, -1.0411947965621948: 1, 0.48456287384033203: 1, 1.3174397945404053: 1, 0.12815426290035248: 1, -0.6981508731842041: 1, -0.6311256289482117: 1, -1.0666824579238892: 1, 0.5256680846214294: 1, -0.3858093321323395: 1, -0.05808640271425247: 1, -1.2014774084091187: 1, 1.5557059049606323: 1, -0.3390061855316162: 1, 1.5299123525619507: 1, -1.1108695268630981: 1, -1.1475187540054321: 1, -0.39169713854789734: 1, 0.11165464669466019: 1, 0.6886505484580994: 1, 0.01178812701255083: 1, -0.6867276430130005: 1, 1.6108869314193726: 1, -0.4003660976886749: 1, -0.22210338711738586: 1, -0.24450848996639252: 1, 0.9832457304000854: 1, 1.2882025241851807: 1, 1.1214849948883057: 1, 1.0926192998886108: 1, 1.5009726285934448: 1, 0.9730016589164734: 1, -1.193730115890503: 1, -0.1565595418214798: 1, 1.471611738204956: 1, -1.1322532892227173: 1, 0.6820655465126038: 1, -0.13443662226200104: 1, 0.592692494392395: 1, -1.0356733798980713: 1, -1.1109116077423096: 1, -1.14544677734375: 1, 1.2373515367507935: 1, 0.6777730584144592: 1, -0.288830429315567: 1, -1.1323087215423584: 1, -0.03975825384259224: 1, 0.18436919152736664: 1, -1.0620733499526978: 1, 1.4639532566070557: 1, -0.8472578525543213: 1, -0.5160067081451416: 1, 0.22540217638015747: 1, -1.0813374519348145: 1, 1.9010276794433594: 1, 1.339892864227295: 1, -1.201583743095398: 1, -0.39157962799072266: 1, -1.1610828638076782: 1, -1.2013626098632812: 1, -0.8993450403213501: 1, -1.1800310611724854: 1, -0.17489729821681976: 1, 0.18457916378974915: 1, 1.3759360313415527: 1, 0.9149655103683472: 1, -0.06736226379871368: 1, 0.3231067657470703: 1, -0.0515512116253376: 1, -1.1954847574234009: 1, 0.6444940567016602: 1, 0.04404761642217636: 1, 0.22667939960956573: 1, 0.7332795858383179: 1, 0.8318881988525391: 1, 0.22794750332832336: 1, -0.1308683305978775: 1, 1.2531977891921997: 1, 0.06883639097213745: 1, -0.13872799277305603: 1, 1.1246896982192993: 1, -1.170555830001831: 1, -1.0281414985656738: 1, 0.05473056063055992: 1, -1.1722731590270996: 1, -0.08719541132450104: 1, 1.281778335571289: 1, 1.5742621421813965: 1, 0.08744630217552185: 1, -1.1925454139709473: 1, -0.08030800521373749: 1, 1.5719680786132812: 1, 0.29415011405944824: 1, -1.1498054265975952: 1, 0.7523799538612366: 1, -1.1402051448822021: 1, 0.2407364845275879: 1, 1.3784377574920654: 1, -0.2937408983707428: 1, -1.0416630506515503: 1, -0.9877877235412598: 1, 1.8886953592300415: 1, -0.16857680678367615: 1, 0.7907249331474304: 1, -1.1953339576721191: 1, -1.1912822723388672: 1, -1.0221163034439087: 1, 1.7097703218460083: 1, -0.11932859569787979: 1, -0.37577909231185913: 1, -1.186226725578308: 1, 1.4504951238632202: 1, -0.4774172008037567: 1, 1.796111822128296: 1, -1.194710373878479: 1, -1.2011888027191162: 1, 1.5870535373687744: 1, 1.2526917457580566: 1, -1.1746125221252441: 1, 0.7244752645492554: 1, 0.055386364459991455: 1, -1.1970044374465942: 1, 0.273947536945343: 1, -1.1859160661697388: 1, -0.19042661786079407: 1, 1.8601784706115723: 1, 0.21325090527534485: 1, 0.27843114733695984: 1, 1.540098786354065: 1, -1.197619080543518: 1, -1.180529236793518: 1, 1.0700626373291016: 1, -0.10739652067422867: 1, -1.0558182001113892: 1, -0.11584310233592987: 1, -0.8443267941474915: 1, 0.49092090129852295: 1, -0.9010990262031555: 1, 1.542358636856079: 1, -1.2016159296035767: 1, 1.8067305088043213: 1, -0.20500345528125763: 1, 0.1322988122701645: 1, -0.07565759867429733: 1, -0.2650972902774811: 1, 1.6906383037567139: 1, 1.1960792541503906: 1, -0.5850008726119995: 1, -0.5088394284248352: 1, -0.9458178281784058: 1, 1.0987391471862793: 1, 0.8007993698120117: 1, 1.3615977764129639: 1, -0.9793020486831665: 1, -1.1779894828796387: 1, -0.957834780216217: 1, -1.2016350030899048: 1, 0.11520501971244812: 1, -0.32526895403862: 1, -1.0519613027572632: 1, -0.19534148275852203: 1, 0.07571198046207428: 1, -0.11078709363937378: 1, -0.459964781999588: 1, -1.201631784439087: 1, -1.1676386594772339: 1, 1.5124294757843018: 1, 0.6508381366729736: 1, -0.4824954569339752: 1, 0.43118155002593994: 1, -0.06838630884885788: 1, -0.7244925498962402: 1, 1.8291547298431396: 1, 0.4876076281070709: 1, -0.7543148398399353: 1, 0.19674475491046906: 1, -0.12463472783565521: 1, 0.7951827049255371: 1, -0.2772659659385681: 1, 0.2642950117588043: 1, 1.491416096687317: 1, 0.32629087567329407: 1, -0.993471086025238: 1, -0.18254651129245758: 1, -1.2009185552597046: 1, 1.3062021732330322: 1, -1.1333893537521362: 1, 0.19680047035217285: 1, -0.35491982102394104: 1, 1.4690353870391846: 1, 1.473071813583374: 1, -1.201625108718872: 1, 0.1011820137500763: 1, -0.30124133825302124: 1, -0.19914157688617706: 1, -0.9635469913482666: 1, -0.538144052028656: 1, 1.368375539779663: 1, 1.2888545989990234: 1, -1.2015430927276611: 1, -0.7914682626724243: 1, 0.10008653253316879: 1, -1.1903178691864014: 1, -0.9919153451919556: 1, 1.5016316175460815: 1, -0.3135724663734436: 1, 1.0347987413406372: 1, -0.5897277593612671: 1, 0.9650787115097046: 1, 1.5793328285217285: 1, -1.0972120761871338: 1, -0.11705746501684189: 1, -0.24564200639724731: 1, -1.1962995529174805: 1, -0.6538054943084717: 1, 0.004675476811826229: 1, -0.1989370882511139: 1, 0.8999701142311096: 1, -0.22351212799549103: 1, -0.10290281474590302: 1, -0.7923315763473511: 1, 0.002965901279821992: 1, -0.16514527797698975: 1, 0.5357815027236938: 1, -1.177890419960022: 1, -0.019765598699450493: 1, -0.5638068318367004: 1, -0.8599510192871094: 1, 1.2984728813171387: 1, -0.37779542803764343: 1, 1.0995265245437622: 1, 1.4056357145309448: 1, -0.30856987833976746: 1, 0.36895880103111267: 1, 1.2405850887298584: 1, -0.42487016320228577: 1, -0.26545509696006775: 1, 1.0147548913955688: 1, -0.8713244199752808: 1, -0.03796708956360817: 1, -1.2016295194625854: 1, -0.16204535961151123: 1, 1.8266348838806152: 1, 0.7471779584884644: 1, -0.4400210678577423: 1, -0.08886344730854034: 1, -1.1406184434890747: 1, 1.4751214981079102: 1, 1.2598285675048828: 1, -1.0825824737548828: 1, -0.9396678805351257: 1, 0.5472216606140137: 1, -0.040548212826251984: 1, -0.021469445899128914: 1, -1.2014974355697632: 1, 1.271135687828064: 1, -0.2168547362089157: 1, 0.5634517669677734: 1, -0.16177639365196228: 1, 0.39953649044036865: 1, -1.201619267463684: 1, 1.1683013439178467: 1, 0.810942530632019: 1, -0.6426911950111389: 1, -0.4194781482219696: 1, 1.887890100479126: 1, -0.34326422214508057: 1, -1.1991721391677856: 1, 0.008013189770281315: 1, 3.259727716445923: 1, 0.540539026260376: 1, 1.6759966611862183: 1, 1.282943606376648: 1, -1.2016282081604004: 1, -0.5025617480278015: 1, 0.9968640804290771: 1, 1.4956955909729004: 1, -1.0713788270950317: 1, -0.3440307080745697: 1, -0.35407769680023193: 1, -1.1744723320007324: 1, 1.481839656829834: 1, 1.3759030103683472: 1, 1.5088152885437012: 1, -1.0395952463150024: 1, 4.282702445983887: 1, 0.6101611256599426: 1, -1.157931923866272: 1, 5.05051851272583: 1, 1.3899286985397339: 1, 1.4899855852127075: 1, 1.0692790746688843: 1, 0.9907589554786682: 1, -0.02133699133992195: 1, 0.9529565572738647: 1, -0.7970835566520691: 1, -0.016240552067756653: 1, 0.7520656585693359: 1, 1.2595564126968384: 1, 0.05425111949443817: 1, -0.8486149311065674: 1, -1.1967262029647827: 1, 0.12959904968738556: 1, 1.2822530269622803: 1, -0.10643515735864639: 1, 1.1462621688842773: 1, -1.1882411241531372: 1, -0.29025569558143616: 1, -1.088794231414795: 1, 1.2427196502685547: 1, 0.7230984568595886: 1, -1.175083041191101: 1, 0.32448264956474304: 1, 0.8794386982917786: 1, 1.477781891822815: 1, 0.6668532490730286: 1, 1.2712597846984863: 1, 0.4001854956150055: 1, 0.7352478504180908: 1, -1.2016136646270752: 1, -0.35842111706733704: 1, -0.18418414890766144: 1, -0.14771254360675812: 1, 0.5713070034980774: 1, 0.19746625423431396: 1, -0.907404899597168: 1, -1.193596363067627: 1, -0.5908447504043579: 1, 4.722275257110596: 1, -1.1212905645370483: 1, 1.4069277048110962: 1, 0.6161129474639893: 1, 0.20812031626701355: 1, 0.6158161163330078: 1, -0.9218448400497437: 1, 1.6304298639297485: 1, -0.4791189730167389: 1, 1.2413678169250488: 1, 1.6493014097213745: 1, -0.9824236631393433: 1, -0.6904935240745544: 1, 1.6013163328170776: 1, -0.14438967406749725: 1, -1.1716605424880981: 1, -0.8027163147926331: 1, -1.2013036012649536: 1, -0.6686071157455444: 1, -0.38419657945632935: 1, 0.07517638802528381: 1, 1.7395148277282715: 1, -0.37206733226776123: 1, -0.05180063098669052: 1, -0.061833277344703674: 1, -0.15591250360012054: 1, -0.39828142523765564: 1, -0.9134752750396729: 1, 1.274375557899475: 1, 1.00320303440094: 1, 1.5747997760772705: 1, 0.5790113210678101: 1, 1.536348581314087: 1, -1.1339654922485352: 1, -0.13019442558288574: 1, -0.024461327120661736: 1, -0.9453350901603699: 1, -1.2000701427459717: 1, 0.6829988956451416: 1, -0.8916471600532532: 1, -0.5300003886222839: 1, 0.6606081128120422: 1, 0.5761882066726685: 1, 0.3640252351760864: 1, -1.2016232013702393: 1, 0.8396581411361694: 1, 0.8847638368606567: 1, -0.556195080280304: 1, 0.9252344369888306: 1, 0.06706182658672333: 1, -0.6896651983261108: 1, -1.169080138206482: 1, -0.6431723237037659: 1, 1.5370275974273682: 1, 0.562964141368866: 1, -0.9035985469818115: 1, -1.2010724544525146: 1, 0.6560998558998108: 1, -0.08804576843976974: 1, -1.1515345573425293: 1, -1.1840753555297852: 1, -1.2016332149505615: 1, -0.08275699615478516: 1, 5.006033897399902: 1, 1.6683679819107056: 1, 0.666642963886261: 1, 1.9020731449127197: 1, -0.5120261311531067: 1, 0.9834553003311157: 1, -0.028692159801721573: 1, 0.8677871227264404: 1, 1.2950940132141113: 1, -1.201636552810669: 1, 1.287703037261963: 1, 0.2662027180194855: 1, 1.6096405982971191: 1, -0.19083698093891144: 1, -0.5348793864250183: 1, -1.1956098079681396: 1, 1.001185417175293: 1, -0.30866673588752747: 1, -1.1697360277175903: 1, 0.637002170085907: 1, 0.24701066315174103: 1, 1.0350605249404907: 1, -1.093284010887146: 1, -0.4005826711654663: 1, -1.1568188667297363: 1, 1.0343732833862305: 1, -0.8594576716423035: 1, -0.15746326744556427: 1, -1.1100589036941528: 1, 1.7476170063018799: 1, 1.111115574836731: 1, -0.56696617603302: 1, -0.21156159043312073: 1, 0.03225273638963699: 1, 1.0769490003585815: 1, 1.9090871810913086: 1, -0.6031805276870728: 1, -0.425843745470047: 1, 1.7558945417404175: 1, -0.2670007050037384: 1, -0.12876513600349426: 1, -0.8705235719680786: 1, 0.1331777125597: 1, 0.2817704975605011: 1, 0.2591017186641693: 1, -0.9714952111244202: 1, 1.4151798486709595: 1, -0.2810556888580322: 1, -0.15606260299682617: 1, 0.7936035394668579: 1, -1.1707801818847656: 1, 0.171223446726799: 1, 0.36215391755104065: 1, 1.0766181945800781: 1, -1.2016162872314453: 1, 1.9172451496124268: 1, 0.7782679200172424: 1, -0.9971945881843567: 1, 0.5755087733268738: 1, -0.2098180055618286: 1, -0.009973025880753994: 1, 0.3124358654022217: 1, -1.1729844808578491: 1, 0.2115481197834015: 1, 1.547389030456543: 1, -1.201622486114502: 1, -0.002722974168136716: 1, 1.1970174312591553: 1, -1.0501618385314941: 1, -0.136034294962883: 1, 0.98076331615448: 1, 0.12906195223331451: 1, 1.2142442464828491: 1, 1.1092147827148438: 1, 0.01664043217897415: 1, 1.5455868244171143: 1, -1.197718858718872: 1, 1.8846700191497803: 1, -0.11230547726154327: 1, -0.20943275094032288: 1, -0.4444347023963928: 1, 1.4051384925842285: 1, -1.1992485523223877: 1, 0.3661552369594574: 1, 0.19889964163303375: 1, 0.780617356300354: 1, -1.1506352424621582: 1, -0.3316475749015808: 1, 0.275499552488327: 1, -0.6411266922950745: 1, -1.0014375448226929: 1, -0.24040797352790833: 1, -0.8740671277046204: 1, -0.9927355051040649: 1, -0.4354245066642761: 1, 0.8696494698524475: 1, -0.7601802349090576: 1, 1.0318180322647095: 1, 0.5170465111732483: 1, -1.0076838731765747: 1, -0.2761753499507904: 1, 0.3744315505027771: 1, -0.6870049834251404: 1, -0.4185396134853363: 1, 5.012721538543701: 1, -0.92970210313797: 1, 0.6983891129493713: 1, -1.1607003211975098: 1, -1.144382119178772: 1, 1.5667779445648193: 1, -0.7234905958175659: 1, -1.2014120817184448: 1, 1.233654499053955: 1, -0.3774075210094452: 1, -1.1150031089782715: 1, -1.1999870538711548: 1, -0.7490406036376953: 1, 0.9129876494407654: 1, 0.053155358880758286: 1, 1.6493046283721924: 1, 0.3489625155925751: 1, 0.5599294900894165: 1, -0.5990430116653442: 1, 0.9840258359909058: 1, 0.8440163731575012: 1, -0.16826894879341125: 1, -0.11703558266162872: 1, -0.6496835947036743: 1, 0.35881760716438293: 1, 0.029840881004929543: 1, -0.04211708903312683: 1, 0.12603989243507385: 1, -0.4268537759780884: 1, -0.42109400033950806: 1, -0.20986565947532654: 1, -1.107023000717163: 1, 1.1129239797592163: 1, 1.3301595449447632: 1, -1.1573199033737183: 1, -1.2016154527664185: 1, -0.5345551371574402: 1, 0.9611315131187439: 1, -0.40274444222450256: 1, 0.3422311544418335: 1, 1.0101338624954224: 1, 0.23876602947711945: 1, -0.7447215914726257: 1, -1.1756606101989746: 1, -1.0657732486724854: 1, -0.11455459892749786: 1, -1.1579643487930298: 1, 0.2889866232872009: 1, 1.3890480995178223: 1, -0.06601618975400925: 1, 0.9352988600730896: 1, -0.6006320714950562: 1, 0.2903974652290344: 1, 1.5724915266036987: 1, 0.4990178942680359: 1, 0.31239357590675354: 1, -1.054260492324829: 1, -1.201563835144043: 1, 1.0972230434417725: 1, 0.033837273716926575: 1, -1.1181161403656006: 1, -1.0877141952514648: 1, 1.0683897733688354: 1, 0.6071187853813171: 1, -1.198028802871704: 1, 0.4937743842601776: 1, -0.9567206501960754: 1, -1.171905517578125: 1, 0.0023630079813301563: 1, -1.193078875541687: 1, -0.6266202926635742: 1, 0.8500742316246033: 1, -1.1623584032058716: 1, -1.201407790184021: 1, 1.9273109436035156: 1, -1.201493740081787: 1, -0.14633353054523468: 1, 0.3313300311565399: 1, 1.8254425525665283: 1, -1.1138004064559937: 1, -1.1503796577453613: 1, 0.20136801898479462: 1, 0.9726781249046326: 1, 0.12876805663108826: 1, -1.0416687726974487: 1, 1.569981575012207: 1, -0.07435453683137894: 1, 1.3129916191101074: 1, -0.20083148777484894: 1, 0.8171444535255432: 1, 1.0165932178497314: 1, 1.5909359455108643: 1, -1.02364981174469: 1, -0.5479841232299805: 1, -1.091170310974121: 1, 0.029728559777140617: 1, -0.5884501934051514: 1, 0.5732383131980896: 1, 1.4807751178741455: 1, -1.2008600234985352: 1, -1.0986745357513428: 1, -0.3237372636795044: 1, -1.1774789094924927: 1, 0.26258811354637146: 1, -1.1923046112060547: 1, 0.34191834926605225: 1, -0.8867827653884888: 1, -0.012521528638899326: 1, 0.5795131325721741: 1, -0.4653182625770569: 1, -0.4105451703071594: 1, 0.3591007590293884: 1, 1.3037681579589844: 1, -1.1260875463485718: 1, -0.7379482984542847: 1, -0.6031686067581177: 1, 1.2443398237228394: 1, 0.6273993253707886: 1, -1.1927686929702759: 1, -0.16245944797992706: 1, 0.7324214577674866: 1, -1.1028145551681519: 1, 1.2671806812286377: 1, -1.040601134300232: 1, 0.8417104482650757: 1, -0.027011625468730927: 1, -1.0309724807739258: 1, 1.5324621200561523: 1, 1.302850365638733: 1, 0.6854439377784729: 1, 0.7587617635726929: 1, -1.1961579322814941: 1, 1.878305196762085: 1, -1.2008949518203735: 1, 1.5722275972366333: 1, 0.2808535397052765: 1, -0.7683218121528625: 1, -0.1397843062877655: 1, -0.5199218392372131: 1, 0.10738043487071991: 1, -0.16393840312957764: 1, -0.02250954695045948: 1, 0.21984633803367615: 1, -0.9954119920730591: 1, 0.9798484444618225: 1, -0.8793452382087708: 1, 1.6888245344161987: 1, -0.9859256744384766: 1, -1.0983095169067383: 1, -0.8270519375801086: 1, 0.5890273451805115: 1, 0.6842910647392273: 1, 1.3033519983291626: 1, -1.1962217092514038: 1, 1.7614209651947021: 1, -1.2015316486358643: 1, -0.6980454921722412: 1, -1.1852269172668457: 1, -0.8602240085601807: 1, 0.1144905686378479: 1, -1.1735186576843262: 1, 1.1691573858261108: 1, 1.478103518486023: 1, -1.2007230520248413: 1, -0.463926762342453: 1, 1.7606215476989746: 1, -0.9047161340713501: 1, -1.1375747919082642: 1, -1.1963841915130615: 1, 0.24872112274169922: 1, -1.051085352897644: 1, 0.8335886001586914: 1, -1.0407896041870117: 1, -0.28641819953918457: 1, 1.5112932920455933: 1, -0.8751402497291565: 1, 0.8861773610115051: 1, 0.5456136465072632: 1, 1.5149681568145752: 1, 0.2826254069805145: 1, 0.7623692750930786: 1, 0.9693878293037415: 1, -1.2016057968139648: 1, 0.2713952362537384: 1, -1.1336802244186401: 1, 1.1985303163528442: 1, -0.7443411946296692: 1, -0.4976504147052765: 1, 1.427628755569458: 1, -1.002498984336853: 1, 0.9285130500793457: 1, -1.1962871551513672: 1, -0.005674127489328384: 1, -1.1957098245620728: 1, 1.4411144256591797: 1, 0.8892757296562195: 1, 0.6535213589668274: 1, -1.201536774635315: 1, 1.192413568496704: 1, -0.6276612877845764: 1, -1.2016189098358154: 1, -0.5378462672233582: 1, 0.0891273021697998: 1, 1.633592128753662: 1, -1.0076677799224854: 1, -1.1646332740783691: 1, 1.0157313346862793: 1, -0.5209143161773682: 1, -0.6757361888885498: 1, -0.008235386572778225: 1, -1.1448140144348145: 1, 1.901644229888916: 1, -1.1760764122009277: 1, 0.9394524693489075: 1, -0.7864221334457397: 1, 1.3913298845291138: 1, 0.23716896772384644: 1, -1.171040415763855: 1, 0.034827083349227905: 1, -1.201562523841858: 1, -1.2002716064453125: 1, 0.5468651056289673: 1, -0.24342182278633118: 1, 1.1942393779754639: 1, 0.28549933433532715: 1, 0.42848649621009827: 1, 0.2222539335489273: 1, 1.2926610708236694: 1, -0.3219013214111328: 1, 1.1086541414260864: 1, -1.1812138557434082: 1, 0.1563034951686859: 1, 1.2604477405548096: 1, -0.42767956852912903: 1, 1.3192821741104126: 1, 0.0860099047422409: 1, 0.3248298168182373: 1, 1.7776927947998047: 1, -0.8111313581466675: 1, -0.3642917275428772: 1, 0.7438257932662964: 1, -1.2016229629516602: 1, -1.2016254663467407: 1, 1.260263442993164: 1, 0.2846507132053375: 1, -0.04390183836221695: 1, 0.04932570457458496: 1, -0.7482910752296448: 1, 0.840314507484436: 1, 0.7984791398048401: 1, 0.6152291297912598: 1, -0.22242674231529236: 1, -0.007039392367005348: 1, 0.8274267315864563: 1, 0.29349300265312195: 1, -1.2015275955200195: 1, -1.2015928030014038: 1, 0.3571586012840271: 1, -1.201560139656067: 1, -1.1136521100997925: 1, -1.1941906213760376: 1, -0.0580214224755764: 1, 0.34432777762413025: 1, 0.042822714895009995: 1, -0.6390724778175354: 1, 0.3915187418460846: 1, -0.28742867708206177: 1, 0.040903303772211075: 1, -0.39423879981040955: 1, 1.088638186454773: 1, 0.8209275603294373: 1, 1.6697852611541748: 1, -1.2015451192855835: 1, 1.3002121448516846: 1, -1.201615810394287: 1, -1.1972460746765137: 1, -0.13113076984882355: 1, 0.7218993902206421: 1, -0.6325321793556213: 1, -1.2015955448150635: 1, 1.4237861633300781: 1, -1.1586220264434814: 1, 0.35647323727607727: 1, -0.23398016393184662: 1, -0.40984249114990234: 1, -1.1868388652801514: 1, -0.7198786735534668: 1, -1.1364892721176147: 1, 0.5530001521110535: 1, -0.6421114802360535: 1, 0.048983871936798096: 1, -0.9669303297996521: 1, -0.011501064524054527: 1, 1.0418422222137451: 1, 1.5550175905227661: 1, -0.22581757605075836: 1, 1.5701237916946411: 1, 0.880368709564209: 1, 0.32263097167015076: 1, 0.005114047322422266: 1, 0.7670885920524597: 1, -0.06900987029075623: 1, -0.03557446971535683: 1, 1.4667671918869019: 1, 0.1497194468975067: 1, 0.7884150743484497: 1, -0.39197561144828796: 1, -0.9789384603500366: 1, -0.5612114071846008: 1, 1.6431117057800293: 1, -0.08915898948907852: 1, 1.1008855104446411: 1, 0.43645578622817993: 1, -0.8622487187385559: 1, -1.162119746208191: 1, -0.766767144203186: 1, 1.127722144126892: 1, -1.1900941133499146: 1, -0.1031346470117569: 1, -0.7184330821037292: 1, 0.2743425965309143: 1, -1.1040070056915283: 1, 1.2280430793762207: 1, -0.3311309218406677: 1, 0.6865427494049072: 1, -0.31671836972236633: 1, 5.072229385375977: 1, -1.1848548650741577: 1, -1.2015702724456787: 1, -1.1205617189407349: 1, 1.8768579959869385: 1, -1.2014681100845337: 1, -0.9288858771324158: 1, -1.1966667175292969: 1, -0.35984915494918823: 1, -0.9092623591423035: 1, -0.9299617409706116: 1, 1.746630072593689: 1, -0.26631277799606323: 1, -0.539626955986023: 1, 1.8896540403366089: 1, -0.3497014045715332: 1, -1.199397087097168: 1, 1.3037792444229126: 1, 1.2464758157730103: 1, -1.2014092206954956: 1, -0.8503693342208862: 1, 1.2778698205947876: 1, -0.07614605128765106: 1, 0.12041562795639038: 1, 0.5059344172477722: 1, -0.2553582787513733: 1, 1.7069745063781738: 1, -0.4622204601764679: 1, -1.1041064262390137: 1, 0.2549906373023987: 1, -0.045158740133047104: 1, -0.34817975759506226: 1, -0.8211961388587952: 1, 0.3664189279079437: 1, -0.032225385308265686: 1, -0.8986467123031616: 1, 1.7425659894943237: 1, -0.1335328370332718: 1, 0.16367900371551514: 1, -0.6085047125816345: 1, -1.1857080459594727: 1, -0.6818874478340149: 1, -0.3594827950000763: 1, -0.8678878545761108: 1, -0.8662946820259094: 1, 0.17100460827350616: 1, -0.0404001921415329: 1, 1.7397531270980835: 1, -0.2681446373462677: 1, 0.004652172327041626: 1, 1.2042120695114136: 1, -0.3503449857234955: 1, 0.31131237745285034: 1, 1.571593999862671: 1, 0.44458866119384766: 1, 1.3986883163452148: 1, -0.3465023636817932: 1, -1.1569617986679077: 1, -1.2016127109527588: 1, 0.7346874475479126: 1, 0.881543755531311: 1, -0.8834558129310608: 1, 1.2588475942611694: 1, -1.130736231803894: 1, 0.5131713151931763: 1, -1.18364417552948: 1, 0.23464715480804443: 1, 1.2994223833084106: 1, 1.5503476858139038: 1, -0.6562255024909973: 1, -1.1325860023498535: 1, 0.1718989461660385: 1, -0.6922621726989746: 1, -0.24759358167648315: 1, -0.35478705167770386: 1, 0.04922454059123993: 1, -1.192929744720459: 1, -0.3422335684299469: 1, -0.36514076590538025: 1, 0.003300704760476947: 1, 0.591416597366333: 1, -0.24634599685668945: 1, -0.10326965153217316: 1, 1.7544053792953491: 1, -0.4610443115234375: 1, -1.2004859447479248: 1, 0.7517246007919312: 1, 0.3015405535697937: 1, -1.1139642000198364: 1, -1.1617506742477417: 1, 1.0176738500595093: 1, -0.3156169652938843: 1, 4.875144004821777: 1, 0.8218473196029663: 1, -1.1329883337020874: 1, 1.4644434452056885: 1, -0.17342792451381683: 1, -0.10037212073802948: 1, -0.19223442673683167: 1, -0.5381679534912109: 1, -0.6403390765190125: 1, -1.167614459991455: 1, 0.46832725405693054: 1, -0.3702247440814972: 1, -0.22176611423492432: 1, -0.28438881039619446: 1, -1.194927453994751: 1, -0.13618627190589905: 1, 0.3153885304927826: 1, -1.201418161392212: 1, 0.24058645963668823: 1, -1.188598394393921: 1, 0.8313724994659424: 1, 1.5401815176010132: 1, 1.1887938976287842: 1, -1.1485586166381836: 1, 0.8028292059898376: 1, -1.2013877630233765: 1, -0.4684484004974365: 1, -1.1816785335540771: 1, 0.23050570487976074: 1, -1.1728081703186035: 1, -0.455705851316452: 1, 0.29691436886787415: 1, 1.3862724304199219: 1, 0.30982303619384766: 1, 1.2996296882629395: 1, 0.0014178809942677617: 1, -0.433001846075058: 1, 1.0659617185592651: 1, 1.2115764617919922: 1, -0.24468590319156647: 1, 0.4647902548313141: 1, 1.1562855243682861: 1, -0.4824763238430023: 1, -0.6864959597587585: 1, -1.2006478309631348: 1, -0.005905755329877138: 1, 1.295539140701294: 1, -0.8877851366996765: 1, 1.094826102256775: 1, 0.07662157714366913: 1, -1.1948002576828003: 1, -1.1987295150756836: 1, -1.1981743574142456: 1, 0.1589841991662979: 1, 1.0407052040100098: 1, 0.8952587246894836: 1, 1.0432312488555908: 1, -1.2016171216964722: 1, -0.002401667181402445: 1, 0.8076177835464478: 1, 1.2035483121871948: 1, 0.02654222585260868: 1, -1.167817234992981: 1, -0.7992537021636963: 1, 0.7365891337394714: 1, -0.5378177762031555: 1, -0.5369265675544739: 1, 1.123262643814087: 1, -0.13838951289653778: 1, 0.6591589450836182: 1, 0.3698660731315613: 1, 1.0540943145751953: 1, 0.7746310830116272: 1, 1.7298698425292969: 1, -0.47614291310310364: 1, 0.025362450629472733: 1, -0.9726563692092896: 1, 0.06307864934206009: 1, -0.6055639982223511: 1, -0.8234555125236511: 1, 1.0135881900787354: 1, 1.256977915763855: 1, -0.20395949482917786: 1, 1.058468222618103: 1, 0.44327667355537415: 1, -1.1852235794067383: 1, -0.518195390701294: 1, -1.0751264095306396: 1, -0.512914776802063: 1, -1.186597228050232: 1, -0.4143541157245636: 1, 0.822748601436615: 1, 1.3262649774551392: 1, 0.853833794593811: 1, -0.9986592531204224: 1, -0.19212104380130768: 1, -0.025104349479079247: 1, -1.2016221284866333: 1, -0.11514480412006378: 1, 1.556864857673645: 1, -0.7811231017112732: 1, -0.9141108393669128: 1, -1.2016135454177856: 1, 0.19668835401535034: 1, 0.05186900869011879: 1, 1.483720302581787: 1, -0.6111128330230713: 1, -1.191677212715149: 1, -0.41735291481018066: 1, -0.019019143655896187: 1, -0.5385211110115051: 1, 1.8749902248382568: 1, 0.9327585697174072: 1, -1.0521938800811768: 1, -0.9202075004577637: 1, 1.0590068101882935: 1, 0.611980676651001: 1, -0.9539603590965271: 1, 0.7785534858703613: 1, -1.1468044519424438: 1, 1.0842758417129517: 1, -0.563433825969696: 1, 0.6789939403533936: 1, 1.865704894065857: 1, -0.41403406858444214: 1, 1.4876383543014526: 1, -1.2006902694702148: 1, -0.9460977911949158: 1, -0.32544630765914917: 1, 0.8533394932746887: 1, -1.188031554222107: 1, -0.923246443271637: 1, -0.6586742401123047: 1, -1.0799649953842163: 1, -1.2015974521636963: 1, -0.15851348638534546: 1, -0.2147151529788971: 1, 0.002877143444493413: 1, -1.1943039894104004: 1, -1.0237786769866943: 1, -0.13542917370796204: 1, -0.4923703670501709: 1, 0.21768306195735931: 1, -0.712040364742279: 1, 1.0069524049758911: 1, 0.9790754318237305: 1, -0.6655375957489014: 1, -0.6306192278862: 1, -0.004194003064185381: 1, -1.1959139108657837: 1, 0.4253986179828644: 1, 0.04068145155906677: 1, 0.7298784255981445: 1, 1.043671727180481: 1, -1.2015763521194458: 1, -0.13886263966560364: 1, 0.8666924238204956: 1, 4.58308219909668: 1, 1.5823100805282593: 1, -0.11206144839525223: 1, -0.1827705055475235: 1, -1.0740134716033936: 1, -0.5749701261520386: 1, 0.31334978342056274: 1, -1.2015992403030396: 1, -0.4367877244949341: 1, -0.4737872779369354: 1, -0.35893186926841736: 1, -1.1972938776016235: 1, 1.1694233417510986: 1, -1.2016340494155884: 1, -0.17077305912971497: 1, 1.457643747329712: 1, -0.16766004264354706: 1, -0.010684509761631489: 1, -0.9129625558853149: 1, -0.6971253156661987: 1, -0.36594024300575256: 1, -1.17979097366333: 1, -1.1624016761779785: 1, -0.7948459982872009: 1, -0.1427413374185562: 1, -1.1405699253082275: 1, 1.629822015762329: 1, -0.3301823139190674: 1, -0.5935716032981873: 1, 1.1279875040054321: 1, -1.0387167930603027: 1, 0.949316143989563: 1, 1.1708778142929077: 1, -0.999508798122406: 1, 0.469992071390152: 1, -0.8279891610145569: 1, -0.20539666712284088: 1, -0.5897085666656494: 1, -0.24570585787296295: 1, 1.1998642683029175: 1, 1.6278821229934692: 1, 1.3844947814941406: 1, -0.5888482928276062: 1, -0.27123570442199707: 1, -0.9966712594032288: 1, -0.3076569437980652: 1, -1.0319366455078125: 1, -1.085170865058899: 1, 0.44933900237083435: 1, -0.5713714957237244: 1, -0.23679472506046295: 1, 1.3268651962280273: 1, -0.37734130024909973: 1, -1.0538722276687622: 1, 1.2882169485092163: 1, -1.0634658336639404: 1, -1.110012173652649: 1, -1.201610803604126: 1, 0.7533062696456909: 1, -1.1811505556106567: 1, -0.37911587953567505: 1, -1.0323843955993652: 1, 0.9158507585525513: 1, 0.8119110465049744: 1, 0.8504006266593933: 1, 1.0329307317733765: 1, -1.2015986442565918: 1, -0.4307803809642792: 1, -0.4220041036605835: 1, -1.1936933994293213: 1, 1.4364150762557983: 1, -0.5802597403526306: 1, -0.5372467637062073: 1, -1.2009141445159912: 1, -0.3739321231842041: 1, 0.9686956405639648: 1, 1.8706549406051636: 1, 1.1854524612426758: 1, -0.4253336787223816: 1, 0.29118791222572327: 1, 1.577986240386963: 1, -1.1924408674240112: 1, 1.8480533361434937: 1, -1.1791499853134155: 1, 0.7611046433448792: 1, -1.024586796760559: 1, 0.276736855506897: 1, -1.194933295249939: 1, -0.4453357756137848: 1, 1.7350994348526: 1, 1.6648616790771484: 1, -1.145255446434021: 1, -0.17054051160812378: 1, -0.22990889847278595: 1, 1.6200270652770996: 1, 0.3488617241382599: 1, -0.5330178141593933: 1, 1.4062000513076782: 1, 0.5583640336990356: 1, 0.454349547624588: 1, -0.5142630338668823: 1, -0.7296833395957947: 1, 0.8895251154899597: 1, -1.1871360540390015: 1, 1.3250715732574463: 1, -1.2016326189041138: 1, -1.2016146183013916: 1, 1.610649824142456: 1, 1.2964091300964355: 1, 0.007953275926411152: 1, 0.27488669753074646: 1, -1.1973918676376343: 1, -0.7001152634620667: 1, -0.41674312949180603: 1, 1.8596769571304321: 1, -0.002975589595735073: 1, -1.2014697790145874: 1, -1.2016299962997437: 1, 0.28860634565353394: 1, 0.23767612874507904: 1, -0.5517370104789734: 1, 0.7950616478919983: 1, -0.0960833728313446: 1, 1.1153340339660645: 1, 0.9011586904525757: 1, 1.6676998138427734: 1, -0.32589226961135864: 1, 0.9889004230499268: 1, 0.17207567393779755: 1, -1.1333073377609253: 1, -1.200080394744873: 1, -1.1995155811309814: 1, 0.3162490129470825: 1, -1.2016372680664062: 1, -1.2013144493103027: 1, 0.10213274508714676: 1, 0.48826223611831665: 1, -1.201594352722168: 1, -1.1490250825881958: 1, -1.1647279262542725: 1, -0.3100615441799164: 1, -1.1416743993759155: 1, -1.09720778465271: 1, -0.1709771454334259: 1, -1.1970343589782715: 1, -0.13741447031497955: 1, -0.1045512706041336: 1, 1.466652274131775: 1, 1.007346510887146: 1, 0.8969712257385254: 1, 0.6905592679977417: 1, -0.02432066947221756: 1, -0.17132940888404846: 1, 0.369517058134079: 1, -0.04312499612569809: 1, -0.16572129726409912: 1, -0.37186357378959656: 1, 1.0331618785858154: 1, 1.1217374801635742: 1, -1.0348321199417114: 1, -1.1798354387283325: 1, -0.0526600144803524: 1, 0.9554869532585144: 1, -1.1871466636657715: 1, 1.5425565242767334: 1, -0.6155209541320801: 1, 0.6448225975036621: 1, 1.4475048780441284: 1, -0.870884895324707: 1, 1.2730098962783813: 1, -1.1350377798080444: 1, -1.1054575443267822: 1, 3.341240167617798: 1, 1.525660514831543: 1, -0.22918304800987244: 1, -0.9704957008361816: 1, -0.9676279425621033: 1, 0.4515918791294098: 1, -1.0399388074874878: 1, 1.6113003492355347: 1, -1.1897622346878052: 1, 1.92928946018219: 1, 0.21555371582508087: 1, -1.2003082036972046: 1, 1.4788439273834229: 1, -0.30685290694236755: 1, 1.0116826295852661: 1, 0.637361466884613: 1, -0.506174623966217: 1, -1.045175313949585: 1, -0.8984062075614929: 1, -0.10418325662612915: 1, 1.2401331663131714: 1, -1.150406837463379: 1, -0.01552529539912939: 1, 1.0355088710784912: 1, 1.8881990909576416: 1, 0.19413244724273682: 1, -1.198868989944458: 1, -0.45158419013023376: 1, 1.114802360534668: 1, 0.2964378595352173: 1, 1.4752575159072876: 1, 0.09289468824863434: 1, -0.3917173445224762: 1, -1.0057076215744019: 1, -0.31709083914756775: 1, -1.0865159034729004: 1, -1.1979888677597046: 1, -0.576421856880188: 1, -1.1991511583328247: 1, -0.3634156584739685: 1, 4.112330913543701: 1, -0.21755054593086243: 1, 1.5002496242523193: 1, -0.07029284536838531: 1, -1.1690752506256104: 1, -1.2015588283538818: 1, 0.28526046872138977: 1, -1.1950762271881104: 1, -0.6588079929351807: 1, -1.0591267347335815: 1, 1.3625078201293945: 1, 1.3024239540100098: 1, 1.1571227312088013: 1, -1.1074978113174438: 1, 0.6052579283714294: 1, -0.48075738549232483: 1, -0.23024950921535492: 1, -0.15476621687412262: 1, -0.22985504567623138: 1, -1.1036909818649292: 1, 1.8893579244613647: 1, 1.4653488397598267: 1, 1.7602899074554443: 1, -0.7266088128089905: 1, 0.35639217495918274: 1, -0.116549052298069: 1, 0.27865079045295715: 1, 0.784543514251709: 1, -0.7624772787094116: 1, 0.9692907333374023: 1, 0.8790757060050964: 1, -1.2011059522628784: 1, 1.1508636474609375: 1, -0.6205415725708008: 1, -0.39972129464149475: 1, 0.8739101886749268: 1, 0.35902467370033264: 1, 1.6146701574325562: 1, -1.2013576030731201: 1, -1.1837621927261353: 1, -0.9720814824104309: 1, 1.593016266822815: 1, 1.1079081296920776: 1, 0.9786769151687622: 1, -0.6223658919334412: 1, -0.9601404666900635: 1, -0.11709770560264587: 1, -0.0026253368705511093: 1, -0.171332448720932: 1, 5.270293235778809: 1, 0.8805737495422363: 1, 1.7616217136383057: 1, -1.1929398775100708: 1, -1.2003904581069946: 1, 0.5139665007591248: 1, -1.0099024772644043: 1, -0.18796367943286896: 1, -1.1945738792419434: 1, 0.36155056953430176: 1, 1.612230896949768: 1, 0.9760450720787048: 1, -0.2711203992366791: 1, -1.0817426443099976: 1, -0.12864308059215546: 1, -0.7745821475982666: 1, -0.4648326635360718: 1, -0.8754668831825256: 1, -0.0695866122841835: 1, 0.9422100782394409: 1, -0.00951747503131628: 1, -1.2003253698349: 1, -1.1891672611236572: 1, -1.1509727239608765: 1, -1.098894715309143: 1, -1.1976145505905151: 1, -0.48821160197257996: 1, -0.8910970091819763: 1, 0.5382747054100037: 1, 0.5874748826026917: 1, 0.7860816717147827: 1, 0.6905925273895264: 1, -0.750208854675293: 1, 0.15043634176254272: 1, -0.7420345544815063: 1, 0.032617583870887756: 1, -0.9307324290275574: 1, 0.7159720659255981: 1, 0.18603742122650146: 1, 0.6975425481796265: 1, 1.3083291053771973: 1, -1.201588749885559: 1, 0.39806419610977173: 1, -1.195853590965271: 1, -1.1028809547424316: 1, 0.5067287683486938: 1, -1.2012838125228882: 1, -0.7500604391098022: 1, 0.5069756507873535: 1, -0.36108481884002686: 1, -0.8494775891304016: 1, -0.16261765360832214: 1, 0.7614589929580688: 1, -1.2014739513397217: 1, 0.6704513430595398: 1, 0.6405824422836304: 1, -1.1865227222442627: 1, 0.04298178479075432: 1, -1.1981457471847534: 1, 1.6643083095550537: 1, 0.35307395458221436: 1, 1.2849032878875732: 1, -1.1562846899032593: 1, -1.201623558998108: 1, 1.9389169216156006: 1, -0.9499993324279785: 1, 1.0590577125549316: 1, -0.22874142229557037: 1, 0.1769435554742813: 1, -1.1940946578979492: 1, -0.12074612081050873: 1, -0.2589719295501709: 1, -1.078048825263977: 1, -0.1759326308965683: 1, -0.09275590628385544: 1, -1.1945018768310547: 1, 1.150854468345642: 1, 0.9452794194221497: 1, -1.2013123035430908: 1, -0.0832226425409317: 1, 1.4680920839309692: 1, 1.621954083442688: 1, -0.17545948922634125: 1, 1.023429274559021: 1, 1.7267848253250122: 1, 0.43482455611228943: 1, -0.040736664086580276: 1, -1.194840431213379: 1, -0.9951556921005249: 1, 0.4917255938053131: 1, 1.0316096544265747: 1, -0.10035426914691925: 1, 0.6872115731239319: 1, -1.2015609741210938: 1, -0.8759819269180298: 1, 0.3951069414615631: 1, -0.8244988322257996: 1, 1.2709132432937622: 1, -1.201630711555481: 1, -0.6204319000244141: 1, -0.5804309248924255: 1, -0.03235474228858948: 1, -0.8920286297798157: 1, 1.2238743305206299: 1, -0.19289743900299072: 1, -1.185569167137146: 1, -0.1808653175830841: 1, -0.6440175771713257: 1, -1.1350090503692627: 1, -0.34284013509750366: 1, 1.7417840957641602: 1, 0.3244344890117645: 1, -0.2782626748085022: 1, -1.1855055093765259: 1, -1.1972260475158691: 1, -1.2016000747680664: 1, -0.8816695213317871: 1, -1.1643073558807373: 1, -0.2992294430732727: 1, -0.3349834084510803: 1, -0.06864805519580841: 1, 0.18425099551677704: 1, -0.47152507305145264: 1, -1.1544640064239502: 1, -0.5622422695159912: 1, 1.3496158123016357: 1, 1.5035943984985352: 1, -1.1654343605041504: 1, -1.1831696033477783: 1, 0.5694330334663391: 1, 1.5063830614089966: 1, 0.8621184229850769: 1, -0.8901136517524719: 1, 1.5984208583831787: 1, 0.21256738901138306: 1, 1.2952117919921875: 1, -1.1929975748062134: 1, 0.6744810342788696: 1, -0.4369107186794281: 1, 0.7875488996505737: 1, -0.879592776298523: 1, -1.0944702625274658: 1, 0.9348665475845337: 1, -0.13237591087818146: 1, 1.561331033706665: 1, -1.1358855962753296: 1, 1.2753889560699463: 1, -0.11317183822393417: 1, -0.8917420506477356: 1, -1.189130187034607: 1, -1.1981604099273682: 1, 1.009254813194275: 1, -0.8241826295852661: 1, 0.7825468182563782: 1, 0.5601547360420227: 1, 1.126368761062622: 1, 0.02118554152548313: 1, -1.0542218685150146: 1, 0.3444092869758606: 1, -0.621107280254364: 1, -0.7876200675964355: 1, 0.8664619326591492: 1, 0.6481048464775085: 1, 0.5923774242401123: 1, -1.2015101909637451: 1, 0.8425688147544861: 1, 1.1628241539001465: 1, -1.2016375064849854: 1, -0.608140766620636: 1, -0.025239035487174988: 1, 1.035197138786316: 1, 0.15106460452079773: 1, -0.5910298824310303: 1, 1.5633379220962524: 1, 1.0467203855514526: 1, 1.585684895515442: 1, -1.200726866722107: 1, 0.6585915088653564: 1, -1.1685289144515991: 1, -0.15267345309257507: 1, -0.4121406674385071: 1, -0.3573164939880371: 1, 1.905008316040039: 1, 1.552185297012329: 1, -0.27776581048965454: 1, 0.29684698581695557: 1, 0.5500550270080566: 1, 0.09744428098201752: 1, -0.8982016444206238: 1, -1.2016292810440063: 1, 0.017385760322213173: 1, 1.224327802658081: 1, 0.523788332939148: 1, -1.1570571660995483: 1, 0.15262554585933685: 1, 1.7461694478988647: 1, 1.556430697441101: 1, -0.059458885341882706: 1, 1.0254307985305786: 1, -0.7589280009269714: 1, -1.1845873594284058: 1, 1.7253838777542114: 1, -1.1558103561401367: 1, -0.1758507341146469: 1, -0.8232764601707458: 1, -1.095828652381897: 1, 1.4735376834869385: 1, -0.2223142683506012: 1, 0.968934953212738: 1, -0.2664187550544739: 1, -0.5744653940200806: 1, 0.04079057276248932: 1, -0.700495719909668: 1, -1.0951330661773682: 1, -0.40758535265922546: 1, 0.5300026535987854: 1, -0.4007585644721985: 1, 1.1072840690612793: 1, -0.26848453283309937: 1, 1.3225599527359009: 1, -1.0963338613510132: 1, 0.3425867557525635: 1, 0.8458276391029358: 1, -1.1919200420379639: 1, -0.3255411982536316: 1, -0.6826592683792114: 1, 0.16982176899909973: 1, -0.15980762243270874: 1, 0.8631362915039062: 1, -0.30772721767425537: 1, 1.3110328912734985: 1, 0.13984030485153198: 1, -1.164048194885254: 1, -0.024909501895308495: 1, -1.0223268270492554: 1, -1.009895920753479: 1, -1.201310396194458: 1, -0.4168057143688202: 1, 0.05316608399152756: 1, 1.0084635019302368: 1, -0.08619289845228195: 1, 1.8212419748306274: 1, -0.8366453051567078: 1, -0.930094301700592: 1, 0.3602719008922577: 1, 0.842113196849823: 1, -1.0719594955444336: 1, 0.7185215353965759: 1, -1.126828908920288: 1, -0.9143604040145874: 1, -0.10448039323091507: 1, -0.11516252905130386: 1, 3.323413610458374: 1, -0.16630633175373077: 1, -0.27813780307769775: 1, -0.35862404108047485: 1, 1.5148382186889648: 1, -0.8267379403114319: 1, -0.5026354193687439: 1, -0.025178229436278343: 1, -0.967963457107544: 1, 0.34220972657203674: 1, -1.1344302892684937: 1, 0.041503969579935074: 1, -0.18857857584953308: 1, 1.488932728767395: 1, 0.753506600856781: 1, -0.3419588804244995: 1, 0.31096193194389343: 1, -1.1501390933990479: 1, -0.5334532856941223: 1, -0.1916339248418808: 1, -1.2015913724899292: 1, -0.9249427914619446: 1, -0.11489463597536087: 1, -0.8232591152191162: 1, 0.8865175843238831: 1, 1.179612159729004: 1, 1.3108209371566772: 1, -1.1605626344680786: 1, -1.1947468519210815: 1, 1.4282907247543335: 1, -1.1552096605300903: 1, 0.3900337219238281: 1, 1.3363116979599: 1, -0.8519163131713867: 1, -0.18136551976203918: 1, 0.021963730454444885: 1, 0.35480692982673645: 1, -0.5466570258140564: 1, -0.12521179020404816: 1, 1.7376213073730469: 1, -0.30726683139801025: 1, 1.4337563514709473: 1, -0.11414719372987747: 1, 0.7242860198020935: 1, -0.8322789072990417: 1, -1.2013384103775024: 1, -1.2015637159347534: 1, -1.1999688148498535: 1, 0.5436715483665466: 1, -0.8634552955627441: 1, 0.13262629508972168: 1, 1.4182209968566895: 1, -0.589444637298584: 1, -0.4057319462299347: 1, 1.9024626016616821: 1, -0.3424704372882843: 1, 0.6232529878616333: 1, 1.14208984375: 1, -0.08644621819257736: 1, 1.1413462162017822: 1, 1.2197668552398682: 1, 1.896323323249817: 1, 0.6647680401802063: 1, 1.852098822593689: 1, 0.8673886060714722: 1, -0.2079317569732666: 1, -1.2002339363098145: 1, 1.143233299255371: 1, -0.8580590486526489: 1, -1.1006834506988525: 1, -0.26783594489097595: 1, -0.7736660242080688: 1, -0.3075839579105377: 1, 0.19487899541854858: 1, -1.1767117977142334: 1, 0.30889958143234253: 1, 0.09368380159139633: 1, -1.1463063955307007: 1, -0.09378552436828613: 1, 0.9064841866493225: 1, -0.04764068126678467: 1, -1.1928194761276245: 1, -1.076475739479065: 1, -1.1363192796707153: 1, -0.7293344736099243: 1, -0.03033183142542839: 1, -1.092740535736084: 1, -1.1849600076675415: 1, -1.1963087320327759: 1, -0.40432149171829224: 1, -0.6465518474578857: 1, 1.1690500974655151: 1, -0.014552570879459381: 1, 0.15484686195850372: 1, -1.2005233764648438: 1, -1.2016280889511108: 1, 1.2151012420654297: 1, 1.0667099952697754: 1, 1.590468406677246: 1, 1.0150052309036255: 1, 0.8287729024887085: 1, 1.114488959312439: 1, 1.703546404838562: 1, 1.5217971801757812: 1, 0.167218878865242: 1, -0.6395750641822815: 1, -0.9769929647445679: 1, 0.16298139095306396: 1, -1.1964715719223022: 1, 1.0410280227661133: 1, -0.4970245659351349: 1, 0.9173278212547302: 1, -0.37317758798599243: 1, -1.1594713926315308: 1, -1.1996524333953857: 1, -1.0178923606872559: 1, -1.1946135759353638: 1, -1.2015936374664307: 1, 0.20394694805145264: 1, -0.13197508454322815: 1, -0.5880576372146606: 1, 1.533202886581421: 1, 1.6284458637237549: 1, -0.2495342493057251: 1, 0.9532740116119385: 1, -0.24131079018115997: 1, -0.4729682505130768: 1, 1.6555308103561401: 1, 1.93668532371521: 1, 1.7650138139724731: 1, -1.197901725769043: 1, -1.2015061378479004: 1, -0.07945768535137177: 1, -1.0483030080795288: 1, -0.44544142484664917: 1, 1.4793431758880615: 1, 0.7402693033218384: 1, -0.9465891718864441: 1, 1.0945255756378174: 1, -0.0674244612455368: 1, -0.2878003716468811: 1, -0.29299479722976685: 1, 0.944926917552948: 1, 1.182131290435791: 1, 0.009196557104587555: 1, -0.9147067666053772: 1, -1.2016206979751587: 1, -0.21501778066158295: 1, 0.3319496512413025: 1, -0.44949105381965637: 1, 0.8521577715873718: 1, -0.20541705191135406: 1, 1.4813575744628906: 1, 0.0384686179459095: 1, -0.9010018706321716: 1, -0.19414900243282318: 1, 0.32160520553588867: 1, 0.9053057432174683: 1, 1.5978120565414429: 1, -0.4707425832748413: 1, -1.1442792415618896: 1, -1.2014148235321045: 1, -0.46590861678123474: 1, 0.31109386682510376: 1, -0.5104274749755859: 1, 1.4887964725494385: 1, -0.5828713178634644: 1, 0.33931559324264526: 1, 1.4972656965255737: 1, -0.16829612851142883: 1, 0.5713223814964294: 1, 0.8512904644012451: 1, -1.1986582279205322: 1, 0.7171676754951477: 1, 0.30057549476623535: 1, 0.5805153250694275: 1, -0.5825971961021423: 1, 0.49536043405532837: 1, -1.1658029556274414: 1, 1.1995047330856323: 1, 1.5116616487503052: 1, -0.24819114804267883: 1, 1.9327584505081177: 1, -0.04230939969420433: 1, 1.4320223331451416: 1, 1.6500415802001953: 1, 0.7184975147247314: 1, -0.11862857639789581: 1, 0.6465133428573608: 1, 1.018097996711731: 1, -1.1434556245803833: 1, 1.215183138847351: 1, -0.8108161091804504: 1, 1.177069067955017: 1, -1.1913617849349976: 1, 1.788615107536316: 1, -1.0068711042404175: 1, 0.1083393469452858: 1, -1.1948130130767822: 1, -0.2558962106704712: 1, -0.46731990575790405: 1, 1.1731380224227905: 1, 0.5326334834098816: 1, 1.5164122581481934: 1, -0.9485399723052979: 1, 0.5780434608459473: 1, -0.4687884449958801: 1, -0.16533638536930084: 1, 0.8217967748641968: 1, 0.5497986674308777: 1, 0.6386443972587585: 1, -1.2014538049697876: 1, 0.8612715601921082: 1, 0.04832748696208: 1, 1.3093386888504028: 1, -0.7597726583480835: 1, -0.5809049010276794: 1, -1.201079249382019: 1, -1.1382324695587158: 1, 1.0187321901321411: 1, -0.4835919141769409: 1, -1.1971925497055054: 1, -0.047685928642749786: 1, -0.9726890325546265: 1, 0.18870316445827484: 1, -0.3552163541316986: 1, 0.8965467214584351: 1, -0.4433523118495941: 1, -1.1379677057266235: 1, 0.1385408341884613: 1, 0.008470889180898666: 1, 0.5490202307701111: 1, -1.2016212940216064: 1, -0.7542659640312195: 1, 1.4984081983566284: 1, -0.9981153011322021: 1, -0.4154701232910156: 1, 1.3042255640029907: 1, -0.3147757053375244: 1, -1.1507015228271484: 1, -1.1476235389709473: 1, 1.6063342094421387: 1, 1.8261455297470093: 1, 0.9224774241447449: 1, -0.9933805465698242: 1, 1.3231751918792725: 1, -1.0555497407913208: 1, 1.030333161354065: 1, 0.6086026430130005: 1, 0.9156388640403748: 1, -1.1107620000839233: 1, 0.8933335542678833: 1, 0.9784976243972778: 1, -0.7995052933692932: 1, 1.5239653587341309: 1, 1.6281145811080933: 1, 0.339458167552948: 1, -1.2002341747283936: 1, -0.12005668878555298: 1, -0.04758370667695999: 1, -0.8736492395401001: 1, 1.8529928922653198: 1, -1.1866750717163086: 1, -1.07969331741333: 1, 1.110692024230957: 1, -1.2015869617462158: 1, 0.09383262693881989: 1, 1.1045739650726318: 1, -0.15097910165786743: 1, -0.19816404581069946: 1, 1.7107861042022705: 1, -0.5718616843223572: 1, 1.3004077672958374: 1, -1.1072322130203247: 1, 1.431997537612915: 1, 0.6801310777664185: 1, -0.11868320405483246: 1, -0.09314227104187012: 1, -0.06465810537338257: 1, -0.14272816479206085: 1, -0.4781683385372162: 1, 1.375723958015442: 1, -0.32108673453330994: 1, 0.9568199515342712: 1, 0.7718502283096313: 1, 4.863577365875244: 1, 1.3818247318267822: 1, -0.11562295258045197: 1, 1.4458893537521362: 1, 0.9924389719963074: 1, 1.6047093868255615: 1, -1.1819733381271362: 1, 0.6907184720039368: 1, -1.2015849351882935: 1, 0.3126457929611206: 1, 0.3778545558452606: 1, -0.5194405913352966: 1, 1.3982725143432617: 1, -0.27802959084510803: 1, 1.8792171478271484: 1, -0.5631559491157532: 1, 1.1449819803237915: 1, 0.41225412487983704: 1, -0.40100592374801636: 1, 1.4127790927886963: 1, -0.4475323557853699: 1, -0.41311657428741455: 1, -1.1526696681976318: 1, -0.042717207223176956: 1, -0.14289136230945587: 1, -1.0867854356765747: 1, -0.5030357241630554: 1, 0.3949566185474396: 1, 5.645717620849609: 1, 1.8180814981460571: 1, 0.39559683203697205: 1, -0.257107138633728: 1, -0.7939543128013611: 1, 1.8800233602523804: 1, 1.4251669645309448: 1, -0.26246213912963867: 1, 0.4620521366596222: 1, -1.1482324600219727: 1, -1.1104997396469116: 1, -0.5931430459022522: 1, -0.5657321810722351: 1, -0.030014334246516228: 1, -1.1986464262008667: 1, 0.812082827091217: 1, -0.32367783784866333: 1, 1.5992790460586548: 1, -0.7701697945594788: 1, 0.804813027381897: 1, -0.2183121144771576: 1, 0.821479320526123: 1, 0.7741647958755493: 1, 0.11128426343202591: 1, -1.2015831470489502: 1, 1.0160198211669922: 1, -0.6967374682426453: 1, -1.1634924411773682: 1, 0.956155002117157: 1, 0.2634084224700928: 1, 0.8838387131690979: 1, 1.4529069662094116: 1, 0.939198911190033: 1, -0.3431845009326935: 1, 0.2954026460647583: 1, -0.9627416729927063: 1, 0.4243623912334442: 1, -0.21699510514736176: 1, 0.5128249526023865: 1, -1.193503737449646: 1, -1.1961623430252075: 1, 0.27857378125190735: 1, 1.059990644454956: 1, 0.7264453172683716: 1, 1.200035810470581: 1, -1.0831377506256104: 1, 0.7561059594154358: 1, -0.6209532618522644: 1, 1.3399251699447632: 1, -0.32669803500175476: 1, 0.8618729114532471: 1, -1.2004907131195068: 1, -1.1989647150039673: 1, 1.1265980005264282: 1, -1.2016305923461914: 1, -1.1948115825653076: 1, 1.5988372564315796: 1, 1.1708914041519165: 1, 0.5880253314971924: 1, -0.03495830297470093: 1, 1.5657020807266235: 1, 1.6581584215164185: 1, 0.028185075148940086: 1, 0.5615567564964294: 1, -0.9566242694854736: 1, 1.4842547178268433: 1, -1.2016196250915527: 1, -0.29541367292404175: 1, 0.9346560835838318: 1, 0.03998654708266258: 1, -0.2210041582584381: 1, -1.1549781560897827: 1, -0.9888867139816284: 1, -0.5884833335876465: 1, 0.40835040807724: 1, -0.3525408208370209: 1, -0.15634003281593323: 1, 1.6441566944122314: 1, -1.1656997203826904: 1, 1.0013794898986816: 1, 0.4845307767391205: 1, -0.15946216881275177: 1, -0.632728099822998: 1, 0.292474627494812: 1, 1.2999117374420166: 1, -0.9050359129905701: 1, -0.9951181411743164: 1, -1.201523780822754: 1, -0.5475073456764221: 1, 0.4334481656551361: 1, 0.23997803032398224: 1, -0.44575440883636475: 1, 1.2202951908111572: 1, -0.9639919996261597: 1, 0.25783771276474: 1, -0.09898030012845993: 1, -0.03138621151447296: 1, -0.7784246206283569: 1, 0.9462845921516418: 1, 0.6039040088653564: 1, -1.0873279571533203: 1, -0.24156887829303741: 1, -1.1832531690597534: 1, 0.6696186065673828: 1, -1.154268741607666: 1, 1.1363762617111206: 1, -1.1895103454589844: 1, 0.21878471970558167: 1, -1.201509714126587: 1, -1.2015154361724854: 1, -1.0024443864822388: 1, 1.256608247756958: 1, 0.20924636721611023: 1, -1.0571757555007935: 1, -1.2007057666778564: 1, 0.5908839702606201: 1, -0.9834045767784119: 1, 0.031881630420684814: 1, 0.7825908064842224: 1, -0.2038322389125824: 1, -1.201606273651123: 1, 1.5027039051055908: 1, 0.5685755014419556: 1, 1.0087279081344604: 1, -1.1560314893722534: 1, -1.1445417404174805: 1, -0.47762712836265564: 1, -0.8136187195777893: 1, 0.6463801264762878: 1, -1.2016048431396484: 1, -0.16252407431602478: 1, 1.2192449569702148: 1, -0.7505127787590027: 1, 0.2748369872570038: 1, -1.0813920497894287: 1, -0.7180438041687012: 1, 1.2327803373336792: 1, 1.2668349742889404: 1, -0.47594985365867615: 1, 0.670230507850647: 1, -1.175873041152954: 1, 1.55558443069458: 1, 1.5686708688735962: 1, -1.1963162422180176: 1, 0.900846004486084: 1, -0.2255825698375702: 1, -0.586733877658844: 1, 0.5997416377067566: 1, -0.43377333879470825: 1, -0.4338090121746063: 1, -0.618209183216095: 1, -0.7650251388549805: 1, -1.1766016483306885: 1, -0.24846623837947845: 1, -1.083876609802246: 1, 0.7781831622123718: 1, 0.2640135586261749: 1, -0.4656817615032196: 1, 0.06790906190872192: 1, -0.39580973982810974: 1, -1.198327660560608: 1, -0.5413272380828857: 1, 1.6464146375656128: 1, 0.9758270978927612: 1, 0.6183062195777893: 1, -0.9432769417762756: 1, -1.1906999349594116: 1, -0.09781666100025177: 1, -1.1923733949661255: 1, -0.5870760679244995: 1, 1.6006011962890625: 1, 0.7021965980529785: 1, 1.4295574426651: 1, 0.7833142876625061: 1, 1.603068470954895: 1, 1.0860453844070435: 1, 1.2978951930999756: 1, 1.0263571739196777: 1, -0.9958106875419617: 1, -0.4845651388168335: 1, -1.1689732074737549: 1, 0.8433452844619751: 1, -0.6867349743843079: 1, 0.4138732850551605: 1, 1.376474142074585: 1, 0.46222802996635437: 1, 1.053705096244812: 1, -0.8842195272445679: 1, -1.1482487916946411: 1, -0.7269378900527954: 1, 0.05797763168811798: 1, 1.30207359790802: 1, -0.9987406134605408: 1, 1.0491001605987549: 1, -0.01062939316034317: 1, -1.1970906257629395: 1, -0.7228063344955444: 1, -1.1882494688034058: 1, -0.5272284746170044: 1, -0.22304323315620422: 1, 1.8303353786468506: 1, 0.521833598613739: 1, -1.196357011795044: 1, -1.1871042251586914: 1, -1.201620101928711: 1, -0.3780987560749054: 1, 0.1708119511604309: 1, 1.0482161045074463: 1, 0.8642333745956421: 1, -1.201564908027649: 1, -1.1486388444900513: 1, 0.6780659556388855: 1, -0.883184015750885: 1, 0.36468705534935: 1, -0.2232077419757843: 1, -0.0690179392695427: 1, -0.19999556243419647: 1, 1.0762102603912354: 1, 1.4342314004898071: 1, 1.069445013999939: 1, 0.9952307939529419: 1, 0.29174771904945374: 1, 1.0025136470794678: 1, -0.16215069591999054: 1, -1.1318936347961426: 1, -1.1019858121871948: 1, -0.24580752849578857: 1, 0.8502970337867737: 1, 4.900933742523193: 1, 0.05546194687485695: 1, 0.525627076625824: 1, -0.655949056148529: 1, 0.12007596343755722: 1, 0.987504780292511: 1, -0.15451399981975555: 1, -0.09802207350730896: 1, 1.5512200593948364: 1, 0.07849415391683578: 1, 0.5222756862640381: 1, -0.31705647706985474: 1, -0.9021695256233215: 1, -0.26542410254478455: 1, -0.07907160371541977: 1, -1.1785725355148315: 1, -0.6442912220954895: 1, 0.006228437647223473: 1, 0.766596257686615: 1, -1.0090559720993042: 1, -1.201634407043457: 1, 0.6026607155799866: 1, -0.21738800406455994: 1, -1.1905653476715088: 1, -0.23146884143352509: 1, -1.1736985445022583: 1, -0.01187801081687212: 1, -1.1207038164138794: 1, -1.0460647344589233: 1, 0.4860861599445343: 1, -1.2011404037475586: 1, -0.9300684928894043: 1, -0.8209242224693298: 1, -0.9450681209564209: 1, -1.0054869651794434: 1, -1.1680830717086792: 1, -0.5754680633544922: 1, 0.4987215995788574: 1, -0.05793027952313423: 1, -1.2012261152267456: 1, -0.10222127288579941: 1, 0.10751932114362717: 1, 0.8198267817497253: 1, 0.9006003737449646: 1, -0.5976389050483704: 1, 0.6513680219650269: 1, 1.5720155239105225: 1, -0.20698606967926025: 1, -0.19042591750621796: 1, -0.5242934823036194: 1, 1.9067574739456177: 1, 1.5496872663497925: 1, 0.963599681854248: 1, -1.1934514045715332: 1, -0.28629931807518005: 1, 0.5479292273521423: 1, 0.36868804693222046: 1, 1.286658525466919: 1, 1.415509581565857: 1, -1.1999285221099854: 1, 0.9657272696495056: 1, 1.7802213430404663: 1, -1.0201867818832397: 1, 1.4755654335021973: 1, -0.5374748706817627: 1, 0.5681759119033813: 1, -1.126016616821289: 1, 1.520749807357788: 1, -1.0537559986114502: 1, -0.832706868648529: 1, -0.6482374668121338: 1, -1.19014310836792: 1, -0.16027171909809113: 1, -0.11124473065137863: 1, 1.2829649448394775: 1, 0.7675086855888367: 1, -1.0079307556152344: 1, -0.29665860533714294: 1, -0.7359880805015564: 1, 1.891126036643982: 1, -0.2265346348285675: 1, -0.21881107985973358: 1, 0.9495259523391724: 1, 0.15707539021968842: 1, -1.192414402961731: 1, 0.9983642101287842: 1, -1.1969208717346191: 1, -0.8602992296218872: 1, 0.9222752451896667: 1, 1.9223994016647339: 1, -1.2015472650527954: 1, 1.3716888427734375: 1, 1.519827961921692: 1, 0.9517895579338074: 1, -0.8456059098243713: 1, 0.7786849141120911: 1, 0.7159395813941956: 1, 0.022439440712332726: 1, -0.28555259108543396: 1, -1.0768063068389893: 1, 1.3494455814361572: 1, 0.71575528383255: 1, 1.1575602293014526: 1, -1.1232612133026123: 1, -1.201271653175354: 1, 0.19260334968566895: 1, 1.4786081314086914: 1, -0.07361169159412384: 1, -1.194837212562561: 1, 1.2197721004486084: 1, 0.3299405574798584: 1, -0.7713357210159302: 1, -0.03429622948169708: 1, 1.0863690376281738: 1, 0.603252649307251: 1, 1.5059622526168823: 1, -1.201627492904663: 1, 0.9434877038002014: 1, 0.9053120613098145: 1, -0.8653848767280579: 1, -0.34825557470321655: 1, -1.0213873386383057: 1, -1.0716415643692017: 1, 0.26557838916778564: 1, 0.10525540262460709: 1, 1.5665374994277954: 1, 0.27008119225502014: 1, 0.3200169503688812: 1, -1.1754673719406128: 1, 0.8532567620277405: 1, 0.6965684294700623: 1, -1.1520825624465942: 1, -0.04817575961351395: 1, 0.5070648193359375: 1, -0.8708242177963257: 1, 0.5241292119026184: 1, 0.465168297290802: 1, 1.7661612033843994: 1, -0.03992554917931557: 1, 1.8530430793762207: 1, -0.016411839053034782: 1, 0.8997297883033752: 1, 0.31122344732284546: 1, 0.23683039844036102: 1, -0.7448302507400513: 1, -0.0920228511095047: 1, -1.1946264505386353: 1, -0.3203604817390442: 1, 1.5516252517700195: 1, 0.22109845280647278: 1, -1.2015341520309448: 1, -0.007546336855739355: 1, 0.38025134801864624: 1, 0.2092854529619217: 1, -1.2016379833221436: 1, -0.3123464286327362: 1, 0.5601941347122192: 1, 1.9158005714416504: 1, -0.9299888610839844: 1, 0.17204155027866364: 1, -0.34107181429862976: 1, -0.2671576142311096: 1, 0.22341269254684448: 1, 0.21761490404605865: 1, 0.08071509003639221: 1, -0.253825306892395: 1, -0.30212122201919556: 1, 1.916335940361023: 1, 0.10260368138551712: 1, -0.5629591941833496: 1, -1.160044550895691: 1, 0.6366953253746033: 1, 1.7223035097122192: 1, 0.9059203267097473: 1, -1.0074002742767334: 1, -0.1383906453847885: 1, -1.1196062564849854: 1, 1.4193427562713623: 1, -0.2865978181362152: 1, 0.7280606031417847: 1, -0.2904500663280487: 1, 1.5599995851516724: 1, -1.2016315460205078: 1, 0.8645955324172974: 1, -0.6611031889915466: 1, 1.1870161294937134: 1, 1.8285820484161377: 1, -0.33075013756752014: 1, -0.6137503981590271: 1, -0.6758065819740295: 1, 0.4542813301086426: 1, 0.7275790572166443: 1, -1.2016215324401855: 1, 0.5979693531990051: 1, -0.25207433104515076: 1, -0.3334684669971466: 1, 1.9331234693527222: 1, 0.6389100551605225: 1, 0.831580638885498: 1, -0.9449849724769592: 1, -0.21517714858055115: 1, -0.5881722569465637: 1, 1.0303751230239868: 1, -1.186979055404663: 1, -0.06609977036714554: 1, 0.29917436838150024: 1, -0.408608615398407: 1, -0.27864202857017517: 1, 1.4656307697296143: 1, -1.1302224397659302: 1, 0.26908448338508606: 1, 0.10331395268440247: 1, 0.33690160512924194: 1, -0.9032037854194641: 1, 1.2229245901107788: 1, -0.8751227855682373: 1, -1.201555848121643: 1, 1.3335411548614502: 1, 1.8360271453857422: 1, 1.8354456424713135: 1, -0.03591597080230713: 1, 1.455495834350586: 1, 0.6376197934150696: 1, 0.3816104531288147: 1, -1.2006280422210693: 1, 0.7334970831871033: 1, -0.392406165599823: 1, -1.0507465600967407: 1, -1.0393953323364258: 1, -0.9069592952728271: 1, -1.201613426208496: 1, -0.4809620976448059: 1, 1.3466922044754028: 1, 0.10832516103982925: 1, -1.1247143745422363: 1, 0.2609155476093292: 1, -1.1582452058792114: 1, 0.9195832014083862: 1, 1.4811328649520874: 1, -0.32787415385246277: 1, 1.8263726234436035: 1, 0.8519235253334045: 1, -0.3399538993835449: 1, -0.5834662318229675: 1, -0.34224218130111694: 1, 0.03431294485926628: 1, 1.2879470586776733: 1, 0.02149348333477974: 1, 1.0611008405685425: 1, -0.34523066878318787: 1, 0.4303920269012451: 1, 0.1467430293560028: 1, 0.4251300096511841: 1, -1.0084080696105957: 1, 0.7045637369155884: 1, 1.7389863729476929: 1, 1.9232004880905151: 1, -0.906280517578125: 1, -0.2950673997402191: 1, 0.1911333203315735: 1, -0.8952922821044922: 1, 0.9750880599021912: 1, -0.29477736353874207: 1, 1.3510494232177734: 1, -0.8635501265525818: 1, 0.3790889084339142: 1, -0.9158876538276672: 1, 0.5611000061035156: 1, 1.090896725654602: 1, -0.816495954990387: 1, -1.200010895729065: 1, -1.180970549583435: 1, -1.2016316652297974: 1, 0.9703378081321716: 1, 0.2588636577129364: 1, 1.166581153869629: 1, 0.2436400204896927: 1, -0.6546655297279358: 1, -0.930620014667511: 1, -0.4605187475681305: 1, -0.5307965278625488: 1, 1.1997345685958862: 1, -0.004799619782716036: 1, -0.2597183287143707: 1, -1.0810678005218506: 1, -0.9762966632843018: 1, -0.30494359135627747: 1, 1.3061707019805908: 1, -1.2016286849975586: 1, 0.7527873516082764: 1, 0.2801852524280548: 1, 0.03461417928338051: 1, -0.11954380571842194: 1, -0.5476839542388916: 1, 0.9666041731834412: 1, 1.2350752353668213: 1, -1.1701372861862183: 1, -1.0467379093170166: 1, 0.23339834809303284: 1, 0.9952530264854431: 1, -1.1969398260116577: 1, -0.6019781231880188: 1, -0.06410756707191467: 1, 1.3920340538024902: 1, -1.2016335725784302: 1, -0.9131625890731812: 1, -0.41700541973114014: 1, 0.7675377726554871: 1, 0.7721536755561829: 1, 1.2057219743728638: 1, 1.7404301166534424: 1, 0.5575955510139465: 1, 0.2615222632884979: 1, 1.1847658157348633: 1, 0.31844547390937805: 1, -1.1412583589553833: 1, 0.09893729537725449: 1, 1.7796648740768433: 1, -0.029267514124512672: 1, 0.9160022139549255: 1, -1.194190263748169: 1, -0.1855221837759018: 1, -1.195172905921936: 1, -0.36745402216911316: 1, -1.2009553909301758: 1, -0.7096079587936401: 1, 0.1928955614566803: 1, 1.5815212726593018: 1, 0.25442907214164734: 1, 0.8610304594039917: 1, -0.17338967323303223: 1, -1.2016358375549316: 1, -0.4437340795993805: 1, 0.15129715204238892: 1, 0.25592291355133057: 1, 0.11681299656629562: 1, -0.9130086302757263: 1, -0.29989245533943176: 1, -0.9981749057769775: 1, -1.1448076963424683: 1, -0.9982122778892517: 1, -0.7622874975204468: 1, -0.5920382142066956: 1, -0.37873539328575134: 1, -1.2016046047210693: 1, 0.14515815675258636: 1, -0.7167530059814453: 1, -0.30134135484695435: 1, -1.2015197277069092: 1, 0.3886134922504425: 1, -0.6676098704338074: 1, -0.1391475349664688: 1, 1.4280599355697632: 1, -0.3189155161380768: 1, -0.8407037854194641: 1, -1.0873202085494995: 1, -0.8935246467590332: 1, -0.24963954091072083: 1, 0.3740043342113495: 1, 1.4998488426208496: 1, -0.4147615134716034: 1, -0.2236318439245224: 1, -0.2299586683511734: 1, 0.9066123366355896: 1, -1.0965174436569214: 1, -0.5115338563919067: 1, -0.8852211833000183: 1, -1.0539400577545166: 1, -0.06596078723669052: 1, 1.7850080728530884: 1, 1.1735796928405762: 1, -0.22727444767951965: 1, -1.1865193843841553: 1, -0.9732658267021179: 1, 0.21643884479999542: 1, 1.3791615962982178: 1, 1.7392624616622925: 1, -1.2016382217407227: 1, 0.2610865831375122: 1, -0.3641962707042694: 1, -1.0031712055206299: 1, 0.41254743933677673: 1, 0.7787987589836121: 1, 0.739153265953064: 1, 0.5786248445510864: 1, 0.2551988363265991: 1, 0.025435535237193108: 1, -0.07969757169485092: 1, 1.6034055948257446: 1, 1.052090048789978: 1, -1.087907314300537: 1, 1.4774726629257202: 1, -0.848010778427124: 1, -1.0349785089492798: 1, -0.9734629392623901: 1, 0.4076603651046753: 1, -0.17838822305202484: 1, 1.0399528741836548: 1, 1.3589857816696167: 1, 0.5858985185623169: 1, -1.182140827178955: 1, -0.3579169511795044: 1, -0.7073397040367126: 1, -0.8350648880004883: 1, 0.9130324721336365: 1, 0.7724436521530151: 1, -1.1343045234680176: 1, -0.4774998426437378: 1, 0.4647965729236603: 1, -0.21827441453933716: 1, -0.3561877906322479: 1, -0.4794164001941681: 1, 0.21266861259937286: 1, -1.1494790315628052: 1, 1.113309621810913: 1, -0.006573406048119068: 1, 1.8227369785308838: 1, 1.2064505815505981: 1, -0.788809597492218: 1, -1.1821529865264893: 1, -1.196489930152893: 1, 0.36046868562698364: 1, 1.533963680267334: 1, -1.1826090812683105: 1, 0.06667295098304749: 1, 1.7719837427139282: 1, 1.0554637908935547: 1, -0.13643299043178558: 1, -0.19495585560798645: 1, 0.9265954494476318: 1, -0.7420557737350464: 1, -0.796614408493042: 1, 0.7108749151229858: 1, -0.14265145361423492: 1, 0.6444322466850281: 1, 1.2513697147369385: 1, -0.12709279358386993: 1, -0.3040960133075714: 1, -0.0861414223909378: 1, 0.5367603302001953: 1, 0.2699461281299591: 1, -0.9153497219085693: 1, 1.25115966796875: 1, 1.7614071369171143: 1, 0.008864369243383408: 1, -0.1799832135438919: 1, 0.32922476530075073: 1, -0.8897131681442261: 1, -0.5582360029220581: 1, 1.8358889818191528: 1, 0.056893277913331985: 1, 0.009732205420732498: 1, 0.938378632068634: 1, -0.7897263169288635: 1, 1.379611611366272: 1, -1.156775951385498: 1, 0.9005318284034729: 1, 0.37810415029525757: 1, -0.180640310049057: 1, 1.6179100275039673: 1, 0.8550933599472046: 1, -1.061142921447754: 1, -1.1626108884811401: 1, -0.1754150390625: 1, 0.35628634691238403: 1, -0.5706648826599121: 1, -1.1971783638000488: 1, -0.1688508540391922: 1, -0.694696843624115: 1, -1.2015864849090576: 1, 1.1998889446258545: 1, -0.9168434739112854: 1, -1.1829675436019897: 1, 1.5597952604293823: 1, -1.2016351222991943: 1, 0.6104775071144104: 1, -0.6007823944091797: 1, -0.881252646446228: 1, -1.0590986013412476: 1, -0.7435175180435181: 1, -1.2001420259475708: 1, 0.15644948184490204: 1, -0.26597675681114197: 1, -1.0583271980285645: 1, 1.5651975870132446: 1, 0.3626178801059723: 1, -0.09905305504798889: 1, -1.0523524284362793: 1, 0.7206960916519165: 1, -0.748826265335083: 1, 0.2464127391576767: 1, -1.1685314178466797: 1, -1.201349139213562: 1, 0.29674577713012695: 1, -1.1674489974975586: 1, 0.2470504194498062: 1, -0.7019102573394775: 1, 1.5731940269470215: 1, -1.18631112575531: 1, -1.2015695571899414: 1, -1.2015215158462524: 1, 0.1461368203163147: 1, -0.466978520154953: 1, -1.201351284980774: 1, 0.8374537825584412: 1, 0.5299685597419739: 1, -0.4422439932823181: 1, 0.912930965423584: 1, -1.055851936340332: 1, 0.2126206010580063: 1, 0.2661615014076233: 1, 1.5698747634887695: 1, -1.155177116394043: 1, -1.2016348838806152: 1, 0.8109627962112427: 1, -0.22866979241371155: 1, -0.014680324122309685: 1, -1.1276121139526367: 1, 0.68455970287323: 1, -0.7557051777839661: 1, -0.19494549930095673: 1, -0.0922759622335434: 1, -0.6103007793426514: 1, 1.4848576784133911: 1, -0.8935588002204895: 1, -1.193153738975525: 1, -1.1569743156433105: 1, 3.9672465324401855: 1, 0.7078472375869751: 1, -1.0434380769729614: 1, -0.03743843734264374: 1, -0.44384631514549255: 1, -0.8510425090789795: 1, 0.5438616275787354: 1, 1.5127235651016235: 1, -0.20442236959934235: 1, -1.2016119956970215: 1, -0.416104793548584: 1, 0.5769325494766235: 1, 1.6552691459655762: 1, -0.5093275308609009: 1, 1.0402084589004517: 1, 1.4624748229980469: 1, -0.1892606019973755: 1, -1.2016255855560303: 1, 1.036679744720459: 1, -0.3055903911590576: 1, 0.729964017868042: 1, -0.22299611568450928: 1, -0.812441349029541: 1, 0.9454357028007507: 1, 0.34270647168159485: 1, -0.6921688914299011: 1, -1.2012096643447876: 1, 1.2256038188934326: 1, -0.1227283924818039: 1, -1.024053692817688: 1, 1.0356202125549316: 1, -1.1068792343139648: 1, -0.9730278849601746: 1, -0.47733497619628906: 1, -0.4993174076080322: 1, -0.9940837025642395: 1, -1.2014310359954834: 1, -0.4171464443206787: 1, 0.7161386013031006: 1, -0.7170498371124268: 1, -1.15325927734375: 1, -0.399366557598114: 1, 0.31799715757369995: 1, -0.05695538595318794: 1, -0.8828660249710083: 1, 1.7609484195709229: 1, 0.2824403941631317: 1, -0.04175446555018425: 1, -1.150158166885376: 1, 1.1922990083694458: 1, -1.193916916847229: 1, -1.0135008096694946: 1, -0.27063482999801636: 1, -0.9751223921775818: 1, 1.593440055847168: 1, -0.7252834439277649: 1, -0.5665901303291321: 1, -0.25122812390327454: 1, 1.9196945428848267: 1, -1.2004797458648682: 1, -1.1703253984451294: 1, -0.8671839237213135: 1, -1.1563661098480225: 1, -1.2013746500015259: 1, -0.2154475301504135: 1, 0.35236239433288574: 1, -1.201621413230896: 1, 0.4001394212245941: 1, 0.9935461282730103: 1, -0.16844011843204498: 1, 1.0024393796920776: 1, -1.05448579788208: 1, 1.8225407600402832: 1, -1.2014168500900269: 1, -1.171350359916687: 1, 0.8342987895011902: 1, 1.2778337001800537: 1, -0.34684932231903076: 1, -1.1966403722763062: 1, 0.1316474825143814: 1, -0.06836508214473724: 1, 0.5143935680389404: 1, -0.10372047126293182: 1, 0.05755604803562164: 1, -0.49563685059547424: 1, 0.6013088822364807: 1, -0.09434226900339127: 1, 1.3847272396087646: 1, 0.3697844445705414: 1, 0.3571058511734009: 1, 1.6178103685379028: 1, -0.39080610871315: 1, 1.0215860605239868: 1, -0.9755853414535522: 1, -0.7628646492958069: 1, 0.7253832817077637: 1, 0.4616207182407379: 1, -0.26975223422050476: 1, 1.3862555027008057: 1, 0.4723914861679077: 1, -0.9803085327148438: 1, -1.1624175310134888: 1, 1.1564749479293823: 1, -0.3753896951675415: 1, 1.5011951923370361: 1, 0.762050449848175: 1, -1.201569676399231: 1, 1.2479115724563599: 1, 0.3338538706302643: 1, -0.13842415809631348: 1, -1.175829291343689: 1, 0.34950539469718933: 1, -1.0224525928497314: 1, 1.571059226989746: 1, 0.6570013761520386: 1, -1.179208755493164: 1, 1.258391261100769: 1, -1.109034538269043: 1, -1.201634168624878: 1, -0.35746997594833374: 1, 0.8018452525138855: 1, -1.010536789894104: 1, -1.1632294654846191: 1, 0.81379234790802: 1, 3.830434560775757: 1, -0.7430113554000854: 1, 0.0863155722618103: 1, -1.1257261037826538: 1, -0.048064157366752625: 1, -1.1980621814727783: 1, -0.3383774757385254: 1, 1.2727433443069458: 1, 0.6492028832435608: 1, 0.1561044156551361: 1, -1.2002416849136353: 1, 0.2669691741466522: 1, -0.638097882270813: 1, -1.2015478610992432: 1, -0.25068432092666626: 1, -0.9225814342498779: 1, 0.47734835743904114: 1, 1.4535586833953857: 1, 0.005373222753405571: 1, -1.1959316730499268: 1, -0.8557604551315308: 1, -0.23531334102153778: 1, 0.7212937474250793: 1, -1.2014594078063965: 1, -0.7811260223388672: 1, -1.1994209289550781: 1, 0.46223318576812744: 1, -0.9642779231071472: 1, -0.27935805916786194: 1, -0.06760650873184204: 1, 1.692647099494934: 1, -0.411021888256073: 1, -0.7931265234947205: 1, 1.231010913848877: 1, -1.1580485105514526: 1, 0.35712626576423645: 1, -1.1809542179107666: 1, -1.0164505243301392: 1, -0.5210259556770325: 1, -1.0433918237686157: 1, -1.1635701656341553: 1, -1.1905823945999146: 1, -0.11716806888580322: 1, 0.6785101294517517: 1, 0.7684015035629272: 1, 0.34516385197639465: 1, -0.15388239920139313: 1, 4.363720893859863: 1, -0.9881011247634888: 1, -1.17500901222229: 1, 1.2916063070297241: 1, -0.08143828064203262: 1, 0.7037346363067627: 1, 0.7288377285003662: 1, 1.8480027914047241: 1, -0.5632508397102356: 1, -0.3213060796260834: 1, -0.7822737693786621: 1, 0.26564425230026245: 1, 0.5000193119049072: 1, 1.176633596420288: 1, 0.672914445400238: 1, 0.26051849126815796: 1, 0.008748067542910576: 1, -1.2007629871368408: 1, 0.16384904086589813: 1, -0.24207068979740143: 1, 1.1870133876800537: 1, 1.8447388410568237: 1, 0.31522658467292786: 1, 0.3443826735019684: 1, -0.07099064439535141: 1, 0.3245508372783661: 1, -0.22097758948802948: 1, -1.140577793121338: 1, 0.8825770616531372: 1, -0.8366922736167908: 1, 1.6089627742767334: 1, -1.1987498998641968: 1, -1.2008846998214722: 1, -0.2519146203994751: 1, 0.21625953912734985: 1, 0.605282723903656: 1, 0.19334031641483307: 1, 1.1463862657546997: 1, 0.22961212694644928: 1, 0.9344565272331238: 1, -0.3344474732875824: 1, 1.595760464668274: 1, -0.5639210939407349: 1, 0.7412875294685364: 1, 0.663663923740387: 1, -1.1798655986785889: 1, -1.2016361951828003: 1, 0.007752139586955309: 1, -0.8772833347320557: 1, -0.7166287899017334: 1, -1.2014809846878052: 1, -0.7642948627471924: 1, 1.1112544536590576: 1, -0.5739816427230835: 1, 1.4424492120742798: 1, 1.310013771057129: 1, 1.5722923278808594: 1, 0.021630888804793358: 1, 0.9797016382217407: 1, 1.7114263772964478: 1, -0.4567924439907074: 1, 1.760259985923767: 1, 0.1478143036365509: 1, 1.622040867805481: 1, -0.7968862652778625: 1, 1.6106586456298828: 1, -0.16474246978759766: 1, -0.5050346255302429: 1, 0.2143784463405609: 1, -1.1969212293624878: 1, 0.8646785616874695: 1, 1.4938230514526367: 1, -0.3169466555118561: 1, -0.19123347103595734: 1, -1.0160380601882935: 1, 1.0554699897766113: 1, -1.20045006275177: 1, 1.2591054439544678: 1, -1.2016234397888184: 1, -1.193373441696167: 1, -0.0996609777212143: 1, 1.8505216836929321: 1, -0.960394024848938: 1, -1.086830973625183: 1, -0.20172715187072754: 1, -1.1420694589614868: 1, 1.622856616973877: 1, -0.5937166213989258: 1, 0.29443448781967163: 1, -0.5884281992912292: 1, 0.80833899974823: 1, -0.04237562045454979: 1, -1.1365838050842285: 1, -0.053435858339071274: 1, -0.6113037467002869: 1, -0.6249680519104004: 1} test data: {-1.201636552810669: 3, -1.2016351222991943: 2, -1.1062737703323364: 1, -1.0625981092453003: 1, -1.1873440742492676: 1, 0.6442342400550842: 1, -0.5042855143547058: 1, 1.3780083656311035: 1, -1.111975073814392: 1, 1.6066374778747559: 1, 1.0823159217834473: 1, 0.8944936394691467: 1, 0.2539007067680359: 1, 0.8548043966293335: 1, -1.1862393617630005: 1, -1.2014601230621338: 1, -1.084214448928833: 1, -0.5724524259567261: 1, -0.8671026229858398: 1, 0.14948004484176636: 1, -0.86516273021698: 1, -0.6162902116775513: 1, -0.37773171067237854: 1, 1.0885628461837769: 1, -0.2831217646598816: 1, 0.5419895052909851: 1, -0.4275071322917938: 1, 0.20390741527080536: 1, -1.2016236782073975: 1, -0.056251220405101776: 1, 1.4580349922180176: 1, 0.2816910445690155: 1, 1.8255465030670166: 1, 0.11391282081604004: 1, -1.0611162185668945: 1, -0.5620038509368896: 1, -0.7673166990280151: 1, -1.1730265617370605: 1, 0.006390336435288191: 1, 1.4536134004592896: 1, 1.5040947198867798: 1, 1.427193522453308: 1, 1.0670119524002075: 1, 0.9712859988212585: 1, 1.5233625173568726: 1, 0.8243502378463745: 1, 1.196900725364685: 1, -0.7029581665992737: 1, -0.07051629573106766: 1, 1.3490053415298462: 1, -0.5772318840026855: 1, 1.0134633779525757: 1, -1.1987890005111694: 1, 1.6143009662628174: 1, -0.233070969581604: 1, 1.1819933652877808: 1, 1.4158756732940674: 1, 0.3499395549297333: 1, -1.2015366554260254: 1, -1.1539435386657715: 1, 1.7429335117340088: 1, -0.20062661170959473: 1, 1.324471354484558: 1, 0.8943637013435364: 1, 0.8741052150726318: 1, -0.19720852375030518: 1, -0.7915438413619995: 1, -0.683555543422699: 1, -1.1653438806533813: 1, -0.8962797522544861: 1, -0.1960129290819168: 1, -1.0791629552841187: 1, 0.014584558084607124: 1, 0.4048600494861603: 1, -0.3605387806892395: 1, 1.0119178295135498: 1, -0.916256844997406: 1, -0.23552395403385162: 1, -0.864342451095581: 1, 1.4221618175506592: 1, -1.0699774026870728: 1, 0.261216938495636: 1, -0.6869497895240784: 1, 0.08808748424053192: 1, -1.1939657926559448: 1, -0.5784834027290344: 1, -0.9751180410385132: 1, -1.1891201734542847: 1, 0.2422785758972168: 1, 0.5247108340263367: 1, 1.0808086395263672: 1, 0.2568971812725067: 1, 1.2379846572875977: 1, -1.2016377449035645: 1, -1.192280888557434: 1, 0.04851381108164787: 1, -1.006115436553955: 1, 0.9156394600868225: 1, 0.2775695323944092: 1, 1.5978947877883911: 1, -0.3348834216594696: 1, 0.723430871963501: 1, -1.2016271352767944: 1, 1.1921144723892212: 1, -0.6771114468574524: 1, 1.0062413215637207: 1, -0.2946179509162903: 1, -1.0668615102767944: 1, 0.18035785853862762: 1, -1.155191421508789: 1, 0.7080846428871155: 1, 1.244690179824829: 1, -0.21433545649051666: 1, -1.1264077425003052: 1, 0.43905025720596313: 1, 1.5277636051177979: 1, -0.6532332301139832: 1, 1.4599251747131348: 1, -0.9877029061317444: 1, -0.807515025138855: 1, -0.8758900761604309: 1, -1.179785132408142: 1, 0.02830575592815876: 1, -0.6645460724830627: 1, -1.1858601570129395: 1, -0.4258005619049072: 1, -1.161582589149475: 1, -1.2016373872756958: 1, 1.6034691333770752: 1, 0.017293494194746017: 1, 0.11351441591978073: 1, 0.8852934837341309: 1, 1.6845048666000366: 1, -0.9308528304100037: 1, -0.01294313371181488: 1, 0.6412140727043152: 1, 1.189407229423523: 1, -1.1320221424102783: 1, 0.05971863865852356: 1, -1.1312958002090454: 1, 0.11328306794166565: 1, -1.188301682472229: 1, -0.6460915207862854: 1, -0.09158548712730408: 1, 0.8505056500434875: 1, -0.009660118259489536: 1, -0.5027830004692078: 1, -0.8235211968421936: 1, 1.0674824714660645: 1, -1.1742432117462158: 1, -0.21185417473316193: 1, 0.699573814868927: 1, -0.5409148931503296: 1, 0.4633817672729492: 1, 0.004596139770001173: 1, -0.46113380789756775: 1, -0.5774872899055481: 1, -0.23251420259475708: 1, 1.605971336364746: 1, 1.3204209804534912: 1, -0.4132004976272583: 1, -0.03317539766430855: 1, -0.24605818092823029: 1, -0.3122004270553589: 1, 1.9238320589065552: 1, 0.49887171387672424: 1, 0.8697875142097473: 1, 0.838370680809021: 1, -0.5620161294937134: 1, -0.4548023045063019: 1, 1.104429006576538: 1, 0.263934850692749: 1, -1.1663979291915894: 1, 0.20332399010658264: 1, 0.30258285999298096: 1, -0.43688738346099854: 1, -1.1383882761001587: 1, 1.0112850666046143: 1, -0.4757806956768036: 1, -0.9494366645812988: 1, -0.23942992091178894: 1, -1.0314160585403442: 1, 0.4960012137889862: 1, -0.31994664669036865: 1, 1.2155990600585938: 1, -1.2006548643112183: 1, -1.1993433237075806: 1, -0.16389766335487366: 1, 0.7778733968734741: 1, 0.9105262160301208: 1, 1.7721431255340576: 1, -0.14631414413452148: 1, -0.3439752459526062: 1, 1.9098440408706665: 1, -0.731924295425415: 1, -0.539240837097168: 1, -0.20047886669635773: 1, 0.28807583451271057: 1, -1.2016326189041138: 1, -1.201277732849121: 1, 1.0595874786376953: 1, 1.2843722105026245: 1, 0.48093563318252563: 1, 1.5410983562469482: 1, 1.6003168821334839: 1, 0.4937030076980591: 1, -0.13060888648033142: 1, -0.9310538172721863: 1, 1.168498158454895: 1, -0.9236016869544983: 1, -1.201622486114502: 1, -0.9930511116981506: 1, -0.15924076735973358: 1, -1.2005667686462402: 1, 0.6150393486022949: 1, -0.9736310243606567: 1, 0.14208731055259705: 1, 0.4746771454811096: 1, 0.22853031754493713: 1, -0.9806917309761047: 1, -0.6876721978187561: 1, -1.137963891029358: 1, -1.2016327381134033: 1, 0.264765202999115: 1, 1.7499927282333374: 1, 1.1509000062942505: 1, 0.17652635276317596: 1, -0.09844925999641418: 1, -1.012891411781311: 1, 0.16830426454544067: 1, -1.194725751876831: 1, -1.1721446514129639: 1, -1.2016315460205078: 1, -0.7456731200218201: 1, -0.8965012431144714: 1, 1.7963730096817017: 1, -0.003650385420769453: 1, -0.9430715441703796: 1, 1.4727312326431274: 1, 1.5291143655776978: 1, 0.03332117572426796: 1, 1.2513844966888428: 1, 1.0934252738952637: 1, 0.669349730014801: 1, 0.6657525897026062: 1, 0.643775999546051: 1, 0.01943967677652836: 1, -0.43114355206489563: 1, -0.3953478932380676: 1, 0.16395387053489685: 1, 1.282376766204834: 1, -0.8890491724014282: 1, -0.3426608741283417: 1, -0.09881063550710678: 1, 1.545008897781372: 1, 0.3604428768157959: 1, -0.040320102125406265: 1, -1.201614260673523: 1, -0.8791208267211914: 1, 0.21012525260448456: 1, -1.2007761001586914: 1, -1.2002415657043457: 1, -0.4644731879234314: 1, -1.2014657258987427: 1, 0.9499619007110596: 1, 1.2749443054199219: 1, -0.7196366786956787: 1, 0.3607413172721863: 1, -1.00275719165802: 1, -0.1517976075410843: 1, -1.042400598526001: 1, -1.199570655822754: 1, -1.0203382968902588: 1, -0.5811882019042969: 1, -0.9900954961776733: 1, -0.5361114740371704: 1, 1.0808240175247192: 1, 1.2736730575561523: 1, -0.5857658982276917: 1, -0.7417653203010559: 1, -1.2010202407836914: 1, 0.015656888484954834: 1, -0.23732632398605347: 1, -1.2016254663467407: 1, 0.8628989458084106: 1, 0.12914451956748962: 1, 0.6509128212928772: 1, -0.1884830892086029: 1, -0.43086937069892883: 1, -0.21730461716651917: 1, -1.031872034072876: 1, 1.2302463054656982: 1, 0.340713769197464: 1, -0.5816406607627869: 1, -1.1954984664916992: 1, -1.201613187789917: 1, -1.196737289428711: 1, -0.03840658441185951: 1, 0.216363787651062: 1, -0.5775644183158875: 1, -1.016434669494629: 1, 1.1383615732192993: 1, 0.741217315196991: 1, -1.1937233209609985: 1, -0.4890022277832031: 1, 1.914624810218811: 1, -0.8054187893867493: 1, -0.8666650652885437: 1, 1.573736548423767: 1, -1.2016273736953735: 1, 1.196738839149475: 1, 1.0541952848434448: 1, 1.1717408895492554: 1, 0.9121710062026978: 1, -0.2417261004447937: 1, 0.339995801448822: 1, 1.8953936100006104: 1, -1.2016195058822632: 1, -0.0411478653550148: 1, 0.21748410165309906: 1, -0.31073465943336487: 1, 0.6880602836608887: 1, -1.0628424882888794: 1, -1.1008306741714478: 1, 0.004906347021460533: 1, -0.4641222357749939: 1, 0.9220188856124878: 1, 0.8005363941192627: 1, -1.1004241704940796: 1, 0.394859254360199: 1, -0.21868036687374115: 1, -0.41031748056411743: 1, 1.2748090028762817: 1, 0.10180643945932388: 1, 0.8476697206497192: 1, -0.35442060232162476: 1, -0.2686515748500824: 1, -0.42906203866004944: 1, -1.1851680278778076: 1, -0.14373785257339478: 1, 0.6649335622787476: 1, 1.4387927055358887: 1, -1.1914112567901611: 1, 1.1114397048950195: 1, -0.7692103385925293: 1, -0.674019455909729: 1, 0.8744557499885559: 1, 0.057195477187633514: 1, 0.7092652916908264: 1, 5.001932144165039: 1, -0.23161835968494415: 1, -0.24153171479701996: 1, 0.3024345636367798: 1, -0.1682627946138382: 1, 0.7566526532173157: 1, 1.5033998489379883: 1, 0.2524677515029907: 1, -0.24653010070323944: 1, -0.4592617452144623: 1, 1.2313082218170166: 1, -0.514695405960083: 1, -1.0994056463241577: 1, 0.9235488772392273: 1, -1.1269394159317017: 1, 1.1650327444076538: 1, 0.043348729610443115: 1, -1.1066040992736816: 1, 0.2097160518169403: 1, 1.0225938558578491: 1, 0.46835482120513916: 1, -0.47023993730545044: 1, -0.9656466245651245: 1, -0.48030614852905273: 1, -0.6966411471366882: 1, -0.2164342701435089: 1, -1.115770697593689: 1, -0.8523722290992737: 1, 1.1094199419021606: 1, -1.2008297443389893: 1, 0.5330069065093994: 1, 0.6296795010566711: 1, -1.2001534700393677: 1, -0.6635236740112305: 1, -0.5079992413520813: 1, 0.6718612909317017: 1, 0.0796559751033783: 1, -1.2016278505325317: 1, -1.144519329071045: 1, -0.2126571536064148: 1, 0.7067909836769104: 1, -1.1784441471099854: 1, -0.012192374095320702: 1, 1.0170906782150269: 1, -1.1999518871307373: 1, -0.7983001470565796: 1, -1.0160771608352661: 1, -0.6454114317893982: 1, 0.37413451075553894: 1, 1.788353681564331: 1, -0.29448574781417847: 1, 0.63859623670578: 1, -0.7990305423736572: 1, 2.0702569484710693: 1, 1.3918044567108154: 1, -0.3081098794937134: 1, 0.06941241025924683: 1, 0.08119866997003555: 1, 0.3276282548904419: 1, -1.2015254497528076: 1, -0.7750424146652222: 1, 0.9848727583885193: 1, 0.5863446593284607: 1, -1.2016297578811646: 1, -0.7289202809333801: 1, -0.12660875916481018: 1, 1.0209075212478638: 1, 0.3683989942073822: 1, -1.2015372514724731: 1, -1.1992239952087402: 1, -0.8680421113967896: 1, -1.1694631576538086: 1, -0.7989376187324524: 1, 0.8691079020500183: 1, -0.21383443474769592: 1, 1.495969295501709: 1, -1.2016159296035767: 1, -0.1501469612121582: 1, -0.4459679424762726: 1, -0.8764730095863342: 1, 0.30879271030426025: 1, -0.17784874141216278: 1, 0.638069212436676: 1, -1.2016363143920898: 1, -1.1484540700912476: 1, 1.5142070055007935: 1, -0.5649014711380005: 1, -1.0858170986175537: 1, -0.9999420642852783: 1, -1.201612949371338: 1, 0.4259852468967438: 1, -0.04164140671491623: 1, -1.1873303651809692: 1, 0.015011060051620007: 1, -1.2016023397445679: 1, 0.5422061085700989: 1, 0.9413108229637146: 1, -1.2015916109085083: 1, 1.6904795169830322: 1, -1.1800106763839722: 1, -0.9175146222114563: 1, -1.1878859996795654: 1, -1.1790684461593628: 1, -1.2009947299957275: 1, 1.2997812032699585: 1, -0.3034926950931549: 1, -0.2970311641693115: 1, 1.1523829698562622: 1, -1.200330376625061: 1, -0.7696746587753296: 1, -0.4297850430011749: 1, 1.0043728351593018: 1, 0.18236172199249268: 1, 1.16111421585083: 1, 1.366266131401062: 1, 0.9015107750892639: 1, -1.2016277313232422: 1, 0.00948107335716486: 1, 0.7778939008712769: 1, -0.94952791929245: 1, 1.2993144989013672: 1, 0.22126778960227966: 1, -0.26343750953674316: 1, 1.2889325618743896: 1, 1.1649004220962524: 1, -0.75212162733078: 1, 0.3431006968021393: 1, 0.19180983304977417: 1, 1.1683435440063477: 1, 0.8562594652175903: 1, -1.1755220890045166: 1, 0.2596873939037323: 1, 1.6772441864013672: 1, 0.86496901512146: 1, -0.2299073189496994: 1, 1.0163378715515137: 1, -1.026742935180664: 1, -0.0211151335388422: 1, -1.20163094997406: 1, 0.7848809361457825: 1, 1.1807068586349487: 1, 1.0866621732711792: 1, -0.01721060648560524: 1, 4.302996635437012: 1, 0.03207547590136528: 1, 0.11682117730379105: 1, 0.4647309482097626: 1, 0.3141234815120697: 1, -1.1866058111190796: 1, -0.25587567687034607: 1, -1.201629400253296: 1, -0.21048426628112793: 1, -0.44190311431884766: 1, -1.1940333843231201: 1, -0.20360040664672852: 1, 0.2675780653953552: 1, 0.2079305797815323: 1, 0.3213244378566742: 1, -1.1747305393218994: 1, 0.15111742913722992: 1, -0.364163339138031: 1, 0.9705496430397034: 1, 0.0663938894867897: 1, -1.173497200012207: 1, 1.7539664506912231: 1, -0.39703771471977234: 1, 1.2261123657226562: 1, 0.8056516647338867: 1, -0.12913857400417328: 1, 0.0010552277090027928: 1, -1.0070439577102661: 1, -1.1964974403381348: 1, 1.6221997737884521: 1, -1.1910632848739624: 1, -0.8923998475074768: 1, -1.20154869556427: 1, 0.7110782265663147: 1, -0.2089931219816208: 1, 0.23268936574459076: 1, 1.6787821054458618: 1, -1.2014458179473877: 1, 0.033690646290779114: 1, -0.27475300431251526: 1, -0.5807573795318604: 1, 0.2561040222644806: 1, 0.3140702247619629: 1, 1.3953920602798462: 1, -0.27164560556411743: 1, 0.23272329568862915: 1, -0.05734042823314667: 1, 1.187366247177124: 1, -1.2016228437423706: 1, -0.8956496715545654: 1, 0.6790488958358765: 1, -1.1612766981124878: 1, 1.6532078981399536: 1, -1.1579349040985107: 1, -1.1774768829345703: 1, -0.11945565789937973: 1, -1.1480247974395752: 1, 0.8423707485198975: 1, 0.4252239763736725: 1, -0.861803412437439: 1, -0.11618001013994217: 1, -0.726171612739563: 1, -1.1990872621536255: 1, -0.8882886171340942: 1, 1.5916389226913452: 1, 0.5211433172225952: 1, 1.5805106163024902: 1, 0.012689988128840923: 1, -0.7376867532730103: 1, 1.7555813789367676: 1, -0.06798223406076431: 1, -1.1760457754135132: 1, 0.3157300651073456: 1, -1.2016263008117676: 1, -0.0449078269302845: 1, 1.3031054735183716: 1, 1.5933094024658203: 1, 1.3996703624725342: 1, 1.280470848083496: 1, 0.1881372481584549: 1, -0.46826034784317017: 1, 0.9216548800468445: 1, -1.2015867233276367: 1, -1.174880027770996: 1, 0.1308048814535141: 1, 0.8569538593292236: 1, 0.050834186375141144: 1, 0.5647678971290588: 1, 1.0952398777008057: 1, 0.342952162027359: 1, 1.7208062410354614: 1, 0.18826737999916077: 1, 1.185150146484375: 1, -0.30786851048469543: 1, 0.7008569240570068: 1, 1.3900312185287476: 1, -0.6185922622680664: 1, -0.8521113991737366: 1, 1.0577486753463745: 1, 1.6950387954711914: 1, -0.1598413586616516: 1, -1.1349726915359497: 1, -1.1586899757385254: 1, 0.2403424084186554: 1, -0.6814844608306885: 1, 1.3463716506958008: 1, 0.2308363914489746: 1, 0.9998847246170044: 1, -0.06991042196750641: 1, -0.7031784653663635: 1, -0.3879204988479614: 1, 0.2788977324962616: 1, -1.1830931901931763: 1, -0.6786049008369446: 1, -0.1196289211511612: 1, -0.8833669424057007: 1, -1.113705039024353: 1, -1.0893759727478027: 1, -1.0186684131622314: 1, 1.3110779523849487: 1, -0.5288078188896179: 1, -1.0589720010757446: 1, -1.1712636947631836: 1, 0.19575154781341553: 1, 0.9470359086990356: 1, -0.862193763256073: 1, 0.3554361164569855: 1, -0.18094922602176666: 1, 1.4103199243545532: 1, -1.1941806077957153: 1, -0.5458275079727173: 1, -0.3469093143939972: 1, -1.0180860757827759: 1, -0.1967451572418213: 1, 0.6605957746505737: 1, 1.7539278268814087: 1, -1.201348900794983: 1, -0.297276109457016: 1, 2.4050354957580566: 1, -1.0461647510528564: 1, 0.10187211632728577: 1, 0.8788592219352722: 1, -1.1422733068466187: 1, 0.021945519372820854: 1, -0.8899372816085815: 1, 0.017721591517329216: 1, 0.02189079485833645: 1, -1.0131398439407349: 1, 0.15925046801567078: 1, 0.7100340127944946: 1, -1.2016352415084839: 1, -1.1595861911773682: 1, 0.0290671493858099: 1, -0.2393776774406433: 1, 1.150696039199829: 1, -1.201637625694275: 1, -0.9657180905342102: 1, 1.3578754663467407: 1, 1.0578463077545166: 1, -0.622269868850708: 1, 0.9746946096420288: 1, -1.2016353607177734: 1, 0.49005118012428284: 1, -1.189503788948059: 1, -1.1691778898239136: 1, -1.2008585929870605: 1, -1.2015223503112793: 1, -1.199107050895691: 1, 5.066896438598633: 1, -0.4477367699146271: 1, 1.7166484594345093: 1, -0.35624369978904724: 1, -0.38882899284362793: 1, 0.1581522524356842: 1, 0.3411409258842468: 1, 0.8222253322601318: 1, -0.35524412989616394: 1, 1.5706232786178589: 1, -1.2016080617904663: 1, 1.5919677019119263: 1, -0.29486238956451416: 1, 0.39954620599746704: 1, 0.46373531222343445: 1, 1.6939563751220703: 1, -1.1991149187088013: 1, 1.1950836181640625: 1, -0.21601253747940063: 1, 0.04192548990249634: 1, -0.5982488989830017: 1, 0.6320856809616089: 1, -1.1898142099380493: 1, -0.9743238091468811: 1, 0.15692748129367828: 1, -0.10225572437047958: 1, -0.6234576106071472: 1, -0.7514510154724121: 1, 0.11675674468278885: 1, -0.9272821545600891: 1, -0.5497206449508667: 1, -1.0579359531402588: 1, 4.5118513107299805: 1, -0.8936908841133118: 1, -0.9465845823287964: 1, -1.187011957168579: 1, -0.40272125601768494: 1, 0.7346283793449402: 1, 0.012895430438220501: 1, -1.1621276140213013: 1, -0.41689032316207886: 1, -1.194821834564209: 1, -0.09393322467803955: 1, -1.1237342357635498: 1, 0.6003371477127075: 1, 1.9054079055786133: 1, 0.16810350120067596: 1, 0.695344090461731: 1, 1.4029184579849243: 1, 0.9127581715583801: 1, 0.9266675710678101: 1, 1.1278204917907715: 1, 0.17733901739120483: 1, 0.654328465461731: 1, 0.06557659059762955: 1, -1.201636791229248: 1, 0.8222996592521667: 1, -1.180148720741272: 1, 0.8297379016876221: 1, 0.047311048954725266: 1, 1.466456651687622: 1, -1.061113953590393: 1, -1.2016375064849854: 1, 1.229008674621582: 1, 1.5396013259887695: 1, 1.0520529747009277: 1, 1.0276689529418945: 1, 0.9881972074508667: 1, -1.1857632398605347: 1, 1.8432141542434692: 1, -1.2015589475631714: 1, 0.5354000329971313: 1, -1.1983946561813354: 1, -0.07709828019142151: 1, -0.33291712403297424: 1, -0.29657527804374695: 1, -0.8769359588623047: 1, 0.5329916477203369: 1, -1.2012841701507568: 1, -1.2015608549118042: 1, -0.2516374886035919: 1, 1.425097107887268: 1, 0.23328566551208496: 1, -1.199577808380127: 1, 0.6647039651870728: 1, 1.2369016408920288: 1, -1.2016295194625854: 1, 0.8098611235618591: 1, -1.2015975713729858: 1, 0.5944019556045532: 1, 0.8834699392318726: 1, 1.1558300256729126: 1, 0.2500914931297302: 1, 1.5357881784439087: 1, 0.6741006970405579: 1, -0.6903147101402283: 1, 1.3359390497207642: 1, -0.31910526752471924: 1, -0.238590806722641: 1, -0.3548189401626587: 1, -0.5888111591339111: 1, 0.5745441317558289: 1, 0.29451489448547363: 1, 1.5550216436386108: 1, -0.6015652418136597: 1, 0.002237366745248437: 1, 0.9138351678848267: 1, 0.5252094268798828: 1, 0.8655160665512085: 1, -0.024080123752355576: 1, -0.6259949207305908: 1, -0.4260459542274475: 1, -1.0714704990386963: 1, -1.2006030082702637: 1, -0.10053509473800659: 1, -0.9003047943115234: 1, 0.5123543739318848: 1, 0.6174435615539551: 1, -0.2588912844657898: 1, -1.1933567523956299: 1, 0.3594037592411041: 1, 0.2692132890224457: 1, 0.5888639092445374: 1, -1.0776159763336182: 1, 1.405086636543274: 1, -1.1513574123382568: 1, 1.0202414989471436: 1, 1.2529374361038208: 1, 1.2726079225540161: 1, 0.36003923416137695: 1, 0.5403873920440674: 1, -1.1639471054077148: 1, 0.43160757422447205: 1, 1.6134487390518188: 1, -0.9931707978248596: 1, 1.0374422073364258: 1, -0.8829452991485596: 1, -1.1907743215560913: 1, 0.2268732339143753: 1, -0.253052294254303: 1, 1.5179330110549927: 1, -1.1721937656402588: 1, 0.9807740449905396: 1, -1.2016345262527466: 1, -0.8747767210006714: 1, 1.1236602067947388: 1, -1.200749397277832: 1, -0.9857455492019653: 1, 0.4205377995967865: 1, -0.07864277809858322: 1, -1.201473593711853: 1, 1.6153100728988647: 1, 1.4854387044906616: 1, 0.02958042360842228: 1, -0.4360010027885437: 1, 1.5315228700637817: 1, -1.103760838508606: 1, 0.3286954462528229: 1, -0.05626079440116882: 1, -1.1659823656082153: 1, 0.81052166223526: 1, -0.974824070930481: 1, -0.67027348279953: 1}
2024-07-15 14:41:43.321813: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory
2024-07-15 14:41:43.321918: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)
2024-07-15 14:41:43.321979: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (UFZ544049): /proc/driver/nvidia/version does not exist
2024-07-15 14:41:43.323084: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
INFO     Network type: REG
INFO     Model: "sequential"
INFO     _________________________________________________________________
INFO     Layer (type)                 Output Shape              Param #
INFO     =================================================================
INFO     dense (Dense)                (None, 1024)              2098176
INFO     _________________________________________________________________
INFO     dropout (Dropout)            (None, 1024)              0
INFO     _________________________________________________________________
INFO     dense_1 (Dense)              (None, 512)               524800
INFO     _________________________________________________________________
INFO     dropout_1 (Dropout)          (None, 512)               0
INFO     _________________________________________________________________
INFO     dense_2 (Dense)              (None, 256)               131328
INFO     _________________________________________________________________
INFO     dropout_2 (Dropout)          (None, 256)               0
INFO     _________________________________________________________________
INFO     dense_3 (Dense)              (None, 128)               32896
INFO     _________________________________________________________________
INFO     dropout_3 (Dropout)          (None, 128)               0
INFO     _________________________________________________________________
INFO     dense_4 (Dense)              (None, 1)                 129
INFO     =================================================================
INFO     Total params: 2,787,329
INFO     Trainable params: 2,787,329
INFO     Non-trainable params: 0
INFO     _________________________________________________________________
WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.
WARNING  `period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.
Epoch 1/1000
2024-07-15 14:41:44.804409: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0
2024-07-15 14:41:44.804663: I tensorflow/core/grappler/clusters/single_machine.cc:357] Starting new session
2024-07-15 14:41:44.816902: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:1137] Optimization results for grappler item: graph_to_optimize
  function_optimizer: function_optimizer did nothing. time = 0.135ms.
  function_optimizer: function_optimizer did nothing. time = 0ms.
WARNING:tensorflow:From /home/shanavas/miniforge3/envs/dfpl_env/lib/python3.9/site-packages/tensorflow/python/ops/math_ops.py:3777: tensor_shape_from_node_def_name (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.graph_util.tensor_shape_from_node_def_name`
WARNING  From /home/shanavas/miniforge3/envs/dfpl_env/lib/python3.9/site-packages/tensorflow/python/ops/math_ops.py:3777: tensor_shape_from_node_def_name (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.graph_util.tensor_shape_from_node_def_name`
2024-07-15 14:41:44.996563: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)
26/26 - 1s - loss: 4.7887 - val_loss: 4.7254
Epoch 2/1000
26/26 - 1s - loss: 4.6896 - val_loss: 4.6530
Epoch 3/1000
26/26 - 1s - loss: 4.6325 - val_loss: 4.6111
Epoch 4/1000
26/26 - 1s - loss: 4.5951 - val_loss: 4.5849
Epoch 5/1000
26/26 - 1s - loss: 4.5682 - val_loss: 4.5632
Epoch 6/1000
26/26 - 1s - loss: 4.5444 - val_loss: 4.5439
Epoch 7/1000
26/26 - 1s - loss: 4.5214 - val_loss: 4.5265
Epoch 8/1000
26/26 - 1s - loss: 4.5021 - val_loss: 4.5114
Epoch 9/1000
26/26 - 1s - loss: 4.4841 - val_loss: 4.4967
Epoch 10/1000
26/26 - 1s - loss: 4.4656 - val_loss: 4.4824
Epoch 00010: val_loss improved from inf to 4.48243, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-0.model.weights.hdf5
Epoch 11/1000
26/26 - 1s - loss: 4.4498 - val_loss: 4.4690
Epoch 12/1000
26/26 - 1s - loss: 4.4298 - val_loss: 4.4554
Epoch 13/1000
26/26 - 1s - loss: 4.4142 - val_loss: 4.4426
Epoch 14/1000
26/26 - 1s - loss: 4.3983 - val_loss: 4.4296
Epoch 15/1000
26/26 - 1s - loss: 4.3829 - val_loss: 4.4174
Epoch 16/1000
26/26 - 1s - loss: 4.3657 - val_loss: 4.4046
Epoch 17/1000
26/26 - 1s - loss: 4.3504 - val_loss: 4.3924
Epoch 18/1000
26/26 - 1s - loss: 4.3346 - val_loss: 4.3804
Epoch 19/1000
26/26 - 1s - loss: 4.3205 - val_loss: 4.3684
Epoch 20/1000
26/26 - 1s - loss: 4.3040 - val_loss: 4.3565
Epoch 00020: val_loss improved from 4.48243 to 4.35645, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-0.model.weights.hdf5
Epoch 21/1000
26/26 - 1s - loss: 4.2892 - val_loss: 4.3447
Epoch 22/1000
26/26 - 1s - loss: 4.2749 - val_loss: 4.3340
Epoch 23/1000
26/26 - 1s - loss: 4.2609 - val_loss: 4.3221
Epoch 24/1000
26/26 - 1s - loss: 4.2468 - val_loss: 4.3109
Epoch 25/1000
26/26 - 1s - loss: 4.2293 - val_loss: 4.2999
Epoch 26/1000
26/26 - 1s - loss: 4.2154 - val_loss: 4.2898
Epoch 27/1000
26/26 - 1s - loss: 4.2034 - val_loss: 4.2799
Epoch 28/1000
26/26 - 1s - loss: 4.1869 - val_loss: 4.2700
Epoch 29/1000
26/26 - 1s - loss: 4.1751 - val_loss: 4.2594
Epoch 30/1000
26/26 - 1s - loss: 4.1589 - val_loss: 4.2496
Epoch 00030: val_loss improved from 4.35645 to 4.24960, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-0.model.weights.hdf5
Epoch 31/1000
26/26 - 1s - loss: 4.1460 - val_loss: 4.2407
Epoch 32/1000
26/26 - 1s - loss: 4.1333 - val_loss: 4.2307
Epoch 33/1000
26/26 - 1s - loss: 4.1194 - val_loss: 4.2215
Epoch 34/1000
26/26 - 1s - loss: 4.1080 - val_loss: 4.2127
Epoch 35/1000
26/26 - 1s - loss: 4.0943 - val_loss: 4.2032
Epoch 36/1000
26/26 - 1s - loss: 4.0812 - val_loss: 4.1944
Epoch 37/1000
26/26 - 1s - loss: 4.0676 - val_loss: 4.1850
Epoch 38/1000
26/26 - 1s - loss: 4.0530 - val_loss: 4.1765
Epoch 39/1000
26/26 - 1s - loss: 4.0417 - val_loss: 4.1693
Epoch 40/1000
26/26 - 1s - loss: 4.0302 - val_loss: 4.1598
Epoch 00040: val_loss improved from 4.24960 to 4.15982, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-0.model.weights.hdf5
Epoch 41/1000
26/26 - 1s - loss: 4.0170 - val_loss: 4.1514
Epoch 42/1000
26/26 - 1s - loss: 4.0036 - val_loss: 4.1419
Epoch 43/1000
26/26 - 1s - loss: 3.9912 - val_loss: 4.1336
Epoch 44/1000
26/26 - 1s - loss: 3.9800 - val_loss: 4.1245
Epoch 45/1000
26/26 - 1s - loss: 3.9672 - val_loss: 4.1168
Epoch 46/1000
26/26 - 1s - loss: 3.9564 - val_loss: 4.1079
Epoch 47/1000
26/26 - 1s - loss: 3.9417 - val_loss: 4.1005
Epoch 48/1000
26/26 - 1s - loss: 3.9330 - val_loss: 4.0923
Epoch 49/1000
26/26 - 1s - loss: 3.9219 - val_loss: 4.0850
Epoch 50/1000
26/26 - 1s - loss: 3.9106 - val_loss: 4.0779
Epoch 00050: val_loss improved from 4.15982 to 4.07794, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-0.model.weights.hdf5
Epoch 51/1000
26/26 - 1s - loss: 3.8985 - val_loss: 4.0706
Epoch 52/1000
26/26 - 1s - loss: 3.8893 - val_loss: 4.0635
Epoch 53/1000
26/26 - 1s - loss: 3.8770 - val_loss: 4.0566
Epoch 54/1000
26/26 - 1s - loss: 3.8677 - val_loss: 4.0490
Epoch 55/1000
26/26 - 1s - loss: 3.8566 - val_loss: 4.0401
Epoch 56/1000
26/26 - 1s - loss: 3.8480 - val_loss: 4.0333
Epoch 57/1000
26/26 - 1s - loss: 3.8377 - val_loss: 4.0267
Epoch 58/1000
26/26 - 1s - loss: 3.8254 - val_loss: 4.0219
Epoch 59/1000
26/26 - 1s - loss: 3.8144 - val_loss: 4.0134
Epoch 60/1000
26/26 - 1s - loss: 3.8059 - val_loss: 4.0074
Epoch 00060: val_loss improved from 4.07794 to 4.00737, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-0.model.weights.hdf5
Epoch 61/1000
26/26 - 1s - loss: 3.7978 - val_loss: 4.0015
Epoch 62/1000
26/26 - 1s - loss: 3.7865 - val_loss: 3.9931
Epoch 63/1000
26/26 - 1s - loss: 3.7788 - val_loss: 3.9872
Epoch 64/1000
26/26 - 1s - loss: 3.7709 - val_loss: 3.9815
Epoch 65/1000
26/26 - 1s - loss: 3.7595 - val_loss: 3.9734
Epoch 66/1000
26/26 - 1s - loss: 3.7501 - val_loss: 3.9657
Epoch 67/1000
26/26 - 1s - loss: 3.7412 - val_loss: 3.9594
Epoch 68/1000
26/26 - 1s - loss: 3.7289 - val_loss: 3.9539
Epoch 69/1000
26/26 - 1s - loss: 3.7260 - val_loss: 3.9484
Epoch 70/1000
26/26 - 1s - loss: 3.7159 - val_loss: 3.9422
Epoch 00070: val_loss improved from 4.00737 to 3.94225, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-0.model.weights.hdf5
Epoch 71/1000
26/26 - 1s - loss: 3.7062 - val_loss: 3.9361
Epoch 72/1000
26/26 - 1s - loss: 3.6961 - val_loss: 3.9307
Epoch 73/1000
26/26 - 1s - loss: 3.6885 - val_loss: 3.9259
Epoch 74/1000
26/26 - 1s - loss: 3.6804 - val_loss: 3.9192
Epoch 75/1000
26/26 - 1s - loss: 3.6705 - val_loss: 3.9137
Epoch 76/1000
26/26 - 1s - loss: 3.6612 - val_loss: 3.9071
Epoch 77/1000
26/26 - 1s - loss: 3.6565 - val_loss: 3.8998
Epoch 78/1000
26/26 - 1s - loss: 3.6476 - val_loss: 3.8935
Epoch 79/1000
26/26 - 1s - loss: 3.6378 - val_loss: 3.8880
Epoch 80/1000
26/26 - 1s - loss: 3.6332 - val_loss: 3.8810
Epoch 00080: val_loss improved from 3.94225 to 3.88105, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-0.model.weights.hdf5
Epoch 81/1000
26/26 - 1s - loss: 3.6225 - val_loss: 3.8748
Epoch 82/1000
26/26 - 1s - loss: 3.6171 - val_loss: 3.8696
Epoch 83/1000
26/26 - 1s - loss: 3.6089 - val_loss: 3.8629
Epoch 84/1000
26/26 - 1s - loss: 3.5991 - val_loss: 3.8571
Epoch 85/1000
26/26 - 1s - loss: 3.5911 - val_loss: 3.8512
Epoch 86/1000
26/26 - 1s - loss: 3.5826 - val_loss: 3.8463
Epoch 87/1000
26/26 - 1s - loss: 3.5778 - val_loss: 3.8418
Epoch 88/1000
26/26 - 1s - loss: 3.5687 - val_loss: 3.8356
Epoch 89/1000
26/26 - 1s - loss: 3.5623 - val_loss: 3.8299
Epoch 90/1000
26/26 - 1s - loss: 3.5549 - val_loss: 3.8264
Epoch 00090: val_loss improved from 3.88105 to 3.82636, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-0.model.weights.hdf5
Epoch 91/1000
26/26 - 1s - loss: 3.5472 - val_loss: 3.8201
Epoch 92/1000
26/26 - 1s - loss: 3.5443 - val_loss: 3.8134
Epoch 93/1000
26/26 - 1s - loss: 3.5325 - val_loss: 3.8097
Epoch 94/1000
26/26 - 1s - loss: 3.5258 - val_loss: 3.8031
Epoch 95/1000
26/26 - 1s - loss: 3.5194 - val_loss: 3.7972
Epoch 96/1000
26/26 - 1s - loss: 3.5147 - val_loss: 3.7925
Epoch 97/1000
26/26 - 1s - loss: 3.5044 - val_loss: 3.7866
Epoch 98/1000
26/26 - 1s - loss: 3.4983 - val_loss: 3.7821
Epoch 99/1000
26/26 - 1s - loss: 3.4943 - val_loss: 3.7762
Epoch 100/1000
26/26 - 1s - loss: 3.4873 - val_loss: 3.7730
Epoch 00100: val_loss improved from 3.82636 to 3.77305, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-0.model.weights.hdf5
Epoch 101/1000
26/26 - 1s - loss: 3.4788 - val_loss: 3.7678
Epoch 102/1000
26/26 - 1s - loss: 3.4699 - val_loss: 3.7621
Epoch 103/1000
26/26 - 1s - loss: 3.4643 - val_loss: 3.7567
Epoch 104/1000
26/26 - 1s - loss: 3.4575 - val_loss: 3.7519
Epoch 105/1000
26/26 - 1s - loss: 3.4518 - val_loss: 3.7463
Epoch 106/1000
26/26 - 1s - loss: 3.4459 - val_loss: 3.7410
Epoch 107/1000
26/26 - 1s - loss: 3.4392 - val_loss: 3.7362
Epoch 108/1000
26/26 - 1s - loss: 3.4340 - val_loss: 3.7322
Epoch 109/1000
26/26 - 1s - loss: 3.4253 - val_loss: 3.7268
Epoch 110/1000
26/26 - 1s - loss: 3.4202 - val_loss: 3.7231
Epoch 00110: val_loss improved from 3.77305 to 3.72314, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-0.model.weights.hdf5
Epoch 111/1000
26/26 - 1s - loss: 3.4147 - val_loss: 3.7165
Epoch 112/1000
26/26 - 1s - loss: 3.4068 - val_loss: 3.7118
Epoch 113/1000
26/26 - 1s - loss: 3.4014 - val_loss: 3.7068
Epoch 114/1000
26/26 - 1s - loss: 3.3963 - val_loss: 3.7008
Epoch 115/1000
26/26 - 1s - loss: 3.3896 - val_loss: 3.6969
Epoch 116/1000
26/26 - 1s - loss: 3.3850 - val_loss: 3.6924
Epoch 117/1000
26/26 - 1s - loss: 3.3761 - val_loss: 3.6884
Epoch 118/1000
26/26 - 1s - loss: 3.3707 - val_loss: 3.6846
Epoch 119/1000
26/26 - 1s - loss: 3.3647 - val_loss: 3.6794
Epoch 120/1000
26/26 - 1s - loss: 3.3590 - val_loss: 3.6744
Epoch 00120: val_loss improved from 3.72314 to 3.67436, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-0.model.weights.hdf5
Epoch 121/1000
26/26 - 1s - loss: 3.3541 - val_loss: 3.6688
Epoch 122/1000
26/26 - 1s - loss: 3.3467 - val_loss: 3.6652
Epoch 123/1000
26/26 - 1s - loss: 3.3429 - val_loss: 3.6595
Epoch 124/1000
26/26 - 1s - loss: 3.3401 - val_loss: 3.6545
Epoch 125/1000
26/26 - 1s - loss: 3.3332 - val_loss: 3.6495
Epoch 126/1000
26/26 - 1s - loss: 3.3265 - val_loss: 3.6455
Epoch 127/1000
26/26 - 1s - loss: 3.3196 - val_loss: 3.6416
Epoch 128/1000
26/26 - 1s - loss: 3.3165 - val_loss: 3.6373
Epoch 129/1000
26/26 - 1s - loss: 3.3086 - val_loss: 3.6323
Epoch 130/1000
26/26 - 1s - loss: 3.3040 - val_loss: 3.6278
Epoch 00130: val_loss improved from 3.67436 to 3.62782, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-0.model.weights.hdf5
Epoch 131/1000
26/26 - 1s - loss: 3.3006 - val_loss: 3.6240
Epoch 132/1000
26/26 - 1s - loss: 3.2911 - val_loss: 3.6192
Epoch 133/1000
26/26 - 1s - loss: 3.2878 - val_loss: 3.6131
Epoch 134/1000
26/26 - 1s - loss: 3.2839 - val_loss: 3.6098
Epoch 135/1000
26/26 - 1s - loss: 3.2776 - val_loss: 3.6052
Epoch 136/1000
26/26 - 1s - loss: 3.2730 - val_loss: 3.6005
Epoch 137/1000
26/26 - 1s - loss: 3.2675 - val_loss: 3.5955
Epoch 138/1000
26/26 - 1s - loss: 3.2594 - val_loss: 3.5916
Epoch 139/1000
26/26 - 1s - loss: 3.2556 - val_loss: 3.5875
Epoch 140/1000
26/26 - 1s - loss: 3.2514 - val_loss: 3.5828
Epoch 00140: val_loss improved from 3.62782 to 3.58279, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-0.model.weights.hdf5
Epoch 141/1000
26/26 - 1s - loss: 3.2443 - val_loss: 3.5787
Epoch 142/1000
26/26 - 1s - loss: 3.2395 - val_loss: 3.5751
Epoch 143/1000
26/26 - 1s - loss: 3.2375 - val_loss: 3.5709
Epoch 144/1000
26/26 - 1s - loss: 3.2324 - val_loss: 3.5660
Epoch 145/1000
26/26 - 1s - loss: 3.2252 - val_loss: 3.5622
Epoch 146/1000
26/26 - 1s - loss: 3.2213 - val_loss: 3.5578
Epoch 147/1000
26/26 - 1s - loss: 3.2200 - val_loss: 3.5521
Epoch 148/1000
26/26 - 1s - loss: 3.2111 - val_loss: 3.5484
Epoch 149/1000
26/26 - 1s - loss: 3.2058 - val_loss: 3.5453
Epoch 150/1000
26/26 - 1s - loss: 3.2020 - val_loss: 3.5408
Epoch 00150: val_loss improved from 3.58279 to 3.54075, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-0.model.weights.hdf5
Epoch 151/1000
26/26 - 1s - loss: 3.1960 - val_loss: 3.5370
Epoch 152/1000
26/26 - 1s - loss: 3.1947 - val_loss: 3.5335
Epoch 153/1000
26/26 - 1s - loss: 3.1861 - val_loss: 3.5280
Epoch 154/1000
26/26 - 1s - loss: 3.1826 - val_loss: 3.5232
Epoch 155/1000
26/26 - 1s - loss: 3.1779 - val_loss: 3.5197
Epoch 156/1000
26/26 - 1s - loss: 3.1717 - val_loss: 3.5151
Epoch 157/1000
26/26 - 1s - loss: 3.1685 - val_loss: 3.5123
Epoch 158/1000
26/26 - 1s - loss: 3.1659 - val_loss: 3.5107
Epoch 159/1000
26/26 - 1s - loss: 3.1608 - val_loss: 3.5064
Epoch 160/1000
26/26 - 1s - loss: 3.1588 - val_loss: 3.5006
Epoch 00160: val_loss improved from 3.54075 to 3.50057, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-0.model.weights.hdf5
Epoch 161/1000
26/26 - 1s - loss: 3.1492 - val_loss: 3.4952
Epoch 162/1000
26/26 - 1s - loss: 3.1471 - val_loss: 3.4908
Epoch 163/1000
26/26 - 1s - loss: 3.1419 - val_loss: 3.4888
Epoch 164/1000
26/26 - 1s - loss: 3.1363 - val_loss: 3.4834
Epoch 165/1000
26/26 - 1s - loss: 3.1346 - val_loss: 3.4784
Epoch 166/1000
26/26 - 1s - loss: 3.1273 - val_loss: 3.4749
Epoch 167/1000
26/26 - 1s - loss: 3.1240 - val_loss: 3.4709
Epoch 168/1000
26/26 - 1s - loss: 3.1176 - val_loss: 3.4675
Epoch 169/1000
26/26 - 1s - loss: 3.1147 - val_loss: 3.4631
Epoch 170/1000
26/26 - 1s - loss: 3.1122 - val_loss: 3.4585
Epoch 00170: val_loss improved from 3.50057 to 3.45846, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-0.model.weights.hdf5
Epoch 171/1000
26/26 - 1s - loss: 3.1083 - val_loss: 3.4556
Epoch 172/1000
26/26 - 1s - loss: 3.1001 - val_loss: 3.4502
Epoch 173/1000
26/26 - 1s - loss: 3.0955 - val_loss: 3.4470
Epoch 174/1000
26/26 - 1s - loss: 3.0926 - val_loss: 3.4429
Epoch 175/1000
26/26 - 1s - loss: 3.0899 - val_loss: 3.4401
Epoch 176/1000
26/26 - 1s - loss: 3.0844 - val_loss: 3.4364
Epoch 177/1000
26/26 - 1s - loss: 3.0796 - val_loss: 3.4317
Epoch 178/1000
26/26 - 1s - loss: 3.0765 - val_loss: 3.4287
Epoch 179/1000
26/26 - 1s - loss: 3.0720 - val_loss: 3.4251
Epoch 180/1000
26/26 - 1s - loss: 3.0669 - val_loss: 3.4220
Epoch 00180: val_loss improved from 3.45846 to 3.42200, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-0.model.weights.hdf5
Epoch 181/1000
26/26 - 1s - loss: 3.0648 - val_loss: 3.4178
Epoch 182/1000
26/26 - 1s - loss: 3.0574 - val_loss: 3.4132
Epoch 183/1000
26/26 - 1s - loss: 3.0551 - val_loss: 3.4097
Epoch 184/1000
26/26 - 1s - loss: 3.0525 - val_loss: 3.4066
Epoch 185/1000
26/26 - 1s - loss: 3.0465 - val_loss: 3.4030
Epoch 186/1000
26/26 - 1s - loss: 3.0426 - val_loss: 3.3994
Epoch 187/1000
26/26 - 1s - loss: 3.0393 - val_loss: 3.3938
Epoch 188/1000
26/26 - 1s - loss: 3.0349 - val_loss: 3.3916
Epoch 189/1000
26/26 - 1s - loss: 3.0302 - val_loss: 3.3885
Epoch 190/1000
26/26 - 1s - loss: 3.0289 - val_loss: 3.3854
Epoch 00190: val_loss improved from 3.42200 to 3.38545, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-0.model.weights.hdf5
Epoch 191/1000
26/26 - 1s - loss: 3.0232 - val_loss: 3.3809
Epoch 192/1000
26/26 - 1s - loss: 3.0204 - val_loss: 3.3767
Epoch 193/1000
26/26 - 1s - loss: 3.0155 - val_loss: 3.3736
Epoch 194/1000
26/26 - 1s - loss: 3.0118 - val_loss: 3.3697
Epoch 195/1000
26/26 - 1s - loss: 3.0042 - val_loss: 3.3667
Epoch 196/1000
26/26 - 1s - loss: 3.0035 - val_loss: 3.3628
Epoch 197/1000
26/26 - 1s - loss: 2.9996 - val_loss: 3.3597
Epoch 198/1000
26/26 - 1s - loss: 2.9956 - val_loss: 3.3565
Epoch 199/1000
26/26 - 1s - loss: 2.9907 - val_loss: 3.3537
Epoch 200/1000
26/26 - 1s - loss: 2.9849 - val_loss: 3.3502
Epoch 00200: val_loss improved from 3.38545 to 3.35023, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-0.model.weights.hdf5
Epoch 201/1000
26/26 - 1s - loss: 2.9821 - val_loss: 3.3477
Epoch 202/1000
26/26 - 1s - loss: 2.9810 - val_loss: 3.3425
Epoch 203/1000
26/26 - 1s - loss: 2.9735 - val_loss: 3.3390
Epoch 204/1000
26/26 - 1s - loss: 2.9711 - val_loss: 3.3348
Epoch 205/1000
26/26 - 1s - loss: 2.9683 - val_loss: 3.3313
Epoch 206/1000
26/26 - 1s - loss: 2.9650 - val_loss: 3.3273
Epoch 207/1000
26/26 - 1s - loss: 2.9607 - val_loss: 3.3238
Epoch 208/1000
26/26 - 1s - loss: 2.9561 - val_loss: 3.3200
Epoch 209/1000
26/26 - 1s - loss: 2.9532 - val_loss: 3.3167
Epoch 210/1000
26/26 - 1s - loss: 2.9501 - val_loss: 3.3147
Epoch 00210: val_loss improved from 3.35023 to 3.31474, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-0.model.weights.hdf5
Epoch 211/1000
26/26 - 1s - loss: 2.9450 - val_loss: 3.3101
Epoch 212/1000
26/26 - 1s - loss: 2.9399 - val_loss: 3.3061
Epoch 213/1000
26/26 - 1s - loss: 2.9368 - val_loss: 3.3021
Epoch 214/1000
26/26 - 1s - loss: 2.9319 - val_loss: 3.2982
Epoch 215/1000
26/26 - 1s - loss: 2.9287 - val_loss: 3.2953
Epoch 216/1000
26/26 - 1s - loss: 2.9267 - val_loss: 3.2924
Epoch 217/1000
26/26 - 1s - loss: 2.9209 - val_loss: 3.2877
Epoch 218/1000
26/26 - 1s - loss: 2.9202 - val_loss: 3.2838
Epoch 219/1000
26/26 - 1s - loss: 2.9146 - val_loss: 3.2809
Epoch 220/1000
26/26 - 1s - loss: 2.9100 - val_loss: 3.2779
Epoch 00220: val_loss improved from 3.31474 to 3.27786, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-0.model.weights.hdf5
Epoch 221/1000
26/26 - 1s - loss: 2.9062 - val_loss: 3.2756
Epoch 222/1000
26/26 - 1s - loss: 2.9031 - val_loss: 3.2720
Epoch 223/1000
26/26 - 1s - loss: 2.8993 - val_loss: 3.2692
Epoch 224/1000
26/26 - 1s - loss: 2.8990 - val_loss: 3.2674
Epoch 225/1000
26/26 - 1s - loss: 2.8947 - val_loss: 3.2626
Epoch 226/1000
26/26 - 1s - loss: 2.8867 - val_loss: 3.2585
Epoch 227/1000
26/26 - 1s - loss: 2.8882 - val_loss: 3.2546
Epoch 228/1000
26/26 - 1s - loss: 2.8813 - val_loss: 3.2501
Epoch 229/1000
26/26 - 1s - loss: 2.8797 - val_loss: 3.2475
Epoch 230/1000
26/26 - 1s - loss: 2.8732 - val_loss: 3.2446
Epoch 00230: val_loss improved from 3.27786 to 3.24455, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-0.model.weights.hdf5
Epoch 231/1000
26/26 - 1s - loss: 2.8687 - val_loss: 3.2416
Epoch 232/1000
26/26 - 1s - loss: 2.8672 - val_loss: 3.2388
Epoch 233/1000
26/26 - 1s - loss: 2.8642 - val_loss: 3.2349
Epoch 234/1000
26/26 - 1s - loss: 2.8598 - val_loss: 3.2317
Epoch 235/1000
26/26 - 1s - loss: 2.8579 - val_loss: 3.2277
Epoch 236/1000
26/26 - 1s - loss: 2.8526 - val_loss: 3.2242
Epoch 237/1000
26/26 - 1s - loss: 2.8504 - val_loss: 3.2214
Epoch 238/1000
26/26 - 1s - loss: 2.8477 - val_loss: 3.2179
Epoch 239/1000
26/26 - 1s - loss: 2.8450 - val_loss: 3.2146
Epoch 240/1000
26/26 - 1s - loss: 2.8386 - val_loss: 3.2113
Epoch 00240: val_loss improved from 3.24455 to 3.21134, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-0.model.weights.hdf5
Epoch 241/1000
26/26 - 1s - loss: 2.8378 - val_loss: 3.2087
Epoch 242/1000
26/26 - 1s - loss: 2.8316 - val_loss: 3.2065
Epoch 243/1000
26/26 - 1s - loss: 2.8289 - val_loss: 3.2021
Epoch 244/1000
26/26 - 1s - loss: 2.8252 - val_loss: 3.1984
Epoch 245/1000
26/26 - 1s - loss: 2.8208 - val_loss: 3.1955
Epoch 246/1000
26/26 - 1s - loss: 2.8184 - val_loss: 3.1936
Epoch 247/1000
26/26 - 1s - loss: 2.8146 - val_loss: 3.1911
Epoch 248/1000
26/26 - 1s - loss: 2.8105 - val_loss: 3.1857
Epoch 249/1000
26/26 - 1s - loss: 2.8077 - val_loss: 3.1828
Epoch 250/1000
26/26 - 1s - loss: 2.8063 - val_loss: 3.1805
Epoch 00250: val_loss improved from 3.21134 to 3.18053, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-0.model.weights.hdf5
Epoch 251/1000
26/26 - 1s - loss: 2.8040 - val_loss: 3.1779
Epoch 252/1000
26/26 - 1s - loss: 2.7986 - val_loss: 3.1749
Epoch 253/1000
26/26 - 1s - loss: 2.7968 - val_loss: 3.1712
Epoch 254/1000
26/26 - 1s - loss: 2.7928 - val_loss: 3.1670
Epoch 255/1000
26/26 - 1s - loss: 2.7869 - val_loss: 3.1651
Epoch 256/1000
26/26 - 1s - loss: 2.7884 - val_loss: 3.1617
Epoch 257/1000
26/26 - 1s - loss: 2.7852 - val_loss: 3.1583
Epoch 258/1000
26/26 - 1s - loss: 2.7768 - val_loss: 3.1555
Epoch 259/1000
26/26 - 1s - loss: 2.7763 - val_loss: 3.1513
Epoch 260/1000
26/26 - 1s - loss: 2.7709 - val_loss: 3.1478
Epoch 00260: val_loss improved from 3.18053 to 3.14780, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-0.model.weights.hdf5
Epoch 261/1000
26/26 - 1s - loss: 2.7687 - val_loss: 3.1450
Epoch 262/1000
26/26 - 1s - loss: 2.7683 - val_loss: 3.1421
Epoch 263/1000
26/26 - 1s - loss: 2.7628 - val_loss: 3.1396
Epoch 264/1000
26/26 - 1s - loss: 2.7593 - val_loss: 3.1348
Epoch 265/1000
26/26 - 1s - loss: 2.7552 - val_loss: 3.1323
Epoch 266/1000
26/26 - 1s - loss: 2.7522 - val_loss: 3.1295
Epoch 267/1000
26/26 - 1s - loss: 2.7493 - val_loss: 3.1273
Epoch 268/1000
26/26 - 1s - loss: 2.7451 - val_loss: 3.1264
Epoch 269/1000
26/26 - 1s - loss: 2.7435 - val_loss: 3.1229
Epoch 270/1000
26/26 - 1s - loss: 2.7390 - val_loss: 3.1206
Epoch 00270: val_loss improved from 3.14780 to 3.12058, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-0.model.weights.hdf5
Epoch 271/1000
26/26 - 1s - loss: 2.7368 - val_loss: 3.1163
Epoch 272/1000
26/26 - 1s - loss: 2.7344 - val_loss: 3.1136
Epoch 273/1000
26/26 - 1s - loss: 2.7282 - val_loss: 3.1097
Epoch 274/1000
26/26 - 1s - loss: 2.7267 - val_loss: 3.1063
Epoch 275/1000
26/26 - 1s - loss: 2.7240 - val_loss: 3.1035
Epoch 276/1000
26/26 - 1s - loss: 2.7218 - val_loss: 3.1000
Epoch 277/1000
26/26 - 1s - loss: 2.7162 - val_loss: 3.0981
Epoch 278/1000
26/26 - 1s - loss: 2.7144 - val_loss: 3.0935
Epoch 279/1000
26/26 - 1s - loss: 2.7096 - val_loss: 3.0910
Epoch 280/1000
26/26 - 1s - loss: 2.7076 - val_loss: 3.0877
Epoch 00280: val_loss improved from 3.12058 to 3.08768, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-0.model.weights.hdf5
Epoch 281/1000
26/26 - 1s - loss: 2.7037 - val_loss: 3.0863
Epoch 282/1000
26/26 - 1s - loss: 2.7019 - val_loss: 3.0823
Epoch 283/1000
26/26 - 1s - loss: 2.7003 - val_loss: 3.0802
Epoch 284/1000
26/26 - 1s - loss: 2.6977 - val_loss: 3.0768
Epoch 285/1000
26/26 - 1s - loss: 2.6916 - val_loss: 3.0765
Epoch 286/1000
26/26 - 1s - loss: 2.6892 - val_loss: 3.0718
Epoch 287/1000
26/26 - 1s - loss: 2.6871 - val_loss: 3.0682
Epoch 288/1000
26/26 - 1s - loss: 2.6824 - val_loss: 3.0657
Epoch 289/1000
26/26 - 1s - loss: 2.6782 - val_loss: 3.0645
Epoch 290/1000
26/26 - 1s - loss: 2.6776 - val_loss: 3.0606
Epoch 00290: val_loss improved from 3.08768 to 3.06064, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-0.model.weights.hdf5
Epoch 291/1000
26/26 - 1s - loss: 2.6736 - val_loss: 3.0575
Epoch 292/1000
26/26 - 1s - loss: 2.6721 - val_loss: 3.0541
Epoch 293/1000
26/26 - 1s - loss: 2.6659 - val_loss: 3.0522
Epoch 294/1000
26/26 - 1s - loss: 2.6652 - val_loss: 3.0507
Epoch 295/1000
26/26 - 1s - loss: 2.6612 - val_loss: 3.0465
Epoch 296/1000
26/26 - 1s - loss: 2.6609 - val_loss: 3.0440
Epoch 297/1000
26/26 - 1s - loss: 2.6575 - val_loss: 3.0402
Epoch 298/1000
26/26 - 1s - loss: 2.6518 - val_loss: 3.0367
Epoch 299/1000
26/26 - 1s - loss: 2.6487 - val_loss: 3.0338
Epoch 300/1000
26/26 - 1s - loss: 2.6477 - val_loss: 3.0306
Epoch 00300: val_loss improved from 3.06064 to 3.03063, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-0.model.weights.hdf5
Epoch 301/1000
26/26 - 1s - loss: 2.6455 - val_loss: 3.0279
Epoch 302/1000
26/26 - 1s - loss: 2.6387 - val_loss: 3.0256
Epoch 303/1000
26/26 - 1s - loss: 2.6362 - val_loss: 3.0223
Epoch 304/1000
26/26 - 1s - loss: 2.6351 - val_loss: 3.0205
Epoch 305/1000
26/26 - 1s - loss: 2.6324 - val_loss: 3.0169
Epoch 306/1000
26/26 - 1s - loss: 2.6273 - val_loss: 3.0136
Epoch 307/1000
26/26 - 1s - loss: 2.6267 - val_loss: 3.0108
Epoch 308/1000
26/26 - 1s - loss: 2.6222 - val_loss: 3.0085
Epoch 309/1000
26/26 - 1s - loss: 2.6193 - val_loss: 3.0061
Epoch 310/1000
26/26 - 1s - loss: 2.6182 - val_loss: 3.0031
Epoch 00310: val_loss improved from 3.03063 to 3.00307, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-0.model.weights.hdf5
Epoch 311/1000
26/26 - 1s - loss: 2.6163 - val_loss: 2.9991
Epoch 312/1000
26/26 - 1s - loss: 2.6145 - val_loss: 2.9975
Epoch 313/1000
26/26 - 1s - loss: 2.6097 - val_loss: 2.9948
Epoch 314/1000
26/26 - 1s - loss: 2.6064 - val_loss: 2.9937
Epoch 315/1000
26/26 - 1s - loss: 2.6004 - val_loss: 2.9911
Epoch 316/1000
26/26 - 1s - loss: 2.6002 - val_loss: 2.9868
Epoch 317/1000
26/26 - 1s - loss: 2.5956 - val_loss: 2.9841
Epoch 318/1000
26/26 - 1s - loss: 2.5925 - val_loss: 2.9807
Epoch 319/1000
26/26 - 1s - loss: 2.5924 - val_loss: 2.9796
Epoch 320/1000
26/26 - 1s - loss: 2.5887 - val_loss: 2.9758
Epoch 00320: val_loss improved from 3.00307 to 2.97584, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-0.model.weights.hdf5
Epoch 321/1000
26/26 - 1s - loss: 2.5848 - val_loss: 2.9731
Epoch 322/1000
26/26 - 1s - loss: 2.5821 - val_loss: 2.9698
Epoch 323/1000
26/26 - 1s - loss: 2.5801 - val_loss: 2.9675
Epoch 324/1000
26/26 - 1s - loss: 2.5743 - val_loss: 2.9650
Epoch 325/1000
26/26 - 1s - loss: 2.5752 - val_loss: 2.9623
Epoch 326/1000
26/26 - 1s - loss: 2.5694 - val_loss: 2.9594
Epoch 327/1000
26/26 - 1s - loss: 2.5674 - val_loss: 2.9561
Epoch 328/1000
26/26 - 1s - loss: 2.5661 - val_loss: 2.9543
Epoch 329/1000
26/26 - 1s - loss: 2.5636 - val_loss: 2.9527
Epoch 330/1000
26/26 - 1s - loss: 2.5586 - val_loss: 2.9512
Epoch 00330: val_loss improved from 2.97584 to 2.95120, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-0.model.weights.hdf5
Epoch 331/1000
26/26 - 1s - loss: 2.5569 - val_loss: 2.9474
Epoch 332/1000
26/26 - 1s - loss: 2.5548 - val_loss: 2.9447
Epoch 333/1000
26/26 - 1s - loss: 2.5510 - val_loss: 2.9419
Epoch 334/1000
26/26 - 1s - loss: 2.5466 - val_loss: 2.9398
Epoch 335/1000
26/26 - 1s - loss: 2.5467 - val_loss: 2.9377
Epoch 336/1000
26/26 - 1s - loss: 2.5416 - val_loss: 2.9342
Epoch 337/1000
26/26 - 1s - loss: 2.5382 - val_loss: 2.9307
Epoch 338/1000
26/26 - 1s - loss: 2.5342 - val_loss: 2.9279
Epoch 339/1000
26/26 - 1s - loss: 2.5355 - val_loss: 2.9242
Epoch 340/1000
26/26 - 1s - loss: 2.5343 - val_loss: 2.9207
Epoch 00340: val_loss improved from 2.95120 to 2.92072, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-0.model.weights.hdf5
Epoch 341/1000
26/26 - 1s - loss: 2.5297 - val_loss: 2.9193
Epoch 342/1000
26/26 - 1s - loss: 2.5270 - val_loss: 2.9181
Epoch 343/1000
26/26 - 1s - loss: 2.5243 - val_loss: 2.9142
Epoch 344/1000
26/26 - 1s - loss: 2.5164 - val_loss: 2.9112
Epoch 345/1000
26/26 - 1s - loss: 2.5180 - val_loss: 2.9089
Epoch 346/1000
26/26 - 1s - loss: 2.5156 - val_loss: 2.9045
Epoch 347/1000
26/26 - 1s - loss: 2.5117 - val_loss: 2.9014
Epoch 348/1000
26/26 - 1s - loss: 2.5101 - val_loss: 2.9002
Epoch 349/1000
26/26 - 1s - loss: 2.5052 - val_loss: 2.8959
Epoch 350/1000
26/26 - 1s - loss: 2.5019 - val_loss: 2.8954
Epoch 00350: val_loss improved from 2.92072 to 2.89544, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-0.model.weights.hdf5
Epoch 351/1000
26/26 - 1s - loss: 2.5017 - val_loss: 2.8935
Epoch 352/1000
26/26 - 1s - loss: 2.4998 - val_loss: 2.8907
Epoch 353/1000
26/26 - 1s - loss: 2.4946 - val_loss: 2.8887
Epoch 354/1000
26/26 - 1s - loss: 2.4937 - val_loss: 2.8847
Epoch 355/1000
26/26 - 1s - loss: 2.4910 - val_loss: 2.8818
Epoch 356/1000
26/26 - 1s - loss: 2.4868 - val_loss: 2.8780
Epoch 357/1000
26/26 - 1s - loss: 2.4852 - val_loss: 2.8785
Epoch 358/1000
26/26 - 1s - loss: 2.4835 - val_loss: 2.8775
Epoch 359/1000
26/26 - 1s - loss: 2.4807 - val_loss: 2.8741
Epoch 360/1000
26/26 - 1s - loss: 2.4779 - val_loss: 2.8706
Epoch 00360: val_loss improved from 2.89544 to 2.87059, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-0.model.weights.hdf5
Epoch 361/1000
26/26 - 1s - loss: 2.4741 - val_loss: 2.8691
Epoch 362/1000
26/26 - 1s - loss: 2.4720 - val_loss: 2.8661
Epoch 363/1000
26/26 - 1s - loss: 2.4690 - val_loss: 2.8637
Epoch 364/1000
26/26 - 1s - loss: 2.4671 - val_loss: 2.8610
Epoch 365/1000
26/26 - 1s - loss: 2.4638 - val_loss: 2.8580
Epoch 366/1000
26/26 - 1s - loss: 2.4615 - val_loss: 2.8561
Epoch 367/1000
26/26 - 1s - loss: 2.4602 - val_loss: 2.8528
Epoch 368/1000
26/26 - 1s - loss: 2.4549 - val_loss: 2.8498
Epoch 369/1000
26/26 - 1s - loss: 2.4523 - val_loss: 2.8470
Epoch 370/1000
26/26 - 1s - loss: 2.4467 - val_loss: 2.8456
Epoch 00370: val_loss improved from 2.87059 to 2.84557, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-0.model.weights.hdf5
Epoch 371/1000
26/26 - 1s - loss: 2.4509 - val_loss: 2.8432
Epoch 372/1000
26/26 - 1s - loss: 2.4462 - val_loss: 2.8403
Epoch 373/1000
26/26 - 1s - loss: 2.4406 - val_loss: 2.8387
Epoch 374/1000
26/26 - 1s - loss: 2.4399 - val_loss: 2.8356
Epoch 375/1000
26/26 - 1s - loss: 2.4412 - val_loss: 2.8323
Epoch 376/1000
26/26 - 1s - loss: 2.4354 - val_loss: 2.8300
Epoch 377/1000
26/26 - 1s - loss: 2.4333 - val_loss: 2.8278
Epoch 378/1000
26/26 - 1s - loss: 2.4273 - val_loss: 2.8255
Epoch 379/1000
26/26 - 1s - loss: 2.4272 - val_loss: 2.8232
Epoch 380/1000
26/26 - 1s - loss: 2.4246 - val_loss: 2.8199
Epoch 00380: val_loss improved from 2.84557 to 2.81988, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-0.model.weights.hdf5
Epoch 381/1000
26/26 - 1s - loss: 2.4229 - val_loss: 2.8185
Epoch 382/1000
26/26 - 1s - loss: 2.4191 - val_loss: 2.8156
Epoch 383/1000
26/26 - 1s - loss: 2.4179 - val_loss: 2.8120
Epoch 384/1000
26/26 - 1s - loss: 2.4140 - val_loss: 2.8088
Epoch 385/1000
26/26 - 1s - loss: 2.4112 - val_loss: 2.8067
Epoch 386/1000
26/26 - 1s - loss: 2.4109 - val_loss: 2.8044
Epoch 387/1000
26/26 - 1s - loss: 2.4036 - val_loss: 2.8005
Epoch 388/1000
26/26 - 1s - loss: 2.4046 - val_loss: 2.7988
Epoch 389/1000
26/26 - 1s - loss: 2.4015 - val_loss: 2.7972
Epoch 390/1000
26/26 - 1s - loss: 2.3966 - val_loss: 2.7949
Epoch 00390: val_loss improved from 2.81988 to 2.79493, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-0.model.weights.hdf5
Epoch 391/1000
26/26 - 1s - loss: 2.3974 - val_loss: 2.7931
Epoch 392/1000
26/26 - 1s - loss: 2.3953 - val_loss: 2.7911
Epoch 393/1000
26/26 - 1s - loss: 2.3922 - val_loss: 2.7873
Epoch 394/1000
26/26 - 1s - loss: 2.3912 - val_loss: 2.7860
Epoch 395/1000
26/26 - 1s - loss: 2.3854 - val_loss: 2.7834
Epoch 396/1000
26/26 - 1s - loss: 2.3848 - val_loss: 2.7813
Epoch 397/1000
26/26 - 1s - loss: 2.3800 - val_loss: 2.7773
Epoch 398/1000
26/26 - 1s - loss: 2.3794 - val_loss: 2.7750
Epoch 399/1000
26/26 - 1s - loss: 2.3766 - val_loss: 2.7739
Epoch 400/1000
26/26 - 1s - loss: 2.3725 - val_loss: 2.7721
Epoch 00400: val_loss improved from 2.79493 to 2.77210, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-0.model.weights.hdf5
Epoch 401/1000
26/26 - 1s - loss: 2.3704 - val_loss: 2.7681
Epoch 402/1000
26/26 - 1s - loss: 2.3693 - val_loss: 2.7662
Epoch 403/1000
26/26 - 1s - loss: 2.3658 - val_loss: 2.7649
Epoch 404/1000
26/26 - 1s - loss: 2.3636 - val_loss: 2.7628
Epoch 405/1000
26/26 - 1s - loss: 2.3620 - val_loss: 2.7594
Epoch 406/1000
26/26 - 1s - loss: 2.3584 - val_loss: 2.7566
Epoch 407/1000
26/26 - 1s - loss: 2.3570 - val_loss: 2.7545
Epoch 408/1000
26/26 - 1s - loss: 2.3545 - val_loss: 2.7527
Epoch 409/1000
26/26 - 1s - loss: 2.3525 - val_loss: 2.7499
Epoch 410/1000
26/26 - 1s - loss: 2.3502 - val_loss: 2.7473
Epoch 00410: val_loss improved from 2.77210 to 2.74730, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-0.model.weights.hdf5
Epoch 411/1000
26/26 - 1s - loss: 2.3480 - val_loss: 2.7459
Epoch 412/1000
26/26 - 1s - loss: 2.3449 - val_loss: 2.7421
Epoch 413/1000
26/26 - 1s - loss: 2.3421 - val_loss: 2.7398
Epoch 414/1000
26/26 - 1s - loss: 2.3418 - val_loss: 2.7386
Epoch 415/1000
26/26 - 1s - loss: 2.3368 - val_loss: 2.7347
Epoch 416/1000
26/26 - 1s - loss: 2.3357 - val_loss: 2.7334
Epoch 417/1000
26/26 - 1s - loss: 2.3311 - val_loss: 2.7293
Epoch 418/1000
26/26 - 1s - loss: 2.3308 - val_loss: 2.7272
Epoch 419/1000
26/26 - 1s - loss: 2.3281 - val_loss: 2.7237
Epoch 420/1000
26/26 - 1s - loss: 2.3256 - val_loss: 2.7223
Epoch 00420: val_loss improved from 2.74730 to 2.72233, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-0.model.weights.hdf5
Epoch 421/1000
26/26 - 1s - loss: 2.3235 - val_loss: 2.7201
Epoch 422/1000
26/26 - 1s - loss: 2.3208 - val_loss: 2.7188
Epoch 423/1000
26/26 - 1s - loss: 2.3162 - val_loss: 2.7160
Epoch 424/1000
26/26 - 1s - loss: 2.3148 - val_loss: 2.7139
Epoch 425/1000
26/26 - 1s - loss: 2.3144 - val_loss: 2.7121
Epoch 426/1000
26/26 - 1s - loss: 2.3134 - val_loss: 2.7105
Epoch 427/1000
26/26 - 1s - loss: 2.3085 - val_loss: 2.7086
Epoch 428/1000
26/26 - 1s - loss: 2.3090 - val_loss: 2.7059
Epoch 429/1000
26/26 - 1s - loss: 2.3011 - val_loss: 2.7024
Epoch 430/1000
26/26 - 1s - loss: 2.3031 - val_loss: 2.6998
Epoch 00430: val_loss improved from 2.72233 to 2.69979, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-0.model.weights.hdf5
Epoch 431/1000
26/26 - 1s - loss: 2.2987 - val_loss: 2.6974
Epoch 432/1000
26/26 - 1s - loss: 2.2958 - val_loss: 2.6945
Epoch 433/1000
26/26 - 1s - loss: 2.2950 - val_loss: 2.6923
Epoch 434/1000
26/26 - 1s - loss: 2.2918 - val_loss: 2.6904
Epoch 435/1000
26/26 - 1s - loss: 2.2909 - val_loss: 2.6887
Epoch 436/1000
26/26 - 1s - loss: 2.2884 - val_loss: 2.6866
Epoch 437/1000
26/26 - 1s - loss: 2.2843 - val_loss: 2.6840
Epoch 438/1000
26/26 - 1s - loss: 2.2852 - val_loss: 2.6816
Epoch 439/1000
26/26 - 1s - loss: 2.2824 - val_loss: 2.6809
Epoch 440/1000
26/26 - 1s - loss: 2.2764 - val_loss: 2.6787
Epoch 00440: val_loss improved from 2.69979 to 2.67873, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-0.model.weights.hdf5
Epoch 441/1000
26/26 - 1s - loss: 2.2756 - val_loss: 2.6773
Epoch 442/1000
26/26 - 1s - loss: 2.2725 - val_loss: 2.6748
Epoch 443/1000
26/26 - 1s - loss: 2.2729 - val_loss: 2.6718
Epoch 444/1000
26/26 - 1s - loss: 2.2683 - val_loss: 2.6701
Epoch 445/1000
26/26 - 1s - loss: 2.2656 - val_loss: 2.6678
Epoch 446/1000
26/26 - 1s - loss: 2.2639 - val_loss: 2.6663
Epoch 447/1000
26/26 - 1s - loss: 2.2635 - val_loss: 2.6632
Epoch 448/1000
26/26 - 1s - loss: 2.2594 - val_loss: 2.6608
Epoch 449/1000
26/26 - 1s - loss: 2.2560 - val_loss: 2.6576
Epoch 450/1000
26/26 - 1s - loss: 2.2563 - val_loss: 2.6545
Epoch 00450: val_loss improved from 2.67873 to 2.65455, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-0.model.weights.hdf5
Epoch 451/1000
26/26 - 1s - loss: 2.2497 - val_loss: 2.6534
Epoch 452/1000
26/26 - 1s - loss: 2.2508 - val_loss: 2.6525
Epoch 453/1000
26/26 - 1s - loss: 2.2479 - val_loss: 2.6496
Epoch 454/1000
26/26 - 1s - loss: 2.2457 - val_loss: 2.6467
Epoch 455/1000
26/26 - 1s - loss: 2.2441 - val_loss: 2.6440
Epoch 456/1000
26/26 - 1s - loss: 2.2408 - val_loss: 2.6424
Epoch 457/1000
26/26 - 1s - loss: 2.2407 - val_loss: 2.6394
Epoch 458/1000
26/26 - 1s - loss: 2.2366 - val_loss: 2.6359
Epoch 459/1000
26/26 - 1s - loss: 2.2341 - val_loss: 2.6341
Epoch 460/1000
26/26 - 1s - loss: 2.2301 - val_loss: 2.6334
Epoch 00460: val_loss improved from 2.65455 to 2.63335, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-0.model.weights.hdf5
Epoch 461/1000
26/26 - 1s - loss: 2.2282 - val_loss: 2.6303
Epoch 462/1000
26/26 - 1s - loss: 2.2275 - val_loss: 2.6294
Epoch 463/1000
26/26 - 1s - loss: 2.2240 - val_loss: 2.6275
Epoch 464/1000
26/26 - 1s - loss: 2.2237 - val_loss: 2.6258
Epoch 465/1000
26/26 - 1s - loss: 2.2202 - val_loss: 2.6238
Epoch 466/1000
26/26 - 1s - loss: 2.2188 - val_loss: 2.6204
Epoch 467/1000
26/26 - 1s - loss: 2.2187 - val_loss: 2.6188
Epoch 468/1000
26/26 - 1s - loss: 2.2134 - val_loss: 2.6167
Epoch 469/1000
26/26 - 1s - loss: 2.2134 - val_loss: 2.6140
Epoch 470/1000
26/26 - 1s - loss: 2.2096 - val_loss: 2.6121
Epoch 00470: val_loss improved from 2.63335 to 2.61215, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-0.model.weights.hdf5
Epoch 471/1000
26/26 - 1s - loss: 2.2042 - val_loss: 2.6097
Epoch 472/1000
26/26 - 1s - loss: 2.2055 - val_loss: 2.6085
Epoch 473/1000
26/26 - 1s - loss: 2.2019 - val_loss: 2.6058
Epoch 474/1000
26/26 - 1s - loss: 2.2000 - val_loss: 2.6026
Epoch 475/1000
26/26 - 1s - loss: 2.1967 - val_loss: 2.6008
Epoch 476/1000
26/26 - 1s - loss: 2.1969 - val_loss: 2.5986
Epoch 477/1000
26/26 - 1s - loss: 2.1931 - val_loss: 2.5974
Epoch 478/1000
26/26 - 1s - loss: 2.1904 - val_loss: 2.5959
Epoch 479/1000
26/26 - 1s - loss: 2.1860 - val_loss: 2.5927
Epoch 480/1000
26/26 - 1s - loss: 2.1887 - val_loss: 2.5894
Epoch 00480: val_loss improved from 2.61215 to 2.58939, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-0.model.weights.hdf5
Epoch 481/1000
26/26 - 1s - loss: 2.1859 - val_loss: 2.5874
Epoch 482/1000
26/26 - 1s - loss: 2.1828 - val_loss: 2.5854
Epoch 483/1000
26/26 - 1s - loss: 2.1818 - val_loss: 2.5844
Epoch 484/1000
26/26 - 1s - loss: 2.1753 - val_loss: 2.5817
Epoch 485/1000
26/26 - 1s - loss: 2.1757 - val_loss: 2.5798
Epoch 486/1000
26/26 - 1s - loss: 2.1753 - val_loss: 2.5771
Epoch 487/1000
26/26 - 1s - loss: 2.1725 - val_loss: 2.5751
Epoch 488/1000
26/26 - 1s - loss: 2.1703 - val_loss: 2.5734
Epoch 489/1000
26/26 - 1s - loss: 2.1684 - val_loss: 2.5719
Epoch 490/1000
26/26 - 1s - loss: 2.1664 - val_loss: 2.5692
Epoch 00490: val_loss improved from 2.58939 to 2.56923, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-0.model.weights.hdf5
Epoch 491/1000
26/26 - 1s - loss: 2.1626 - val_loss: 2.5670
Epoch 492/1000
26/26 - 1s - loss: 2.1627 - val_loss: 2.5651
Epoch 493/1000
26/26 - 1s - loss: 2.1607 - val_loss: 2.5625
Epoch 494/1000
26/26 - 1s - loss: 2.1586 - val_loss: 2.5593
Epoch 495/1000
26/26 - 1s - loss: 2.1540 - val_loss: 2.5580
Epoch 496/1000
26/26 - 1s - loss: 2.1548 - val_loss: 2.5569
Epoch 497/1000
26/26 - 1s - loss: 2.1508 - val_loss: 2.5538
Epoch 498/1000
26/26 - 1s - loss: 2.1474 - val_loss: 2.5517
Epoch 499/1000
26/26 - 1s - loss: 2.1476 - val_loss: 2.5501
Epoch 500/1000
26/26 - 1s - loss: 2.1447 - val_loss: 2.5484
Epoch 00500: val_loss improved from 2.56923 to 2.54844, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-0.model.weights.hdf5
Epoch 501/1000
26/26 - 1s - loss: 2.1422 - val_loss: 2.5461
Epoch 502/1000
26/26 - 1s - loss: 2.1391 - val_loss: 2.5439
Epoch 503/1000
26/26 - 1s - loss: 2.1389 - val_loss: 2.5423
Epoch 504/1000
26/26 - 1s - loss: 2.1347 - val_loss: 2.5414
Epoch 505/1000
26/26 - 1s - loss: 2.1336 - val_loss: 2.5387
Epoch 506/1000
26/26 - 1s - loss: 2.1331 - val_loss: 2.5351
Epoch 507/1000
26/26 - 1s - loss: 2.1298 - val_loss: 2.5330
Epoch 508/1000
26/26 - 1s - loss: 2.1280 - val_loss: 2.5313
Epoch 509/1000
26/26 - 1s - loss: 2.1240 - val_loss: 2.5301
Epoch 510/1000
26/26 - 1s - loss: 2.1239 - val_loss: 2.5261
Epoch 00510: val_loss improved from 2.54844 to 2.52612, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-0.model.weights.hdf5
Epoch 511/1000
26/26 - 1s - loss: 2.1219 - val_loss: 2.5260
Epoch 512/1000
26/26 - 1s - loss: 2.1189 - val_loss: 2.5241
Epoch 513/1000
26/26 - 1s - loss: 2.1143 - val_loss: 2.5228
Epoch 514/1000
26/26 - 1s - loss: 2.1158 - val_loss: 2.5198
Epoch 515/1000
26/26 - 1s - loss: 2.1119 - val_loss: 2.5177
Epoch 516/1000
26/26 - 1s - loss: 2.1096 - val_loss: 2.5154
Epoch 517/1000
26/26 - 1s - loss: 2.1089 - val_loss: 2.5138
Epoch 518/1000
26/26 - 1s - loss: 2.1080 - val_loss: 2.5128
Epoch 519/1000
26/26 - 1s - loss: 2.1043 - val_loss: 2.5103
Epoch 520/1000
26/26 - 1s - loss: 2.1020 - val_loss: 2.5094
Epoch 00520: val_loss improved from 2.52612 to 2.50944, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-0.model.weights.hdf5
Epoch 521/1000
26/26 - 1s - loss: 2.1003 - val_loss: 2.5073
Epoch 522/1000
26/26 - 1s - loss: 2.0987 - val_loss: 2.5040
Epoch 523/1000
26/26 - 1s - loss: 2.0963 - val_loss: 2.5026
Epoch 524/1000
26/26 - 1s - loss: 2.0918 - val_loss: 2.5008
Epoch 525/1000
26/26 - 1s - loss: 2.0893 - val_loss: 2.4993
Epoch 526/1000
26/26 - 1s - loss: 2.0899 - val_loss: 2.4975
Epoch 527/1000
26/26 - 1s - loss: 2.0894 - val_loss: 2.4946
Epoch 528/1000
26/26 - 1s - loss: 2.0866 - val_loss: 2.4934
Epoch 529/1000
26/26 - 1s - loss: 2.0849 - val_loss: 2.4912
Epoch 530/1000
26/26 - 1s - loss: 2.0812 - val_loss: 2.4886
Epoch 00530: val_loss improved from 2.50944 to 2.48860, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-0.model.weights.hdf5
Epoch 531/1000
26/26 - 1s - loss: 2.0777 - val_loss: 2.4872
Epoch 532/1000
26/26 - 1s - loss: 2.0770 - val_loss: 2.4854
Epoch 533/1000
26/26 - 1s - loss: 2.0753 - val_loss: 2.4835
Epoch 534/1000
26/26 - 1s - loss: 2.0744 - val_loss: 2.4808
Epoch 535/1000
26/26 - 1s - loss: 2.0717 - val_loss: 2.4784
Epoch 536/1000
26/26 - 1s - loss: 2.0689 - val_loss: 2.4764
Epoch 537/1000
26/26 - 1s - loss: 2.0673 - val_loss: 2.4745
Epoch 538/1000
26/26 - 1s - loss: 2.0643 - val_loss: 2.4717
Epoch 539/1000
26/26 - 1s - loss: 2.0624 - val_loss: 2.4705
Epoch 540/1000
26/26 - 1s - loss: 2.0623 - val_loss: 2.4688
Epoch 00540: val_loss improved from 2.48860 to 2.46884, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-0.model.weights.hdf5
Epoch 541/1000
26/26 - 1s - loss: 2.0593 - val_loss: 2.4685
Epoch 542/1000
26/26 - 1s - loss: 2.0565 - val_loss: 2.4658
Epoch 543/1000
26/26 - 1s - loss: 2.0550 - val_loss: 2.4634
Epoch 544/1000
26/26 - 1s - loss: 2.0536 - val_loss: 2.4623
Epoch 545/1000
26/26 - 1s - loss: 2.0533 - val_loss: 2.4598
Epoch 546/1000
26/26 - 1s - loss: 2.0498 - val_loss: 2.4574
Epoch 547/1000
26/26 - 1s - loss: 2.0475 - val_loss: 2.4557
Epoch 548/1000
26/26 - 1s - loss: 2.0432 - val_loss: 2.4533
Epoch 549/1000
26/26 - 1s - loss: 2.0447 - val_loss: 2.4515
Epoch 550/1000
26/26 - 1s - loss: 2.0399 - val_loss: 2.4502
Epoch 00550: val_loss improved from 2.46884 to 2.45020, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-0.model.weights.hdf5
Epoch 551/1000
26/26 - 1s - loss: 2.0382 - val_loss: 2.4486
Epoch 552/1000
26/26 - 1s - loss: 2.0370 - val_loss: 2.4467
Epoch 553/1000
26/26 - 1s - loss: 2.0373 - val_loss: 2.4456
Epoch 554/1000
26/26 - 1s - loss: 2.0341 - val_loss: 2.4423
Epoch 555/1000
26/26 - 1s - loss: 2.0324 - val_loss: 2.4388
Epoch 556/1000
26/26 - 1s - loss: 2.0310 - val_loss: 2.4370
Epoch 557/1000
26/26 - 1s - loss: 2.0270 - val_loss: 2.4351
Epoch 558/1000
26/26 - 1s - loss: 2.0251 - val_loss: 2.4327
Epoch 559/1000
26/26 - 1s - loss: 2.0231 - val_loss: 2.4309
Epoch 560/1000
26/26 - 1s - loss: 2.0227 - val_loss: 2.4300
Epoch 00560: val_loss improved from 2.45020 to 2.43000, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-0.model.weights.hdf5
Epoch 561/1000
26/26 - 1s - loss: 2.0196 - val_loss: 2.4268
Epoch 562/1000
26/26 - 1s - loss: 2.0200 - val_loss: 2.4239
Epoch 563/1000
26/26 - 1s - loss: 2.0188 - val_loss: 2.4230
Epoch 564/1000
26/26 - 1s - loss: 2.0168 - val_loss: 2.4218
Epoch 565/1000
26/26 - 1s - loss: 2.0101 - val_loss: 2.4200
Epoch 566/1000
26/26 - 1s - loss: 2.0111 - val_loss: 2.4193
Epoch 567/1000
26/26 - 1s - loss: 2.0099 - val_loss: 2.4178
Epoch 568/1000
26/26 - 1s - loss: 2.0081 - val_loss: 2.4156
Epoch 569/1000
26/26 - 1s - loss: 2.0037 - val_loss: 2.4133
Epoch 570/1000
26/26 - 1s - loss: 2.0026 - val_loss: 2.4106
Epoch 00570: val_loss improved from 2.43000 to 2.41060, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-0.model.weights.hdf5
Epoch 571/1000
26/26 - 1s - loss: 2.0006 - val_loss: 2.4084
Epoch 572/1000
26/26 - 1s - loss: 1.9982 - val_loss: 2.4070
Epoch 573/1000
26/26 - 1s - loss: 1.9980 - val_loss: 2.4051
Epoch 574/1000
26/26 - 1s - loss: 1.9944 - val_loss: 2.4024
Epoch 575/1000
26/26 - 1s - loss: 1.9940 - val_loss: 2.4011
Epoch 576/1000
26/26 - 1s - loss: 1.9945 - val_loss: 2.3994
Epoch 577/1000
26/26 - 1s - loss: 1.9927 - val_loss: 2.3968
Epoch 578/1000
26/26 - 1s - loss: 1.9879 - val_loss: 2.3965
Epoch 579/1000
26/26 - 1s - loss: 1.9876 - val_loss: 2.3949
Epoch 580/1000
26/26 - 1s - loss: 1.9856 - val_loss: 2.3932
Epoch 00580: val_loss improved from 2.41060 to 2.39316, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-0.model.weights.hdf5
Epoch 581/1000
26/26 - 1s - loss: 1.9838 - val_loss: 2.3916
Epoch 582/1000
26/26 - 1s - loss: 1.9825 - val_loss: 2.3893
Epoch 583/1000
26/26 - 1s - loss: 1.9779 - val_loss: 2.3869
Epoch 584/1000
26/26 - 1s - loss: 1.9792 - val_loss: 2.3852
Epoch 585/1000
26/26 - 1s - loss: 1.9737 - val_loss: 2.3829
Epoch 586/1000
26/26 - 1s - loss: 1.9752 - val_loss: 2.3821
Epoch 587/1000
26/26 - 1s - loss: 1.9709 - val_loss: 2.3800
Epoch 588/1000
26/26 - 1s - loss: 1.9681 - val_loss: 2.3785
Epoch 589/1000
26/26 - 1s - loss: 1.9661 - val_loss: 2.3759
Epoch 590/1000
26/26 - 1s - loss: 1.9656 - val_loss: 2.3743
Epoch 00590: val_loss improved from 2.39316 to 2.37432, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-0.model.weights.hdf5
Epoch 591/1000
26/26 - 1s - loss: 1.9632 - val_loss: 2.3708
Epoch 592/1000
26/26 - 1s - loss: 1.9642 - val_loss: 2.3687
Epoch 593/1000
26/26 - 1s - loss: 1.9606 - val_loss: 2.3671
Epoch 594/1000
26/26 - 1s - loss: 1.9598 - val_loss: 2.3664
Epoch 595/1000
26/26 - 1s - loss: 1.9562 - val_loss: 2.3656
Epoch 596/1000
26/26 - 1s - loss: 1.9539 - val_loss: 2.3640
Epoch 597/1000
26/26 - 1s - loss: 1.9543 - val_loss: 2.3608
Epoch 598/1000
26/26 - 1s - loss: 1.9510 - val_loss: 2.3589
Epoch 599/1000
26/26 - 1s - loss: 1.9473 - val_loss: 2.3573
Epoch 600/1000
26/26 - 1s - loss: 1.9478 - val_loss: 2.3560
Epoch 00600: val_loss improved from 2.37432 to 2.35603, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-0.model.weights.hdf5
Epoch 601/1000
26/26 - 1s - loss: 1.9450 - val_loss: 2.3535
Epoch 602/1000
26/26 - 1s - loss: 1.9425 - val_loss: 2.3518
Epoch 603/1000
26/26 - 1s - loss: 1.9405 - val_loss: 2.3509
Epoch 604/1000
26/26 - 1s - loss: 1.9424 - val_loss: 2.3486
Epoch 605/1000
26/26 - 1s - loss: 1.9391 - val_loss: 2.3467
Epoch 606/1000
26/26 - 1s - loss: 1.9362 - val_loss: 2.3453
Epoch 607/1000
26/26 - 1s - loss: 1.9359 - val_loss: 2.3442
Epoch 608/1000
26/26 - 1s - loss: 1.9312 - val_loss: 2.3430
Epoch 609/1000
26/26 - 1s - loss: 1.9319 - val_loss: 2.3399
Epoch 610/1000
26/26 - 1s - loss: 1.9293 - val_loss: 2.3380
Epoch 00610: val_loss improved from 2.35603 to 2.33797, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-0.model.weights.hdf5
Epoch 611/1000
26/26 - 1s - loss: 1.9262 - val_loss: 2.3378
Epoch 612/1000
26/26 - 1s - loss: 1.9239 - val_loss: 2.3351
Epoch 613/1000
26/26 - 1s - loss: 1.9245 - val_loss: 2.3331
Epoch 614/1000
26/26 - 1s - loss: 1.9222 - val_loss: 2.3306
Epoch 615/1000
26/26 - 1s - loss: 1.9200 - val_loss: 2.3295
Epoch 616/1000
26/26 - 1s - loss: 1.9180 - val_loss: 2.3282
Epoch 617/1000
26/26 - 1s - loss: 1.9174 - val_loss: 2.3261
Epoch 618/1000
26/26 - 1s - loss: 1.9170 - val_loss: 2.3248
Epoch 619/1000
26/26 - 1s - loss: 1.9146 - val_loss: 2.3232
Epoch 620/1000
26/26 - 1s - loss: 1.9091 - val_loss: 2.3209
Epoch 00620: val_loss improved from 2.33797 to 2.32088, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-0.model.weights.hdf5
Epoch 621/1000
26/26 - 1s - loss: 1.9090 - val_loss: 2.3190
Epoch 622/1000
26/26 - 1s - loss: 1.9086 - val_loss: 2.3175
Epoch 623/1000
26/26 - 1s - loss: 1.9067 - val_loss: 2.3147
Epoch 624/1000
26/26 - 1s - loss: 1.9054 - val_loss: 2.3133
Epoch 625/1000
26/26 - 1s - loss: 1.9019 - val_loss: 2.3121
Epoch 626/1000
26/26 - 1s - loss: 1.9002 - val_loss: 2.3112
Epoch 627/1000
26/26 - 1s - loss: 1.8976 - val_loss: 2.3096
Epoch 628/1000
26/26 - 1s - loss: 1.8983 - val_loss: 2.3076
Epoch 629/1000
26/26 - 1s - loss: 1.8953 - val_loss: 2.3054
Epoch 630/1000
26/26 - 1s - loss: 1.8947 - val_loss: 2.3029
Epoch 00630: val_loss improved from 2.32088 to 2.30291, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-0.model.weights.hdf5
Epoch 631/1000
26/26 - 1s - loss: 1.8931 - val_loss: 2.3026
Epoch 632/1000
26/26 - 1s - loss: 1.8899 - val_loss: 2.3000
Epoch 633/1000
26/26 - 1s - loss: 1.8897 - val_loss: 2.2987
Epoch 634/1000
26/26 - 1s - loss: 1.8843 - val_loss: 2.2960
Epoch 635/1000
26/26 - 1s - loss: 1.8838 - val_loss: 2.2956
Epoch 636/1000
26/26 - 1s - loss: 1.8820 - val_loss: 2.2930
Epoch 637/1000
26/26 - 1s - loss: 1.8815 - val_loss: 2.2918
Epoch 638/1000
26/26 - 1s - loss: 1.8812 - val_loss: 2.2895
Epoch 639/1000
26/26 - 1s - loss: 1.8772 - val_loss: 2.2884
Epoch 640/1000
26/26 - 1s - loss: 1.8770 - val_loss: 2.2858
Epoch 00640: val_loss improved from 2.30291 to 2.28581, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-0.model.weights.hdf5
Epoch 641/1000
26/26 - 1s - loss: 1.8754 - val_loss: 2.2855
Epoch 642/1000
26/26 - 1s - loss: 1.8724 - val_loss: 2.2832
Epoch 643/1000
26/26 - 1s - loss: 1.8721 - val_loss: 2.2825
Epoch 644/1000
26/26 - 1s - loss: 1.8698 - val_loss: 2.2800
Epoch 645/1000
26/26 - 1s - loss: 1.8669 - val_loss: 2.2776
Epoch 646/1000
26/26 - 1s - loss: 1.8669 - val_loss: 2.2754
Epoch 647/1000
26/26 - 1s - loss: 1.8645 - val_loss: 2.2746
Epoch 648/1000
26/26 - 1s - loss: 1.8613 - val_loss: 2.2733
Epoch 649/1000
26/26 - 1s - loss: 1.8598 - val_loss: 2.2720
Epoch 650/1000
26/26 - 1s - loss: 1.8571 - val_loss: 2.2688
Epoch 00650: val_loss improved from 2.28581 to 2.26885, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-0.model.weights.hdf5
Epoch 651/1000
26/26 - 1s - loss: 1.8582 - val_loss: 2.2674
Epoch 652/1000
26/26 - 1s - loss: 1.8566 - val_loss: 2.2654
Epoch 653/1000
26/26 - 1s - loss: 1.8538 - val_loss: 2.2637
Epoch 654/1000
26/26 - 1s - loss: 1.8528 - val_loss: 2.2616
Epoch 655/1000
26/26 - 1s - loss: 1.8494 - val_loss: 2.2599
Epoch 656/1000
26/26 - 1s - loss: 1.8491 - val_loss: 2.2586
Epoch 657/1000
26/26 - 1s - loss: 1.8478 - val_loss: 2.2572
Epoch 658/1000
26/26 - 1s - loss: 1.8458 - val_loss: 2.2558
Epoch 659/1000
26/26 - 1s - loss: 1.8452 - val_loss: 2.2552
Epoch 660/1000
26/26 - 1s - loss: 1.8409 - val_loss: 2.2532
Epoch 00660: val_loss improved from 2.26885 to 2.25318, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-0.model.weights.hdf5
Epoch 661/1000
26/26 - 1s - loss: 1.8408 - val_loss: 2.2517
Epoch 662/1000
26/26 - 1s - loss: 1.8378 - val_loss: 2.2499
Epoch 663/1000
26/26 - 1s - loss: 1.8375 - val_loss: 2.2489
Epoch 664/1000
26/26 - 1s - loss: 1.8380 - val_loss: 2.2454
Epoch 665/1000
26/26 - 1s - loss: 1.8327 - val_loss: 2.2448
Epoch 666/1000
26/26 - 1s - loss: 1.8307 - val_loss: 2.2434
Epoch 667/1000
26/26 - 1s - loss: 1.8299 - val_loss: 2.2417
Epoch 668/1000
26/26 - 1s - loss: 1.8288 - val_loss: 2.2409
Epoch 669/1000
26/26 - 1s - loss: 1.8277 - val_loss: 2.2386
Epoch 670/1000
26/26 - 1s - loss: 1.8260 - val_loss: 2.2370
Epoch 00670: val_loss improved from 2.25318 to 2.23699, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-0.model.weights.hdf5
Epoch 671/1000
26/26 - 1s - loss: 1.8239 - val_loss: 2.2348
Epoch 672/1000
26/26 - 1s - loss: 1.8223 - val_loss: 2.2317
Epoch 673/1000
26/26 - 1s - loss: 1.8197 - val_loss: 2.2308
Epoch 674/1000
26/26 - 1s - loss: 1.8197 - val_loss: 2.2285
Epoch 675/1000
26/26 - 1s - loss: 1.8176 - val_loss: 2.2263
Epoch 676/1000
26/26 - 1s - loss: 1.8148 - val_loss: 2.2239
Epoch 677/1000
26/26 - 1s - loss: 1.8135 - val_loss: 2.2222
Epoch 678/1000
26/26 - 1s - loss: 1.8131 - val_loss: 2.2212
Epoch 679/1000
26/26 - 1s - loss: 1.8106 - val_loss: 2.2224
Epoch 680/1000
26/26 - 1s - loss: 1.8107 - val_loss: 2.2201
Epoch 00680: val_loss improved from 2.23699 to 2.22010, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-0.model.weights.hdf5
Epoch 681/1000
26/26 - 1s - loss: 1.8084 - val_loss: 2.2175
Epoch 682/1000
26/26 - 1s - loss: 1.8054 - val_loss: 2.2157
Epoch 683/1000
26/26 - 1s - loss: 1.8061 - val_loss: 2.2146
Epoch 684/1000
26/26 - 1s - loss: 1.8028 - val_loss: 2.2136
Epoch 685/1000
26/26 - 1s - loss: 1.8002 - val_loss: 2.2127
Epoch 686/1000
26/26 - 1s - loss: 1.7989 - val_loss: 2.2116
Epoch 687/1000
26/26 - 1s - loss: 1.8003 - val_loss: 2.2108
Epoch 688/1000
26/26 - 1s - loss: 1.7948 - val_loss: 2.2080
Epoch 689/1000
26/26 - 1s - loss: 1.7934 - val_loss: 2.2070
Epoch 690/1000
26/26 - 1s - loss: 1.7935 - val_loss: 2.2054
Epoch 00690: val_loss improved from 2.22010 to 2.20545, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-0.model.weights.hdf5
Epoch 691/1000
26/26 - 1s - loss: 1.7916 - val_loss: 2.2041
Epoch 692/1000
26/26 - 1s - loss: 1.7926 - val_loss: 2.2014
Epoch 693/1000
26/26 - 1s - loss: 1.7888 - val_loss: 2.2003
Epoch 694/1000
26/26 - 1s - loss: 1.7821 - val_loss: 2.1987
Epoch 695/1000
26/26 - 1s - loss: 1.7838 - val_loss: 2.1959
Epoch 696/1000
26/26 - 1s - loss: 1.7816 - val_loss: 2.1953
Epoch 697/1000
26/26 - 1s - loss: 1.7807 - val_loss: 2.1931
Epoch 698/1000
26/26 - 1s - loss: 1.7788 - val_loss: 2.1919
Epoch 699/1000
26/26 - 1s - loss: 1.7782 - val_loss: 2.1902
Epoch 700/1000
26/26 - 1s - loss: 1.7757 - val_loss: 2.1886
Epoch 00700: val_loss improved from 2.20545 to 2.18862, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-0.model.weights.hdf5
Epoch 701/1000
26/26 - 1s - loss: 1.7756 - val_loss: 2.1878
Epoch 702/1000
26/26 - 1s - loss: 1.7746 - val_loss: 2.1857
Epoch 703/1000
26/26 - 1s - loss: 1.7725 - val_loss: 2.1826
Epoch 704/1000
26/26 - 1s - loss: 1.7708 - val_loss: 2.1817
Epoch 705/1000
26/26 - 1s - loss: 1.7701 - val_loss: 2.1800
Epoch 706/1000
26/26 - 1s - loss: 1.7651 - val_loss: 2.1792
Epoch 707/1000
26/26 - 1s - loss: 1.7646 - val_loss: 2.1787
Epoch 708/1000
26/26 - 1s - loss: 1.7647 - val_loss: 2.1763
Epoch 709/1000
26/26 - 1s - loss: 1.7633 - val_loss: 2.1749
Epoch 710/1000
26/26 - 1s - loss: 1.7617 - val_loss: 2.1728
Epoch 00710: val_loss improved from 2.18862 to 2.17278, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-0.model.weights.hdf5
Epoch 711/1000
26/26 - 1s - loss: 1.7615 - val_loss: 2.1709
Epoch 712/1000
26/26 - 1s - loss: 1.7568 - val_loss: 2.1693
Epoch 713/1000
26/26 - 1s - loss: 1.7542 - val_loss: 2.1688
Epoch 714/1000
26/26 - 1s - loss: 1.7552 - val_loss: 2.1668
Epoch 715/1000
26/26 - 1s - loss: 1.7525 - val_loss: 2.1655
Epoch 716/1000
26/26 - 1s - loss: 1.7509 - val_loss: 2.1637
Epoch 717/1000
26/26 - 1s - loss: 1.7505 - val_loss: 2.1603
Epoch 718/1000
26/26 - 1s - loss: 1.7478 - val_loss: 2.1596
Epoch 719/1000
26/26 - 1s - loss: 1.7479 - val_loss: 2.1580
Epoch 720/1000
26/26 - 1s - loss: 1.7439 - val_loss: 2.1567
Epoch 00720: val_loss improved from 2.17278 to 2.15667, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-0.model.weights.hdf5
Epoch 721/1000
26/26 - 1s - loss: 1.7440 - val_loss: 2.1550
Epoch 722/1000
26/26 - 1s - loss: 1.7423 - val_loss: 2.1538
Epoch 723/1000
26/26 - 1s - loss: 1.7420 - val_loss: 2.1521
Epoch 724/1000
26/26 - 1s - loss: 1.7398 - val_loss: 2.1511
Epoch 725/1000
26/26 - 1s - loss: 1.7383 - val_loss: 2.1501
Epoch 726/1000
26/26 - 1s - loss: 1.7366 - val_loss: 2.1486
Epoch 727/1000
26/26 - 1s - loss: 1.7332 - val_loss: 2.1473
Epoch 728/1000
26/26 - 1s - loss: 1.7339 - val_loss: 2.1445
Epoch 729/1000
26/26 - 1s - loss: 1.7328 - val_loss: 2.1455
Epoch 730/1000
26/26 - 1s - loss: 1.7316 - val_loss: 2.1432
Epoch 00730: val_loss improved from 2.15667 to 2.14317, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-0.model.weights.hdf5
Epoch 731/1000
26/26 - 1s - loss: 1.7287 - val_loss: 2.1414
Epoch 732/1000
26/26 - 1s - loss: 1.7265 - val_loss: 2.1405
Epoch 733/1000
26/26 - 1s - loss: 1.7266 - val_loss: 2.1384
Epoch 734/1000
26/26 - 1s - loss: 1.7236 - val_loss: 2.1368
Epoch 735/1000
26/26 - 1s - loss: 1.7217 - val_loss: 2.1347
Epoch 736/1000
26/26 - 1s - loss: 1.7221 - val_loss: 2.1339
Epoch 737/1000
26/26 - 1s - loss: 1.7199 - val_loss: 2.1331
Epoch 738/1000
26/26 - 1s - loss: 1.7178 - val_loss: 2.1310
Epoch 739/1000
26/26 - 1s - loss: 1.7171 - val_loss: 2.1293
Epoch 740/1000
26/26 - 1s - loss: 1.7137 - val_loss: 2.1278
Epoch 00740: val_loss improved from 2.14317 to 2.12780, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-0.model.weights.hdf5
Epoch 741/1000
26/26 - 1s - loss: 1.7116 - val_loss: 2.1264
Epoch 742/1000
26/26 - 1s - loss: 1.7099 - val_loss: 2.1254
Epoch 743/1000
26/26 - 1s - loss: 1.7099 - val_loss: 2.1238
Epoch 744/1000
26/26 - 1s - loss: 1.7084 - val_loss: 2.1216
Epoch 745/1000
26/26 - 1s - loss: 1.7053 - val_loss: 2.1216
Epoch 746/1000
26/26 - 1s - loss: 1.7074 - val_loss: 2.1200
Epoch 747/1000
26/26 - 1s - loss: 1.7052 - val_loss: 2.1188
Epoch 748/1000
26/26 - 1s - loss: 1.7022 - val_loss: 2.1178
Epoch 749/1000
26/26 - 1s - loss: 1.7013 - val_loss: 2.1155
Epoch 750/1000
26/26 - 1s - loss: 1.7013 - val_loss: 2.1135
Epoch 00750: val_loss improved from 2.12780 to 2.11345, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-0.model.weights.hdf5
Epoch 751/1000
26/26 - 1s - loss: 1.6998 - val_loss: 2.1129
Epoch 752/1000
26/26 - 1s - loss: 1.6971 - val_loss: 2.1098
Epoch 753/1000
26/26 - 1s - loss: 1.6954 - val_loss: 2.1076
Epoch 754/1000
26/26 - 1s - loss: 1.6927 - val_loss: 2.1074
Epoch 755/1000
26/26 - 1s - loss: 1.6910 - val_loss: 2.1065
Epoch 756/1000
26/26 - 1s - loss: 1.6906 - val_loss: 2.1049
Epoch 757/1000
26/26 - 1s - loss: 1.6898 - val_loss: 2.1024
Epoch 758/1000
26/26 - 1s - loss: 1.6875 - val_loss: 2.1016
Epoch 759/1000
26/26 - 1s - loss: 1.6873 - val_loss: 2.1001
Epoch 760/1000
26/26 - 1s - loss: 1.6853 - val_loss: 2.0981
Epoch 00760: val_loss improved from 2.11345 to 2.09810, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-0.model.weights.hdf5
Epoch 761/1000
26/26 - 1s - loss: 1.6825 - val_loss: 2.0976
Epoch 762/1000
26/26 - 1s - loss: 1.6822 - val_loss: 2.0952
Epoch 763/1000
26/26 - 1s - loss: 1.6824 - val_loss: 2.0931
Epoch 764/1000
26/26 - 1s - loss: 1.6804 - val_loss: 2.0912
Epoch 765/1000
26/26 - 1s - loss: 1.6787 - val_loss: 2.0893
Epoch 766/1000
26/26 - 1s - loss: 1.6764 - val_loss: 2.0888
Epoch 767/1000
26/26 - 1s - loss: 1.6760 - val_loss: 2.0882
Epoch 768/1000
26/26 - 1s - loss: 1.6745 - val_loss: 2.0855
Epoch 769/1000
26/26 - 1s - loss: 1.6716 - val_loss: 2.0844
Epoch 770/1000
26/26 - 1s - loss: 1.6726 - val_loss: 2.0830
Epoch 00770: val_loss improved from 2.09810 to 2.08302, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-0.model.weights.hdf5
Epoch 771/1000
26/26 - 1s - loss: 1.6679 - val_loss: 2.0805
Epoch 772/1000
26/26 - 1s - loss: 1.6679 - val_loss: 2.0790
Epoch 773/1000
26/26 - 1s - loss: 1.6673 - val_loss: 2.0783
Epoch 774/1000
26/26 - 1s - loss: 1.6644 - val_loss: 2.0770
Epoch 775/1000
26/26 - 1s - loss: 1.6628 - val_loss: 2.0758
Epoch 776/1000
26/26 - 1s - loss: 1.6637 - val_loss: 2.0751
Epoch 777/1000
26/26 - 1s - loss: 1.6597 - val_loss: 2.0730
Epoch 778/1000
26/26 - 1s - loss: 1.6586 - val_loss: 2.0710
Epoch 779/1000
26/26 - 1s - loss: 1.6567 - val_loss: 2.0703
Epoch 780/1000
26/26 - 1s - loss: 1.6561 - val_loss: 2.0695
Epoch 00780: val_loss improved from 2.08302 to 2.06947, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-0.model.weights.hdf5
Epoch 781/1000
26/26 - 1s - loss: 1.6545 - val_loss: 2.0676
Epoch 782/1000
26/26 - 1s - loss: 1.6531 - val_loss: 2.0656
Epoch 783/1000
26/26 - 1s - loss: 1.6531 - val_loss: 2.0654
Epoch 784/1000
26/26 - 1s - loss: 1.6529 - val_loss: 2.0644
Epoch 785/1000
26/26 - 1s - loss: 1.6490 - val_loss: 2.0633
Epoch 786/1000
26/26 - 1s - loss: 1.6475 - val_loss: 2.0611
Epoch 787/1000
26/26 - 1s - loss: 1.6467 - val_loss: 2.0601
Epoch 788/1000
26/26 - 1s - loss: 1.6452 - val_loss: 2.0583
Epoch 789/1000
26/26 - 1s - loss: 1.6427 - val_loss: 2.0570
Epoch 790/1000
26/26 - 1s - loss: 1.6423 - val_loss: 2.0555
Epoch 00790: val_loss improved from 2.06947 to 2.05552, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-0.model.weights.hdf5
Epoch 791/1000
26/26 - 1s - loss: 1.6414 - val_loss: 2.0543
Epoch 792/1000
26/26 - 1s - loss: 1.6374 - val_loss: 2.0533
Epoch 793/1000
26/26 - 1s - loss: 1.6384 - val_loss: 2.0510
Epoch 794/1000
26/26 - 1s - loss: 1.6370 - val_loss: 2.0499
Epoch 795/1000
26/26 - 1s - loss: 1.6370 - val_loss: 2.0478
Epoch 796/1000
26/26 - 1s - loss: 1.6338 - val_loss: 2.0471
Epoch 797/1000
26/26 - 1s - loss: 1.6323 - val_loss: 2.0465
Epoch 798/1000
26/26 - 1s - loss: 1.6312 - val_loss: 2.0441
Epoch 799/1000
26/26 - 1s - loss: 1.6316 - val_loss: 2.0425
Epoch 800/1000
26/26 - 1s - loss: 1.6294 - val_loss: 2.0417
Epoch 00800: val_loss improved from 2.05552 to 2.04175, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-0.model.weights.hdf5
Epoch 801/1000
26/26 - 1s - loss: 1.6256 - val_loss: 2.0403
Epoch 802/1000
26/26 - 1s - loss: 1.6263 - val_loss: 2.0390
Epoch 803/1000
26/26 - 1s - loss: 1.6226 - val_loss: 2.0382
Epoch 804/1000
26/26 - 1s - loss: 1.6236 - val_loss: 2.0364
Epoch 805/1000
26/26 - 1s - loss: 1.6226 - val_loss: 2.0353
Epoch 806/1000
26/26 - 1s - loss: 1.6197 - val_loss: 2.0346
Epoch 807/1000
26/26 - 1s - loss: 1.6193 - val_loss: 2.0332
Epoch 808/1000
26/26 - 1s - loss: 1.6180 - val_loss: 2.0325
Epoch 809/1000
26/26 - 1s - loss: 1.6166 - val_loss: 2.0307
Epoch 810/1000
26/26 - 1s - loss: 1.6146 - val_loss: 2.0282
Epoch 00810: val_loss improved from 2.04175 to 2.02818, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-0.model.weights.hdf5
Epoch 811/1000
26/26 - 1s - loss: 1.6140 - val_loss: 2.0263
Epoch 812/1000
26/26 - 1s - loss: 1.6127 - val_loss: 2.0246
Epoch 813/1000
26/26 - 1s - loss: 1.6108 - val_loss: 2.0233
Epoch 814/1000
26/26 - 1s - loss: 1.6102 - val_loss: 2.0217
Epoch 815/1000
26/26 - 1s - loss: 1.6066 - val_loss: 2.0204
Epoch 816/1000
26/26 - 1s - loss: 1.6054 - val_loss: 2.0180
Epoch 817/1000
26/26 - 1s - loss: 1.6055 - val_loss: 2.0178
Epoch 818/1000
26/26 - 1s - loss: 1.6043 - val_loss: 2.0173
Epoch 819/1000
26/26 - 1s - loss: 1.6044 - val_loss: 2.0149
Epoch 820/1000
26/26 - 1s - loss: 1.6008 - val_loss: 2.0131
Epoch 00820: val_loss improved from 2.02818 to 2.01308, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-0.model.weights.hdf5
Epoch 821/1000
26/26 - 1s - loss: 1.6005 - val_loss: 2.0124
Epoch 822/1000
26/26 - 1s - loss: 1.5983 - val_loss: 2.0110
Epoch 823/1000
26/26 - 1s - loss: 1.5975 - val_loss: 2.0095
Epoch 824/1000
26/26 - 1s - loss: 1.5970 - val_loss: 2.0068
Epoch 825/1000
26/26 - 1s - loss: 1.5941 - val_loss: 2.0078
Epoch 826/1000
26/26 - 1s - loss: 1.5923 - val_loss: 2.0066
Epoch 827/1000
26/26 - 1s - loss: 1.5906 - val_loss: 2.0042
Epoch 828/1000
26/26 - 1s - loss: 1.5908 - val_loss: 2.0023
Epoch 829/1000
26/26 - 1s - loss: 1.5885 - val_loss: 2.0034
Epoch 830/1000
26/26 - 1s - loss: 1.5881 - val_loss: 2.0015
Epoch 00830: val_loss improved from 2.01308 to 2.00147, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-0.model.weights.hdf5
Epoch 831/1000
26/26 - 1s - loss: 1.5859 - val_loss: 2.0010
Epoch 832/1000
26/26 - 1s - loss: 1.5847 - val_loss: 1.9991
Epoch 833/1000
26/26 - 1s - loss: 1.5820 - val_loss: 1.9972
Epoch 834/1000
26/26 - 1s - loss: 1.5823 - val_loss: 1.9958
Epoch 835/1000
26/26 - 1s - loss: 1.5789 - val_loss: 1.9940
Epoch 836/1000
26/26 - 1s - loss: 1.5798 - val_loss: 1.9933
Epoch 837/1000
26/26 - 1s - loss: 1.5779 - val_loss: 1.9927
Epoch 838/1000
26/26 - 1s - loss: 1.5773 - val_loss: 1.9914
Epoch 839/1000
26/26 - 1s - loss: 1.5749 - val_loss: 1.9905
Epoch 840/1000
26/26 - 1s - loss: 1.5731 - val_loss: 1.9887
Epoch 00840: val_loss improved from 2.00147 to 1.98871, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-0.model.weights.hdf5
Epoch 841/1000
26/26 - 1s - loss: 1.5726 - val_loss: 1.9872
Epoch 842/1000
26/26 - 1s - loss: 1.5699 - val_loss: 1.9856
Epoch 843/1000
26/26 - 1s - loss: 1.5707 - val_loss: 1.9845
Epoch 844/1000
26/26 - 1s - loss: 1.5684 - val_loss: 1.9832
Epoch 845/1000
26/26 - 1s - loss: 1.5669 - val_loss: 1.9808
Epoch 846/1000
26/26 - 1s - loss: 1.5655 - val_loss: 1.9798
Epoch 847/1000
26/26 - 1s - loss: 1.5662 - val_loss: 1.9798
Epoch 848/1000
26/26 - 1s - loss: 1.5640 - val_loss: 1.9782
Epoch 849/1000
26/26 - 1s - loss: 1.5630 - val_loss: 1.9762
Epoch 850/1000
26/26 - 1s - loss: 1.5613 - val_loss: 1.9757
Epoch 00850: val_loss improved from 1.98871 to 1.97571, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-0.model.weights.hdf5
Epoch 851/1000
26/26 - 1s - loss: 1.5590 - val_loss: 1.9757
Epoch 852/1000
26/26 - 1s - loss: 1.5603 - val_loss: 1.9739
Epoch 853/1000
26/26 - 1s - loss: 1.5576 - val_loss: 1.9710
Epoch 854/1000
26/26 - 1s - loss: 1.5585 - val_loss: 1.9694
Epoch 855/1000
26/26 - 1s - loss: 1.5540 - val_loss: 1.9684
Epoch 856/1000
26/26 - 1s - loss: 1.5553 - val_loss: 1.9673
Epoch 857/1000
26/26 - 1s - loss: 1.5534 - val_loss: 1.9663
Epoch 858/1000
26/26 - 1s - loss: 1.5523 - val_loss: 1.9646
Epoch 859/1000
26/26 - 1s - loss: 1.5493 - val_loss: 1.9634
Epoch 860/1000
26/26 - 1s - loss: 1.5473 - val_loss: 1.9633
Epoch 00860: val_loss improved from 1.97571 to 1.96328, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-0.model.weights.hdf5
Epoch 861/1000
26/26 - 1s - loss: 1.5501 - val_loss: 1.9614
Epoch 862/1000
26/26 - 1s - loss: 1.5448 - val_loss: 1.9616
Epoch 863/1000
26/26 - 1s - loss: 1.5435 - val_loss: 1.9600
Epoch 864/1000
26/26 - 1s - loss: 1.5434 - val_loss: 1.9584
Epoch 865/1000
26/26 - 1s - loss: 1.5406 - val_loss: 1.9574
Epoch 866/1000
26/26 - 1s - loss: 1.5418 - val_loss: 1.9560
Epoch 867/1000
26/26 - 1s - loss: 1.5404 - val_loss: 1.9549
Epoch 868/1000
26/26 - 1s - loss: 1.5377 - val_loss: 1.9523
Epoch 869/1000
26/26 - 1s - loss: 1.5350 - val_loss: 1.9512
Epoch 870/1000
26/26 - 1s - loss: 1.5345 - val_loss: 1.9489
Epoch 00870: val_loss improved from 1.96328 to 1.94891, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-0.model.weights.hdf5
Epoch 871/1000
26/26 - 1s - loss: 1.5349 - val_loss: 1.9482
Epoch 872/1000
26/26 - 1s - loss: 1.5316 - val_loss: 1.9464
Epoch 873/1000
26/26 - 1s - loss: 1.5317 - val_loss: 1.9462
Epoch 874/1000
26/26 - 1s - loss: 1.5318 - val_loss: 1.9460
Epoch 875/1000
26/26 - 1s - loss: 1.5290 - val_loss: 1.9429
Epoch 876/1000
26/26 - 1s - loss: 1.5278 - val_loss: 1.9415
Epoch 877/1000
26/26 - 1s - loss: 1.5274 - val_loss: 1.9423
Epoch 878/1000
26/26 - 1s - loss: 1.5240 - val_loss: 1.9410
Epoch 879/1000
26/26 - 1s - loss: 1.5229 - val_loss: 1.9394
Epoch 880/1000
26/26 - 1s - loss: 1.5236 - val_loss: 1.9385
Epoch 00880: val_loss improved from 1.94891 to 1.93850, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-0.model.weights.hdf5
Epoch 881/1000
26/26 - 1s - loss: 1.5221 - val_loss: 1.9369
Epoch 882/1000
26/26 - 1s - loss: 1.5191 - val_loss: 1.9352
Epoch 883/1000
26/26 - 1s - loss: 1.5216 - val_loss: 1.9342
Epoch 884/1000
26/26 - 1s - loss: 1.5190 - val_loss: 1.9315
Epoch 885/1000
26/26 - 1s - loss: 1.5153 - val_loss: 1.9302
Epoch 886/1000
26/26 - 1s - loss: 1.5141 - val_loss: 1.9300
Epoch 887/1000
26/26 - 1s - loss: 1.5132 - val_loss: 1.9279
Epoch 888/1000
26/26 - 1s - loss: 1.5119 - val_loss: 1.9265
Epoch 889/1000
26/26 - 1s - loss: 1.5120 - val_loss: 1.9260
Epoch 890/1000
26/26 - 1s - loss: 1.5102 - val_loss: 1.9245
Epoch 00890: val_loss improved from 1.93850 to 1.92445, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-0.model.weights.hdf5
Epoch 891/1000
26/26 - 1s - loss: 1.5086 - val_loss: 1.9243
Epoch 892/1000
26/26 - 1s - loss: 1.5069 - val_loss: 1.9222
Epoch 893/1000
26/26 - 1s - loss: 1.5070 - val_loss: 1.9216
Epoch 894/1000
26/26 - 1s - loss: 1.5051 - val_loss: 1.9202
Epoch 895/1000
26/26 - 1s - loss: 1.5033 - val_loss: 1.9192
Epoch 896/1000
26/26 - 1s - loss: 1.5029 - val_loss: 1.9166
Epoch 897/1000
26/26 - 1s - loss: 1.5010 - val_loss: 1.9151
Epoch 898/1000
26/26 - 1s - loss: 1.5005 - val_loss: 1.9129
Epoch 899/1000
26/26 - 1s - loss: 1.4992 - val_loss: 1.9125
Epoch 900/1000
26/26 - 1s - loss: 1.5004 - val_loss: 1.9122
Epoch 00900: val_loss improved from 1.92445 to 1.91219, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-0.model.weights.hdf5
Epoch 901/1000
26/26 - 1s - loss: 1.4967 - val_loss: 1.9102
Epoch 902/1000
26/26 - 1s - loss: 1.4950 - val_loss: 1.9094
Epoch 903/1000
26/26 - 1s - loss: 1.4951 - val_loss: 1.9078
Epoch 904/1000
26/26 - 1s - loss: 1.4944 - val_loss: 1.9069
Epoch 905/1000
26/26 - 1s - loss: 1.4907 - val_loss: 1.9054
Epoch 906/1000
26/26 - 1s - loss: 1.4903 - val_loss: 1.9053
Epoch 907/1000
26/26 - 1s - loss: 1.4882 - val_loss: 1.9040
Epoch 908/1000
26/26 - 1s - loss: 1.4880 - val_loss: 1.9033
Epoch 909/1000
26/26 - 1s - loss: 1.4894 - val_loss: 1.9024
Epoch 910/1000
26/26 - 1s - loss: 1.4868 - val_loss: 1.9016
Epoch 00910: val_loss improved from 1.91219 to 1.90162, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-0.model.weights.hdf5
Epoch 911/1000
26/26 - 1s - loss: 1.4864 - val_loss: 1.9012
Epoch 912/1000
26/26 - 1s - loss: 1.4828 - val_loss: 1.9001
Epoch 913/1000
26/26 - 1s - loss: 1.4834 - val_loss: 1.8979
Epoch 914/1000
26/26 - 1s - loss: 1.4823 - val_loss: 1.8961
Epoch 915/1000
26/26 - 1s - loss: 1.4813 - val_loss: 1.8952
Epoch 916/1000
26/26 - 1s - loss: 1.4803 - val_loss: 1.8937
Epoch 917/1000
26/26 - 1s - loss: 1.4780 - val_loss: 1.8931
Epoch 918/1000
26/26 - 1s - loss: 1.4765 - val_loss: 1.8914
Epoch 919/1000
26/26 - 1s - loss: 1.4758 - val_loss: 1.8904
Epoch 920/1000
26/26 - 1s - loss: 1.4742 - val_loss: 1.8886
Epoch 00920: val_loss improved from 1.90162 to 1.88860, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-0.model.weights.hdf5
Epoch 921/1000
26/26 - 1s - loss: 1.4732 - val_loss: 1.8875
Epoch 922/1000
26/26 - 1s - loss: 1.4718 - val_loss: 1.8863
Epoch 923/1000
26/26 - 1s - loss: 1.4698 - val_loss: 1.8855
Epoch 924/1000
26/26 - 1s - loss: 1.4672 - val_loss: 1.8838
Epoch 925/1000
26/26 - 1s - loss: 1.4688 - val_loss: 1.8821
Epoch 926/1000
26/26 - 1s - loss: 1.4650 - val_loss: 1.8817
Epoch 927/1000
26/26 - 1s - loss: 1.4656 - val_loss: 1.8806
Epoch 928/1000
26/26 - 1s - loss: 1.4657 - val_loss: 1.8780
Epoch 929/1000
26/26 - 1s - loss: 1.4623 - val_loss: 1.8777
Epoch 930/1000
26/26 - 1s - loss: 1.4614 - val_loss: 1.8771
Epoch 00930: val_loss improved from 1.88860 to 1.87714, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-0.model.weights.hdf5
Epoch 931/1000
26/26 - 1s - loss: 1.4622 - val_loss: 1.8763
Epoch 932/1000
26/26 - 1s - loss: 1.4580 - val_loss: 1.8746
Epoch 933/1000
26/26 - 1s - loss: 1.4575 - val_loss: 1.8725
Epoch 934/1000
26/26 - 1s - loss: 1.4573 - val_loss: 1.8718
Epoch 935/1000
26/26 - 1s - loss: 1.4540 - val_loss: 1.8717
Epoch 936/1000
26/26 - 1s - loss: 1.4556 - val_loss: 1.8706
Epoch 937/1000
26/26 - 1s - loss: 1.4515 - val_loss: 1.8693
Epoch 938/1000
26/26 - 1s - loss: 1.4527 - val_loss: 1.8670
Epoch 939/1000
26/26 - 1s - loss: 1.4514 - val_loss: 1.8656
Epoch 940/1000
26/26 - 1s - loss: 1.4505 - val_loss: 1.8652
Epoch 00940: val_loss improved from 1.87714 to 1.86523, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-0.model.weights.hdf5
Epoch 941/1000
26/26 - 1s - loss: 1.4490 - val_loss: 1.8640
Epoch 942/1000
26/26 - 1s - loss: 1.4474 - val_loss: 1.8641
Epoch 943/1000
26/26 - 1s - loss: 1.4463 - val_loss: 1.8645
Epoch 944/1000
26/26 - 1s - loss: 1.4468 - val_loss: 1.8608
Epoch 945/1000
26/26 - 1s - loss: 1.4459 - val_loss: 1.8611
Epoch 946/1000
26/26 - 1s - loss: 1.4433 - val_loss: 1.8599
Epoch 947/1000
26/26 - 1s - loss: 1.4442 - val_loss: 1.8589
Epoch 948/1000
26/26 - 1s - loss: 1.4408 - val_loss: 1.8565
Epoch 949/1000
26/26 - 1s - loss: 1.4400 - val_loss: 1.8554
Epoch 950/1000
26/26 - 1s - loss: 1.4387 - val_loss: 1.8541
Epoch 00950: val_loss improved from 1.86523 to 1.85407, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-0.model.weights.hdf5
Epoch 951/1000
26/26 - 1s - loss: 1.4370 - val_loss: 1.8517
Epoch 952/1000
26/26 - 1s - loss: 1.4362 - val_loss: 1.8511
Epoch 953/1000
26/26 - 1s - loss: 1.4338 - val_loss: 1.8503
Epoch 954/1000
26/26 - 1s - loss: 1.4355 - val_loss: 1.8501
Epoch 955/1000
26/26 - 1s - loss: 1.4333 - val_loss: 1.8503
Epoch 956/1000
26/26 - 1s - loss: 1.4297 - val_loss: 1.8481
Epoch 957/1000
26/26 - 1s - loss: 1.4301 - val_loss: 1.8474
Epoch 958/1000
26/26 - 1s - loss: 1.4302 - val_loss: 1.8449
Epoch 959/1000
26/26 - 1s - loss: 1.4297 - val_loss: 1.8428
Epoch 960/1000
26/26 - 1s - loss: 1.4248 - val_loss: 1.8427
Epoch 00960: val_loss improved from 1.85407 to 1.84270, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-0.model.weights.hdf5
Epoch 961/1000
26/26 - 1s - loss: 1.4257 - val_loss: 1.8416
Epoch 962/1000
26/26 - 1s - loss: 1.4251 - val_loss: 1.8411
Epoch 963/1000
26/26 - 1s - loss: 1.4236 - val_loss: 1.8401
Epoch 964/1000
26/26 - 1s - loss: 1.4245 - val_loss: 1.8391
Epoch 965/1000
26/26 - 1s - loss: 1.4208 - val_loss: 1.8381
Epoch 966/1000
26/26 - 1s - loss: 1.4191 - val_loss: 1.8375
Epoch 967/1000
26/26 - 1s - loss: 1.4186 - val_loss: 1.8362
Epoch 968/1000
26/26 - 1s - loss: 1.4183 - val_loss: 1.8334
Epoch 969/1000
26/26 - 1s - loss: 1.4173 - val_loss: 1.8325
Epoch 970/1000
26/26 - 1s - loss: 1.4139 - val_loss: 1.8322
Epoch 00970: val_loss improved from 1.84270 to 1.83217, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-0.model.weights.hdf5
Epoch 971/1000
26/26 - 1s - loss: 1.4150 - val_loss: 1.8316
Epoch 972/1000
26/26 - 1s - loss: 1.4159 - val_loss: 1.8302
Epoch 973/1000
26/26 - 1s - loss: 1.4131 - val_loss: 1.8290
Epoch 974/1000
26/26 - 1s - loss: 1.4135 - val_loss: 1.8271
Epoch 975/1000
26/26 - 1s - loss: 1.4111 - val_loss: 1.8264
Epoch 976/1000
26/26 - 1s - loss: 1.4096 - val_loss: 1.8255
Epoch 977/1000
26/26 - 1s - loss: 1.4081 - val_loss: 1.8248
Epoch 978/1000
26/26 - 1s - loss: 1.4064 - val_loss: 1.8233
Epoch 979/1000
26/26 - 1s - loss: 1.4054 - val_loss: 1.8220
Epoch 980/1000
26/26 - 1s - loss: 1.4051 - val_loss: 1.8217
Epoch 00980: val_loss improved from 1.83217 to 1.82168, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-0.model.weights.hdf5
Epoch 981/1000
26/26 - 1s - loss: 1.4039 - val_loss: 1.8198
Epoch 982/1000
26/26 - 1s - loss: 1.4020 - val_loss: 1.8199
Epoch 983/1000
26/26 - 1s - loss: 1.4011 - val_loss: 1.8171
Epoch 984/1000
26/26 - 1s - loss: 1.4006 - val_loss: 1.8152
Epoch 985/1000
26/26 - 1s - loss: 1.3996 - val_loss: 1.8144
Epoch 986/1000
26/26 - 1s - loss: 1.3987 - val_loss: 1.8139
Epoch 987/1000
26/26 - 1s - loss: 1.3984 - val_loss: 1.8120
Epoch 988/1000
26/26 - 1s - loss: 1.3976 - val_loss: 1.8116
Epoch 989/1000
26/26 - 1s - loss: 1.3970 - val_loss: 1.8106
Epoch 990/1000
26/26 - 1s - loss: 1.3931 - val_loss: 1.8088
Epoch 00990: val_loss improved from 1.82168 to 1.80877, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-0.model.weights.hdf5
Epoch 991/1000
26/26 - 1s - loss: 1.3928 - val_loss: 1.8094
Epoch 992/1000
26/26 - 1s - loss: 1.3914 - val_loss: 1.8068
Epoch 993/1000
26/26 - 1s - loss: 1.3918 - val_loss: 1.8054
Epoch 994/1000
26/26 - 1s - loss: 1.3898 - val_loss: 1.8043
Epoch 995/1000
26/26 - 1s - loss: 1.3887 - val_loss: 1.8028
Epoch 996/1000
26/26 - 1s - loss: 1.3888 - val_loss: 1.8024
Epoch 997/1000
26/26 - 1s - loss: 1.3850 - val_loss: 1.8019
Epoch 998/1000
26/26 - 1s - loss: 1.3845 - val_loss: 1.8007
Epoch 999/1000
26/26 - 1s - loss: 1.3850 - val_loss: 1.8000
Epoch 1000/1000
26/26 - 1s - loss: 1.3830 - val_loss: 1.7994
Epoch 01000: val_loss improved from 1.80877 to 1.79938, saving model to /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_single-labeled_Fold-0.model.weights.hdf5
INFO     Computation time for training the single-label model for AR: 17.36 min
INFO     Network type: REG
INFO     Model: "sequential"
INFO     _________________________________________________________________
INFO     Layer (type)                 Output Shape              Param #
INFO     =================================================================
INFO     dense (Dense)                (None, 1024)              2098176
INFO     _________________________________________________________________
INFO     dropout (Dropout)            (None, 1024)              0
INFO     _________________________________________________________________
INFO     dense_1 (Dense)              (None, 512)               524800
INFO     _________________________________________________________________
INFO     dropout_1 (Dropout)          (None, 512)               0
INFO     _________________________________________________________________
INFO     dense_2 (Dense)              (None, 256)               131328
INFO     _________________________________________________________________
INFO     dropout_2 (Dropout)          (None, 256)               0
INFO     _________________________________________________________________
INFO     dense_3 (Dense)              (None, 128)               32896
INFO     _________________________________________________________________
INFO     dropout_3 (Dropout)          (None, 128)               0
INFO     _________________________________________________________________
INFO     dense_4 (Dense)              (None, 1)                 129
INFO     =================================================================
INFO     Total params: 2,787,329
INFO     Trainable params: 2,787,329
INFO     Non-trainable params: 0
INFO     _________________________________________________________________
INFO     Evaluating trained model 'AR single-labeled Fold-0' on test data
INFO     Network type: REG
INFO     Model: "sequential_1"
INFO     _________________________________________________________________
INFO     Layer (type)                 Output Shape              Param #
INFO     =================================================================
INFO     dense_5 (Dense)              (None, 1024)              2098176
INFO     _________________________________________________________________
INFO     dropout_4 (Dropout)          (None, 1024)              0
INFO     _________________________________________________________________
INFO     dense_6 (Dense)              (None, 512)               524800
INFO     _________________________________________________________________
INFO     dropout_5 (Dropout)          (None, 512)               0
INFO     _________________________________________________________________
INFO     dense_7 (Dense)              (None, 256)               131328
INFO     _________________________________________________________________
INFO     dropout_6 (Dropout)          (None, 256)               0
INFO     _________________________________________________________________
INFO     dense_8 (Dense)              (None, 128)               32896
INFO     _________________________________________________________________
INFO     dropout_7 (Dropout)          (None, 128)               0
INFO     _________________________________________________________________
INFO     dense_9 (Dense)              (None, 1)                 129
INFO     =================================================================
INFO     Total params: 2,787,329
INFO     Trainable params: 2,787,329
INFO     Non-trainable params: 0
INFO     _________________________________________________________________
2024-07-15 14:59:06.324145: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.
INFO:tensorflow:Assets written to: /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_saved_model/assets
INFO     Assets written to: /home/shanavas/PycharmProjects/deepFPlearn/example/dilshana-results-train/scaled_data_AR/AR_saved_model/assets
Traceback (most recent call last):
  File "/home/shanavas/PycharmProjects/deepFPlearn/dfpl/__main__.py", line 198, in <module>
    main()
  File "/home/shanavas/PycharmProjects/deepFPlearn/dfpl/__main__.py", line 173, in main
    train(fixed_opts)
  File "/home/shanavas/PycharmProjects/deepFPlearn/dfpl/__main__.py", line 90, in train
    sl.train_single_label_models(df=df, opts=opts)
  File "/home/shanavas/PycharmProjects/deepFPlearn/dfpl/single_label_model.py", line 532, in train_single_label_models
    pd
  File "/home/shanavas/miniforge3/envs/dfpl_env/lib/python3.9/site-packages/pandas/util/_decorators.py", line 311, in wrapper
    return func(*args, **kwargs)
  File "/home/shanavas/miniforge3/envs/dfpl_env/lib/python3.9/site-packages/pandas/core/frame.py", line 6304, in sort_values
    keys = [self._get_label_or_level_values(x, axis=axis) for x in by]
  File "/home/shanavas/miniforge3/envs/dfpl_env/lib/python3.9/site-packages/pandas/core/frame.py", line 6304, in <listcomp>
    keys = [self._get_label_or_level_values(x, axis=axis) for x in by]
  File "/home/shanavas/miniforge3/envs/dfpl_env/lib/python3.9/site-packages/pandas/core/generic.py", line 1840, in _get_label_or_level_values
    raise KeyError(key)
KeyError: 'p_1'